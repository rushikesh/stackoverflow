[{"t":"Plain English explanation of Big O","l":"http://stackoverflow.com/questions/487258/plain-english-explanation-of-big-o","q":"\n\n<p>What is a plain English explanation of Big O notation? I'd prefer as little formal definition as possible and simple mathematics.</p>\n    ","a":"\n<p>Quick note, this is almost certainly confusing <a href=\"http://en.wikipedia.org/wiki/Big_O_notation\">Big O notation</a> (which is an upper bound) with Theta notation (which is a two-side bound). In my experience this is actually typical of discussions in non-academic settings. Apologies for any confusion caused.</p>\n\n<hr>\n\n<p>Big O complexity can be visualized with this graph:</p>\n\n<p><img src=\"http://i.stack.imgur.com/WcBRI.png\" alt=\"Big O Analysis\"></p>\n\n<p>The simplest definition I can give for Big-O notation is this:</p>\n\n<p><strong>Big-O notation is a relative representation of the complexity of an algorithm.</strong></p>\n\n<p>There are some important and deliberately chosen words in that sentence:</p>\n\n<blockquote>\n  <ul>\n  <li><strong>relative:</strong> you can only compare apples to apples.  You can't compare an algorithm to do arithmetic multiplication to an algorithm that sorts a list of integers.  But a comparison of two algorithms to do arithmetic operations (one multiplication, one addition) will tell you something meaningful;</li>\n  <li><strong>representation:</strong> Big-O (in its simplest form) reduces the comparison between algorithms to a single variable.  That variable is chosen based on observations or assumptions.  For example, sorting algorithms are typically compared based on comparison operations (comparing two nodes to determine their relative ordering).  This assumes that comparison is expensive.  But what if comparison is cheap but swapping is expensive?  It changes the comparison; and</li>\n  <li><strong>complexity:</strong> if it takes me one second to sort 10,000 elements how long will it take me to sort one million?  Complexity in this instance is a relative measure to something else.</li>\n  </ul>\n</blockquote>\n\n<p>Come back and reread the above when you've read the rest.</p>\n\n<p>The best example of Big-O I can think of is doing arithmetic.  Take two numbers (123456 and 789012).  The basic arithmetic operations we learnt in school were:</p>\n\n<blockquote>\n  <ul>\n  <li>addition;</li>\n  <li>subtraction;</li>\n  <li>multiplication; and</li>\n  <li>division.</li>\n  </ul>\n</blockquote>\n\n<p>Each of these is an operation or a problem.  A method of solving these is called an <strong>algorithm</strong>.</p>\n\n<p>Addition is the simplest.  You line the numbers up (to the right) and add the digits in a column writing the last number of that addition in the result.  The 'tens' part of that number is carried over to the next column.</p>\n\n<p>Let's assume that the addition of these numbers is the most expensive operation in this algorithm.  It stands to reason that to add these two numbers together we have to add together 6 digits (and possibly carry a 7th).  If we add two 100 digit numbers together we have to do 100 additions.  If we add <strong>two</strong> 10,000 digit numbers we have to do 10,000 additions.</p>\n\n<p>See the pattern?  The <strong>complexity</strong> (being the number of operations) is directly proportional to the number of digits <em>n</em> in the larger number.  We call this <strong>O(n)</strong> or <strong>linear complexity</strong>.</p>\n\n<p>Subtraction is similar (except you may need to borrow instead of carry).</p>\n\n<p>Multiplication is different.  You line the numbers up, take the first digit in the bottom number and multiply it in turn against each digit in the top number and so on through each digit.  So to multiply our two 6 digit numbers we must do 36 multiplications.  We may need to do as many as 10 or 11 column adds to get the end result too.</p>\n\n<p>If we have two 100-digit numbers we need to do 10,000 multiplications and 200 adds.  For two one million digit numbers we need to do one trillion (10<sup>12</sup>) multiplications and two million adds.</p>\n\n<p>As the algorithm scales with n-<em>squared</em>, this is <strong>O(n<sup>2</sup>)</strong> or <strong>quadratic complexity</strong>.  This is a good time to introduce another important concept:</p>\n\n<p><strong>We only care about the most significant portion of complexity.</strong></p>\n\n<p>The astute may have realized that we could express the number of operations as: n<sup>2</sup> + 2n.  But as you saw from our example with two numbers of a million digits apiece, the second term (2n) becomes insignificant (accounting for 0.0002% of the total operations by that stage).</p>\n\n<p>One can notice that we've assumed the worst case scenario here. While multiplying 6 digit numbers if one of them is 4 digit and the other one is 6 digit, then we only have 24 multiplications. Still we calculate the worst case scenario for that 'n', i.e when both are 6 digit numbers. Hence Big-O notation is about the Worst-case scenario of an algorithm</p>\n\n<h1>The Telephone Book</h1>\n\n<p>The next best example I can think of is the telephone book, normally called the White Pages or similar but it'll vary from country to country.  But I'm talking about the one that lists people by surname and then initials or first name, possibly address and then telephone numbers.</p>\n\n<p>Now if you were instructing a computer to look up the phone number for \"John Smith\" in a telephone book that contains 1,000,000 names, what would you do?  Ignoring the fact that you could guess how far in the S's started (let's assume you can't), what would you do?</p>\n\n<p>A typical implementation might be to open up to the middle, take the 500,000<sup>th</sup> and compare it to \"Smith\".  If it happens to be \"Smith, John\", we just got real lucky.  Far more likely is that \"John Smith\" will be before or after that name.  If it's after we then divide the last half of the phone book in half and repeat.  If it's before then we divide the first half of the phone book in half and repeat.  And so on.</p>\n\n<p>This is called a <strong>binary search</strong> and is used every day in programming whether you realize it or not.</p>\n\n<p>So if you want to find a name in a phone book of a million names you can actually find any name by doing this at most 20 times.  In comparing search algorithms we decide that this comparison is our 'n'.</p>\n\n<blockquote>\n  <ul>\n  <li>For a phone book of 3 names it takes 2 comparisons (at most).</li>\n  <li>For 7 it takes at most 3.</li>\n  <li>For 15 it takes 4.</li>\n  <li>…</li>\n  <li>For 1,000,000 it takes 20.</li>\n  </ul>\n</blockquote>\n\n<p>That is staggeringly good isn't it?</p>\n\n<p>In Big-O terms this is <strong>O(log n)</strong> or <strong>logarithmic complexity</strong>.  Now the logarithm in question could be ln (base e), log<sub>10</sub>, log<sub>2</sub> or some other base.  It doesn't matter it's still O(log n) just like O(2n<sup>2</sup>) and O(100n<sup>2</sup>) are still both O(n<sup>2</sup>).</p>\n\n<p>It's worthwhile at this point to explain that Big O can be used to determine three cases with an algorithm:</p>\n\n<blockquote>\n  <ul>\n  <li><strong>Best Case:</strong> In the telephone book search, the best case is that we find the name in one comparison.  This is <strong>O(1)</strong> or <strong>constant complexity</strong>;</li>\n  <li><strong>Expected Case:</strong> As discussed above this is O(log n); and</li>\n  <li><strong>Worst Case:</strong> This is also O(log n).</li>\n  </ul>\n</blockquote>\n\n<p>Normally we don't care about the best case.  We're interested in the expected and worst case.  Sometimes one or the other of these will be more important.</p>\n\n<p>Back to the telephone book.</p>\n\n<p>What if you have a phone number and want to find a name?  The police have a reverse phone book but such look-ups are denied to the general public.  Or are they?  Technically you can reverse look-up a number in an ordinary phone book.  How?</p>\n\n<p>You start at the first name and compare the number.  If it's a match, great, if not, you move on to the next.  You have to do it this way because the phone book is <strong>unordered</strong> (by phone number anyway).</p>\n\n<p>So to find a name given the phone number (reverse lookup):</p>\n\n<blockquote>\n  <ul>\n  <li><strong>Best Case:</strong> O(1);</li>\n  <li><strong>Expected Case:</strong> O(n) (for 500,000); and</li>\n  <li><strong>Worst Case:</strong> O(n) (for 1,000,000).</li>\n  </ul>\n</blockquote>\n\n<h1>The Travelling Salesman</h1>\n\n<p>This is quite a famous problem in computer science and deserves a mention.  In this problem you have N towns. Each of those towns is linked to 1 or more other towns by a road of a certain distance. The Travelling Salesman problem is to find the shortest tour that visits every town.</p>\n\n<p>Sounds simple?  Think again.</p>\n\n<p>If you have 3 towns A, B and C with roads between all pairs then you could go:</p>\n\n<blockquote>\n  <ul>\n  <li>A → B → C</li>\n  <li>A → C → B</li>\n  <li>B → C → A</li>\n  <li>B → A → C</li>\n  <li>C → A → B</li>\n  <li>C → B → A</li>\n  </ul>\n</blockquote>\n\n<p>Well actually there's less than that because some of these are equivalent (A → B → C and C → B → A are equivalent, for example, because they use the same roads, just in reverse).</p>\n\n<p>In actuality there are 3 possibilities.</p>\n\n<blockquote>\n  <ul>\n  <li>Take this to 4 towns and you have (iirc) 12 possibilities.</li>\n  <li>With 5 it's 60.</li>\n  <li>6 becomes 360.</li>\n  </ul>\n</blockquote>\n\n<p>This is a function of a mathematical operation called a <strong>factorial</strong>.  Basically:</p>\n\n<blockquote>\n  <ul>\n  <li>5! = 5 × 4 × 3 × 2 × 1 = 120</li>\n  <li>6! = 6 × 5 × 4 × 3 × 2 × 1 = 720</li>\n  <li>7! = 7 × 6 × 5 × 4 × 3 × 2 × 1 = 5040</li>\n  <li>…</li>\n  <li>25! = 25 × 24 × … × 2 × 1 = 15,511,210,043,330,985,984,000,000</li>\n  <li>…</li>\n  <li>50! = 50 × 49 × … × 2 × 1 = 3.04140932 × 10<sup>64</sup></li>\n  </ul>\n</blockquote>\n\n<p>So the Big-O of the Travelling Salesman problem is <strong>O(n!)</strong> or <strong>factorial or combinatorial complexity</strong>.</p>\n\n<p><strong>By the time you get to 200 towns there isn't enough time left in the universe to solve the problem with traditional computers.</strong></p>\n\n<p>Something to think about.</p>\n\n<h1>Polynomial Time</h1>\n\n<p>Another point I wanted to make quick mention of is that any algorithm that has a complexity of <strong>O(n<sup>a</sup>)</strong> is said to have <strong>polynomial complexity</strong> or is solvable in <strong>polynomial time</strong>.</p>\n\n<p>O(n), O(n<sup>2</sup>) etc are all polynomial time. Some problems cannot be solved in polynomial time.  Certain things are used in the world because of this.  Public Key Cryptography is a prime example.  It is computationally hard to find two prime factors of a very large number.  If it wasn't, we couldn't use the public key systems we use.</p>\n\n<p>Anyway, that's it for my (hopefully plain English) explanation of Big O (revised).</p>\n    "},{"t":"Pair socks from a pile efficiently?","l":"http://stackoverflow.com/questions/14415881/pair-socks-from-a-pile-efficiently","q":"\n\n<p>Yesterday I was pairing the socks from the clean laundry and figured out the way I was doing it is not very efficient. I was doing a naive search&nbsp;— picking one sock and \"iterating\" the pile in order to find its pair. This requires iterating over n/2 * n/4 = n<sup>2</sup>/8 socks on average.</p>\n\n<p>As a computer scientist I was thinking what I could do? Sorting (according to size/color/...) of course came to mind to achieve an O(NlogN) solution.</p>\n\n<p>Hashing or other not-in-place solutions are not an option, because I am not able to duplicate my socks (though it could be nice if I could).</p>\n\n<p><strong>So, the question is basically:</strong></p>\n\n<p>Given a pile of <code>n</code> pairs of socks, containing <code>2n</code> elements (assume each sock has exactly one matching pair), what is the best way to pair them up efficiently with up to logarithmic extra space? (I believe I can remember that amount of info if needed.)</p>\n\n<p>I will appreciate an answer that addresses the following aspects:</p>\n\n<ul>\n<li>A general <em>theoretical</em> solution for a huge number of socks.</li>\n<li>The actual number of socks is not that large, I don't believe my spouse and I have more than 30 pairs. (And it is fairly easy to distinguish between my socks and hers; can this be used as well?)</li>\n<li>Is it equivalent to the <a href=\"http://en.wikipedia.org/wiki/Element_distinctness_problem\">element distinctness problem</a>?</li>\n</ul>\n    ","a":"\n<p>Sorting solutions have been proposed, but <strong>sorting is a little too much</strong>: We don't need order; <strong>we just need equality groups</strong>.</p>\n\n<p>So <strong>hashing</strong> would be enough (and faster).</p>\n\n<ol>\n<li>For each color of socks, <strong>form a pile</strong>. Iterate over all socks in your input basket <strong>and distribute them onto the color piles</strong>.</li>\n<li>Iterate over each pile and <strong>distribute it by some other metric</strong> (e.g. pattern) into a second set of piles</li>\n<li><strong>Recursively apply this scheme</strong> until you have distributed all socks onto <strong>very small piles that you can visually process immediately</strong></li>\n</ol>\n\n<p>This kind of recursive hash partitioning is actually being done by <a href=\"http://en.wikipedia.org/wiki/Microsoft_SQL_Server\">SQL Server</a> when it needs to hash join or hash aggregate over huge data sets. It distributes its build input stream into many partitions which are independent. This scheme scales to arbitrary amounts of data and multiple CPUs linearly.</p>\n\n<p>You don't need recursive partitioning if you can find a distribution key (hash key) that <strong>provides enough buckets</strong> that each bucket is small enough to be processed very quickly. Unfortunately, I don't think socks have such a property.</p>\n\n<p>If each sock had an integer called \"PairID\" one could easily distribute them into 10 buckets according to <code>PairID % 10</code> (the last digit).</p>\n\n<p>The best real-world partitioning I can think of is creating a <strong>rectangle of piles</strong>: one dimension is color, the other is pattern. Why a rectangle? Because we need O(1) random-access to piles. (A 3D <a href=\"http://en.wikipedia.org/wiki/Cuboid\">cuboid</a> would also work, but that is not very practical.)</p>\n\n<hr>\n\n<p>Update:</p>\n\n<p>What about <strong>parallelism</strong>? Can multiple humans match the socks faster?</p>\n\n<ol>\n<li>The simplest parallization strategy is to have multiple workers take from the input basket and put the socks onto the piles. This only scales up so much - imagine 100 people fighting over 10 piles. <strong>The synchronization costs</strong> (manifesting themselves as hand-collisions and human communication) <strong>destroy efficiency and speed-up</strong> (see the <a href=\"http://www.perfdynamics.com/Manifesto/USLscalability.html\">Universal Scalability Law</a>!). Is this prone to <strong>deadlocks</strong>? No, because each worker only needs to access one pile at a time. With just one \"lock\" there cannot be a deadlock. <strong>Livelocks</strong> might be possible depending on how the humans coordinate access to piles. They might just use <a href=\"http://en.wikipedia.org/wiki/Exponential_backoff\">random backoff</a> like network cards do that on a physical level to determine what card can exclusively access the network wire. If it works for <a href=\"http://en.wikipedia.org/wiki/Network_interface_controller\">NICs</a>, it should work for humans as well.</li>\n<li>It scales nearly indefinitely if <strong>each worker has its own set of piles</strong>. Workers can then take big chunks of socks from the input basket (very little contention as they are doing it rarely) and they do not need to synchronise when distributing the socks at all (because they have thread-local piles). At the end, all workers need to union their pile-sets. I believe that can be done in O(log (worker count * piles per worker)) if the workers form an <strong>aggregation tree</strong>.</li>\n</ol>\n\n<p>What about the <a href=\"http://en.wikipedia.org/wiki/Element_distinctness_problem\">element distinctness problem</a>? As the article states, the element distinctness problem can be solved in <code>O(N)</code>. This is the same for the socks problem (also <code>O(N)</code>, if you need only one distribution step (I proposed multiple steps only because humans are bad at calculations - one step is enough if you distribute on <code>md5(color, length, pattern, ...)</code>, i.e. a <strong>perfect hash</strong> of all attributes)).</p>\n\n<p>Clearly, one cannot go faster than <code>O(N)</code>, so we have reached the <strong>optimal lower bound</strong>.</p>\n\n<p>Although the outputs are not exactly the same (in one case, just a boolean. In the other case, the pairs of socks), the asymptotic complexities are the same.</p>\n    "},{"t":"What is the optimal algorithm for the game 2048?","l":"http://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048","q":"\n\n<p>I have recently stumbled upon the game <a href=\"http://gabrielecirulli.github.io/2048/\">2048</a>. You merge similar tiles by moving them in any of the four directions to make \"bigger\" tiles. After each move, a new tile appears at random empty position with value of either <code>2</code> or <code>4</code>. The game terminates when all the boxes are filled and there are no moves that can merge tiles, or you create a tile with a value of <code>2048</code>.</p>\n\n<p>One, I need to follow a well-defined strategy to reach the goal. So, I thought of writing a program for it.</p>\n\n<p>My current algorithm:</p>\n\n<pre><code>while (!game_over) {\n    for each possible move:\n        count_no_of_merges_for_2-tiles and 4-tiles\n    choose the move with large number of merges\n}\n</code></pre>\n\n<p>What I am doing is at any point, I will try to merge the tiles with values <code>2</code> and <code>4</code>, that is, I try to have <code>2</code> and <code>4</code> tiles, as minimum as possible. If I try it this way, all other tiles were automatically getting merged and the strategy seems good.</p>\n\n<p>But, when I actually use this algorithm, I only get around 4000 points before the game terminates. Maximum points AFAIK is slightly more than 20,000 points which is way larger than my current score. Is there a better algorithm than the above?</p>\n    ","a":"\n<p>I developed a 2048 AI using <em>expectimax</em> optimization, instead of the minimax search used by @ovolve's algorithm. The AI simply performs maximization over all possible moves, followed by expectation over all possible tile spawns (weighted by the probability of the tiles, i.e. 10% for a 4 and 90% for a 2). As far as I'm aware, it is not possible to prune expectimax optimization (except to remove branches that are exceedingly unlikely), and so the algorithm used is a carefully optimized brute force search.</p>\n\n<h2>Performance</h2>\n\n<p>The AI in its default configuration (max search depth of 8) takes anywhere from 10ms to 200ms to execute a move, depending on the complexity of the board position. In testing, the AI achieves an average move rate of 5-10 moves per second over the course of an entire game. If the search depth is limited to 6 moves, the AI can easily execute 20+ moves per second, which makes for some <a href=\"https://www.youtube.com/watch?v=96ab_dK6JM0\">interesting watching</a>.</p>\n\n<p>To assess the score performance of the AI, I ran the AI 100 times (connected to the browser game via remote control). For each tile, here are the proportions of games in which that tile was achieved at least once:</p>\n\n<pre><code>2048: 100%\n4096: 100%\n8192: 100%\n16384: 94%\n32768: 36%\n</code></pre>\n\n<p>The minimum score over all runs was 124024; the maximum score achieved was 794076. The median score is 387222. The AI never failed to obtain the 2048 tile (so it never lost the game even once in 100 games); in fact, it achieved the <strong>8192</strong> tile at least once in every run!</p>\n\n<p>Here's the screenshot of the best run:</p>\n\n<p><img src=\"http://i.stack.imgur.com/jG2CL.png\" alt=\"32768 tile, score 794076\"></p>\n\n<p>This game took 27830 moves over 96 minutes, or an average of 4.8 moves per second. </p>\n\n<h2>Implementation</h2>\n\n<p>My approach encodes the entire board (16 entries) as a single 64-bit integer (where tiles are the nybbles, i.e. 4-bit chunks). On a 64-bit machine, this enables the entire board to be passed around in a single machine register.</p>\n\n<p>Bit shift operations are used to extract individual rows and columns. A single row or column is a 16-bit quantity, so a table of size 65536 can encode transformations which operate on a single row or column. For example, moves are implemented as 4 lookups into a precomputed \"move effect table\" which describes how each move affects a single row or column (for example, the \"move right\" table contains the entry \"1122 -&gt; 0023\" describing how the row [2,2,4,4] becomes the row [0,0,4,8] when moved to the right).</p>\n\n<p>Scoring is also done using table lookup. The tables contain heuristic scores computed on all possible rows/columns, and the resultant score for a board is simply the sum of the table values across each row and column.</p>\n\n<p>This board representation, along with the table lookup approach for movement and scoring, allows the AI to search a huge number of game states in a short period of time (over 10,000,000 game states per second on one core of my mid-2011 laptop).</p>\n\n<p>The expectimax search itself is coded as a recursive search which alternates between \"expectation\" steps (testing all possible tile spawn locations and values, and weighting their optimized scores by the probability of each possibility), and \"maximization\" steps (testing all possible moves and selecting the one with the best score). The tree search terminates when it sees a previously-seen position (using a <a href=\"http://en.wikipedia.org/wiki/Transposition_table\">transposition table</a>), when it reaches a predefined depth limit, or when it reaches a board state that is highly unlikely (e.g. it was reached by getting 6 \"4\" tiles in a row from the starting position). The typical search depth is 4-8 moves.</p>\n\n<h2>Heuristics</h2>\n\n<p>Several heuristics are used to direct the optimization algorithm towards favorable positions. The precise choice of heuristic has a huge effect on the performance of the algorithm. The various heuristics are weighted and combined into a positional score, which determines how \"good\" a given board position is. The optimization search will then aim to maximize the average score of all possible board positions. The actual score, as shown by the game, is <em>not</em> used to calculate the board score, since it is too heavily weighted in favor of merging tiles (when delayed merging could produce a large benefit).</p>\n\n<p>Initially, I used two very simple heuristics, granting \"bonuses\" for open squares and for having large values on the edge. These heuristics performed pretty well, frequently achieving 16384 but never getting to 32768.</p>\n\n<p>Petr Morávek (@xificurk) took my AI and added two new heuristics. The first heuristic was a penalty for having non-monotonic rows and columns which increased as the ranks increased, ensuring that  non-monotonic rows of small numbers would not strongly affect the score, but non-monotonic rows of large numbers hurt the score substantially. The second heuristic counted the number of potential merges (adjacent equal values) in addition to open spaces. These two heuristics served to push the algorithm towards monotonic boards (which are easier to merge), and towards board positions with lots of merges (encouraging it to align merges where possible for greater effect).</p>\n\n<p>Furthermore, Petr also optimized the heuristic weights using a \"meta-optimization\" strategy (using an algorithm called <a href=\"https://en.wikipedia.org/wiki/CMA-ES\">CMA-ES</a>), where the weights themselves were adjusted to obtain the highest possible average score.</p>\n\n<p>The effect of these changes are extremely significant. The algorithm went from achieving the 16384 tile around 13% of the time to achieving it over 90% of the time, and the algorithm began to achieve 32768 over 1/3 of the time (whereas the old heuristics never once produced a 32768 tile).</p>\n\n<p>I believe there's still room for improvement on the heuristics. This algorithm definitely isn't yet \"optimal\", but I feel like it's getting pretty close.</p>\n\n<hr>\n\n<p>That the AI achieves the 32768 tile in over a third of its games is a huge milestone; I will be surprised to hear if any human players have achieved 32768 on the official game (i.e. without using tools like savestates or undo). I think the 65536 tile is within reach!</p>\n\n<p>You can try the AI for yourself. The code is available at <a href=\"https://github.com/nneonneo/2048-ai\">https://github.com/nneonneo/2048-ai</a>.</p>\n    "},{"t":"Algorithm improvement for Coca-Cola can shape recognition","l":"http://stackoverflow.com/questions/10168686/algorithm-improvement-for-coca-cola-can-shape-recognition","q":"\n\n<p>One of the most interesting projects I've worked in the past couple years as I was still a student, was a final project about image processing. The goal was to develop a system to be able to recognize Coca-Cola <strong>cans</strong> (note that I'm stressing the word cans, you'll see why in a minute). You can see a sample below, with the can recognized in the green rectangle with scale and rotation.</p>\n\n<p><img src=\"http://i.stack.imgur.com/irQtR.png\" alt=\"Template matching\"></p>\n\n<p>Some contraints on the project:</p>\n\n<ul>\n<li>The background could be very noisy.</li>\n<li>The can could have any scale or rotation or even orientation (within reasonable limits)</li>\n<li>The image could have some degree of fuziness (contours could be not really straight)</li>\n<li>There could be Coca-Cola bottles in the image, and the algorithm should only detect the can !</li>\n<li>The brightness of the image could vary a lot (so you can't rely \"too much\" on color detection.</li>\n<li>The can could be partly hidden on the sides or the middle (and possibly partly hidden behind the bottle !)</li>\n<li>There could be no cans at all in the image, in which case you had to find nothing and write a message saying so.</li>\n</ul>\n\n<p>So you could end up with tricky things like this (which in this case had my algorithm totally fail):</p>\n\n<p><img src=\"http://i.stack.imgur.com/Byw82.png\" alt=\"Total fail\"></p>\n\n<p>Now I've done this project obviously as it was a while ago, and had a lot of fun doing it, and I had a decent implementation. Here are some details about my implementation:</p>\n\n<p><strong>Language</strong>: Done in C++ using OpenCV library.</p>\n\n<p><strong>Pre-processing</strong>: Regarding image pre-processing I mean how to transform it in a more raw form to give to the algorithm. I used 2 methods:</p>\n\n<ol>\n<li>Changing color domain from RGB to HSV (<a href=\"http://en.wikipedia.org/wiki/HSL_and_HSV\">Hue Saturation Value</a>) and filtering based on \"red\" hue, saturation above a certain threshold to avoid orange-like colors, and filtering of low value to avoid dark tones. The end result was a binary black and white image, where all white pixels would represent the pixels that match this threshold. Obviously there is still a lot of crap in the image, but this reduces the number of dimensions you have to work with).\n<img src=\"http://i.stack.imgur.com/ktdAB.png\" alt=\"Binarized image\"></li>\n<li>Noise filtering using median filtering (taking the median pixel value of all neighbors and replace the pixel by this value) to reduce noise.</li>\n<li>Using <a href=\"http://en.wikipedia.org/wiki/Canny_edge_detector\">Canny Edge Detection Filter</a> to get the contours of all items after 2 precedent steps.\n<img src=\"http://i.stack.imgur.com/F9319.png\" alt=\"Contour detection\"></li>\n</ol>\n\n<p><strong>Algorithm</strong>: The algorithm itself I chose for this task was taken from this (awesome) <a href=\"http://rads.stackoverflow.com/amzn/click/0123725380\">book on feature extraction</a> and called <a href=\"http://en.wikipedia.org/wiki/Generalised_Hough_transform\">Generalized Hough Transform</a> (pretty different from the regular Hough Transform). It basically says a few things:</p>\n\n<ul>\n<li>You can describe an object in space without knowing its analytical equation (which is the case here).</li>\n<li>It is resistent to image deformations such as scaling and rotation, as it will basically test your image for every combination of scale factor and rotation factor.</li>\n<li>It uses a base model (a template) that the algorithm will \"learn\".</li>\n<li>Each pixel remaining in the contour image will vote for another pixel which will supposedly be the center (in terms of gravity) of your object, based on what it learned from the model.</li>\n</ul>\n\n<p>In the end, you end up with a heat map of the votes, for example here all the pixels of the contour of the can will vote for its gravitational center, so you'll have a lot of votes in the same pixel corresponding to the center, and will see a peak in the heat map as below.</p>\n\n<p><img src=\"http://i.stack.imgur.com/wxrT1.png\" alt=\"GHT\"></p>\n\n<p>Once you have that, a simple threshold-based heuristic can give you the location of the center pixel, from which you can derive the scale and rotation and then plot your little rectangle around it (final scale and rotation factor will obviously be relative to your original template). In theory at least...</p>\n\n<p><strong>Results</strong>: Now, while this approach worked in the basic cases, it was severely lacking in some areas:</p>\n\n<ul>\n<li>It is <strong>extremely slow</strong> ! I'm not stressing this enough. Almost a full day was needed to process the 30 test images, obviously because I had a very high scaling factor for rotation and translation, since some of the cans were very small.</li>\n<li>It was completely lost when bottles were in the image, and for some reason almost always found the bottle instead of the can (perhaps because bottles were bigger, thus had more pixels, thus more votes)</li>\n<li>Fuzzy images were also no good, since the votes ended up in pixel at random locations around the center, thus ending with a very noisy heat map.</li>\n<li>Invariance in translation and rotation was achieved, but not in orientation, meaning that a can that was not directly facing the camera objective wasn't recognized.</li>\n</ul>\n\n<p>Can you help me improve my <strong>specific</strong> algorithm, using <strong>exclusively</strong> OpenCV features, to resolve the <strong>four specific</strong> issues mentionned?</p>\n\n<p>I hope some people will also learn something out of it as well, after all I think not only people who ask questions should learn :)</p>\n    ","a":"\n<p>An alternative approach would be to extract features (keypoints) using the <a href=\"http://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">scale-invariant feature transform</a> (SIFT) or <a href=\"http://en.wikipedia.org/wiki/SURF\">Speeded Up Robust Features</a> (SURF).</p>\n\n<p>It is implemented in <a href=\"http://en.wikipedia.org/wiki/OpenCV\">OpenCV</a> 2.3.1.</p>\n\n<p>You can find a nice code example using features in <em><a href=\"http://opencv.itseez.com/doc/tutorials/features2d/feature_homography/feature_homography.html#feature-homography\">Features2D + Homography to find a known object</a></em></p>\n\n<p>Both algorithms are invariant to scaling and rotation. Since they work with features, you can also handle <a href=\"http://en.wikipedia.org/wiki/Ambient_occlusion\">occlusion</a> (as long as enough keypoints are visible).</p>\n\n<p><img src=\"http://i.stack.imgur.com/kF63R.jpg\" alt=\"Enter image description here\"></p>\n\n<p>Image source: tutorial example</p>\n\n<p>The processing takes a few hundred ms for SIFT, SURF is bit faster, but it not suitable for real-time applications. ORB uses FAST which is weaker regarding rotation invariance.</p>\n\n<h3>The original papers</h3>\n\n<ul>\n<li><a href=\"http://www.vision.ee.ethz.ch/~surf/eccv06.pdf\">SURF: Speeded Up Robust Features</a></li>\n<li><a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\">Distinctive Image Features\nfrom Scale-Invariant Keypoints</a></li>\n<li><a href=\"http://www.willowgarage.com/sites/default/files/orb_final.pdf\">ORB: an efficient alternative to SIFT or SURF</a></li>\n</ul>\n    "},{"t":"What is tail recursion?","l":"http://stackoverflow.com/questions/33923/what-is-tail-recursion","q":"\n\n<p>Whilst starting to learn lisp, I've come across the term <em>tail-recursive</em>. What does it mean?</p>\n    ","a":"\n<p>Tail recursion is well-described in previous answers, but I think an example in action would help to illustrate the concept. </p>\n\n<p>Consider a simple function that adds the first N integers. (e.g. <code>sum(5) = 1 + 2 + 3 + 4 + 5 = 15</code>).</p>\n\n<p>Here is a simple Python implementation that uses recursion:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def recsum(x):\n if x == 1:\n  return x\n else:\n  return x + recsum(x - 1)\n</code></pre>\n\n<p>If you called <code>recsum(5)</code>, this is what the Python interpreter would evaluate.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>recsum(5)\n5 + recsum(4)\n5 + (4 + recsum(3))\n5 + (4 + (3 + recsum(2)))\n5 + (4 + (3 + (2 + recsum(1))))\n5 + (4 + (3 + (2 + 1)))\n15\n</code></pre>\n\n<p>Note how every recursive call has to complete before the Python interpreter begins to actually do the work of calculating the sum.</p>\n\n<p>Here's a tail-recursive version of the same function:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def tailrecsum(x, running_total=0):\n  if x == 0:\n    return running_total\n  else:\n    return tailrecsum(x - 1, running_total + x)\n</code></pre>\n\n<p>Here's the sequence of events that would occur if you called <code>tailrecsum(5)</code>, (which would effectively be <code>tailrecsum(5, 0)</code>, because of the default second argument).</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tailrecsum(5, 0)\ntailrecsum(4, 5)\ntailrecsum(3, 9)\ntailrecsum(2, 12)\ntailrecsum(1, 14)\ntailrecsum(0, 15)\n15\n</code></pre>\n\n<p>In the tail-recursive case, with each evaluation of the recursive call, the <code>running_total</code> is updated.</p>\n\n<p><em>Note: As mentioned in the comments, Python doesn't have built-in support for optimizing away tail calls, so there's no advantage to doing this in Python. However, you can use a <a href=\"http://code.activestate.com/recipes/474088/\">decorator</a> to achieve the optimization.</em></p>\n    "},{"t":"Ukkonen's suffix tree algorithm in plain English?","l":"http://stackoverflow.com/questions/9452701/ukkonens-suffix-tree-algorithm-in-plain-english","q":"\n\n<p>I feel a bit thick at this point. I've spent days trying to fully wrap my head around suffix tree construction, but because I don't have a mathematical background, many of the explanations elude me as they start to make excessive use of mathematical symbology. The closest to a good explanation that I've found is <em><a href=\"http://marknelson.us/1996/08/01/suffix-trees/\">Fast String Searching With Suffix Trees</a></em>, but he glosses over various points and some aspects of the algorithm remain unclear.</p>\n\n<p>A step-by-step explanation of this algorithm here on Stack&nbsp;Overflow would be invaluable for many others besides me, I'm sure.</p>\n\n<p>For reference, here's Ukkonen's paper on the algorithm: <a href=\"http://www.cs.helsinki.fi/u/ukkonen/SuffixT1withFigs.pdf\">http://www.cs.helsinki.fi/u/ukkonen/SuffixT1withFigs.pdf</a></p>\n\n<p>My basic understanding, so far:</p>\n\n<ul>\n<li>I need to iterate through each prefix P of a given string T</li>\n<li>I need to iterate through each suffix S in prefix P and add that to tree</li>\n<li>To add suffix S to the tree, I need to iterate through each character in S, with the iterations consisting of either walking down an existing branch that starts with the same set of characters C in S and potentially splitting an edge into descendent nodes when I reach a differing character in the suffix, OR if there was no matching edge to walk down. When no matching edge is found to walk down for C, a new leaf edge is created for C.</li>\n</ul>\n\n<p>The basic algorithm appears to be O(n<sup>2</sup>), as is pointed out in most explanations, as we need to step through all of the prefixes, then we need to step through each of the suffixes for each prefix. Ukkonen's algorithm is apparently unique because of the suffix pointer technique he uses, though I think <em>that</em> is what I'm having trouble understanding.</p>\n\n<p>I'm also having trouble understanding:</p>\n\n<ul>\n<li>exactly when and how the \"active point\" is assigned, used and changed</li>\n<li>what is going on with the canonization aspect of the algorithm</li>\n<li>Why the implementations I've seen need to \"fix\" bounding variables that they are using</li>\n</ul>\n\n<hr>\n\n<p><strong>EDIT</strong> (April 13, 2012)</p>\n\n<p>Here is the completed source code that I've written and output based on jogojapan's answer below. The code outputs a detailed description and text-based diagram of the steps it takes as it builds the tree. It is a first version and could probably do with optimization and so forth, but it works, which is the main thing.</p>\n\n<p>[Redacted URL, see updated link below]</p>\n\n<p><strong>EDIT</strong> (April 15, 2012)</p>\n\n<p>The source code has been completely rewritten from scratch and now not only works correctly, but it supports automatic canonization and renders a nicer looking text graph of the output. Source code and sample output is at:</p>\n\n<blockquote>\n  <p><strong><a href=\"https://gist.github.com/2373868\">https://gist.github.com/2373868</a></strong></p>\n</blockquote>\n    ","a":"\n<p>The following is an attempt to describe the Ukkonen algorithm by first showing what it does when the string is simple (i.e. does not contain any repeated characters), and then extending it to the full algorithm.</p>\n\n<p><strong>First, a few preliminary statements.</strong></p>\n\n<ol>\n<li><p>What we are building, is <em>basically</em> like a search trie. So there\nis a root node, edges going out of it leading to new nodes, and\nfurther edges going out of those, and so forth</p></li>\n<li><p><strong>But</strong>: Unlike in a search trie, the edge labels are not single\ncharacters. Instead, each edge is labeled using a pair of integers\n<code>[from,to]</code>. These are pointers into the text. In this sense, each\nedge carries a string label of arbitrary length, but takes only O(1)\nspace (two pointers).</p></li>\n</ol>\n\n<h2>Basic principle</h2>\n\n<p>I would like to first demonstrate how to create the suffix tree of a\nparticularly simple string, a string with no repeated characters:</p>\n\n<pre><code>abc\n</code></pre>\n\n<p>The algorithm <strong>works in steps, from left to right</strong>. There is <strong>one step for every character of the string</strong>. Each step might involve more than one individual operation, but we will see (see the final observations at the end) that the total number of operations is O(n).</p>\n\n<p>So, we start from the <em>left</em>, and first insert only the single character\n<code>a</code> by creating an edge from the root node (on the left) to a leaf,\nand labeling it as <code>[0,#]</code>, which means the edge represents the\nsubstring starting at position 0 and ending at <em>the current end</em>. I\nuse the symbol <code>#</code> to mean <em>the current end</em>, which is at position 1\n(right after <code>a</code>).</p>\n\n<p>So we have an initial tree, which looks like this:</p>\n\n<p><img src=\"http://i.stack.imgur.com/aOwIL.png\" alt=\"\"></p>\n\n<p>And what it means is this:</p>\n\n<p><img src=\"http://i.stack.imgur.com/SZH4k.png\" alt=\"\"></p>\n\n<p>Now we progress to position 2 (right after <code>b</code>). <strong>Our goal at each step</strong>\nis to insert <strong>all suffixes up to the current position</strong>. We do this\nby</p>\n\n<ul>\n<li>expanding the existing <code>a</code>-edge to <code>ab</code></li>\n<li>inserting one new edge for <code>b</code></li>\n</ul>\n\n<p>In our representation this looks like</p>\n\n<p><img src=\"http://i.stack.imgur.com/onmqt.png\" alt=\"enter image description here\"></p>\n\n<p>And what it means is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/tchAx.png\" alt=\"\"></p>\n\n<p><strong>We observe</strong> two things:</p>\n\n<ul>\n<li>The edge representation for <code>ab</code> is <strong>the same</strong> as it used to be\nin the initial tree: <code>[0,#]</code>. Its meaning has automatically changed\nbecause we updated the current position <code>#</code> from 1 to 2.</li>\n<li>Each edge consumes O(1) space, because it consists of only two\npointers into the text, regardless of how many characters it\nrepresents.</li>\n</ul>\n\n<p>Next we increment the position again and update the tree by appending\na <code>c</code> to every existing edge and inserting one new edge for the new\nsuffix <code>c</code>.</p>\n\n<p>In our representation this looks like</p>\n\n<p><img src=\"http://i.stack.imgur.com/wCEdI.png\" alt=\"\"></p>\n\n<p>And what it means is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/UpUFw.png\" alt=\"\"></p>\n\n<p><strong>We observe:</strong></p>\n\n<ul>\n<li>The tree is the correct suffix tree <em>up to the current position</em>\nafter each step</li>\n<li>There are as many steps as there are characters in the text</li>\n<li>The amount of work in each step is O(1), because all existing edges\nare updated automatically by incrementing <code>#</code>, and inserting the\none new edge for the final character can be done in O(1)\ntime. Hence for a string of length n, only O(n) time is required.</li>\n</ul>\n\n<h2>First extension: Simple repetitions</h2>\n\n<p>Of course this works so nicely only because our string does not\ncontain any repetitions. We now look at a more realistic string:</p>\n\n<pre><code>abcabxabcd\n</code></pre>\n\n<p>It starts with <code>abc</code> as in the previous example, then <code>ab</code> is repeated\nand followed by <code>x</code>, and then <code>abc</code> is repeated followed by <code>d</code>.</p>\n\n<p><strong>Steps 1 through 3:</strong> After the first 3 steps we have the tree from the previous example:</p>\n\n<p><img src=\"http://i.stack.imgur.com/AclCh.png\" alt=\"\"></p>\n\n<p><strong>Step 4:</strong> We move <code>#</code> to position 4. This implicitly updates all existing\nedges to this:</p>\n\n<p><img src=\"http://i.stack.imgur.com/xhVMY.png\" alt=\"\"></p>\n\n<p>and we need to insert the final suffix of the current step, <code>a</code>, at\nthe root.</p>\n\n<p>Before we do this, we introduce <strong>two more variables</strong> (in addition to\n<code>#</code>), which of course have been there all the time but we haven't used\nthem so far:</p>\n\n<ul>\n<li>The <strong>active point</strong>, which is a triple\n<code>(active_node,active_edge,active_length)</code></li>\n<li>The <code>remainder</code>, which is an integer indicating how many new suffixes\nwe need to insert</li>\n</ul>\n\n<p>The exact meaning of these two will become clear soon, but for now\nlet's just say:</p>\n\n<ul>\n<li>In the simple <code>abc</code> example, the active point was always\n<code>(root,'\\0x',0)</code>, i.e. <code>active_node</code> was the root node, <code>active_edge</code> was specified as the null character <code>'\\0x'</code>, and <code>active_length</code> was zero. The effect of this was that the one new edge that\nwe inserted in every step was inserted at the root node as a\nfreshly created edge. We will see soon why a triple is necessary to\nrepresent this information.</li>\n<li>The <code>remainder</code> was always set to 1 at the beginning of each\nstep. The meaning of this was that the number of suffixes we had to\nactively insert at the end of each step was 1 (always just the\nfinal character).</li>\n</ul>\n\n<p>Now this is going to change. When we insert the current final\ncharacter <code>a</code> at the root, we notice that there is already an outgoing\nedge starting with <code>a</code>, specifically: <code>abca</code>. Here is what we do in\nsuch a case:</p>\n\n<ul>\n<li>We <strong>do not</strong> insert a fresh edge <code>[4,#]</code> at the root node. Instead we\nsimply notice that the suffix <code>a</code> is already in our\ntree. It ends in the middle of a longer edge, but we are not\nbothered by that. We just leave things the way they are.</li>\n<li>We <strong>set the active point</strong> to <code>(root,'a',1)</code>. That means the active\npoint is now somewhere in the middle of outgoing edge of the root node that starts with <code>a</code>, specifically, after position 1 on that edge. We\nnotice that the edge is specified simply by its first\ncharacter <code>a</code>. That suffices because there can be <em>only one</em> edge\nstarting with any particular character (confirm that this is true after reading through the entire description).</li>\n<li>We also increment <code>remainder</code>, so at the beginning of the next step\nit will be 2.</li>\n</ul>\n\n<p><strong>Observation:</strong> When the final <strong>suffix we need to insert is found to\nexist in the tree already</strong>, the tree itself is <strong>not changed</strong> at all (we only update the active point and <code>remainder</code>). The tree\nis then not an accurate representation of the suffix tree <em>up to the\ncurrent position</em> any more, but it <strong>contains</strong> all suffixes (because the final\nsuffix <code>a</code> is contained <em>implicitly</em>). Hence, apart from updating the\nvariables (which are all of fixed length, so this is O(1)), there was\n<strong>no work</strong> done in this step.</p>\n\n<p><strong>Step 5:</strong> We update the current position <code>#</code> to 5. This\nautomatically updates the tree to this:</p>\n\n<p><img src=\"http://i.stack.imgur.com/XL6bg.png\" alt=\"\"></p>\n\n<p>And <strong>because <code>remainder</code> is 2</strong>, we need to insert two final\nsuffixes of the current position: <code>ab</code> and <code>b</code>. This is basically because:</p>\n\n<ul>\n<li>The <code>a</code> suffix from the previous step has never been properly\ninserted. So it has <em>remained</em>, and since we have progressed one\nstep, it has now grown from <code>a</code> to <code>ab</code>.</li>\n<li>And we need to insert the new final edge <code>b</code>.</li>\n</ul>\n\n<p>In practice this means that we go to the active point (which points to\nbehind the <code>a</code> on what is now the <code>abcab</code> edge), and insert the\ncurrent final character <code>b</code>. <strong>But:</strong> Again, it turns out that <code>b</code> is\nalso already present on that same edge.</p>\n\n<p>So, again, we do not change the tree. We simply:</p>\n\n<ul>\n<li>Update the active point to <code>(root,'a',2)</code> (same node and edge\nas before, but now we point to behind the <code>b</code>)</li>\n<li>Increment the <code>remainder</code> to 3 because we still have not properly\ninserted the final edge from the previous step, and we don't insert\nthe current final edge either.</li>\n</ul>\n\n<p>To be clear: We had to insert <code>ab</code> and <code>b</code> in the current step, but\nbecause <code>ab</code> was already found, we updated the active point and did\nnot even attempt to insert <code>b</code>. Why? Because if <code>ab</code> is in the tree,\n<strong>every suffix</strong> of it (including <code>b</code>) must be in the tree,\ntoo. Perhaps only <em>implicitly</em>, but it must be there, because of the\nway we have built the tree so far.</p>\n\n<p>We proceed to <strong>step 6</strong> by incrementing <code>#</code>. The tree is\nautomatically updated to:</p>\n\n<p><img src=\"http://i.stack.imgur.com/bLLT9.png\" alt=\"\"></p>\n\n<p>Because <strong><code>remainder</code> is 3</strong>, we have to insert <code>abx</code>, <code>bx</code> and\n<code>x</code>. The active point tells us where <code>ab</code> ends, so we only need to\njump there and insert the <code>x</code>. Indeed, <code>x</code> is not there yet, so we\nsplit the <code>abcabx</code> edge and insert an internal node:</p>\n\n<p><img src=\"http://i.stack.imgur.com/6HYtR.png\" alt=\"\"></p>\n\n<p>The edge representations are still pointers into the text, so\nsplitting and inserting an internal node can be done in O(1) time.</p>\n\n<p>So we have dealt with <code>abx</code> and decrement <code>remainder</code> to 2. Now we\nneed to insert the next remaining suffix, <code>bx</code>. But before we do that\nwe need to update the active point. The rule for this, after splitting\nand inserting an edge, will be called <strong>Rule 1</strong> below, and it applies whenever the\n<code>active_node</code> is root (we will learn rule 3 for other cases further\nbelow). Here is rule 1:</p>\n\n<p>After an insertion from root,</p>\n\n<ul>\n<li><code>active_node</code> remains root</li>\n<li><code>active_edge</code> is set to the first character of the new suffix we\nneed to insert, i.e. <code>b</code></li>\n<li><code>active_length</code> is reduced by 1</li>\n</ul>\n\n<p>Hence, the new active-point triple <code>(root,'b',1)</code> indicates that the\nnext insert has to be made at the <code>bcabx</code> edge, behind 1 character,\ni.e. behind <code>b</code>. We can identify the insertion point in O(1) time and\ncheck whether <code>x</code> is already present or not. If it was present, we\nwould end the current step and leave everything the way it is. But <code>x</code>\nis not present, so we insert it by splitting the edge:</p>\n\n<p><img src=\"http://i.stack.imgur.com/YVvbJ.png\" alt=\"\"></p>\n\n<p>Again, this took O(1) time and we update <code>remainder</code> to 1 and the\nactive point to <code>(root,'x',0)</code> as rule 1 states.</p>\n\n<p>But there is one more thing we need to do. We'll call this <strong>Rule 2:</strong></p>\n\n<blockquote>\n  <p>If we split an edge and insert a new node, and if that is <em>not the\n  first node</em> created during the current step, we connect the previously\n  inserted node and the new node through a special pointer, a <strong>suffix\n  link</strong>. We will later see why that is useful. Here is what we get, the\n  suffix link is represented as a dotted edge:</p>\n</blockquote>\n\n<p><img src=\"http://i.stack.imgur.com/zL9yl.png\" alt=\"\"></p>\n\n<p>We still need to insert the final suffix of the current step,\n<code>x</code>. Since the <code>active_length</code> component of the active node has fallen\nto 0, the final insert is made at the root directly. Since there is no\noutgoing edge at the root node starting with <code>x</code>, we insert a new\nedge:</p>\n\n<p><img src=\"http://i.stack.imgur.com/992gV.png\" alt=\"\"></p>\n\n<p>As we can see, in the current step all remaining inserts were made.</p>\n\n<p>We proceed to <strong>step 7</strong> by setting <code>#</code>=7, which automatically appends the next character,\n<code>a</code>, to all leaf edges, as always. Then we attempt to insert the new final\ncharacter to the active point (the root), and find that it is there\nalready. So we end the current step without inserting anything and\nupdate the active point to <code>(root,'a',1)</code>.</p>\n\n<p>In <strong>step 8</strong>,, <code>#</code>=8, we append <code>b</code>, and as seen before, this only\nmeans we update the active point to <code>(root,'a',2)</code> and increment <code>remainder</code> without doing\nanything else, because <code>b</code> is already present. <strong>However,</strong> we notice (in O(1) time) that the active point\nis now at the end of an edge. We reflect this by re-setting it to\n<code>(node1,'\\0x',0)</code>. Here, I use <code>node1</code> to refer to the\ninternal node the <code>ab</code> edge ends at.</p>\n\n<p>Then, in <strong>step <code>#</code>=9</strong>, we need to insert 'c' and this will help us to\nunderstand the final trick:</p>\n\n<h2>Second extension: Using suffix links</h2>\n\n<p>As always, the <code>#</code> update appends <code>c</code> automatically to the leaf edges\nand we go to the active point to see if we can insert 'c'. It turns\nout 'c' exists already at that edge, so we set the active point to\n<code>(node1,'c',1)</code>, increment <code>remainder</code> and do nothing else.</p>\n\n<p>Now in <strong>step <code>#</code>=10</strong>, <code>remainder is 4</code>, and so we first need to insert\n<code>abcd</code> (which remains from 3 steps ago) by inserting <code>d</code> at the active\npoint.</p>\n\n<p>Attempting to insert <code>d</code> at the active point causes an edge split in\nO(1) time:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Rkdzd.png\" alt=\"\"></p>\n\n<p>The <code>active_node</code>, from which the split was initiated, is marked in\nred above. Here is the final rule, <strong>Rule 3:</strong></p>\n\n<blockquote>\n  <p>After splitting an edge from an <code>active_node</code> that is not the root\n  node, we follow the suffix link going out of that node, if there is\n  any, and reset the <code>active_node</code> to the node it points to. If there is\n  no suffix link, we set the <code>active_node</code> to the root. <code>active_edge</code>\n  and <code>active_length</code> remain unchanged.</p>\n</blockquote>\n\n<p>So the active point is now <code>(node2,'c',1)</code>, and <code>node2</code> is marked in\nred below:</p>\n\n<p><img src=\"http://i.stack.imgur.com/0IS5C.png\" alt=\"\"></p>\n\n<p>Since the insertion of <code>abcd</code> is complete, we decrement <code>remainder</code> to\n3 and consider the next remaining suffix of the current step,\n<code>bcd</code>. Rule 3 has set the active point to just the right node and edge\nso inserting <code>bcd</code> can be done by simply inserting its final character\n<code>d</code> at the active point.</p>\n\n<p>Doing this causes another edge split, and <strong>because of rule 2</strong>, we\nmust create a suffix link from the previously inserted node to the new\none:</p>\n\n<p><img src=\"http://i.stack.imgur.com/DNVQO.png\" alt=\"\"></p>\n\n<p><strong>We observe:</strong> Suffix links enable us to reset the active point so we\n  can make the next <em>remaining insert</em> at O(1) effort. Look at the\n  graph above to confirm that indeed node at label <code>ab</code> is linked to\n  the node at <code>b</code> (its suffix), and the node at <code>abc</code> is linked to\n  <code>bc</code>.</p>\n\n<p>The current step is not finished yet. <code>remainder</code> is now 2, and we\nneed to follow rule 3 to reset the active point again. Since the\ncurrent <code>active_node</code> (red above) has no suffix link, we reset to\nroot. The active point is now <code>(root,'c',1)</code>.</p>\n\n<p>Hence the next insert occurs at the one outgoing edge of the root node\nwhose label starts with <code>c</code>: <code>cabxabcd</code>, behind the first character,\ni.e. behind <code>c</code>. This causes another split:</p>\n\n<p><img src=\"http://i.stack.imgur.com/wZ7Bj.png\" alt=\"\"></p>\n\n<p>And since this involves the creation of a new internal node,we follow\nrule 2 and set a new suffix link from the previously created internal\nnode:</p>\n\n<p><img src=\"http://i.stack.imgur.com/urgol.png\" alt=\"\"></p>\n\n<p>(I am using <a href=\"http://www.graphviz.org/\">Graphviz Dot</a> for these little\ngraphs. The new suffix link caused dot to re-arrange the existing\nedges, so check carefully to confirm that the only thing that was\ninserted above is a new suffix link.)</p>\n\n<p>With this, <code>remainder</code> can be set to 1 and since the <code>active_node</code> is\nroot, we use rule 1 to update the active point to <code>(root,'d',0)</code>. This\nmeans the final insert of the current step is to insert a single <code>d</code>\nat root:</p>\n\n<p><img src=\"http://i.stack.imgur.com/TPxLe.png\" alt=\"\"></p>\n\n<p>That was the final step and we are done. There are number of <strong>final\nobservations</strong>, though:</p>\n\n<ul>\n<li><p>In each step we move <code>#</code> forward by 1 position. This automatically\nupdates all leaf nodes in O(1) time.</p></li>\n<li><p>But it does not deal with a) any suffixes <em>remaining</em> from previous\nsteps, and b) with the one final character of the current step.</p></li>\n<li><p><code>remainder</code> tells us how many additional inserts we need to\nmake. These inserts correspond one-to-one to the final suffixes of\nthe string that ends at the current position <code>#</code>. We consider one\nafter the other and make the insert. <strong>Important:</strong> Each insert is\ndone in O(1) time since the active point tells us exactly where to\ngo, and we need to add only one single character at the active\npoint. Why? Because the other characters are <em>contained implicitly</em>\n(otherwise the active point would not be where it is).</p></li>\n<li><p>After each such insert, we decrement <code>remainder</code> and follow the\nsuffix link if there is any. If not we go to root (rule 3). If we\nare at root already, we modify the active point using rule 1. In\nany case, it takes only O(1) time.</p></li>\n<li><p>If, during one of these inserts, we find that the character we want\nto insert is already there, we don't do anything and end the\ncurrent step, even if <code>remainder</code>&gt;0. The reason is that any\ninserts that remain will be suffixes of the one we just tried to\nmake. Hence they are all <em>implicit</em> in the current tree. The fact\nthat <code>remainder</code>&gt;0 makes sure we deal with the remaining suffixes\nlater.</p></li>\n<li><p>What if at the end of the algorithm <code>remainder</code>&gt;0? This will be the\ncase whenever the end of the text is a substring that occurred\nsomewhere before. In that case we must append one extra character\nat the end of the string that has not occurred before. In the\nliterature, usually the dollar sign <code>$</code> is used as a symbol for\nthat. <strong>Why does that matter?</strong> --&gt; If later we use the completed suffix tree to search for suffixes, we must accept matches only if they <em>end at a leaf</em>. Otherwise we would get a lot of spurious matches, because there are <em>many</em> strings <em>implicitly</em> contained in the tree that are not actual suffixes of the main string. Forcing <code>remainder</code> to be 0 at the end is essentially a way to ensure that all suffixes end at a leaf node. <strong>However,</strong> if we want to use the tree to search for <em>general substrings</em>, not only <em>suffixes</em> of the main string, this final step is indeed not required, as suggested by the OP's comment below.</p></li>\n<li><p>So what is the complexity of the entire algorithm? If the text is n\ncharacters in length, there are obviously n steps (or n+1 if we add\nthe dollar sign). In each step we either do nothing (other than\nupdating the variables), or we make <code>remainder</code> inserts, each taking O(1)\ntime. Since <code>remainder</code> indicates how many times we have done nothing\nin previous steps, and is decremented for every insert that we make\nnow, the total number of times we do something is exactly n (or\nn+1). Hence, the total complexity is O(n).</p></li>\n<li><p>However, there is one small thing that I did not properly explain:\nIt can happen that we follow a suffix link, update the active\npoint, and then find that its <code>active_length</code> component does not\nwork well with the new <code>active_node</code>. For example, consider a situation\nlike this:</p></li>\n</ul>\n\n<p><img src=\"http://i.stack.imgur.com/7t0dg.png\" alt=\"\"></p>\n\n<p>(The dashed lines indicate the rest of the tree. The dotted line is a\nsuffix link.)</p>\n\n<p>Now let the active point be <code>(red,'d',3)</code>, so it points to the place\nbehind the <code>f</code> on the <code>defg</code> edge. Now assume we made the necessary\nupdates and now follow the suffix link to update the active point\naccording to rule 3. The new active point is <code>(green,'d',3)</code>. However,\nthe <code>d</code>-edge going out of the green node is <code>de</code>, so it has only 2\ncharacters. In order to find the correct active point, we obviously\nneed to follow that edge to the blue node and reset to <code>(blue,'f',1)</code>.</p>\n\n<p>In a particularly bad case, the <code>active_length</code> could be as large as\n<code>remainder</code>, which can be as large as n. And it might very well happen\nthat to find the correct active point, we need not only jump over one\ninternal node, but perhaps many, up to n in the worst case. Does that\nmean the algorithm has a hidden O(n<sup>2</sup>) complexity, because\nin each step <code>remainder</code> is generally O(n), and the post-adjustments\nto the active node after following a suffix link could be O(n), too?</p>\n\n<p>No. The reason is that if indeed we have to adjust the active point\n(e.g. from green to blue as above), that brings us to a new node that\nhas its own suffix link, and <code>active_length</code> will be reduced. As\nwe follow down the chain of suffix links we make the remaining inserts, <code>active_length</code> can only\ndecrease, and the number of active-point adjustments we can make on\nthe way can't be larger than <code>active_length</code> at any given time. Since\n<code>active_length</code> can never be larger than <code>remainder</code>, and <code>remainder</code>\nis O(n) not only in every single step, but the total sum of increments\never made to <code>remainder</code> over the course of the entire process is\nO(n) too, the number of active point adjustments is also bounded by\nO(n).</p>\n    "},{"t":"Easy interview question got harder: given numbers 1..100, find the missing number(s)","l":"http://stackoverflow.com/questions/3492302/easy-interview-question-got-harder-given-numbers-1-100-find-the-missing-numbe","q":"\n\n<p>I had an interesting job interview experience a while back. The question started really easy:</p>\n\n<blockquote>\n  <p><strong>Q1</strong>: We have a bag containing numbers <code>1</code>, <code>2</code>, <code>3</code>, …, <code>100</code>. Each number appears exactly once, so there are 100 numbers. Now one number is randomly picked out of the bag. Find the missing number.</p>\n</blockquote>\n\n<p>I've heard this interview question before, of course, so I very quickly answered along the lines of:</p>\n\n<blockquote>\n  <p><strong>A1</strong>: Well, the sum of the numbers <code>1 + 2 + 3 + … + N</code> is <code>(N+1)(N/2)</code> (see <a href=\"http://en.wikipedia.org/wiki/Arithmetic_sum#Sum\">Wikipedia: sum of arithmetic series</a>). For <code>N = 100</code>, the sum is <code>5050</code>.</p>\n  \n  <p>Thus, if all numbers are present in the bag, the sum will be exactly <code>5050</code>. Since one number is missing, the sum will be less than this, and the difference is that number. So we can find that missing number in <code>O(N)</code> time and <code>O(1)</code> space.</p>\n</blockquote>\n\n<p>At this point I thought I had done well, but all of a sudden the question took an unexpected turn:</p>\n\n<blockquote>\n  <p><strong>Q2</strong>: That is correct, but now how would you do this if <em>TWO</em> numbers are missing?</p>\n</blockquote>\n\n<p>I had never seen/heard/considered this variation before, so I panicked and couldn't answer the question. The interviewer insisted on knowing my thought process, so I mentioned that perhaps we can get more information by comparing against the expected product, or perhaps doing a second pass after having gathered some information from the first pass, etc, but I really was just shooting in the dark rather than actually having a clear path to the solution.</p>\n\n<p>The interviewer did try to encourage me by saying that having a second equation is indeed one way to solve the problem. At this point I was kind of upset (for not knowing the answer before hand), and asked if this is a general (read: \"useful\") programming technique, or if it's just a trick/gotcha answer.</p>\n\n<p>The interviewer's answer surprised me: you can generalize the technique to find 3 missing numbers. In fact, you can generalize it to find <em>k</em> missing numbers.</p>\n\n<blockquote>\n  <p><strong>Qk</strong>: If exactly <em>k</em> numbers are missing from the bag, how would you find it efficiently?</p>\n</blockquote>\n\n<p>This was a few months ago, and I still couldn't figure out what this technique is.  Obviously there's a <code>Ω(N)</code> time lower bound since we must scan all the numbers at least once, but the interviewer insisted that the <em>TIME</em> and <em>SPACE</em> complexity of the solving technique (minus the <code>O(N)</code> time input scan) is defined in <em>k</em> not <em>N</em>.</p>\n\n<p>So the question here is simple:</p>\n\n<ul>\n<li>How would you solve <strong>Q2</strong>?</li>\n<li>How would you solve <strong>Q3</strong>?</li>\n<li>How would you solve <strong>Qk</strong>?</li>\n</ul>\n\n<hr>\n\n<h3>Clarifications</h3>\n\n<ul>\n<li>Generally there are <em>N</em> numbers from 1..<em>N</em>, not just 1..100.</li>\n<li>I'm not looking for the obvious set-based solution, e.g. using a <a href=\"http://en.wikipedia.org/wiki/Bit_array\">bit set</a>, encoding the presence/absence each number by the value of a designated bit, therefore using <code>O(N)</code> bits in additional space. We can't afford any additional space proportional to <em>N</em>.</li>\n<li>I'm also not looking for the obvious sort-first approach. This and the set-based approach are worth mentioning in an interview (they are easy to implement, and depending on <em>N</em>, can be very practical). I'm looking for the Holy Grail solution (which may or may not be practical to implement, but has the desired asymptotic characteristics nevertheless).</li>\n</ul>\n\n<p>So again, of course you must scan the input in <code>O(N)</code>, but you can only capture small amount of information (defined in terms of <em>k</em> not <em>N</em>), and must then find the <em>k</em> missing numbers somehow.</p>\n    ","a":"\n<p>Here's a summary of <a href=\"http://stackoverflow.com/questions/3492302/easy-interview-question-got-harder-given-numbers-1-100-find-the-missing-number/3492664#3492664\">Dimitris Andreou's</a> link.</p>\n\n<p>Remember sum of i-th powers, where i=1,2,..,k. This reduces the problem to solving the system of equations</p>\n\n<p>a<sub>1</sub> + a<sub>2</sub> + ... + a<sub>k</sub> = b<sub>1</sub></p>\n\n<p>a<sub>1</sub><sup>2</sup> + a<sub>2</sub><sup>2</sup> + ... + a<sub>k</sub><sup>2</sup> = b<sub>2</sub></p>\n\n<p>...</p>\n\n<p>a<sub>1</sub><sup>k</sup> + a<sub>2</sub><sup>k</sup> + ... + a<sub>k</sub><sup>k</sup> = b<sub>k</sub></p>\n\n<p>Using <a href=\"http://en.wikipedia.org/wiki/Newton_identities#Formulation_in_terms_of_symmetric_polynomials\">Newton's identities</a>, knowing b<sub>i</sub> allows to compute</p>\n\n<p>c<sub>1</sub> = a<sub>1</sub> + a<sub>2</sub> + ... a<sub>k</sub></p>\n\n<p>c<sub>2</sub> = a<sub>1</sub>a<sub>2</sub> + a<sub>1</sub>a<sub>3</sub> + ... + a<sub>k-1</sub>a<sub>k</sub></p>\n\n<p>...</p>\n\n<p>c<sub>k</sub> = a<sub>1</sub>a<sub>2</sub> ... a<sub>k</sub></p>\n\n<p>If you expand the polynomial (x-a<sub>1</sub>)...(x-a<sub>k</sub>) the coefficients will be exactly c<sub>1</sub>, ..., c<sub>k</sub> - see <a href=\"http://en.wikipedia.org/wiki/Vi%C3%A8te%27s_formulas\">Viète's formulas</a>. Since every polynomial factors uniquely (ring of polynomials is an <a href=\"http://en.wikipedia.org/wiki/Euclidean_domain\">Euclidean domain</a>), this means a<sub>i</sub> are uniquely determined, up to permutation.</p>\n\n<p>This ends a proof that remembering powers is enough to recover the numbers. For constant k, this is a good approach.</p>\n\n<p>However, when k is varying, the direct approach of computing c<sub>1</sub>,...,c<sub>k</sub> is prohibitely expensive, since e.g. c<sub>k</sub> is the product of all missing numbers, magnitude n!/(n-k)!. To overcome this, perform computations in Z<sub>q</sub> field, where q is a prime such that n &lt;= q &lt; 2n - it exists by <a href=\"http://en.wikipedia.org/wiki/Bertrand%27s_postulate\">Bertrand's postulate</a>. The proof doesn't need to be changed, since the formulas still hold, and factorization of polynomials is still unique. You also need an algorithm for factorization over finite fields, for example the one by <a href=\"http://en.wikipedia.org/wiki/Berlekamp%27s_algorithm\">Berlekamp</a> or <a href=\"http://en.wikipedia.org/wiki/Cantor%E2%80%93Zassenhaus_algorithm\">Cantor-Zassenhaus</a>.</p>\n\n<p>High level pseudocode for constant k:</p>\n\n<ul>\n<li>Compute i-th powers of given numbers</li>\n<li>Subtract to get sums of i-th powers of unknown numbers. Call the sums b<sub>i</sub>.</li>\n<li>Use Newton's identities to compute coefficients from b<sub>i</sub>; call them c<sub>i</sub>. Basically, c<sub>1</sub> = b<sub>1</sub>; c<sub>2</sub> = (c<sub>1</sub>b<sub>1</sub> - b<sub>2</sub>)/2; see Wikipedia for exact formulas</li>\n<li>Factor the polynomial x<sup>k</sup>-c<sub>1</sub>x<sup>k-1</sup> + ... + c<sub>k</sub>.</li>\n<li>The roots of the polynomial are the needed numbers a<sub>1</sub>, ..., a<sub>k</sub>.</li>\n</ul>\n\n<p>For varying k, find a prime n &lt;= q &lt; 2n using e.g. Miller-Rabin, and perform the steps with all numbers reduced modulo q.</p>\n\n<p>As Heinrich Apfelmus commented, instead of a prime q you can use q=2<sup>⌈log n⌉</sup> and perform <a href=\"http://en.wikipedia.org/wiki/Finite_field_arithmetic\">arithmetic in finite field</a>.</p>\n    "},{"t":"How to determine whether my calculation of pi is accurate?","l":"http://stackoverflow.com/questions/14283270/how-to-determine-whether-my-calculation-of-pi-is-accurate","q":"\n\n<p>I was trying various methods to implement a program that gives the digits of pi sequentially. I tried the <a href=\"http://en.wikipedia.org/wiki/Taylor_series\">Taylor series</a> method, but it proved to converge extremely slowly (when I compared my result with the online values after some time). Anyway, I am trying better algorithms.</p>\n\n<p>So, while writing the program I got stuck on a problem, as with all algorithms: How do I know that the <code>n</code> digits that I've calculated are accurate?</p>\n    ","a":"\n<p>Since I'm the current world record holder for the most digits of pi, I'll add my <a href=\"http://en.wiktionary.org/wiki/two_cents#Noun\">two cents</a>:</p>\n\n<p>Unless you're actually setting a new world record, the common practice is just to verify the computed digits against the known values. So that's simple enough.</p>\n\n<p>In fact, I have a webpage that lists snippets of digits for the purpose of verifying computations against them: <a href=\"http://www.numberworld.org/digits/Pi/\">http://www.numberworld.org/digits/Pi/</a></p>\n\n<hr>\n\n<p>But when you get into world-record territory, there's nothing to compare against.</p>\n\n<p>Historically, the standard approach for verifying that computed digits are correct is to recompute the digits using a second algorithm. So if either computation goes bad, the digits at the end won't match.</p>\n\n<p>This does typically more than double the amount of time needed (since the second algorithm is usually slower). But it's the only way to verify the computed digits once you've wandered into the uncharted territory of never-before-computed digits and a new world record.</p>\n\n<hr>\n\n<p>Back in the days where supercomputers were setting the records, two different <a href=\"http://en.wikipedia.org/wiki/AGM_method\">AGM algorithms</a> were commonly used:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Gauss%E2%80%93Legendre_algorithm\">Gauss–Legendre algorithm</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Borwein%27s_algorithm\">Borwein's algorithm</a></li>\n</ul>\n\n<p>These are both <code>O(N log(N)^2)</code> algorithms that were fairly easy to implement.</p>\n\n<p>However, nowadays, things are a bit different. In the last three world records, instead of performing two computations, we performed only one computation using the fastest known formula (<a href=\"http://en.wikipedia.org/wiki/Chudnovsky_algorithm\">Chudnovsky Formula</a>):</p>\n\n<p><img src=\"http://mathurl.com/bjkw6tu.png\" alt=\"Enter image description here\"></p>\n\n<p>This algorithm is much harder to implement, but it is a lot faster than the AGM algorithms.</p>\n\n<p>Then we verify the binary digits using the <a href=\"http://en.wikipedia.org/wiki/Bailey%E2%80%93Borwein%E2%80%93Plouffe_formula\">BBP formulas for digit extraction</a>.</p>\n\n<p><img src=\"http://mathurl.com/b56zl9d.png\" alt=\"Enter image description here\"></p>\n\n<p>This formula allows you to compute arbitrary binary digits <em>without</em> computing all the digits before it. So it is used to verify the last few computed binary digits. Therefore it is <strong><em>much</em></strong> faster than a full computation.</p>\n\n<p>The advantage of this is:</p>\n\n<ol>\n<li>Only one expensive computation is needed.</li>\n</ol>\n\n<p>The disadvantage is:</p>\n\n<ol>\n<li>An implementation of the <a href=\"http://en.wikipedia.org/wiki/Bailey%E2%80%93Borwein%E2%80%93Plouffe_formula\">Bailey–Borwein–Plouffe</a> (BBP) formula is needed.</li>\n<li>An additional step is needed to verify the radix conversion from binary to decimal.</li>\n</ol>\n\n<p><sub>I've glossed over some details of why verifying the last few digits implies that all the digits are correct. But it is easy to see this since any computation error will propagate to the last digits.</sub></p>\n\n<hr>\n\n<p>Now this last step (verifying the conversion) is actually fairly important. One of the previous world record holders <strong><em>actually called us out</em></strong> on this because, initially, I didn't give a sufficient description of how it worked.</p>\n\n<p>So I've pulled this snippet from my blog:</p>\n\n<pre><code>N = # of decimal digits desired\np = 64-bit prime number\n</code></pre>\n\n<p><img src=\"http://mathurl.com/csv4fyo.png\" alt=\"Enter image description here\"><br></p>\n\n<p>Compute A using base 10 arithmetic and B using binary arithmetic.</p>\n\n<p><img src=\"http://mathurl.com/cu4slf5.png\" alt=\"enter image description here\"></p>\n\n<p>If <code>A = B</code>, then with \"extremely high probability\", the conversion is correct.</p>\n\n<hr>\n\n<p>For further reading, see my blog post <strong><a href=\"http://www.numberworld.org/misc_runs/pi-5t/details.html\">Pi - 5 Trillion Digits</a></strong>.</p>\n    "},{"t":"Expand a random range from 1–5 to 1–7","l":"http://stackoverflow.com/questions/137783/expand-a-random-range-from-1-5-to-1-7","q":"\n\n<p>Given a function which produces a random integer in the range 1 to 5, write a function which produces a random integer in the range 1 to 7.</p>\n\n<ol>\n<li>What is a simple solution?</li>\n<li>What is an effective solution to reduce memory usage or run on a slower CPU?</li>\n</ol>\n    ","a":"\n<p>This is equivalent to Adam Rosenfield's solution, but may be a bit more clear for some readers. It assumes rand5() is a function that returns a statistically random integer in the range 1 through 5 inclusive.</p>\n\n<pre><code>int rand7()\n{\n    int vals[5][5] = {\n        { 1, 2, 3, 4, 5 },\n        { 6, 7, 1, 2, 3 },\n        { 4, 5, 6, 7, 1 },\n        { 2, 3, 4, 5, 6 },\n        { 7, 0, 0, 0, 0 }\n    };\n\n    int result = 0;\n    while (result == 0)\n    {\n        int i = rand5();\n        int j = rand5();\n        result = vals[i-1][j-1];\n    }\n    return result;\n}\n</code></pre>\n\n<p>How does it work? Think of it like this: imagine printing out this double-dimension array on paper, tacking it up to a dart board and randomly throwing darts at it. If you hit a non-zero value, it's a statistically random value between 1 and 7, since there are an equal number of non-zero values to choose from. If you hit a zero, just keep throwing the dart until you hit a non-zero. That's what this code is doing: the i and j indexes randomly select a location on the dart board, and if we don't get a good result, we keep throwing darts.</p>\n\n<p>Like Adam said, this can run forever in the worst case, but statistically the worst case never happens. :)</p>\n    "},{"t":"Find an integer not among four billion given ones","l":"http://stackoverflow.com/questions/7153659/find-an-integer-not-among-four-billion-given-ones","q":"\n\n<p>It is an interview question:</p>\n\n<blockquote>\n  <p>Given an input file with four billion integers, provide an algorithm to generate an integer which is not contained in the file. Assume you have 1&nbsp;GiB memory. Follow up with what you would do if you have only 10&nbsp;MiB of memory.</p>\n</blockquote>\n\n<p>My analysis:</p>\n\n<p>The size of the file is 4 * 10<sup>9</sup> * 4 bytes = 16&nbsp;GiB.</p>\n\n<p>We can do external sorting, thus we get to know the range of the integers. My question is what is the best way to detect the missing integer in the sorted big integer sets?</p>\n\n<p>My understanding(after reading all answers):</p>\n\n<p>Assuming we are talking about 32-bit integers. There are 2^32 = 4*10<sup>9</sup> distinct integers.</p>\n\n<p>Case 1: we have 1&nbsp;GiB = 1 * 10<sup>9</sup> bytes * 8 bits/byte = 8 billion bits memory.\n  Solution: if we use one bit representing one distinct integer, it is enough. we don't\n  need sort.\n  Implementation:\n</p>\n\n<pre><code>int radix = 8;\nbyte[] bitfield = new byte[0xffffffff/radix];\nvoid F() throws FileNotFoundException{\n    Scanner in = new Scanner(new FileReader(\"a.txt\"));\n    while(in.hasNextInt()){\n        int n = in.nextInt();\n        bitfield[n/radix] |= (1 &lt;&lt; (n%radix));\n    }\n\n    for(int i = 0; i&lt; bitfield.lenght; i++){\n        for(int j =0; j&lt;radix; j++){\n            if( (bitfield[i] &amp; (1&lt;&lt;j)) == 0) System.out.print(i*radix+j);\n        }\n    }\n}\n</code></pre>\n\n<p>Case 2: 10&nbsp;MB memory = 10 * 10<sup>6</sup> * 8 bits = 80 million bits</p>\n\n<pre><code>Solution: For all possible 16-bit prefixes, there are 2^16 number of\nintegers = 65536, we need 2^16 * 4 * 8 = 2 million bits. We need build\n65536 buckets. For each bucket, we need 4 bytes holding all possibilities because\n the worst case is all the 4 billion integers belong to the same bucket.\n\nStep 1: Build the counter of each bucket through the first pass through the file.\nStep 2: Scan the buckets, find the first one who has less than 65536 hit.\nStep 3: Build new buckets whose high 16-bit prefixes are we found in step2\nthrough second pass of the file\nStep 4: Scan the buckets built in step3, find the first bucket which doesnt\nhave a hit.\n\nThe code is very similar to above one.\n</code></pre>\n\n<p>Conclusion:\n    We decrease memory through increasing file pass.</p>\n\n<hr>\n\n<p><em>A clarification for those arriving late: The question, as asked, does not say that there is exactly one integer that is not contained in the file -- at least that's not how most people interpret it. Many comments in the comment thread <strong>are</strong> about that variation of the task, though. Unfortunately the comment that <strong>introduced</strong> it to the comment thread was later deleted by its author, so now it looks like the orphaned replies to it just misunderstood everything. It's very confusing. Sorry.</em></p>\n    ","a":"\n<p><strong>Assuming that \"integer\" means 32 bits</strong>: Having 10 MB of space is more than enough for you to count how many numbers there are in the input file with any given 16-bit prefix, for all possible 16-bit prefixes in one pass through the input file. At least one of the buckets will have be hit less than 2^16 times. Do a second pass to find of which of the possible numbers in that bucket are used already.</p>\n\n<p><strong>If it means more than 32 bits, but still of bounded size</strong>: Do as above, ignoring all input numbers that happen to fall outside the (signed or unsigned; your choice) 32-bit range.</p>\n\n<p><strong>If \"integer\" means mathematical integer</strong>: Read through the input once and keep track of the <s>largest number</s> length of the longest number you've ever seen. When you're done, output <s>the maximum plus one</s> a random number that has one more digit. (One of the numbers in the file may be a bignum that takes more than 10 MB to represent exactly, but if the input is a file, then you can at least represent the <em>length</em> of anything that fits in it).</p>\n    "},{"t":"Sorting 1 million 8-digit numbers in 1MB of RAM","l":"http://stackoverflow.com/questions/12748246/sorting-1-million-8-digit-numbers-in-1mb-of-ram","q":"\n\n<p>I have a computer with 1M of RAM and no other local storage. I must use it to accept 1 million 8-digit decimal numbers over a TCP connection, sort them, and then send the sorted list out over another TCP connection. The list of numbers may contain duplicates, which I must not discard. The code will be placed in ROM, so I need not subtract the size of my code from the 1M. I already have code to drive the ethernet port and handle TCP/IP connections, and it requires 2k for its state data, including a 1k buffer via which the code will read and write data. Is there a solution to this problem?</p>\n\n<p><strong>Sources Of Question And Answer:</strong><br>\n<a href=\"http://tech.slashdot.org/comments.pl?sid=232757&amp;cid=18925745\">http://tech.slashdot.org/comments.pl?sid=232757&amp;cid=18925745</a><br>\n<a href=\"http://nick.cleaton.net/ramsort.html\">http://nick.cleaton.net/ramsort.html</a></p>\n    ","a":"\n<p>A solution is possible only because of the difference between 1 megabyte and 1 million bytes. There are about 2 to the power 8093729.5 different ways to choose 1 million 8-digit numbers with duplicates allowed and order unimportant, so a machine with only 1 million bytes of RAM doesn't have enough states to represent all the possibilities. But 1M (less 2k for TCP/IP) is 1022*1024*8 = 8372224 bits, so a solution is possible.</p>\n\n<p><strong>Part 1, initial solution</strong></p>\n\n<p>This approach needs a little more than 1M, I'll refine it to fit into 1M later.</p>\n\n<p>I'll store a compact sorted list of numbers in the range 0 to 99999999 as a sequence of sublists of 7-bit numbers. The first sublist holds numbers from 0 to 127, the second sublist holds numbers from 128 to 255, etc. 100000000/128 is exactly 781250, so 781250 such sublists will be needed.</p>\n\n<p>Each sublist consists of a 2-bit sublist header followed by a sublist body. The sublist body takes up 7 bits per sublist entry. The sublists are all concatenated together, and the format makes it possible to tell where one sublist ends and the next begins. The total storage required for a fully populated list is 2*781250 + 7*1000000 = 8562500 bits, which is about 1.021 M-bytes.</p>\n\n<p>The 4 possible sublist header values are:</p>\n\n<p><strong>00</strong> Empty sublist, nothing follows.</p>\n\n<p><strong>01</strong> Singleton, there is only one entry in the sublist and and next 7 bits hold it.</p>\n\n<p><strong>10</strong> The sublist holds at least 2 distinct numbers. The entries are stored in non-decreasing order, except that the last entry is less than or equal to the first. This allows the end of the sublist to be identified. For example, the numbers 2,4,6 would be stored as (4,6,2). The numbers 2,2,3,4,4 would be stored as (2,3,4,4,2).</p>\n\n<p><strong>11</strong> The sublist holds 2 or more repetitions of a single number. The next 7 bits give the number. Then come zero or more 7-bit entries with the value 1, followed by a 7-bit entry with the value 0. The length of the sublist body dictates the number of repetitions. For example, the numbers 12,12 would be stored as (12,0), the numbers 12,12,12 would be stored as (12,1,0), 12,12,12,12 would be (12,1,1,0) and so on.</p>\n\n<p>I start off with an empty list, read a bunch of numbers in and store them as 32 bit integers, sort the new numbers in place (using heapsort, probably) and then merge them into a new compact sorted list. Repeat until there are no more numbers to read, then walk the compact list once more to generate the output.</p>\n\n<p>The line below represents memory just before the start of the list merge operation. The \"O\"s are the region that hold the sorted 32-bit integers. The \"X\"s are the region that hold the old compact list. The \"=\" signs are the expansion room for the compact list, 7 bits for each integer in the \"O\"s. The \"Z\"s are other random overhead.</p>\n\n<pre><code>ZZZOOOOOOOOOOOOOOOOOOOOOOOOOO==========XXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre>\n\n<p>The merge routine starts reading at the leftmost \"O\" and at the leftmost \"X\", and starts writing at the leftmost \"=\". The write pointer doesn't catch the compact list read pointer until all of the new integers are merged, because both pointers advance 2 bits for each sublist and 7 bits for each entry in the old compact list, and there is enough extra room for the 7-bit entries for the new numbers.</p>\n\n<p><strong>Part 2, cramming it into 1M</strong></p>\n\n<p>To Squeeze the solution above into 1M, I need to make the compact list format a bit more compact. I'll get rid of one of the sublist types, so that there will be just 3 different possible sublist header values. Then I can use \"00\", \"01\" and \"1\" as the sublist header values and save a few bits. The sublist types are:</p>\n\n<p>A Empty sublist, nothing follows.</p>\n\n<p>B Singleton, there is only one entry in the sublist and and next 7 bits hold it.</p>\n\n<p>C The sublist holds at least 2 distinct numbers. The entries are stored in non-decreasing order, except that the last entry is less than or equal to the first. This allows the end of the sublist to be identified. For example, the numbers 2,4,6 would be stored as (4,6,2). The numbers 2,2,3,4,4 would be stored as (2,3,4,4,2).</p>\n\n<p>D The sublist consists of 2 or more repetitions of a single number.</p>\n\n<p>My 3 sublist header values will be \"A\", \"B\" and \"C\", so I need a way to represent D-type sublists.</p>\n\n<p>Suppose I have the C-type sublist header followed by 3 entries, such as \"C[17][101][58]\". This can't be part of a valid C-type sublist as described above, since the third entry is less than the second but more than the first. I can use this type of construct to represent a D-type sublist. In bit terms, anywhere I have \"C{00?????}{1??????}{01?????}\" is an impossible C-type sublist. I'll use this to represent a sublist consisting of 3 or more repetitions of a single number. The first two 7-bit words encode the number (the \"N\" bits below) and are followed by zero or more {0100001} words followed by a {0100000} word.</p>\n\n<pre><code>For example, 3 repetitions: \"C{00NNNNN}{1NN0000}{0100000}\", 4 repetitions: \"C{00NNNNN}{1NN0000}{0100001}{0100000}\", and so on.\n</code></pre>\n\n<p>That just leaves lists that hold exactly 2 repetitions of a single number. I'll represent those with another impossible C-type sublist pattern: \"C{0??????}{11?????}{10?????}\". There's plenty of room for the 7 bits of the number in the first 2 words, but this pattern is longer than the sublist that it represents, which makes things a bit more complex. The five question-marks at the end can be considered not part of the pattern, so I have: \"C{0NNNNNN}{11N????}10\" as my pattern, with the number to be repeated stored in the \"N\"s. That's 2 bits too long.</p>\n\n<p>I'll have to borrow 2 bits and pay them back from the 4 unused bits in this pattern. When reading, on encountering \"C{0NNNNNN}{11N00AB}10\", output 2 instances of the number in the \"N\"s, overwrite the \"10\" at the end with bits A and B, and rewind the read pointer by 2 bits. Destructive reads are ok for this algorithm, since each compact list gets walked only once.</p>\n\n<p>When writing a sublist of 2 repetitions of a single number, write \"C{0NNNNNN}11N00\" and set the borrowed bits counter to 2. At every write where the borrowed bits counter is non-zero, it is decremented for each bit written and \"10\" is written when the counter hits zero. So the next 2 bits written will go into slots A and B, and then the \"10\" will get dropped onto the end.</p>\n\n<p>With 3 sublist header values represented by \"00\", \"01\" and \"1\", I can assign \"1\" to the most popular sublist type. I'll need a small table to map sublist header values to sublist types, and I'll need an occurrence counter for each sublist type so that I know what the best sublist header mapping is.</p>\n\n<p>The worst case minimal representation of a fully populated compact list occurs when all the sublist types are equally popular. In that case I save 1 bit for every 3 sublist headers, so the list size is 2*781250 + 7*1000000 - 781250/3 = 8302083.3 bits. Rounding up to a 32 bit word boundary, thats 8302112 bits, or 1037764 bytes.</p>\n\n<p>1M minus the 2k for TCP/IP state and buffers is 1022*1024 = 1046528 bytes, leaving me 8764 bytes to play with.</p>\n\n<p>But what about the process of changing the sublist header mapping ? In the memory map below, \"Z\" is random overhead, \"=\" is free space, \"X\" is the compact list.</p>\n\n<pre><code>ZZZ=====XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre>\n\n<p>Start reading at the leftmost \"X\" and start writing at the leftmost \"=\" and work right. When it's done the compact list will be a little shorter and it will be at the wrong end of memory:</p>\n\n<pre><code>ZZZXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX=======\n</code></pre>\n\n<p>So then I'll need to shunt it to the right:</p>\n\n<pre><code>ZZZ=======XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre>\n\n<p>In the header mapping change process, up to 1/3 of the sublist headers will be changing from 1-bit to 2-bit. In the worst case these will all be at the head of the list, so I'll need at least 781250/3 bits of free storage before I start, which takes me back to the memory requirements of the previous version of the compact list :(</p>\n\n<p>To get around that, I'll split the 781250 sublists into 10 sublist groups of 78125 sublists each. Each group has its own independent sublist header mapping. Using the letters A to J for the groups:</p>\n\n<pre><code>ZZZ=====AAAAAABBCCCCDDDDDEEEFFFGGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\n</code></pre>\n\n<p>Each sublist group shrinks or stays the same during a sublist header mapping change:</p>\n\n<pre><code>ZZZ=====AAAAAABBCCCCDDDDDEEEFFFGGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAA=====BBCCCCDDDDDEEEFFFGGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABB=====CCCCDDDDDEEEFFFGGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCC======DDDDDEEEFFFGGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCCDDDDD======EEEFFFGGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCCDDDDDEEE======FFFGGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCCDDDDDEEEFFF======GGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCCDDDDDEEEFFFGGGGGGGGGG=======HHIJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCCDDDDDEEEFFFGGGGGGGGGGHH=======IJJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCCDDDDDEEEFFFGGGGGGGGGGHHI=======JJJJJJJJJJJJJJJJJJJJ\nZZZAAAAAABBCCCDDDDDEEEFFFGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ=======\nZZZ=======AAAAAABBCCCDDDDDEEEFFFGGGGGGGGGGHHIJJJJJJJJJJJJJJJJJJJJ\n</code></pre>\n\n<p>The worst case temporary expansion of a sublist group during a mapping change is 78125/3 = 26042 bits, under 4k. If I allow 4k plus the 1037764 bytes for a fully populated compact list, that leaves me 8764 - 4096 = 4668 bytes for the \"Z\"s in the memory map.</p>\n\n<p>That should be plenty for the 10 sublist header mapping tables, 30 sublist header occurrence counts and the other few counters, pointers and small buffers I'll need, and space I've used without noticing, like stack space for function call return addresses and local variables.</p>\n\n<p><strong>Part 3, how long would it take to run ?</strong></p>\n\n<p>With an empty compact list the 1-bit list header will be used for an empty sublist, and the starting size of the list will be 781250 bits. In the worst case the list grows 8 bits for each number added, so 32 + 8 = 40 bits of free space are needed for each of the 32-bit numbers to be placed at the top of the list buffer and then sorted and merged. In the worst case, changing the sublist header mapping results in a space usage of 2*781250 + 7*entries - 781250/3 bits.</p>\n\n<p>With a policy of changing the sublist header mapping after every fifth merge once there are at least 800000 numbers in the list, a worst case run would involve a total of about 30M of compact list reading and writing activity.</p>\n\n<p><strong>Source:</strong></p>\n\n<p><strong><a href=\"http://nick.cleaton.net/ramsortsol.html\">http://nick.cleaton.net/ramsortsol.html</a></strong></p>\n    "},{"t":"Big O, how do you calculate/approximate it?","l":"http://stackoverflow.com/questions/3255/big-o-how-do-you-calculate-approximate-it","q":"\n\n<p>Most people with a degree in CS will certainly know what <a href=\"http://www.nist.gov/dads/HTML/bigOnotation.html\">Big O stands for</a>.\nIt helps us to measure how (in)efficient an algorithm really is and if you know in <a href=\"http://en.wikipedia.org/wiki/List_of_complexity_classes\">what category the problem you are trying to solve lays in</a> you can figure out if it is still possible to squeeze out that little extra performance.<sup>1</sup></p>\n\n<p>But I'm curious, how do <em>you</em> calculate or approximate the complexity of your algorithms?</p>\n\n<p><sup>1</sup> <sub>but as they say, don't overdo it, <a href=\"http://en.wikipedia.org/wiki/Optimization_%28computer_science%29#When_to_optimize\">premature optimization is the root of all evil</a>, and optimization without a justified cause should deserve that name as well.</sub></p>\n    ","a":"\n<p>I'm a professor assistant at my local university on the Data Structures and Algorithms course. I'll do my best to explain it here on simple terms, but be warned that this topic takes my students a couple of months to finally grasp. You can find more information on the Chapter 2 of the <a href=\"http://rads.stackoverflow.com/amzn/click/0321370139\">Data Structures and Algorithms in Java</a> book.</p>\n\n<hr>\n\n<p>There is no <a href=\"http://en.wikipedia.org/wiki/Halting_problem\">mechanical procedure</a> that can be used to get the BigOh.</p>\n\n<p>As a \"cookbook\", to obtain the <a href=\"http://en.wikipedia.org/wiki/Big_Oh_notation\">BigOh</a> from a piece of code you first need to realize that you are creating a math formula to count how many steps of computations get executed given an input of some size.</p>\n\n<p>The purpose is simple: to compare algorithms from a theoretical point of view, without the need to execute the code. The lesser the number of steps, the faster the algorithm.</p>\n\n<p>For example, let's say you have this piece of code:</p>\n\n<pre><code>int sum(int* data, int N) {\n    int result = 0;               // 1\n\n    for (int i = 0; i &lt; N; i++) { // 2\n        result += data[i];        // 3\n    }\n\n    return result;                // 4\n}\n</code></pre>\n\n<p>This function returns the sum of all the elements of the array, and we want to create a formula to count the <a href=\"http://en.wikipedia.org/wiki/Computational_complexity_theory\">computational complexity</a> of that function:</p>\n\n<pre><code>Number_Of_Steps = f(N)\n</code></pre>\n\n<p>So we have <code>f(N)</code>, a function to count the number of computational steps. The input of the function is the size of the structure to process. It means that this function is called suchs as:</p>\n\n<pre><code>Number_Of_Steps = f(data.length)\n</code></pre>\n\n<p>The parameter <code>N</code> takes the <code>data.length</code> value. Now we need the actual definition of the function <code>f()</code>. This is done from the source code, in which each interesting line is numbered from 1 to 4.</p>\n\n<p>There are many ways to calculate the BigOh. From this point forward we are going to assume that every sentence that doesn't depend on the size of the input data takes a constant <code>C</code> number computational steps.</p>\n\n<p>We are going to add the individual number of steps of the function, and neither the local variable declaration nor the return statement depends on the size of the <code>data</code> array.</p>\n\n<p>That means that lines 1 and 4 takes C amount of steps each, and the function is somewhat like this:</p>\n\n<pre><code>f(N) = C + ??? + C\n</code></pre>\n\n<p>The next part is to define the value of the <code>for</code> statement. Remember that we are counting the number of computational steps, meaning that the body of the for statement gets executed N times. That's the same as adding <code>C</code>, <code>N</code> times:</p>\n\n<pre><code>f(N) = C + (C + C + ... + C) + C = C + N * C + C\n</code></pre>\n\n<p>There is no mechanical rule to count how many times the body of the <code>for</code> gets executed, you need to count it by looking at what does the code do. To simplify the calculations, we are ignoring the variable initialization, condition and increment parts of the <code>for</code> statement.</p>\n\n<p>To get the actual BigOh we need the <a href=\"http://en.wikipedia.org/wiki/Asymptotic_analysis\">Asymptotic analysis</a> of the function. This is roughly done like this:</p>\n\n<ol>\n<li>Take away all the constants <code>C</code>.</li>\n<li>From <code>f()</code> get the <a href=\"http://en.wikipedia.org/wiki/Polynomial\">polynomium</a> in its <code>standard form</code>.</li>\n<li>Divide the terms of the polynomium and sort them by the rate of growth.</li>\n<li>Keep the one that grows bigger when <code>N</code> approaches <code>infinity</code>.</li>\n</ol>\n\n<p>Our <code>f()</code> has two terms:</p>\n\n<pre><code>f(N) = 2 * C * N ^ 0 + 1 * C * N ^ 1\n</code></pre>\n\n<p>Taking away all the <code>C</code> constants and redundant parts:</p>\n\n<pre><code>f(N) = 1 + N ^ 1\n</code></pre>\n\n<p>Since the last term is the one which grows bigger when <code>f()</code> approaches infinity (think on <a href=\"http://en.wikipedia.org/wiki/Limit_%28mathematics%29\">limits</a>) this is the BigOh argument, and the <code>sum()</code> function has a BigOh of:</p>\n\n<pre><code>O(N)\n</code></pre>\n\n<hr>\n\n<p>There are a few tricks to solve some tricky ones: use <a href=\"http://en.wikipedia.org/wiki/Summation\">summations</a> whenever you can. There are some handy <a href=\"http://en.wikipedia.org/wiki/Summation#Identities\">summation identities</a> already proven to be correct.</p>\n\n<p>As another example, this code can be easily solved using summations:</p>\n\n<pre><code>for (i = 0; i &lt; 2*n; i += 2) {  // 1\n    for (j=n; j &gt; i; j--) {     // 2\n        foo();                  // 3\n    }\n}\n</code></pre>\n\n<p>The first thing you needed to be asked is the order of execution of <code>foo()</code>. While the usual is to be <code>O(1)</code>, you need to ask your professors about it. <code>O(1)</code> means (almost, mostly) constant <code>C</code>, independent of the size <code>N</code>.</p>\n\n<p>The <code>for</code> statement on the sentence number one is tricky. While the index ends at <code>2 * N</code>, the increment is done by two. That means that the first <code>for</code> gets executed only <code>N</code> steps, and we need to divide the count by two.</p>\n\n<pre><code>f(N) = Summation(i from 1 to 2 * N / 2)( ... ) = \n     = Summation(i from 1 to N)( ... )\n</code></pre>\n\n<p>The sentence number <em>two</em> is even trickier since it depends on the value of <code>i</code>. Take a look: the index i takes the values: 0, 2, 4, 6, 8, ..., 2 * N, and the second <code>for</code> get executed: N times the first one, N - 2 the second, N - 4 the third... up to the N / 2 stage, on which the second <code>for</code> never gets executed.</p>\n\n<p>On formula, that means:</p>\n\n<pre><code>f(N) = Summation(i from 1 to N)( Summation(j = ???)(  ) )\n</code></pre>\n\n<p>Again, we are counting <strong>the number of steps</strong>. And by definition, every summation should always start at one, and end at a number bigger-or-equal than one.</p>\n\n<pre><code>f(N) = Summation(i from 1 to N)( Summation(j = 1 to (N - (i - 1) * 2)( C ) )\n</code></pre>\n\n<p>(We are assuming that <code>foo()</code> is <code>O(1)</code> and takes <code>C</code> steps.)</p>\n\n<p>We have a problem here: when <code>i</code> takes the value <code>N / 2 + 1</code> upwards, the inner Summation ends at a negative number! That's impossible and wrong. We need to split the summation in two, being the pivotal point the moment <code>i</code> takes <code>N / 2 + 1</code>.</p>\n\n<pre><code>f(N) = Summation(i from 1 to N / 2)( Summation(j = 1 to (N - (i - 1) * 2)) * ( C ) ) + Summation(i from 1 to N / 2) * ( C )\n</code></pre>\n\n<p>Since the pivotal moment <code>i &gt; N / 2</code>, the inner for won't get executed, and we are assuming a constant C execution complexity on its body.</p>\n\n<p>Now the summations can be simplified using some identity rules:</p>\n\n<ol>\n<li>Summation(w from 1 to N)( C ) = N * C</li>\n<li>Summation(w from 1 to N)( A (+/-) B ) = Summation(w from 1 to N)( A ) (+/-) Summation(w from 1 to N)( B )</li>\n<li>Summation(w from 1 to N)( w * C ) = C * Summation(w from 1 to N)( w ) (C is a constant, independent of <code>w</code>)</li>\n<li>Summation(w from 1 to N)( w ) = (w * (w + 1)) / 2</li>\n</ol>\n\n<p>Applying some algebra:</p>\n\n<pre><code>f(N) = Summation(i from 1 to N / 2)( (N - (i - 1) * 2) * ( C ) ) + (N / 2)( C )\n\nf(N) = C * Summation(i from 1 to N / 2)( (N - (i - 1) * 2)) + (N / 2)( C )\n\nf(N) = C * (Summation(i from 1 to N / 2)( N ) - Summation(i from 1 to N / 2)( (i - 1) * 2)) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * Summation(i from 1 to N / 2)( i - 1 )) + (N / 2)( C )\n\n=&gt; Summation(i from 1 to N / 2)( i - 1 ) = Summation(i from 1 to N / 2 - 1)( i )\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * Summation(i from 1 to N / 2 - 1)( i )) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * ( (N / 2 - 1) * (N / 2 - 1 + 1) / 2) ) + (N / 2)( C )\n\n=&gt; (N / 2 - 1) * (N / 2 - 1 + 1) / 2 = \n\n   (N / 2 - 1) * (N / 2) / 2 = \n\n   ((N ^ 2 / 4) - (N / 2)) / 2 = \n\n   (N ^ 2 / 8) - (N / 4)\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * ( (N ^ 2 / 8) - (N / 4) )) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - ( (N ^ 2 / 4) - (N / 2) )) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - (N ^ 2 / 4) + (N / 2)) + (N / 2)( C )\n\nf(N) = C * ( N ^ 2 / 4 ) + C * (N / 2) + C * (N / 2)\n\nf(N) = C * ( N ^ 2 / 4 ) + 2 * C * (N / 2)\n\nf(N) = C * ( N ^ 2 / 4 ) + C * N\n\nf(N) = C * 1/4 * N ^ 2 + C * N\n</code></pre>\n\n<p>And the BigOh is:</p>\n\n<pre><code>O(N ^ 2)\n</code></pre>\n    "},{"t":"How to count the number of set bits in a 32-bit integer?","l":"http://stackoverflow.com/questions/109023/how-to-count-the-number-of-set-bits-in-a-32-bit-integer","q":"\n\n<p>8 bits representing the number 7 look like this:</p>\n\n<pre><code>00000111\n</code></pre>\n\n<p>Three bits are set.   </p>\n\n<p>What are algorithms to determine the number of set bits in a 32-bit integer?</p>\n    ","a":"\n<p>This is known as the '<a href=\"http://en.wikipedia.org/wiki/Hamming_weight\">Hamming Weight</a>', 'popcount' or 'sideways addition'.</p>\n\n<p>The 'best' algorithm really depends on which CPU you are on and what your usage pattern is.</p>\n\n<p>Some CPUs have a single built-in instruction to do it and others have parallel instructions which act on bit vectors. The parallel instructions will almost certainly be fastest, however, the single-instruction algorithms are 'usually microcoded loops that test a bit per cycle; a log-time algorithm coded in C is often faster'.</p>\n\n<p>A pre-populated table lookup method can be very fast if your CPU has a large cache and/or you are doing lots of these instructions in a tight loop. However it can suffer because of the expense of a 'cache miss', where the CPU has to fetch some of the table from main memory.</p>\n\n<p>If you know that your bytes will be mostly 0's or mostly 1's then there are very efficient algorithms for these scenarios.</p>\n\n<p>I believe a very good general purpose algorithm is the following, known as 'parallel' or 'variable-precision SWAR algorithm'. I have expressed this in a C-like pseudo language, you may need to adjust it to work for a particular language (e.g. using uint32_t for C++ and &gt;&gt;&gt; in Java):</p>\n\n<pre><code>int NumberOfSetBits(int i)\n{\n     i = i - ((i &gt;&gt; 1) &amp; 0x55555555);\n     i = (i &amp; 0x33333333) + ((i &gt;&gt; 2) &amp; 0x33333333);\n     return (((i + (i &gt;&gt; 4)) &amp; 0x0F0F0F0F) * 0x01010101) &gt;&gt; 24;\n}\n</code></pre>\n\n<p>This is because it has the best worst-case behaviour of any of the algorithms discussed, so will efficiently deal with any usage pattern or values you throw at it.</p>\n\n<p>References:</p>\n\n<p><a href=\"http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel\">http://graphics.stanford.edu/~seander/bithacks.html</a></p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Hamming_weight\">http://en.wikipedia.org/wiki/Hamming_weight</a></p>\n\n<p><a href=\"http://gurmeetsingh.wordpress.com/2008/08/05/fast-bit-counting-routines/\">http://gurmeetsingh.wordpress.com/2008/08/05/fast-bit-counting-routines/</a></p>\n\n<p><a href=\"http://aggregate.ee.engr.uky.edu/MAGIC/#Population%20Count%20%28Ones%20Count%29\">http://aggregate.ee.engr.uky.edu/MAGIC/#Population%20Count%20(Ones%20Count)</a></p>\n    "},{"t":"What are the differences between NP, NP-Complete and NP-Hard?","l":"http://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard","q":"\n\n<p>What are the differences between <strong>NP</strong>, <strong>NP-Complete</strong> and <strong>NP-Hard</strong>?</p>\n\n<p>I am aware of many resources all over the web. I'd like to read your explanations, and the reason is they might be different then what's out there, or it's out there and I'm not aware.</p>\n    ","a":"\n<p>I assume that you are looking for intuitive definitions, since the technical definitions require quite some time to understand. First of all, let's remember a preliminary needed concept to understand those definitions.</p>\n\n<ul>\n<li><strong>Decision problem</strong>: A problem with a <strong>yes</strong> or <strong>no</strong> answer.</li>\n</ul>\n\n<hr>\n\n<p>Now, let us define those <em>complexity classes</em>.</p>\n\n<h1>P</h1>\n\n<p><em>P is a complexity class that represents the set of all decision problems that can be solved in polynomial time</em>. That is, given an instance of the problem, the answer yes or no can be decided in polynomial time.</p>\n\n<p><strong>Example</strong></p>\n\n<p>Given a graph connected <code>G</code>, can its vertices be coloured using two colours so that no edge is monochromatic?</p>\n\n<p>Algorithm: start with an arbitrary vertex, color it red and all of its neighbours blue and continue. Stop when you run out of vertices or you are forced to make an edge have both of its endpoints be the same color.</p>\n\n<hr>\n\n<h1>NP</h1>\n\n<p><em>NP is a complexity class that represents the set of all decision problems for which the answer is yes have proofs that can be verified in polynomial time.</em></p>\n\n<p>This means that if someone gives us an instance of the problem and a certificate (sometimes called a witness) to the answer being yes, we can check that it is correct in polynomial time.</p>\n\n<p><strong>Example</strong></p>\n\n<p><em>Integer factorisation</em> is in NP. This is the problem that given integers <code>n</code> and <code>m</code>, is there an integer <code>f</code> with <code>1 &lt; f &lt; m</code>, such that <code>f</code> divides <code>n</code> (<code>f</code> is a small factor of <code>n</code>)? </p>\n\n<p>This is a decision problem because the answers are yes or no. If someone hands us an instance of the problem (so they hand us integers <code>n</code> and <code>m</code>) and an integer <code>f</code> with <code>1 &lt; f &lt; m</code>, and claim that <code>f</code> is a factor of <code>n</code> (the certificate), we can check the answer in <em>polynomial time</em> by performing the division <code>n / f</code>.</p>\n\n<hr>\n\n<h1>NP-Complete</h1>\n\n<p><em>NP is a complexity class which represents the set of all problems <code>X</code> for which it is possible to reduce any other NP problem <code>Y</code> to <code>X</code> in polynomial time.</em></p>\n\n<p>Intuitively this means that we can solve <code>Y</code> quickly if we know how to solve <code>X</code> quickly. Precisely, <code>Y</code> is reducible to <code>X</code>, if there is a polynomial time algorithm <code>f</code> to transform instances <code>y</code> of <code>Y</code> to instances <code>x = f(y)</code> of <code>X</code> in polynomial time, with the property that the answer to <code>y</code> is yes, if and only if the answer to <code>f(y)</code> is yes.</p>\n\n<p><strong>Example</strong> </p>\n\n<p><code>3-SAT</code>. This is the problem wherein we are given a conjunction (ANDs) of 3-clause disjunctions (ORs), statements of the form</p>\n\n<pre><code>(x_v11 OR x_v21 OR x_v31) AND \n(x_v12 OR x_v22 OR x_v32) AND \n...                       AND \n(x_v1n OR x_v2n OR x_v3n)\n</code></pre>\n\n<p>where each <code>x_vij</code> is a boolean variable or the negation of a variable from a finite predefined list <code>(x_1, x_2, ... x_n)</code>. </p>\n\n<p>It can be shown that <em>every NP problem can be reduced to 3-SAT</em>. The proof of this is technical and requires use of the technical definition of NP (<em>based on non-deterministic Turing machines</em>). This is known as <em>Cook's theorem</em>.</p>\n\n<p>What makes NP-complete problems important is that if a deterministic polynomial time algorithm can be found to solve one of them, every NP problem is solvable in polynomial time (one problem to rule them all).</p>\n\n<hr>\n\n<h1>NP-hard</h1>\n\n<p>Intuitively, these are the problems that are <em>even harder than the NP-complete problems</em>. Note that NP-hard problems <em>do not have to be in NP</em>, and <em>they do not have to be decision problems</em>. </p>\n\n<p>The precise definition here is that <em>a problem <code>X</code> is NP-hard, if there is an NP-complete problem <code>Y</code>, such that <code>Y</code> is reducible to <code>X</code> in polynomial time</em>.</p>\n\n<p>But since any NP-complete problem can be reduced to any other NP-complete problem in polynomial time, all NP-complete problems can be reduced to any NP-hard problem in polynomial time. Then, if there is a solution to one NP-hard problem in polynomial time, there is a solution to all NP problems in polynomial time.</p>\n\n<p><strong>Example</strong></p>\n\n<p>The <em>halting problem</em> is the classic NP-hard problem. This is the problem that given a program <code>P</code> and input <code>I</code>, will it halt? This is a decision problem but it is not in NP. It is clear that any NP-complete problem can be reduced to this one.</p>\n\n<p>My favorite NP-complete problem is the <a href=\"http://web.mat.bham.ac.uk/R.W.Kaye/minesw/ordmsw.htm\" rel=\"nofollow\">Minesweeper problem</a>.</p>\n\n<hr>\n\n<h1>P = NP</h1>\n\n<p>This one of most famous problem in computer science, and one of the most important outstanding questions in the mathematical sciences. In fact, the <a href=\"http://www.claymath.org/millennium/P_vs_NP/\" rel=\"nofollow\">Clay Institute</a> is offering one million dollars for a solution to the problem (Stephen Cook's <a href=\"http://www.claymath.org/millennium/P_vs_NP/pvsnp.pdf\" rel=\"nofollow\">writeup</a> on the Clay website is quite good). </p>\n\n<p>It's clear that P is a subset of NP. The open question is whether or not NP problems have deterministic polynomial time solutions. It is largely believed that they do not. Here is an outstanding recent article on the latest (and the importance) of the P = NP problem: <a href=\"http://cacm.acm.org/magazines/2009/9/38904-the-status-of-the-p-versus-np-problem/fulltext\" rel=\"nofollow\">The Status of the P versus NP problem</a>. </p>\n\n<p>The best book on the subject is <a href=\"http://rads.stackoverflow.com/amzn/click/0716710455\" rel=\"nofollow\">Computers and Intractability</a> by Garey and Johnson. </p>\n    "},{"t":"What algorithms compute directions from point A to point B on a map?","l":"http://stackoverflow.com/questions/430142/what-algorithms-compute-directions-from-point-a-to-point-b-on-a-map","q":"\n\n<p>How do map providers (such as Google or Yahoo! Maps) suggest directions?</p>\n\n<p>I mean, they probably have real-world data in some form, certainly including distances but also perhaps things like driving speeds, presence of sidewalks, train schedules, etc.  But suppose the data were in a simpler format, say a very large directed graph with edge weights reflecting distances.  I want to be able to quickly compute directions from one arbitrary point to another.  Sometimes these points will be close together (within one city) while sometimes they will be far apart (cross-country).</p>\n\n<p>Graph algorithms like Dijkstra's algorithm will not work because the graph is enormous.  Luckily, heuristic algorithms like A* will probably work.  However, our data is very structured, and perhaps some kind of tiered approach might work?  (For example, store precomputed directions between certain \"key\" points far apart, as well as some local directions.  Then directions for two far-away points will involve local directions to a key points, global directions to another key point, and then local directions again.)</p>\n\n<p>What algorithms are actually used in practice?</p>\n\n<p>PS.  This question was motivated by finding quirks in online mapping directions.  Contrary to the triangle inequality, sometimes Google Maps thinks that <a href=\"http://maps.google.com/maps?f=d&amp;saddr=Place+Jacques+Bonsergent,+75010+Paris&amp;daddr=Place+Louis+Lepine,+75004+Paris&amp;hl=en&amp;geocode=&amp;mra=ls&amp;dirflg=w&amp;sll=48.861295,2.35161&amp;sspn=0.013778,0.029655&amp;ie=UTF8&amp;z=14\">X-Z</a> takes longer and is farther than using an intermediate point as in <a href=\"http://maps.google.com/maps?f=d&amp;saddr=Place+Jacques+Bonsergent,+75010+Paris&amp;daddr=Square+Emile+Chautemps,+75003+Paris+to%3aPlace+Louis+Lepine,+75004+Paris&amp;hl=en&amp;geocode=&amp;mra=ls&amp;dirflg=w&amp;sll=48.869359,2.357833&amp;sspn=0.006888,0.014827&amp;ie=UTF8&amp;z=14\">X-Y-Z</a>.  But maybe their walking directions optimize for another parameter, too?</p>\n\n<p>PPS.  Here's another violation of the triangle inequality that suggests (to me) that they use some kind of tiered approach: <a href=\"http://maps.google.com/maps?f=d&amp;saddr=214,+boulevard+de+la+Villette,+75019+Paris&amp;daddr=Passage+des+Patriarches,+75005+Paris&amp;hl=en&amp;geocode=&amp;sll=48.86278,2.35595&amp;sspn=0.05511,0.118618&amp;mra=cc&amp;dirflg=w&amp;ie=UTF8&amp;z=13\">X-Z</a> versus <a href=\"http://maps.google.com/maps?f=d&amp;saddr=214,+boulevard+de+la+Villette,+75019+Paris&amp;daddr=Square+Emile+Chautemps,+75003+Paris+to%3aPassage+des+Patriarches,+75005+Paris&amp;hl=en&amp;geocode=&amp;mra=ls&amp;dirflg=w&amp;sll=48.86278,2.35595&amp;sspn=0.05511,0.118618&amp;ie=UTF8&amp;ll=48.862682,2.357941&amp;spn=0.05511,0.118618&amp;z=13\">X-Y-Z</a>.  The former seems to use prominent Boulevard de Sebastopol even though it's slightly out of the way.</p>\n\n<p><strong>Edit</strong>: Neither of these examples seem to work anymore, but both did at the time of the original post.</p>\n    ","a":"\n<p>Speaking as someone who spent 18 months working at a mapping company, which included working on the routing algorithm... yes, <a href=\"http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\">Dijkstra's</a> does work, with a couple of modifications:</p>\n\n<ul>\n<li>Instead of doing <a href=\"http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\">Dijkstra's</a> once from source to dest, you start at each end, and expand both sides until they meet in the middle. This eliminates roughly half the work (2*pi*(r/2)^2 vs pi*r^2).</li>\n<li>To avoid exploring the back-alleys of every city between your source and destination, you can have several layers of map data: A 'highways' layer that contains only highways, a 'secondary' layer that contains only secondary streets, and so forth. Then, you explore only smaller sections of the more detailed layers, expanding as necessary. Obviously this description leaves out a lot of detail, but you get the idea.</li>\n</ul>\n\n<p>With modifications along those lines, you can do even cross-country routing in a very reasonable timeframe.</p>\n    "},{"t":"How to check if a number is a power of 2","l":"http://stackoverflow.com/questions/600293/how-to-check-if-a-number-is-a-power-of-2","q":"\n\n<p>Today I needed a simple algorithm for checking if a number is a power of 2.</p>\n\n<p>The algorithm needs to be:</p>\n\n<ol>\n<li>Simple</li>\n<li>Correct for any <code>ulong</code> value.</li>\n</ol>\n\n<p>I came up with this simple algorithm:</p>\n\n<pre><code>private bool IsPowerOfTwo(ulong number)\n{\n    if (number == 0)\n        return false;\n\n    for (ulong power = 1; power &gt; 0; power = power &lt;&lt; 1)\n    {\n        // This for loop used shifting for powers of 2, meaning\n        // that the value will become 0 after the last shift\n        // (from binary 1000...0000 to 0000...0000) then, the 'for'\n        // loop will break out.\n\n        if (power == number)\n            return true;\n        if (power &gt; number)\n            return false;\n    }\n    return false;\n}\n</code></pre>\n\n<p>But then I thought, how about checking if <code>log<sub>2</sub> x</code> is an exactly round number? But when I checked for 2^63+1, <code>Math.Log</code> returned exactly 63 because of rounding. So I checked if 2 to the power 63 is equal to the original number - and it is, because the calculation is done in <code>double</code>s and not in exact numbers:</p>\n\n<pre><code>private bool IsPowerOfTwo_2(ulong number)\n{\n    double log = Math.Log(number, 2);\n    double pow = Math.Pow(2, Math.Round(log));\n    return pow == number;\n}\n</code></pre>\n\n<p>This returned <code>true</code> for the given wrong value: <code>9223372036854775809</code>.</p>\n\n<p>Is there a better algorithm?</p>\n    ","a":"\n<p>There's a simple trick for this problem:</p>\n\n<pre><code>bool IsPowerOfTwo(ulong x)\n{\n    return (x &amp; (x - 1)) == 0;\n}\n</code></pre>\n\n<p>For completeness, zero is not a power of two. If you want to take into account that edge case, here's how:</p>\n\n<pre><code>bool IsPowerOfTwo(ulong x)\n{\n    return (x != 0) &amp;&amp; ((x &amp; (x - 1)) == 0);\n}\n</code></pre>\n\n<h3>Explanation</h3>\n\n<p>First and foremost the bitwise binary &amp; operator from MSDN definition:</p>\n\n<blockquote>\n  <p>Binary &amp; operators are predefined for the integral types and bool. For\n  integral types, &amp; computes the logical bitwise AND of its operands.\n  For bool operands, &amp; computes the logical AND of its operands; that\n  is, the result is true if and only if both its operands are true.</p>\n</blockquote>\n\n<p>Now let's take a look at how this all plays out:</p>\n\n<p>The function returns boolean (true / false) and accepts one incoming parameter of type unsigned long (x, in this case).  Let us for the sake of simplicity assume that someone has passed the value 4 and called the function like so:</p>\n\n<pre><code>bool b = IsPowerOfTwo(4)\n</code></pre>\n\n<p>Now we replace each occurrence of x with 4:</p>\n\n<pre><code>return (4 != 0) &amp;&amp; ((4 &amp; (4-1)) == 0);\n</code></pre>\n\n<p>Well we already know that 4 != 0 evals to true, so far so good.  But what about:</p>\n\n<pre><code>((4 &amp; (4-1)) == 0)\n</code></pre>\n\n<p>This translates to this of course:</p>\n\n<pre><code>((4 &amp; 3) == 0)\n</code></pre>\n\n<p>But what exactly is <code>4&amp;3</code>?</p>\n\n<p>The binary representation of 4 is 100 and the binary representation of 3 is 011 (remember the &amp; takes the binary representation of these numbers.  So we have:</p>\n\n<pre><code>100 = 4\n011 = 3\n</code></pre>\n\n<p>Imagine these values being stacked up much like elementary addition. The <code>&amp;</code> operator says that if both values are equal to 1 then the result is 1, otherwise it is 0. So <code>1 &amp; 1 = 1</code>, <code>1 &amp; 0 = 0</code>, <code>0 &amp; 0 = 0</code>, and <code>0 &amp; 1 = 0</code>. So we do the math:</p>\n\n<pre><code>100\n011\n----\n000\n</code></pre>\n\n<p>The result is simply 0. So we go back and look at what our return statement now translates to:</p>\n\n<pre><code>return (4 != 0) &amp;&amp; ((4 &amp; 3) == 0);\n</code></pre>\n\n<p>Which translates now to:</p>\n\n<pre><code>return true &amp;&amp; (0 == 0);\n</code></pre>\n\n\n\n<pre><code>return true &amp;&amp; true;\n</code></pre>\n\n<p>We all know that <code>true &amp;&amp; true</code> is simply <code>true</code>, and this shows that for our example, 4 is a power of 2.</p>\n    "},{"t":"How to code a URL shortener?","l":"http://stackoverflow.com/questions/742013/how-to-code-a-url-shortener","q":"\n\n<p>I want to create a URL shortener service where you can write a long URL into an input field and the service shortens the URL to \"<code>http://www.example.org/abcdef</code>\". Instead of \"<code>abcdef</code>\" there can be any other string with six characters containing <code>a-z, A-Z and 0-9</code>. That makes 56~57 billion possible strings.</p>\n\n<p><strong>Edit:</strong> Due to the ongoing interest in this topic, I've uploaded the code that I used <a href=\"https://github.com/delight-im/ShortURL\">to GitHub</a>, with implementations for <a href=\"https://github.com/delight-im/ShortURL/blob/master/Java/ShortURL.java\">Java</a>, <a href=\"https://github.com/delight-im/ShortURL/blob/master/PHP/ShortURL.php\">PHP</a> and <a href=\"https://github.com/delight-im/ShortURL/blob/master/JavaScript/ShortURL.js\">JavaScript</a>. Add your solutions if you like :)</p>\n\n<p>My approach:</p>\n\n<p>I have a database table with three columns:</p>\n\n<ol>\n<li>id, integer, auto-increment</li>\n<li>long, string, the long URL the user entered</li>\n<li>short, string, the shortened URL (or just the six characters)</li>\n</ol>\n\n<p>I would then insert the long URL into the table. Then I would select the auto-increment value for \"<code>id</code>\" and build a hash of it. This hash should then be inserted as \"<code>short</code>\". But what sort of hash should I build? Hash algorithms like MD5 create too long strings. I don't use these algorithms, I think. A self-built algorithm will work, too.</p>\n\n<p>My idea:</p>\n\n<p>For \"<code>http://www.google.de/</code>\" I get the auto-increment id <code>239472</code>. Then I do the following steps:</p>\n\n<pre><code>short = '';\nif divisible by 2, add \"a\"+the result to short\nif divisible by 3, add \"b\"+the result to short\n... until I have divisors for a-z and A-Z.\n</code></pre>\n\n<p>That could be repeated until the number isn't divisible any more. Do you think this is a good approach? Do you have a better idea?</p>\n    ","a":"\n<p>I would continue your \"convert number to string\" approach. However you will realize that your proposed algorithm fails if your ID is a <em>prime and greater than 52</em>.</p>\n\n<h3>Theoretical background</h3>\n\n<p>You need a <a href=\"http://en.wikipedia.org/wiki/Bijection\">Bijective Function</a> <em>f</em>. This is necessary so that you can find a inverse function <em>g('abc') = 123</em> for your <em>f(123) = 'abc'</em> function. This means:</p>\n\n<ul>\n<li>There must be no <em>x1, x2 (with x1 ≠ x2)</em> that will make <em>f(x1) = f(x2)</em>,</li>\n<li>and for every <em>y</em> you must be able to find an <em>x</em> so that <em>f(x) = y</em>.</li>\n</ul>\n\n<h3>How to convert the ID to a shortened URL</h3>\n\n<ol>\n<li>Think of an alphabet we want to use. In your case that's <code>[a-zA-Z0-9]</code>. It contains <em>62 letters</em>.</li>\n<li><p>Take an auto-generated, unique numerical key (the auto-incremented <code>id</code> of a MySQL table for example).</p>\n\n<p>For this example I will use 125<sub>10</sub> (125 with a base of 10).</p></li>\n<li><p>Now you have to convert 125<sub>10</sub> to X<sub>62</sub> (base 62).</p>\n\n<p>125<sub>10</sub> = 2×62<sup>1</sup> + 1×62<sup>0</sup> = <code>[2,1]</code></p>\n\n<p>This requires use of integer division and modulo. A pseudo-code example:</p>\n\n<pre><code>digits = []\n\nwhile num &gt; 0\n  remainder = modulo(num, 62)\n  digits.push(remainder)\n  num = divide(num, 62)\n\ndigits = digits.reverse\n</code></pre>\n\n<p>Now map the <em>indices 2 and 1</em> to your alphabet. This is how your mapping (with an array for example) could look like:</p>\n\n<pre><code>0  → a\n1  → b\n...\n25 → z\n...\n52 → 0\n61 → 9\n</code></pre>\n\n<p>With 2 → c and 1 → b you will receive cb<sub>62</sub> as the shortened URL.</p>\n\n<pre><code>http://shor.ty/cb\n</code></pre></li>\n</ol>\n\n<h3>How to resolve a shortened URL to the initial ID</h3>\n\n<p>The reverse is even easier. You just do a reverse lookup in your alphabet.</p>\n\n<ol>\n<li><p>e9a<sub>62</sub> will be resolved to \"4th, 61st, and 0th letter in alphabet\".</p>\n\n<p>e9a<sub>62</sub> = <code>[4,61,0]</code> = 4×62<sup>2</sup> + 61×62<sup>1</sup> + 0×62<sup>0</sup> = 19158<sub>10</sub></p></li>\n<li><p>Now find your database-record with <code>WHERE id = 19158</code> and do the redirect.</p></li>\n</ol>\n\n<h3>Some implementations (provided by commenters)</h3>\n\n<ul>\n<li><a href=\"https://gist.github.com/1073996\">Ruby</a></li>\n<li><a href=\"https://gist.github.com/778542\">Python</a></li>\n<li><a href=\"https://gist.github.com/1158171\">CoffeeScript</a></li>\n<li><a href=\"https://gist.github.com/4626401\">Haskell</a></li>\n<li><a href=\"https://metacpan.org/pod/Short::URL\">Perl</a></li>\n<li><a href=\"https://gist.github.com/9554733\">C#</a></li>\n</ul>\n    "},{"t":"Efficiency of purely functional programming","l":"http://stackoverflow.com/questions/1990464/efficiency-of-purely-functional-programming","q":"\n\n<p>Does anyone know what is the worst possible asymptotic slowdown that can happen when programming purely functionally as opposed to imperatively (i.e. allowing side-effects)?</p>\n\n<p><em>Clarification from comment by itowlson</em>: is there any problem for which the best known non-destructive algorithm is asymptotically worse than the best known destructive algorithm, and if so by how much?</p>\n    ","a":"\n<p>According to <a href=\"http://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf\">Pippenger [1996]</a>, when comparing a Lisp system that is purely functional (and has strict evaluation semantics, not lazy) to one that can mutate data, an algorithm written for the impure Lisp that runs in O(<em>n</em>) can be translated to an algorithm in the pure Lisp that runs in O(<em>n</em> log <em>n</em>) time (based on work by <a href=\"http://www2.mta.ac.il/~amirben/downloadable/jacm.ps.gz\">Ben-Amram and Galil [1992]</a> about simulating random access memory using only pointers). Pippenger also establishes that there are algorithms for which that is the best you can do; there are problems which are O(<em>n</em>) in the impure system which are Ω(<em>n</em> log <em>n</em>) in the pure system.</p>\n\n<p>There are a few caveats to be made about this paper. The most significant is that it does not address lazy functional languages, such as Haskell. <a href=\"ftp://ftp.comlab.ox.ac.uk/pub/Documents/techpapers/Geraint.Jones/FP-1-96.ps.Z\">Bird, Jones and De Moor [1997]</a> demonstrate that the problem constructed by Pippenger can be solved in a lazy functional language in O(<em>n</em>) time, but they do not establish (and as far as I know, no one has) whether or not a lazy functional language can solve all problems in the same asymptotic running time as a language with mutation.</p>\n\n<p>The problem constructed by Pippenger requires Ω(<em>n</em> log <em>n</em>) is specifically constructed to achieve this result, and is not necessarily representative of practical, real-world problems. There are a few restrictions on the problem that are a bit unexpected, but necessary for the proof to work; in particular, the problem requires that results are computed on-line, without being able to access future input, and that the input consists of a sequence of atoms from an unbounded set of possible atoms, rather than a fixed size set. And the paper only establishes (lower bound) results for an impure algorithm of linear running time; for problems that require a greater running time, it is possible that the extra O(log <em>n</em>) factor seen in the linear problem may be able to be \"absorbed\" in the process of extra operations necessary for algorithms with greater running times. These clarifications and open questions are explored briefly by <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.3024\">Ben-Amram [1996]</a>.</p>\n\n<p>In practice, many algorithms can be implemented in a pure functional language at the same efficiency as in a language with mutable data structures. For a good reference on techniques to use for implementing purely functional data structures efficiently, see <a href=\"http://rads.stackoverflow.com/amzn/click/0521663504\">Chris Okasaki's \"Purely Functional Data Structures\" [Okasaki 1998]</a> (which is an expanded version of his thesis <a href=\"http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf\">[Okasaki 1996]</a>).</p>\n\n<p>Anyone who needs to implement algorithms on purely-functional data structures should read Okasaki. You can always get at worst an O(log <em>n</em>) slowdown per operation by simulating mutable memory with a balanced binary tree, but in many cases you can do considerably better than that, and Okasaki describes many useful techniques, from amortized techniques to real-time ones that do the amortized work incrementally. Purely functional data structures can be a bit difficult to work with and analyze, but they provide many benefits like referential transparency that are helpful in compiler optimization, in parallel and distributed computing, and in implementation of features like versioning, undo, and rollback.</p>\n\n<p>Note also that all of this discusses only asymptotic running times. Many techniques for implementing purely functional data structures give you a certain amount of constant factor slowdown, due to extra bookkeeping necessary for them to work, and implementation details of the language in question. The benefits of purely functional data structures may outweigh these constant factor slowdowns, so you will generally need to make trade-offs based on the problem in question.</p>\n\n<h3>References</h3>\n\n<ul>\n<li>Ben-Amram, Amir and Galil, Zvi 1992. <a href=\"http://www2.mta.ac.il/~amirben/downloadable/jacm.ps.gz\">\"On Pointers versus Addresses\"</a> Journal of the ACM, 39(3), pp. 617-648, July 1992</li>\n<li>Ben-Amram, Amir 1996. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.3024\">\"Notes on Pippenger's Comparison of Pure and Impure Lisp\"</a> Unpublished manuscript, DIKU, University of Copenhagen, Denmark</li>\n<li>Bird, Richard, Jones, Geraint, and De Moor, Oege 1997. <a href=\"ftp://ftp.comlab.ox.ac.uk/pub/Documents/techpapers/Geraint.Jones/FP-1-96.ps.Z\">\"More haste, less speed: lazy versus eager evaluation\"</a> Journal of Functional Programming 7, 5 pp. 541–547, September 1997</li>\n<li>Okasaki, Chris 1996. <a href=\"http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf\">\"Purely Functional Data Structures\"</a> PhD Thesis, Carnegie Mellon University</li>\n<li>Okasaki, Chris 1998. <a href=\"http://rads.stackoverflow.com/amzn/click/0521663504\">\"Purely Functional Data Structures\"</a> Cambridge University Press, Cambridge, UK</li>\n<li>Pippenger, Nicholas 1996. <a href=\"http://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf\">\"Pure Versus Impure Lisp\"</a> ACM Symposium on Principles of Programming Languages, pages 104–109, January 1996</li>\n</ul>\n    "},{"t":"What's the Hi/Lo algorithm?","l":"http://stackoverflow.com/questions/282099/whats-the-hi-lo-algorithm","q":"\n\n<p>What's the Hi/Lo algorithm?</p>\n\n<p>I've found this in the <a href=\"http://en.wikipedia.org/wiki/NHibernate\">NHibernate</a> documentation (it's one method to generate unique keys, section 5.1.4.2), but I haven't found any good explanation of how does it work.</p>\n\n<p>I know that Nhibernate handles it, and I don't need to know the inside, but I'm just curious.</p>\n    ","a":"\n<p>The basic idea is that you have two numbers to make up a primary key- a \"high\" number and a \"low\" number. A client can basically increment the \"high\" sequence, knowing that it can then safely generate keys from the entire range of the previous \"high\" value with the variety of \"low\" values.</p>\n\n<p>For instance, supposing you have a \"high\" sequence with a current value of 35, and the \"low\" number is in the range 0-1023. Then the client can increment the sequence to 36 (for other clients to be able to generate keys while it's using 35) and know that keys 35/0, 35/1, 35/2, 35/3... 35/1023 are all available.</p>\n\n<p>It can be very useful (particularly with ORMs) to be able to set the primary keys on the client side, instead of inserting values without primary keys and then fetching them back onto the client. Aside from anything else, it means you can easily make parent/child relationships and have the keys all in place before you do <em>any</em> inserts, which makes batching them simpler.</p>\n    "},{"t":"How to find list of possible words from a letter matrix [Boggle Solver]","l":"http://stackoverflow.com/questions/746082/how-to-find-list-of-possible-words-from-a-letter-matrix-boggle-solver","q":"\n\n<p>Lately I have been playing a game on my iPhone called Scramble. Some of you may know this game as Boggle. Essentially, when the game starts you get a matrix of letters like so:</p>\n\n<pre><code>F X I E\nA M L O\nE W B X\nA S T U\n</code></pre>\n\n<p>The goal of the game is to find as many words as you can that can be formed by chaining letters together. You can start with any letter, and all the letters that surround it are fair game, and then once you move on to the next letter, all the letters that surround that letter are fair game, <strong>except for any previously used letters</strong>. So in the grid above, for example, I could come up with the words <code>LOB</code>, <code>TUX</code>, <code>SEA</code>, <code>FAME</code>, etc. Words must be at least 3 characters, and no more than NxN characters, which would be 16 in this game but can vary in some implementations.  While this game is fun and addictive, I am apparently not very good at it and I wanted to cheat a little bit by making a program that would give me the best possible words (the longer the word the more points you get).</p>\n\n<p><img src=\"http://www.boggled.org/sample.gif\" alt=\"Sample Boggle\"></p>\n\n<p>I am, unfortunately, not very good with algorithms or their efficiencies and so forth. My first attempt uses a dictionary <a href=\"http://www.freebsd.org/cgi/cvsweb.cgi/src/share/dict/web2?rev=1.12;content-type=text%2Fplain\">such as this one</a> (~2.3MB) and does a linear search trying to match combinations with dictionary entries. This takes a <em>very</em> long time to find the possible words, and since you only get 2 minutes per round, it is simply not adequate.</p>\n\n<p>I am interested to see if any Stackoverflowers can come up with more efficient solutions. I am mostly looking for solutions using the Big 3 Ps: Python, PHP, and Perl, although anything with Java or C++ is cool too, since speed is essential.</p>\n\n<p><strong>CURRENT SOLUTIONS</strong>:</p>\n\n<ul>\n<li><a href=\"#746102\">Adam Rosenfield</a>, Python, ~20s  </li>\n<li><a href=\"#746345\">John Fouhy</a>, Python, ~3s  </li>\n<li><a href=\"#746955\">Kent Fredric</a>, Perl, ~1s  </li>\n<li><a href=\"#750012\">Darius Bacon</a>, Python, ~1s  </li>\n<li><a href=\"#750722\">rvarcher</a>, VB.NET <a href=\"http://www.myvrad.com/boggle/default.aspx\">(live link)</a>, ~1s</li>\n<li><a href=\"#757165\">Paolo Bergantino</a>, PHP <a href=\"http://www.rootspot.com/stackoverflow/boggle.php\">(live link)</a>, ~5s (~2s locally)</li>\n</ul>\n\n<p><strong>BOUNTY</strong>:</p>\n\n<p>I am adding a bounty to this question as my way of saying thanks to all the people who pitched in with their programs. Unfortunately I can only give the accepted answer to one of you, so I'll measure who has the fastest boggle solver 7 days from now and award the winner the bounty.</p>\n\n<p>Bounty awarded. Thanks to everyone that participated.</p>\n    ","a":"\n<p>My answer works like the others here, but I'll post it because it looks a bit faster than the other Python solutions, from setting up the dictionary faster. (I checked this against John Fouhy's solution.) After setup, the time to solve is down in the noise.</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>grid = \"fxie amlo ewbx astu\".split()\nnrows, ncols = len(grid), len(grid[0])\n\n# A dictionary word that could be a solution must use only the grid's\n# letters and have length &gt;= 3. (With a case-insensitive match.)\nimport re\nalphabet = ''.join(set(''.join(grid)))\nbogglable = re.compile('[' + alphabet + ']{3,}$', re.I).match\n\nwords = set(word.rstrip('\\n') for word in open('words') if bogglable(word))\nprefixes = set(word[:i] for word in words\n               for i in range(2, len(word)+1))\n\ndef solve():\n    for y, row in enumerate(grid):\n        for x, letter in enumerate(row):\n            for result in extending(letter, ((x, y),)):\n                yield result\n\ndef extending(prefix, path):\n    if prefix in words:\n        yield (prefix, path)\n    for (nx, ny) in neighbors(path[-1]):\n        if (nx, ny) not in path:\n            prefix1 = prefix + grid[ny][nx]\n            if prefix1 in prefixes:\n                for result in extending(prefix1, path + ((nx, ny),)):\n                    yield result\n\ndef neighbors((x, y)):\n    for nx in range(max(0, x-1), min(x+2, ncols)):\n        for ny in range(max(0, y-1), min(y+2, nrows)):\n            yield (nx, ny)\n</code></pre>\n\n<p>Sample usage:</p>\n\n<pre><code># Print a maximal-length word and its path:\nprint max(solve(), key=lambda (word, path): len(word))\n</code></pre>\n\n<p><strong>Edit:</strong> Filter out words less than 3 letters long.</p>\n\n<p><strong>Edit 2:</strong> I was curious why Kent Fredric's Perl solution was faster; it turns out to use regular-expression matching instead of a set of characters. Doing the same in Python about doubles the speed.</p>\n    "},{"t":"What is the most efficient/elegant way to parse a flat table into a tree?","l":"http://stackoverflow.com/questions/192220/what-is-the-most-efficient-elegant-way-to-parse-a-flat-table-into-a-tree","q":"\n\n<p>Assume you have a flat table that stores an ordered tree hierarchy:</p>\n\n<pre><code>Id   Name         ParentId   Order\n 1   'Node 1'            0      10\n 2   'Node 1.1'          1      10\n 3   'Node 2'            0      20\n 4   'Node 1.1.1'        2      10\n 5   'Node 2.1'          3      10\n 6   'Node 1.2'          1      20\n</code></pre>\n\n<p>Here's a diagram, where we have <code>[id] Name</code>.  Root node 0 is fictional.</p>\n\n<pre>                       [0] ROOT\n                          /    \\ \n              [1] Node 1          [3] Node 2\n              /       \\                   \\\n    [2] Node 1.1     [6] Node 1.2      [5] Node 2.1\n          /          \n [4] Node 1.1.1\n</pre>\n\n<p>What minimalistic approach would you use to output that to HTML (or text, for that matter) as a correctly ordered, correctly indented tree? </p>\n\n<p>Assume further you only have basic data structures (arrays and hashmaps), no fancy objects with parent/children references, no ORM, no framework, just your two hands. The table is represented as a result set, which can be accessed randomly. </p>\n\n<p>Pseudo code or plain English is okay, this is purely a conceptional question.</p>\n\n<p>Bonus question: Is there a fundamentally better way to store a tree structure like this in a RDBMS?</p>\n\n<hr>\n\n<p><strong>EDITS AND ADDITIONS</strong></p>\n\n<p>To answer one commenter's (<a href=\"http://stackoverflow.com/users/17826/mark-bessey\">Mark Bessey</a>'s) question: A root node is not necessary, because it is never going to be displayed anyway. ParentId = 0 is the convention to express \"these are top level\". The Order column defines how nodes with the same parent are going to be sorted.</p>\n\n<p>The \"result set\" I spoke of can be pictured as an array of hashmaps (to stay in that terminology). For my example was meant to be already there. Some answers go the extra mile and construct it first, but thats okay.</p>\n\n<p>The tree can be arbitrarily deep. Each node can have N children. I did not exactly have a \"millions of entries\" tree in mind, though.</p>\n\n<p>Don't mistake my choice of node naming ('Node 1.1.1') for something to rely on. The nodes could equally well be called 'Frank' or 'Bob', no naming structure is implied, this was merely to make it readable.</p>\n\n<p><em>I have posted my own solution so you guys can pull it to pieces.</em></p>\n    ","a":"\n<p>There are several ways to store tree-structured data in a relational database.  What you show in your example uses two methods:  </p>\n\n<ul>\n<li><strong>Adjacency List</strong> (the \"parent\" column) and </li>\n<li><strong>Path Enumeration</strong> (the dotted-numbers in your name column).  </li>\n</ul>\n\n<p>Another solution is called <strong>Nested Sets</strong>, and it can be stored in the same table too.  Read \"<a href=\"http://rads.stackoverflow.com/amzn/click/1558609202\">Trees and Hierarchies in SQL for Smarties</a>\" by Joe Celko for a lot more information on these designs.</p>\n\n<p>I usually prefer a design called <strong>Closure Table</strong> (aka \"Adjacency Relation\") for storing tree-structured data.  It requires another table, but then querying trees is pretty easy.</p>\n\n<p>I cover Closure Table in my presentation <a href=\"http://www.slideshare.net/billkarwin/models-for-hierarchical-data\">Models for Hierarchical Data with SQL and PHP</a> and in my book <a href=\"http://pragprog.com/book/bksqla/sql-antipatterns\">SQL Antipatterns: Avoiding the Pitfalls of Database Programming</a>.</p>\n\n<pre><code>CREATE TABLE ClosureTable (\n  ancestor_id   INT NOT NULL REFERENCES FlatTable(id),\n  descendant_id INT NOT NULL REFERENCES FlatTable(id),\n  PRIMARY KEY (ancestor_id, descendant_id)\n);\n</code></pre>\n\n<p>Store all paths in the Closure Table, where there is a direct ancestry from one node to another.  Include a row for each node to reference itself.  For example, using the data set you showed in your question:</p>\n\n<pre><code>INSERT INTO ClosureTable (ancestor_id, descendant_id) VALUES\n  (1,1), (1,2), (1,4), (1,6),\n  (2,2), (2,4),\n  (3,3), (3,5),\n  (4,4),\n  (5,5),\n  (6,6);\n</code></pre>\n\n<p>Now you can get a tree starting at node 1 like this:</p>\n\n<pre><code>SELECT f.* \nFROM FlatTable f \n  JOIN ClosureTable a ON (f.id = a.descendant_id)\nWHERE a.ancestor_id = 1;\n</code></pre>\n\n<p>The output (in MySQL client) looks like the following:</p>\n\n<pre><code>+----+\n| id |\n+----+\n|  1 | \n|  2 | \n|  4 | \n|  6 | \n+----+\n</code></pre>\n\n<p>In other words, nodes 3 and 5 are excluded, because they're part of a separate hierarchy, not descending from node 1.</p>\n\n<hr>\n\n<p>Re: comment from e-satis about immediate children (or immediate parent).  You can add a \"<code>path_length</code>\" column to the <code>ClosureTable</code> to make it easier to query specifically for an immediate child or parent (or any other distance).</p>\n\n<pre><code>INSERT INTO ClosureTable (ancestor_id, descendant_id, path_length) VALUES\n  (1,1,0), (1,2,1), (1,4,2), (1,6,1),\n  (2,2,0), (2,4,1),\n  (3,3,0), (3,5,1),\n  (4,4,0),\n  (5,5,0),\n  (6,6,0);\n</code></pre>\n\n<p>Then you can add a term in your search for querying the immediate children of a given node.  These are descendants whose <code>path_length</code> is 1.</p>\n\n<pre><code>SELECT f.* \nFROM FlatTable f \n  JOIN ClosureTable a ON (f.id = a.descendant_id)\nWHERE a.ancestor_id = 1\n  AND path_length = 1;\n\n+----+\n| id |\n+----+\n|  2 | \n|  6 | \n+----+\n</code></pre>\n\n<hr>\n\n<p>Re comment from @ashraf: \"How about sorting the whole tree [by name]?\"</p>\n\n<p>Here's an example query to return all nodes that are descendants of node 1, join them to the FlatTable that contains other node attributes such as <code>name</code>, and sort by the name.</p>\n\n<pre><code>SELECT f.name\nFROM FlatTable f \nJOIN ClosureTable a ON (f.id = a.descendant_id)\nWHERE a.ancestor_id = 1\nORDER BY f.name;\n</code></pre>\n\n<hr>\n\n<p>Re comment from @Nate:</p>\n\n<pre><code>SELECT f.name, GROUP_CONCAT(b.ancestor_id order by b.path_length desc) AS breadcrumbs\nFROM FlatTable f \nJOIN ClosureTable a ON (f.id = a.descendant_id) \nJOIN ClosureTable b ON (b.descendant_id = a.descendant_id) \nWHERE a.ancestor_id = 1 \nGROUP BY a.descendant_id \nORDER BY f.name\n\n+------------+-------------+\n| name       | breadcrumbs |\n+------------+-------------+\n| Node 1     | 1           |\n| Node 1.1   | 1,2         |\n| Node 1.1.1 | 1,2,4       |\n| Node 1.2   | 1,6         |\n+------------+-------------+\n</code></pre>\n\n<hr>\n\n<p>A user suggested an edit today. SO moderators approved the edit, but I am reversing it.</p>\n\n<p>The edit suggested that the ORDER BY in the last query above should be <code>ORDER BY b.path_length, f.name</code>, presumably to make sure the ordering matches the hierarchy. But this doesn't work, because it would order \"Node 1.1.1\" after \"Node 1.2\".</p>\n\n<p>If you want the ordering to match the hierarchy in a sensible way, that is possible, but not simply by ordering by the path length. For example, see my answer to <a href=\"http://stackoverflow.com/questions/8252323/mysql-closure-table-hierarchical-database-how-to-pull-information-out-in-the-c\">MySQL Closure Table hierarchical database - How to pull information out in the correct order</a>.</p>\n    "},{"t":"How do you detect Credit card type based on number?","l":"http://stackoverflow.com/questions/72768/how-do-you-detect-credit-card-type-based-on-number","q":"\n\n<p>I'm trying to figure out how to detect the type of credit card based purely on its number. Does anyone know of a definitive, reliable way to find this?</p>\n    ","a":"\n<p>The credit/debit card number is referred to as a <strong>PAN</strong>, or <em>Primary Account Number</em>.  The first six digits of the PAN are taken from the <strong>IIN</strong>, or <em>Issuer Identification Number</em>, belonging to the issuing bank (IINs were previously known as BIN — Bank Identification Numbers — so you may see references to that terminology in some documents).  These six digits are subject to an international standard, <a href=\"http://en.wikipedia.org/wiki/ISO/IEC_7812\">ISO/IEC 7812</a>, and can be used to determine the type of card from the number.</p>\n\n<p>Unfortunately the actual ISO/IEC 7812 database is not publicly available, however there are unofficial lists, both commercial and free, including <a href=\"http://en.wikipedia.org/wiki/Bank_card_number\">on Wikipedia</a>.</p>\n\n<p>Anyway, to detect the type from the number, you can use a regular expression like the ones below: <a href=\"http://www.regular-expressions.info/creditcard.html\">Credit for original expressions</a></p>\n\n<p><strong>Visa:</strong> <code>^4[0-9]{6,}$</code> Visa card numbers start with a 4.</p>\n\n<p><strong>MasterCard:</strong> <code>^5[1-5][0-9]{5,}$</code> MasterCard numbers start with the numbers 51 through 55, <strong>but this will only detect MasterCard credit cards</strong>; there are other cards issued using the MasterCard system that do not fall into this IIN range.</p>\n\n<p><strong>American Express:</strong> <code>^3[47][0-9]{5,}$</code> American Express card numbers start with 34 or 37.</p>\n\n<p><strong>Diners Club:</strong> <code>^3(?:0[0-5]|[68][0-9])[0-9]{4,}$</code> Diners Club card numbers begin with 300 through 305, 36 or 38. There are Diners Club cards that begin with 5 and have 16 digits. These are a joint venture between Diners Club and MasterCard, and should be processed like a MasterCard.</p>\n\n<p><strong>Discover:</strong> <code>^6(?:011|5[0-9]{2})[0-9]{3,}$</code> Discover card numbers begin with 6011 or 65.</p>\n\n<p><strong>JCB:</strong> <code>^(?:2131|1800|35[0-9]{3})[0-9]{3,}$</code> JCB cards begin with 2131, 1800 or 35.</p>\n\n<p>Unfortunately there are a number of card types processed with the MasterCard system that do not live in MasterCard’s IIN range (numbers starting 51...55); the most important case is that of Maestro cards, many of which have been issued from other banks’ IIN ranges and so are located all over the number space.  As a result, <strong>it may be best to assume that any card that is not of some other type you accept must be a MasterCard</strong>.</p>\n\n<p><strong>Important</strong>: card numbers do vary in length; for instance, Visa has in the past issued cards with 13 digit PANs and cards with 16 digit PANs.  Visa’s documentation currently indicates that it may issue or may have issued numbers with between 12 and 19 digits.  <strong>Therefore, you should not check the length of the card number, other than to verify that it has at least 7 digits</strong> (for a complete IIN plus one check digit, which should match the value predicted by <a href=\"http://en.wikipedia.org/wiki/Luhn_algorithm\">the Luhn algorithm</a>).</p>\n\n<p>One further hint: <strong>before processing a cardholder PAN, strip any whitespace and punctuation characters from the input</strong>.  Why?  Because it’s typically <em>much</em> easier to enter the digits in groups, similar to how they’re displayed on the front of an actual credit card, i.e.</p>\n\n<pre><code>4444 4444 4444 4444\n</code></pre>\n\n<p>is much easier to enter correctly than</p>\n\n<pre><code>4444444444444444\n</code></pre>\n\n<p>There’s really no benefit in chastising the user because they’ve entered characters you don't expect here.</p>\n\n<p><strong>This also implies making sure that your entry fields have room for <em>at least</em> 24 characters, otherwise users who enter spaces will run out of room.</strong>  I’d recommend that you make the field wide enough to display 32 characters and allow up to 64; that gives plenty of headroom for expansion.</p>\n\n<p>Here's an image that gives a little more insight:</p>\n\n<p><strong>UPDATE (2014):</strong> The checksum method <strong>no longer appears to be a valid way of verifying a card's authenticity</strong> as noted in the comments on this answer.</p>\n\n<p><img src=\"http://i.stack.imgur.com/Cu7PG.jpg\" alt=\"Credit Card Verification\"></p>\n    "},{"t":"Algorithm to return all combinations of k elements from n","l":"http://stackoverflow.com/questions/127704/algorithm-to-return-all-combinations-of-k-elements-from-n","q":"\n\n<p>I want to write a function that takes an array of letters as an argument and a number of those letters to select. </p>\n\n<p>Say you provide an array of 8 letters and want to select 3 letters from that. Then you should get:</p>\n\n<pre><code>8! / ((8 - 3)! * 3!) = 56\n</code></pre>\n\n<p>Arrays (or words) in return consisting of 3 letters each.</p>\n    ","a":"\n<p><a href=\"http://portal.acm.org/citation.cfm?id=1036677&amp;dl=&amp;coll=\">Art of Computer Programming Volume 4: Fascicle 3</a> has a ton of these that might fit your particular situation better than how I describe.</p>\n\n<h2>Gray Codes</h2>\n\n<p>An issue that you will come across is of course memory and pretty quickly, you'll have problems by 20 elements in your set -- <sup>20</sup>C<sub>3</sub> = 1140. And if you want to iterate over the set it's best to use a modified gray code algorithm so you aren't holding all of them in memory. There are many of these, and for different uses, based on the distance of elements. Do we want to maximize the differences between successive applications? minimize? et cetera.</p>\n\n<p>Some of the original papers describing gray codes: </p>\n\n<ol>\n<li><a href=\"http://portal.acm.org/citation.cfm?id=2422.322413\">Some Hamilton Paths and a Minimal Change Algorithm</a></li>\n<li><a href=\"http://portal.acm.org/citation.cfm?id=49203&amp;jmp=indexterms&amp;coll=GUIDE&amp;dl=GUIDE&amp;CFID=81503149&amp;CFTOKEN=96444237\">Adjacent Interchange Combination Generation Algorithm</a></li>\n</ol>\n\n<p>Here are some other papers covering the topic:</p>\n\n<ol>\n<li><a href=\"http://www.cs.uvic.ca/~ruskey/Publications/EHR/HoughRuskey.pdf\">An Efficient Implementation of the Eades, Hickey, Read Adjacent Interchange Combination Generation Algorithm</a> (PDF, with code in Pascal)</li>\n<li><a href=\"http://portal.acm.org/citation.cfm?doid=355826.355830\">Combination Generators</a></li>\n<li><a href=\"http://www4.ncsu.edu/~savage/AVAILABLE_FOR_MAILING/survey.ps\">Survey of Combinatorial Gray Codes</a> (PostScript)</li>\n<li><a href=\"http://www.springerlink.com/content/7lvmm575n85xv5v0/\">An Algorithm for Gray Codes</a></li>\n</ol>\n\n<h2>Chase's Twiddle (algorithm)</h2>\n\n<p>Phillip J Chase, `<a href=\"http://portal.acm.org/citation.cfm?id=362502\">Algorithm 382: Combinations of M out of N Objects</a>' (1970)</p>\n\n<p><a href=\"http://www.netlib.no/netlib/toms/382\">The algorithm in C</a>...</p>\n\n<h2>Index of Combinations in Lexicographical Order (Buckles Algorithm 515)</h2>\n\n<p>You can also reference a combination by it's index (in lexicographical order),  realizing that the index should be some amount of change from right to left based on the index.</p>\n\n<p>So, we have a set {1,2,3,4,5,6}... when we want three elements {1,2,3} we can say that the difference between the elements is one and in order. {1,2,4} has one change, and is lexicographically number 2. So the number of 'changes' in the last place accounts for one change in the lexicographical ordering. The second place, with one change {1,3,4} has one change, but accounts for more change since it's in the second place.</p>\n\n<p>The method I've described is a deconstruction, as it seems, from set to the index, we need to do the reverse --which is much trickier. This is how <a href=\"http://portal.acm.org/citation.cfm?id=355739\">Buckles</a> solves the problem. I wrote some <a href=\"http://stackoverflow.com/questions/561/using-combinations-of-sets-as-test-data#794\">C to compute them</a>, with other minor changes --I used the index of the sets rather then a number range to represent the set, so we are always working from 0...n.\nNote:</p>\n\n<ol>\n<li>Since combinations are unordered, {1,3,2} = {1,2,3} --we order them to be lexicographical.</li>\n<li>This method has an implicit 0 to start the set for the first difference.</li>\n</ol>\n\n<h2>Index of Combinations in Lexicographical Order (McCaffrey)</h2>\n\n<p>There is <a href=\"http://msdn.microsoft.com/en-us/library/aa289166.aspx\">another way</a>:, it's concept is easier to grasp and program but it's without the optimizations of Buckles. Fortunately, it does not produce duplicate combinations:</p>\n\n<p>The set, $x_k...x_1 \\in \\mathbb{N}$ that maximizes $i = C(x_1,k) + C(x_2,k-1) + ... C(x_k,1)$, where $C(n,r) = {n \\choose r}$.</p>\n\n<p>For an example: $27 = C(6,4) + C(5,3) + C(2,2) + C(1,1)$. So, the 27th lexicographical combination of four things is: {1,2,5,6}, those are the indexes of whatever set you want to look at. Example below (ocaml), requires <code>choose</code> function, left to reader:</p>\n\n<pre><code>(* this will find the [x] combination of a [set] list when taking [k] elements *)\nlet combination_maccaffery set k x =\n    (* maximize function -- maximize a that is aCb              *)\n    (* return largest c where c &lt; i and choose(c,i) &lt;= z        *)\n    let rec maximize a b x =\n        if (choose a b ) &lt;= x then a else maximize (a-1) b x\n    in\n    let rec iterate n x i = match i with\n        | 0 -&gt; []\n        | i -&gt;\n            let max = maximize n i x in\n            max :: iterate n (x - (choose max i)) (i-1)\n    in\n    if x &lt; 0 then failwith \"errors\" else\n    let idxs =  iterate (List.length set) x k in\n    List.map (List.nth set) (List.sort (-) idxs)\n</code></pre>\n    "},{"t":"How does the Google “Did you mean?” Algorithm work?","l":"http://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work","q":"\n\n<p>I've been developing an internal website for a portfolio management tool.  There is a lot of text data, company names etc.  I've been really impressed with some search engines ability to very quickly respond to queries with \"Did you mean: xxxx\".</p>\n\n<p>I need to be able to intelligently take a user query and respond with not only raw search results but also with a \"Did you mean?\" response when there is a highly likely alternative answer etc</p>\n\n<p>[I'm developing in <a href=\"http://en.wikipedia.org/wiki/ASP.NET\">ASP.NET</a> (VB - don't hold it against me! )]</p>\n\n<p>UPDATE:\nOK, how can I mimic this without the millions of 'unpaid users'?</p>\n\n<ul>\n<li>Generate typos for each 'known' or 'correct' term and perform lookups?</li>\n<li>Some other more elegant method?</li>\n</ul>\n    ","a":"\n<p>Here's the explanation directly from the source ( almost ) </p>\n\n<h2><strong><a href=\"http://www.youtube.com/watch?v=syKY8CrHkck#t=22m03s\">Search 101!</a></strong></h2>\n\n<p>at min 22:03</p>\n\n<p>Worth watching!</p>\n\n<p>Basically and according  to Douglas Merrill former CTO of Google it is like this:</p>\n\n<p>1) You write a  ( misspelled )  word  in google </p>\n\n<p>2) You don't find what you wanted ( don't click on any results )</p>\n\n<p>3) You realize you misspelled the word  so you rewrite the word in the search box.</p>\n\n<p>4) You find what you want ( you click in the first links ) </p>\n\n<p>This pattern multiplied millions of times, shows what are the most common misspells and what are the most \"common\" corrections. </p>\n\n<p>This way Google can almost instantaneously, offer spell correction in every language.</p>\n\n<p>Also this means if overnight everyone start to spell night as \"nigth\" google would suggest that word instead.  </p>\n\n<p><strong>EDIT</strong></p>\n\n<p>@ThomasRutter: Douglas describe it as \"statistical machine learning\". </p>\n\n<p>They know who correct the query, because they know which query comes from which user ( using cookies ) </p>\n\n<p>If the users perform a query, and only 10% of the users click on a result and 90% goes back and type another query ( with the corrected word ) and this time that 90% clicks on a result, then they know they have found a correction. </p>\n\n<p>They can also know if those are \"related\" queries of two different, because they have information of all the links they show. </p>\n\n<p>Furthermore, they are now including the context into the spell check, so they can even suggest different word depending on the context. </p>\n\n<p>See this <a href=\"http://www.youtube.com/watch?v=v_UyVmITiYQ#t=44m06s\">demo of google wave</a> ( @ 44m 06s )  that shows how the context is taken into account to automatically correct the spelling.</p>\n\n<p><a href=\"http://www.youtube.com/watch?v=Sx3Fpw0XCXk\">Here</a> it is explained how that natural language processing works.</p>\n\n<p>And finally here is an awesome demo of what can be done adding automatic <a href=\"http://www.youtube.com/watch?v=v_UyVmITiYQ#t=1h12m47s\">machine translation</a> ( @ 1h 12m 47s )  to the mix. </p>\n\n<p><sub>\n  I've added anchors of minute and seconds to the videos to skip directly to the content, if they don't work, try reloading the page or scrolling by hand to the mark. \n</sub></p>\n    "},{"t":"How does the algorithm to color the song list in iTunes 11 work?","l":"http://stackoverflow.com/questions/13637892/how-does-the-algorithm-to-color-the-song-list-in-itunes-11-work","q":"\n\n<p>The new iTunes 11 has a very nice view for the song list of an album, picking the colors for the fonts and background in function of album cover. Anyone figured out how the algorithm works?</p>\n\n<p><img src=\"http://i.stack.imgur.com/SNXX2.png\" alt=\"Third Example\"></p>\n    ","a":"\n<p><img src=\"http://i.imgur.com/qodsu.png\" alt=\"Example 1\"></p>\n\n<p>I approximated the iTunes 11 color algorithm in Mathematica given the album cover as input:</p>\n\n<p><img src=\"http://i.imgur.com/lhVJv.png\" alt=\"Output 1\"></p>\n\n<h2>How I did it</h2>\n\n<p>Through trial and error, I came up with an algorithm that works on ~80% of the albums with which I've tested it.</p>\n\n<h3>Color Differences</h3>\n\n<p>The bulk of the algorithm deals with finding the dominant color of an image.  A prerequisite to finding dominant colors, however, is calculating a quantifiable difference between two colors. One way to calculate the difference between two colors is to calculate their Euclidean distance in the RGB color space.  However, human color perception doesn't match up very well with distance in the RGB color space.</p>\n\n<p>Therefore, I wrote a function to convert RGB colors (in the form <code>{1,1,1}</code>) to <a href=\"http://en.wikipedia.org/wiki/YUV\">YUV</a>, a color space which is much better at approximating color perception:</p>\n\n<p><sub>(EDIT: <a href=\"http://stackoverflow.com/users/933113/cormullion\">@cormullion</a> and <a href=\"http://stackoverflow.com/users/261718/drake\">@Drake</a> pointed out that Mathematica's built-in CIELAB and CIELUV color spaces would be just as suitable... looks like I reinvented the wheel a bit here)</sub></p>\n\n<pre class=\"lang-js prettyprint-override\"><code>convertToYUV[rawRGB_] :=\n    Module[{yuv},\n        yuv = {{0.299, 0.587, 0.114}, {-0.14713, -0.28886, 0.436},\n            {0.615, -0.51499, -0.10001}};\n        yuv . rawRGB\n    ]\n</code></pre>\n\n<p>Next, I wrote a function to calculate color distance with the above conversion:</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>ColorDistance[rawRGB1_, rawRGB2_] := \n    EuclideanDistance[convertToYUV @ rawRGB1, convertToYUV @ rawRGB2]\n</code></pre>\n\n<h3>Dominant Colors</h3>\n\n<p>I quickly discovered that the built-in Mathematica function <code>DominantColors</code> doesn't allow enough fine-grained control to approximate the algorithm that iTunes uses.  I wrote my own function instead...</p>\n\n<p>A simple method to calculate the dominant color in a group of pixels is to collect all pixels into buckets of similar colors and then find the largest bucket.</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>DominantColorSimple[pixelArray_] :=\n    Module[{buckets},\n        buckets = Gather[pixelArray, ColorDistance[#1,#2] &lt; .1 &amp;];\n        buckets = Sort[buckets, Length[#1] &gt; Length[#2] &amp;];\n        RGBColor @@ Mean @ First @ buckets\n    ]\n</code></pre>\n\n<p><sub>Note that <code>.1</code> is the tolerance for how different colors must be to be considered separate.  Also note that although the input is an array of pixels in raw triplet form (<code>{{1,1,1},{0,0,0}}</code>), I return a Mathematica <code>RGBColor</code> element to better approximate the built-in <code>DominantColors</code> function.</sub></p>\n\n<p>My actual function <code>DominantColorsNew</code> adds the option of returning up to <code>n</code> dominant colors after filtering out a given other color.  It also exposes tolerances for each color comparison:</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>DominantColorsNew[pixelArray_, threshold_: .1, n_: 1, \n    numThreshold_: .2, filterColor_: 0, filterThreshold_: .5] :=\n    Module[\n        {buckets, color, previous, output},\n        buckets = Gather[pixelArray, ColorDistance[#1, #2] &lt; threshold &amp;];\n        If[filterColor =!= 0, \n        buckets = \n            Select[buckets, \n                ColorDistance[ Mean[#1], filterColor] &gt; filterThreshold &amp;]];\n        buckets = Sort[buckets, Length[#1] &gt; Length[#2] &amp;];\n        If[Length @ buckets == 0, Return[{}]];\n        color = Mean @ First @ buckets;\n        buckets = Drop[buckets, 1];\n        output = List[RGBColor @@ color];\n        previous = color;\n        Do[\n            If[Length @ buckets == 0, Return[output]];\n            While[\n                ColorDistance[(color = Mean @ First @ buckets), previous] &lt; \n                    numThreshold, \n                If[Length @ buckets != 0, buckets = Drop[buckets, 1], \n                    Return[output]]\n            ];\n            output = Append[output, RGBColor @@ color];\n            previous = color,\n            {i, n - 1}\n        ];\n        output\n    ]\n</code></pre>\n\n<h3>The Rest of the Algorithm</h3>\n\n<p>First I resized the album cover (<code>36px</code>, <code>36px</code>) &amp; reduced detail with a bilateral filter</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>image = Import[\"http://i.imgur.com/z2t8y.jpg\"]\nthumb = ImageResize[ image, 36, Resampling -&gt; \"Nearest\"];\nthumb = BilateralFilter[thumb, 1, .2, MaxIterations -&gt; 2];\n</code></pre>\n\n<p>iTunes picks the background color by finding the dominant color along the edges of the album.  However, it ignores narrow album cover borders by cropping the image.</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>thumb = ImageCrop[thumb, 34];\n</code></pre>\n\n<p>Next, I found the dominant color (with the new function above) along the outermost edge of the image with a default tolerance of <code>.1</code>.</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>border = Flatten[\n    Join[ImageData[thumb][[1 ;; 34 ;; 33]] , \n        Transpose @ ImageData[thumb][[All, 1 ;; 34 ;; 33]]], 1];\nbackground = DominantColorsNew[border][[1]];\n</code></pre>\n\n<p>Lastly, I returned 2 dominant colors in the image as a whole, telling the function to filter out the background color as well.</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>highlights = DominantColorsNew[Flatten[ImageData[thumb], 1], .1, 2, .2, \n    List @@ background, .5];\ntitle = highlights[[1]];\nsongs = highlights[[2]];\n</code></pre>\n\n<p><sub>The tolerance values above are as follows: <code>.1</code> is the minimum difference between \"separate\" colors; <code>.2</code> is the minimum difference between numerous dominant colors (A lower value might return black and dark gray, while a higher value ensures more diversity in the dominant colors); <code>.5</code> is the minimum difference between dominant colors and the background (A higher value will yield higher-contrast color combinations)</sub></p>\n\n<p><em>Voila!</em></p>\n\n<pre class=\"lang-js prettyprint-override\"><code>Graphics[{background, Disk[]}]\nGraphics[{title, Disk[]}]\nGraphics[{songs, Disk[]}]\n</code></pre>\n\n<p><img src=\"http://i.imgur.com/VIufx.png\" alt=\"Final Output\"></p>\n\n<h3>Notes</h3>\n\n<p>The algorithm can be applied very generally.  I tweaked the above settings and tolerance values to the point where they work to produce generally correct colors for ~80% of the  album covers I tested.  A few edge cases occur when <code>DominantColorsNew</code> doesn't find two colors to return for the highlights (i.e. when the album cover is monochrome).  My algorithm doesn't address these cases, but it would be trivial to duplicate iTunes' functionality: when the album yields less than two highlights, the title becomes white or black depending on the best contrast with the background.  Then the songs become the one highlight color if there is one, or the title color faded into the background a bit.</p>\n\n<h2>More Examples</h2>\n\n<p><img src=\"http://i.imgur.com/YPxA3.png\" alt=\"More Examples\"></p>\n    "},{"t":"What Is Tail Call Optimization?","l":"http://stackoverflow.com/questions/310974/what-is-tail-call-optimization","q":"\n\n<p>Very simply, what is tail-call optimization?  More specifically, Can anyone show some small code snippets where it could be applied, and where not, with an explanation of why?</p>\n    ","a":"\n<p>Tail-call optimization is where you are able to avoid allocating a new stack frame for a function because the calling function will simply return the value that it gets from the called function. The most common use is tail-recursion, where a recursive function written to take advantage of tail-call optimization can use constant stack space.</p>\n\n<p>Scheme is one of the few programming languages that guarantee in the spec that any implementation must provide this optimization <em>(JavaScript will also, once ES6 is finalized)</em>, so here are two examples of the factorial function in Scheme:</p>\n\n<pre class=\"lang-lisp prettyprint-override\"><code>(define (fact x)\n  (if (= x 0) 1\n      (* x (fact (- x 1)))))\n\n(define (fact x)\n  (define (fact-tail x accum)\n    (if (= x 0) accum\n        (fact-tail (- x 1) (* x accum))))\n  (fact-tail x 1))\n</code></pre>\n\n<p>The first function is not tail recursive because when the recursive call is made, the function needs to keep track of the multiplication it needs to do with the result after the call returns. As such, the stack looks as follows:</p>\n\n<pre class=\"lang-scheme prettyprint-override\"><code>(fact 3)\n(* 3 (fact 2))\n(* 3 (* 2 (fact 1)))\n(* 3 (* 2 (* 1 (fact 0))))\n(* 3 (* 2 (* 1 1)))\n(* 3 (* 2 1))\n(* 3 2)\n6\n</code></pre>\n\n<p>In contrast, the stack trace for the tail recursive factorial looks as follows:</p>\n\n<pre class=\"lang-scheme prettyprint-override\"><code>(fact 3)\n(fact-tail 3 1)\n(fact-tail 2 3)\n(fact-tail 1 6)\n(fact-tail 0 6)\n6\n</code></pre>\n\n<p>As you can see, we only need to keep track of the same amount of data for every call to fact-tail because we are simply returning the value we get right through to the top. This means that even if I were to call (fact 1000000), I need only the same amount of space as (fact 3). This is not the case with the non-tail-recursive fact, and as such large values may cause a stack overflow.</p>\n    "},{"t":"Most effective way for float and double comparison","l":"http://stackoverflow.com/questions/17333/most-effective-way-for-float-and-double-comparison","q":"\n\n<p>What would be the most efficient way to compare two <code>double</code> or two <code>float</code> values?</p>\n\n<p>Simply doing this is not correct:</p>\n\n<pre><code>bool CompareDoubles1 (double A, double B)\n{\n   return A == B;\n}\n</code></pre>\n\n<p>But something like:</p>\n\n<pre><code>bool CompareDoubles2 (double A, double B) \n{\n   diff = A - B;\n   return (diff &lt; EPSILON) &amp;&amp; (-diff &lt; EPSILON);\n}\n</code></pre>\n\n<p>Seems to waste processing.</p>\n\n<p>Does anyone know a smarter float comparer?</p>\n    ","a":"\n<p>Be extremely careful using any of the suggestions above. It all depends on context. </p>\n\n<p>I have spent a long time tracing a bugs in a system that presumed <em>a==b</em> if <em>|a-b|&lt;epsion</em>. The underlying problems were:</p>\n\n<ol>\n<li><p>The implicit presumption in an algorithm that if <em>a==b</em> and <em>b==c</em> then <em>a==c</em>. </p></li>\n<li><p>Using the same epsilon for lines measured in inches and lines measured in mils (.001 inch). That is <em>a==b</em> but <em>1000a!=1000b</em>. (This is why AlmostEqual2sComplement asks for the epsilon or max ULPS).</p></li>\n<li><p>The use of the same epsilon for both the cosine of angles and the length of lines!</p></li>\n<li><p>Using such a compare function to sort items in a collection. (In this case using the builtin C++ operator == for doubles produced correct results.)</p></li>\n</ol>\n\n<p>Like I said: it all depends on context and the expected size of <em>a</em> and <em>b</em>.</p>\n\n<p>BTW, std::numeric_limits&lt;double&gt;::epsilon() is the \"machine epsilon\". It is the difference between 1.0 and the next value representable by a double. I guess that it could be used in the compare function but only if the expected values are less than 1. </p>\n\n<p>Also, if you basically have int arithmetic in doubles (here we use doubles to hold int values in certain cases) your arithmetic will be correct. For example 4.0/2.0 will be the same as 1.0+1.0. This is as long as you do not do things that result in fractions (4.0/3.0) or do not go outside of the size of an int.</p>\n    "},{"t":"Write a program to find 100 largest numbers out of an array of 1 billion numbers","l":"http://stackoverflow.com/questions/19227698/write-a-program-to-find-100-largest-numbers-out-of-an-array-of-1-billion-numbers","q":"\n\n<p>I recently attended an interview where I was asked \"write a program to find 100 largest numbers out of an array of 1 billion numbers.\"</p>\n\n<p>I was only able to give a brute force solution which was to sort the array in O(nlogn) time complexity and take the last 100 numbers. </p>\n\n<pre><code>Arrays.sort(array);\n</code></pre>\n\n<p>The interviewer was looking for a better time complexity, I tried a couple of other solutions but failed to answer him. Is there a better time complexity solution?</p>\n    ","a":"\n<p>You can keep a priority queue of the 100 biggest numbers, iterate through the billion numbers, whenever you encounter a number greater than the smallest number in the queue (the head of the queue), remove the head of the queue and add the new number to the queue.</p>\n\n<p><strong>EDIT:</strong>\nas Dev noted, with a priority queue implemented with a heap, the complexity of insertion to queue is <code>O(logN)</code></p>\n\n<p>In the worst case you get <code>billion*log<sub>2</sub>(100)</code> which is better than <code>billion*log<sub>2</sub>(billion)</code></p>\n\n<p>In general, if you need the largest K numbers from a set of N numbers, the complexity is <code>O(NlogK)</code> rather than <code>O(NlogN)</code>, this can be very significant when K is very small comparing to N.</p>\n\n<p><strong>EDIT2:</strong></p>\n\n<p>The expected time of this algorithm is pretty interesting, since in each iteration an insertion may or may not occur. The probability of the i'th number to be inserted to the queue is the probability of a random variable being larger than at least <code>i-K</code> random variables from the same distribution (the first k numbers are automatically added to the queue). We can use order statistics (see <a href=\"http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29#Order_statistics\">link</a>) to calculate this probability. For example, lets assume the numbers were randomly selected uniformly from <code>{0, 1}</code>, the expected value of (i-K)th number (out of i numbers) is <code>(i-k)/i</code>, and chance of a random variable being larger than this value is <code>1-[(i-k)/i] = k/i</code>. </p>\n\n<p>Thus, the expected number of insertions is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/6Vgjk.jpg\" alt=\"enter image description here\"></p>\n\n<p>And the expected running time can be expressed as:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Mg6J1.jpg\" alt=\"enter image description here\"></p>\n\n<p>(<code>k</code> time to generate the queue with the first <code>k</code> elements, then <code>n-k</code> comparisons, and the expected number of insertions as described above, each takes an average <code>log(k)/2</code> time)</p>\n\n<p>Note that when <code>N</code> is very large comparing to <code>K</code>, this expression is a lot closer to <code>n</code> rather than <code>NlogK</code>. This is somewhat intuitive, as in the case of the question, even after 10000 iterations (which is very small comparing to a billion), the chance of a number to be inserted to the queue is very small.</p>\n    "},{"t":"Big-O for Eight Year Olds?","l":"http://stackoverflow.com/questions/107165/big-o-for-eight-year-olds","q":"\n\n<p>I'm asking more about what this means to my code.  I understand the concepts mathematically, I just have a hard time wrapping my head around what they mean conceptually.  For example, if one were to perform an O(1) operation on a data structure, I understand that the amount of operations it has to perform won't grow because there are more items.  And an O(n) operation would mean that you would perform a set of operations on each element.  Could somebody fill in the blanks here?</p>\n\n<ul>\n<li>Like what exactly would an O(n^2) operation do?</li>\n<li>And what the heck does it mean if an operation is O(n log(n))?</li>\n<li>And does somebody have to smoke crack to write an O(x!)?</li>\n</ul>\n    ","a":"\n<p>One way of thinking about it is this:</p>\n\n<p>O(N^2) means for every element, you're doing something with every other element, such as comparing them.  Bubble sort is an example of this.</p>\n\n<p>O(N log N) means for every element, you're doing something that only needs to look at log N of the elements.  This is usually because you know something about the elements that lets you make an efficient choice.  Most efficient sorts are an example of this, such as merge sort.</p>\n\n<p>O(N!) means do something for all possible permutations of the N elements.  Traveling salesman is an example of this, where there are N! ways to visit the nodes, and the brute force solution is to look at the total cost of every possible permutation to find the optimal one.</p>\n    "},{"t":"Fastest sort of fixed length 6 int array","l":"http://stackoverflow.com/questions/2786899/fastest-sort-of-fixed-length-6-int-array","q":"\n\n<p>Answering to another Stack Overflow question (<a href=\"http://stackoverflow.com/questions/2775774/what-is-the-best-algorithm-for-this-array-comparison-problem/2777202#2777202\">this one</a>) I stumbled upon an interesting sub-problem. What is the fastest way to sort an array of 6 ints?</p>\n\n<p>As the question is very low level:</p>\n\n<ul>\n<li>we can't assume libraries are available (and the call itself has its cost), only plain C</li>\n<li>to avoid emptying instruction pipeline (that has a <em>very</em> high cost) we should probably minimize branches, jumps, and every other kind of control flow breaking (like those hidden behind sequence points in &amp;&amp; or ||).</li>\n<li>room is constrained and minimizing registers and memory use is an issue, ideally in place sort is probably best.</li>\n</ul>\n\n<p>Really this question is a kind of Golf where the goal is not to minimize source length but execution time. I call it 'Zening` code as used in the title of the book <a href=\"http://rads.stackoverflow.com/amzn/click/1883577039\">Zen of Code optimization</a> by <a href=\"http://en.wikipedia.org/wiki/Michael_Abrash\">Michael Abrash</a> and its <a href=\"http://www.codinghorror.com/blog/2008/02/there-aint-no-such-thing-as-the-fastest-code.html\">sequels</a>.</p>\n\n<p>As for why it is interesting, there is several layers:</p>\n\n<ul>\n<li>the example is simple and easy to understand and measure, not much C skill involved</li>\n<li>it shows effects of choice of a good algorithm for the problem, but also effects of the compiler and underlying hardware.</li>\n</ul>\n\n<p>Here is my reference (naive, not optimized) implementation and my test set.</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\nstatic __inline__ int sort6(int * d){\n\n    char j, i, imin;\n    int tmp;\n    for (j = 0 ; j &lt; 5 ; j++){\n        imin = j;\n        for (i = j + 1; i &lt; 6 ; i++){\n            if (d[i] &lt; d[imin]){\n                imin = i;\n            }\n        }\n        tmp = d[j];\n        d[j] = d[imin];\n        d[imin] = tmp;\n    }\n}\n\nstatic __inline__ unsigned long long rdtsc(void)\n{\n  unsigned long long int x;\n     __asm__ volatile (\".byte 0x0f, 0x31\" : \"=A\" (x));\n     return x;\n}\n\nint main(int argc, char ** argv){\n    int i;\n    int d[6][5] = {\n        {1, 2, 3, 4, 5, 6},\n        {6, 5, 4, 3, 2, 1},\n        {100, 2, 300, 4, 500, 6},\n        {100, 2, 3, 4, 500, 6},\n        {1, 200, 3, 4, 5, 600},\n        {1, 1, 2, 1, 2, 1}\n    };\n\n&nbsp; &nbsp; unsigned long long cycles = rdtsc();\n&nbsp; &nbsp; for (i = 0; i &lt; 6 ; i++){\n    &nbsp; &nbsp; sort6(d[i]);\n&nbsp; &nbsp;     /*\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;* printf(\"d%d : %d %d %d %d %d %d\\n\", i,\n    &nbsp; &nbsp; &nbsp;* &nbsp;d[i][0], d[i][6], d[i][7],\n  &nbsp; &nbsp; &nbsp;  * &nbsp;d[i][8], d[i][9], d[i][10]);\n&nbsp; &nbsp; &nbsp; &nbsp; */\n&nbsp; &nbsp; }\n&nbsp; &nbsp; cycles = rdtsc() - cycles;\n&nbsp; &nbsp; printf(\"Time is %d\\n\", (unsigned)cycles);\n}\n</code></pre>\n\n<h2><strong>Raw results</strong></h2>\n\n<p>As number of variants is becoming large, I gathered them all in a test suite that can be found <a href=\"http://pastebin.com/azzuk072\">here</a>. The actual tests used are a bit less naive than those showed above, thanks to Kevin Stock. You can compile and execute it in your own environment. I'm quite interested by behavior on different target architecture/compilers. (OK guys, put it in answers, I will +1 every contributor of a new resultset). </p>\n\n<p>I gave the answer to Daniel Stutzbach (for golfing) one year ago as he was at the source of the fastest solution at that time (sorting networks).</p>\n\n<p><strong>Linux 64 bits, gcc 4.6.1 64 bits, Intel Core 2 Duo E8400, -O2</strong></p>\n\n<ul>\n<li>Direct call to qsort library function      : 689.38</li>\n<li>Naive implementation (insertion sort)      : 285.70</li>\n<li>Insertion Sort (Daniel Stutzbach)          : 142.12</li>\n<li>Insertion Sort Unrolled                    : 125.47</li>\n<li>Rank Order                                 : 102.26</li>\n<li>Rank Order with registers                  : 58.03</li>\n<li>Sorting Networks (Daniel Stutzbach)        : 111.68</li>\n<li>Sorting Networks (Paul R)                  : 66.36</li>\n<li>Sorting Networks 12 with Fast Swap         : 58.86</li>\n<li>Sorting Networks 12 reordered Swap         : 53.74</li>\n<li>Sorting Networks 12 reordered Simple Swap  : 31.54</li>\n<li>Reordered Sorting Network w/ fast swap     : 31.54</li>\n<li>Reordered Sorting Network w/ fast swap V2  : 33.63</li>\n<li>Inlined Bubble Sort (Paolo Bonzini)        : 48.85</li>\n<li>Unrolled Insertion Sort (Paolo Bonzini)    : 75.30</li>\n</ul>\n\n<p><strong>Linux 64 bits, gcc 4.6.1 64 bits, Intel Core 2 Duo E8400, -O1</strong></p>\n\n<ul>\n<li>Direct call to qsort library function      : 705.93</li>\n<li>Naive implementation (insertion sort)      : 135.60</li>\n<li>Insertion Sort (Daniel Stutzbach)          : 142.11</li>\n<li>Insertion Sort Unrolled                    : 126.75</li>\n<li>Rank Order                                 : 46.42</li>\n<li>Rank Order with registers                  : 43.58</li>\n<li>Sorting Networks (Daniel Stutzbach)        : 115.57</li>\n<li>Sorting Networks (Paul R)                  : 64.44</li>\n<li>Sorting Networks 12 with Fast Swap         : 61.98</li>\n<li>Sorting Networks 12 reordered Swap         : 54.67</li>\n<li>Sorting Networks 12 reordered Simple Swap  : 31.54</li>\n<li>Reordered Sorting Network w/ fast swap     : 31.24</li>\n<li>Reordered Sorting Network w/ fast swap V2  : 33.07</li>\n<li>Inlined Bubble Sort (Paolo Bonzini)        : 45.79</li>\n<li>Unrolled Insertion Sort (Paolo Bonzini)    : 80.15</li>\n</ul>\n\n<p>I included both -O1 and -O2 results because surprisingly for several programs O2 is <strong>less</strong> efficient than O1. I wonder what specific optimization has this effect ?</p>\n\n<h2><strong>Comments on proposed solutions</strong></h2>\n\n<p><strong>Insertion Sort (Daniel Stutzbach)</strong></p>\n\n<p>As expected minimizing branches is indeed a good idea.</p>\n\n<p><strong>Sorting Networks (Daniel Stutzbach)</strong></p>\n\n<p>Better than insertion sort. I wondered if the main effect was not get from avoiding the external loop. I gave it a try by unrolled insertion sort to check and indeed we get roughly the same figures (code is <a href=\"http://www.copypastecode.com/28231/\">here</a>).</p>\n\n<p><strong>Sorting Networks (Paul R)</strong></p>\n\n<p>The best so far. The actual code I used to test is <a href=\"http://www.copypastecode.com/28227/\">here</a>. Don't know yet why it is nearly two times as fast as the other sorting network implementation. Parameter passing ? Fast max ?</p>\n\n<p><strong>Sorting Networks 12 SWAP with Fast Swap</strong></p>\n\n<p>As suggested by Daniel Stutzbach, I combined his 12 swap sorting network with branchless fast swap (code is <a href=\"http://www.copypastecode.com/28235/\">here</a>). It is indeed faster, the best so far with a small margin (roughly 5%) as could be expected using 1 less swap. </p>\n\n<p>It is also interesting to notice that the branchless swap seems to be much (4 times) less efficient than the simple one using if on PPC architecture.</p>\n\n<p><strong>Calling Library qsort</strong></p>\n\n<p>To give another reference point I also tried as suggested to just call library qsort (code is <a href=\"http://www.copypastecode.com/28239/\">here</a>). As expected it is much slower : 10 to 30 times slower...  as it became obvious with the new test suite, the main problem seems to be the initial load of the library after the first call, and it compares not so poorly with other version. It is just between 3 and 20 times slower on my Linux. On some architecture used for tests by others it seems even to be faster (I'm really surprised by that one, as library qsort use a more complex API).</p>\n\n<p><strong>Rank order</strong></p>\n\n<p>Rex Kerr proposed another completely different method : for each item of the array compute directly its final position. This is efficient because computing rank order do not need branch. The drawback of this method is that it takes three times the amount of memory of the array (one copy of array and variables to store rank orders). The performance results are very surprising (and interesting). On my reference architecture with 32 bits OS and Intel Core2 Quad E8300, cycle count was slightly below 1000 (like sorting networks with branching swap). But when compiled and executed on my 64 bits box (Intel Core2 Duo) it performed much better : it became the fastest so far. I finally found out the true reason. My 32bits box use gcc 4.4.1 and my 64bits box gcc 4.4.3 and the last one seems much better at optimising this particular code (there was very little difference for other proposals).</p>\n\n<p><strong>Sorting Networks 12 with reordered Swap</strong></p>\n\n<p>The amazing efficiency of the Rex Kerr proposal with gcc 4.4.3 made me wonder : how could a program with 3 times as much memory usage be faster than branchless sorting networks? My hypothesis was that it had less dependencies of the kind read after write, allowing for better use of the superscalar instruction scheduler of the x86. That gave me an idea: reorder swaps to minimize read after write dependencies. More simply put: when you do <code>SWAP(1, 2); SWAP(0, 2);</code> you have to wait for the first swap to be finished before performing the second one because both access to a common memory cell. When you do <code>SWAP(1, 2); SWAP(4, 5);</code>the processor can execute both in parallel. I tried it and it works as expected, the sorting networks is running about 10% faster. </p>\n\n<p><strong>Sorting Networks 12 with Simple Swap</strong></p>\n\n<p>One year after the original post Steinar H. Gunderson suggested, that we should not try to outsmart the compiler and keep the swap code simple. It's indeed a good idea as the resulting code is about 40% faster! He also proposed a swap optimized by hand using x86 inline assembly code that can still spare some more cycles. The most surprising (it says volumes on programmer's psychology) is that one year ago none of used tried that version of swap. Code I used to test is <a href=\"http://www.copypastecode.com/76995/\">here</a>. Others suggested other ways to write a C fast swap, but it yields the same performances as the simple one with a decent compiler.</p>\n\n<p>The \"best\" code is now as follow:</p>\n\n<pre><code>static inline void sort6_sorting_network_simple_swap(int * d){\n#define min(x, y) (x&lt;y?x:y)\n#define max(x, y) (x&lt;y?y:x) \n#define SWAP(x,y) { const int a = min(d[x], d[y]); const int b = max(d[x], d[y]); d[x] = a; d[y] = b;}\n    SWAP(1, 2);\n    SWAP(4, 5);\n    SWAP(0, 2);\n    SWAP(3, 5);\n    SWAP(0, 1);\n    SWAP(3, 4);\n    SWAP(1, 4);\n    SWAP(0, 3);\n    SWAP(2, 5);\n    SWAP(1, 3);\n    SWAP(2, 4);\n    SWAP(2, 3);\n#undef SWAP\n#undef min\n#undef max\n}\n</code></pre>\n\n<p>If we believe our test set (and, yes it is quite poor, it's mere benefit is being short, simple and easy to understand what we are measuring), the average number of cycles of the resulting code for one sort is below 40 cycles (6 tests are executed). That put each swap at an average of 4 cycles. I call that amazingly fast. Any other improvements possible ?</p>\n    ","a":"\n<p>For any optimization, it's always best to test, test, test.  I would try at least sorting networks and insertion sort.  If I were betting, I'd put my money on insertion sort based on past experience.</p>\n\n<p>Do you know anything about the input data?  Some algorithms will perform better with certain kinds of data.  For example, insertion sort performs better on sorted or almost-sorted dat, so it will be the better choice if there's an above-average chance of almost-sorted data.</p>\n\n<p>The algorithm you posted is similar to an insertion sort, but it looks like you've minimized the number of swaps at the cost of more comparisons.  Comparisons are far more expensive than swaps, though, because branches can cause the instruction pipeline to stall.</p>\n\n<p>Here's an insertion sort implementation:</p>\n\n<pre><code>static __inline__ int sort6(int *d){\n        int i, j;\n        for (i = 1; i &lt; 6; i++) {\n                int tmp = d[i];\n                for (j = i; j &gt;= 1 &amp;&amp; tmp &lt; d[j-1]; j--)\n                        d[j] = d[j-1];\n                d[j] = tmp;\n        }\n}\n</code></pre>\n\n<p>Here's how I'd build a sorting network.  First, use <a href=\"http://pages.ripco.net/~jgamble/nw.html\">this site</a> to generate a minimal set of SWAP macros for a network of the appropriate length.  Wrapping that up in a function gives me:</p>\n\n<pre><code>static __inline__ int sort6(int * d){\n#define SWAP(x,y) if (d[y] &lt; d[x]) { int tmp = d[x]; d[x] = d[y]; d[y] = tmp; }\n    SWAP(1, 2);\n    SWAP(0, 2);\n    SWAP(0, 1);\n    SWAP(4, 5);\n    SWAP(3, 5);\n    SWAP(3, 4);\n    SWAP(0, 3);\n    SWAP(1, 4);\n    SWAP(2, 5);\n    SWAP(2, 4);\n    SWAP(1, 3);\n    SWAP(2, 3);\n#undef SWAP\n}\n</code></pre>\n    "},{"t":"Getting the closest string match","l":"http://stackoverflow.com/questions/5859561/getting-the-closest-string-match","q":"\n\n<p>I need a way to compare multiple strings to a test string and return the string that closely resembles it:</p>\n\n<pre><code>TEST STRING: THE BROWN FOX JUMPED OVER THE RED COW\n\nCHOICE A   : THE RED COW JUMPED OVER THE GREEN CHICKEN\nCHOICE B   : THE RED COW JUMPED OVER THE RED COW\nCHOICE C   : THE RED FOX JUMPED OVER THE BROWN COW\n</code></pre>\n\n<p>(If I did this correctly) The closest string to the \"TEST STRING\" should be \"CHOICE C\".  What is the easiest way to do this?</p>\n\n<p>I plan on implementing this into multiple languages including VB.net, Lua, and JavaScript.  At this point, pseudo code is acceptable.  If you can provide an example for a specific language, this is appreciated too!</p>\n    ","a":"\n<p>I was presented with this problem about a year ago when it came to looking up user entered information about a oil rig in a database of miscellaneous information. The goal was to do some sort of fuzzy string search that could identify the database entry with the most common elements.</p>\n\n<p>Part of the research involved implementing the <a href=\"http://en.wikipedia.org/wiki/Levenshtein_distance\">Levenshtein distance</a> algorithm, which determines how many changes must be made to a string or phrase to turn it into another string or phrase.</p>\n\n<p>The implementation I came up with was relatively simple, and involved a weighted comparison of the length of the two phrases, the number of changes between each phrase, and whether each word could be found in the target entry.</p>\n\n<p>The article is on a private site so I'll do my best to append the relevant contents here:</p>\n\n<hr>\n\n<p>Fuzzy String Matching is the process of performing a human-like estimation of the similarity of two words or phrases. In many cases, it involves identifying words or phrases which are most similar to each other. This article describes an in-house solution to the fuzzy string matching problem and its usefulness in solving a variety of problems which can allow us to automate tasks which previously required tedious user involvement. </p>\n\n<p><strong>Introduction</strong></p>\n\n<p>The need to do fuzzy string matching originally came about while developing the Gulf of Mexico Validator tool. What existed was a database of known gulf of Mexico oil rigs and platforms, and people buying insurance would give us some badly typed out information about their assets and we had to match it to the database of known platforms. When there was very little information given, the best we could do is rely on an underwriter to \"recognize\" the one they were referring to and call up the proper information. This is where this automated solution comes in handy.</p>\n\n<p>I spent a day researching methods of fuzzy string matching, and eventually stumbled upon the very useful Levenshtein distance algorithm on Wikipedia.</p>\n\n<p><strong>Implementation</strong></p>\n\n<p>After reading about the theory behind it, I implemented and found ways to optimize it. This is how my code looks like in VBA: </p>\n\n<pre class=\"lang-vb prettyprint-override\"><code>'Calculate the Levenshtein Distance between two strings (the number of insertions,\n'deletions, and substitutions needed to transform the first string into the second)\nPublic Function LevenshteinDistance(ByRef S1 As String, ByVal S2 As String) As Long\n    Dim L1 As Long, L2 As Long, D() As Long 'Length of input strings and distance matrix\n    Dim i As Long, j As Long, cost As Long 'loop counters and cost of substitution for current letter\n    Dim cI As Long, cD As Long, cS As Long 'cost of next Insertion, Deletion and Substitution\n    L1 = Len(S1): L2 = Len(S2)\n    ReDim D(0 To L1, 0 To L2)\n    For i = 0 To L1: D(i, 0) = i: Next i\n    For j = 0 To L2: D(0, j) = j: Next j\n\n    For j = 1 To L2\n        For i = 1 To L1\n            cost = Abs(StrComp(Mid$(S1, i, 1), Mid$(S2, j, 1), vbTextCompare))\n            cI = D(i - 1, j) + 1\n            cD = D(i, j - 1) + 1\n            cS = D(i - 1, j - 1) + cost\n            If cI &lt;= cD Then 'Insertion or Substitution\n                If cI &lt;= cS Then D(i, j) = cI Else D(i, j) = cS\n            Else 'Deletion or Substitution\n                If cD &lt;= cS Then D(i, j) = cD Else D(i, j) = cS\n            End If\n        Next i\n    Next j\n    LevenshteinDistance = D(L1, L2)\nEnd Function\n</code></pre>\n\n<p>Simple, speedy, and a very useful metric. Using this, I created two separate metrics for evaluating the similarity of two strings. One I call \"valuePhrase\" and one I call \"valueWords\". valuePhrase is just the Levenshtein distance between the two phrases, and valueWords splits the string into individual words, based on delimiters such as spaces, dashes, and anything else you'd like, and compares each word to each other word, summing up the shortest Levenshtein distance connecting any two words. Essentially, it measures whether the information in one 'phrase' is really contained in another, just as a word-wise permutation. I spent a few days as a side project coming up with the most efficient way possible of splitting a string based on delimiters.</p>\n\n<p>valueWords, valuePhrase, and Split function:</p>\n\n<pre class=\"lang-vb prettyprint-override\"><code>Public Function valuePhrase#(ByRef S1$, ByRef S2$)\n    valuePhrase = LevenshteinDistance(S1, S2)\nEnd Function\n\nPublic Function valueWords#(ByRef S1$, ByRef S2$)\n    Dim wordsS1$(), wordsS2$()\n    wordsS1 = SplitMultiDelims(S1, \" _-\")\n    wordsS2 = SplitMultiDelims(S2, \" _-\")\n    Dim word1%, word2%, thisD#, wordbest#\n    Dim wordsTotal#\n    For word1 = LBound(wordsS1) To UBound(wordsS1)\n        wordbest = Len(S2)\n        For word2 = LBound(wordsS2) To UBound(wordsS2)\n            thisD = LevenshteinDistance(wordsS1(word1), wordsS2(word2))\n            If thisD &lt; wordbest Then wordbest = thisD\n            If thisD = 0 Then GoTo foundbest\n        Next word2\nfoundbest:\n        wordsTotal = wordsTotal + wordbest\n    Next word1\n    valueWords = wordsTotal\nEnd Function\n\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n' SplitMultiDelims\n' This function splits Text into an array of substrings, each substring\n' delimited by any character in DelimChars. Only a single character\n' may be a delimiter between two substrings, but DelimChars may\n' contain any number of delimiter characters. It returns a single element\n' array containing all of text if DelimChars is empty, or a 1 or greater\n' element array if the Text is successfully split into substrings.\n' If IgnoreConsecutiveDelimiters is true, empty array elements will not occur.\n' If Limit greater than 0, the function will only split Text into 'Limit'\n' array elements or less. The last element will contain the rest of Text.\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\nFunction SplitMultiDelims(ByRef Text As String, ByRef DelimChars As String, _\n        Optional ByVal IgnoreConsecutiveDelimiters As Boolean = False, _\n        Optional ByVal Limit As Long = -1) As String()\n    Dim ElemStart As Long, N As Long, M As Long, Elements As Long\n    Dim lDelims As Long, lText As Long\n    Dim Arr() As String\n\n    lText = Len(Text)\n    lDelims = Len(DelimChars)\n    If lDelims = 0 Or lText = 0 Or Limit = 1 Then\n        ReDim Arr(0 To 0)\n        Arr(0) = Text\n        SplitMultiDelims = Arr\n        Exit Function\n    End If\n    ReDim Arr(0 To IIf(Limit = -1, lText - 1, Limit))\n\n    Elements = 0: ElemStart = 1\n    For N = 1 To lText\n        If InStr(DelimChars, Mid(Text, N, 1)) Then\n            Arr(Elements) = Mid(Text, ElemStart, N - ElemStart)\n            If IgnoreConsecutiveDelimiters Then\n                If Len(Arr(Elements)) &gt; 0 Then Elements = Elements + 1\n            Else\n                Elements = Elements + 1\n            End If\n            ElemStart = N + 1\n            If Elements + 1 = Limit Then Exit For\n        End If\n    Next N\n    'Get the last token terminated by the end of the string into the array\n    If ElemStart &lt;= lText Then Arr(Elements) = Mid(Text, ElemStart)\n    'Since the end of string counts as the terminating delimiter, if the last character\n    'was also a delimiter, we treat the two as consecutive, and so ignore the last elemnent\n    If IgnoreConsecutiveDelimiters Then If Len(Arr(Elements)) = 0 Then Elements = Elements - 1\n\n    ReDim Preserve Arr(0 To Elements) 'Chop off unused array elements\n    SplitMultiDelims = Arr\nEnd Function\n</code></pre>\n\n<p><strong>Measures of Similarity</strong></p>\n\n<p>Using these two metrics, and a third which simply computes the distance between two strings, I have a series of variables which I can run an optimization algorithm to achieve the greatest number of matches. Fuzzy string matching is, itself, a fuzzy science, and so by creating linearly independent metrics for measuring string similarity, and having a known set of strings we wish to match to each other, we can find the parameters that, for our specific styles of strings, give the best fuzzy match results.</p>\n\n<p>Initially, the goal of the metric was to have a low search value for for an exact match, and increasing search values for increasingly permuted measures. In an impractical case, this was fairly easy to define using a set of well defined permutations, and engineering the final formula such that they had increasing search values results as desired.</p>\n\n<p><img src=\"http://i.stack.imgur.com/ltCIu.png\" alt=\"Fuzzy String Matching Permutations\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/inSyO.png\" alt=\"Fuzzy String Matching Value Phrase\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/kCrqF.png\" alt=\"Fuzzy String Matching Value Words\"></p>\n\n<p>As you can see, the last two metrics, which are fuzzy string matching metrics, already have a natural tendency to give low scores to strings that are meant to match (down the diagonal). This is very good. </p>\n\n<p><strong>Application</strong>\nTo allow the optimization of fuzzy matching, I weight each metric. As such, every application of fuzzy string match can weight the parameters differently. The formula that defines the final score is a simply combination of the metrics and their weights:</p>\n\n<pre><code>value = Min(phraseWeight*phraseValue, wordsWeight*wordsValue)*minWeight\n      + Max(phraseWeight*phraseValue, wordsWeight*wordsValue)*maxWeight\n      + lengthWeight*lengthValue\n</code></pre>\n\n<p>Using an optimization algorithm (neural network is best here because it is a discrete, multi-dimentional problem), the goal is now to maximize the number of matches. I created a function that detects the number of correct matches of each set to each other, as can be seen in this final screenshot. A column or row gets a point if the lowest score is assigned the the string that was meant to be matched, and partial points are given if there is a tie for the lowest score, and the correct match is among the tied matched strings. I then optimized it. You can see that a green cell is the column that best matches the current row, and a blue square around the cell is the row that best matches the current column. The score in the bottom corner is roughly the number of successful matches and this is what we tell our optimization problem to maximize. </p>\n\n<p><img src=\"http://i.stack.imgur.com/XPglZ.png\" alt=\"Fuzzy String Matching Optimized Metric\"></p>\n\n<p>The algorithm was a wonderful success, and the solution parameters say a lot about this type of problem. You'll notice the optimized score was 44, and the best possible score is 48. The 5 columns at the end are decoys, and do not have any match at all to the row values. The more decoys there are, the harder it will naturally be to find the best match.</p>\n\n<p>In this particular matching case, the length of the strings are irrelevant, because we are expecting abbreviations that represent longer words, so the optimal weight for length is -0.3, which means we do not penalize strings which vary in length. We reduce the score in anticipation of these abbreviations, giving more room for partial word matches to supersede non-word matches that simply require less substitutions because the string is shorter.</p>\n\n<p>The word weight is 1.0 while the phrase weight is only 0.5, which means that we penalize whole words missing from one string and value more the entire phrase being intact. This is useful because a lot of these strings have one word in common (the peril) where what really matters is whether or not the combination (region and peril) are maintained.</p>\n\n<p>Finally, the min weight is optimized at 10 and the max weight at 1. What this means is that if the best of the two scores (value phrase and value words) isn't very good, the match is greatly penalized, but we don't greatly penalize the worst of the two scores. Essentially, this puts emphasis on requiring <em>either</em> the valueWord or valuePhrase to have a good score, but not both. A sort of \"take what we can get\" mentality.</p>\n\n<p>It's really fascinating what the optimized value of these 5 weights say about the sort of fuzzy string matching taking place. For completely different practical cases of fuzzy string matching, these parameters are very different. I've used it for 3 separate applications so far.</p>\n\n<p>While unused in the final optimization, a benchmarking sheet was established which matches columns to themselves for all perfect results down the diagonal, and lets the user change parameters to control the rate at which scores diverge from 0, and note innate similarities between search phrases (which could in theory be used to offset false positives in the results) </p>\n\n<p><img src=\"http://i.stack.imgur.com/XV5Wp.png\" alt=\"Fuzzy String Matching Benchmark\"></p>\n\n<p><strong>Further Applications</strong></p>\n\n<p>This solution has potential to be used anywhere where the user wishes to have a computer system identify a string in a set of strings where there is no perfect match. (Like an approximate match vlookup for strings).</p>\n\n<hr>\n\n<p>So what you should take from this, is that you probably want to use a combination of high level heuristics (finding words from one phrase in the other phrase, length of both phrases, etc) along with the implementation of the Levenshtein distance algorithm. Because deciding which is the \"best\" match is a heuristic (fuzzy) determination - you'll have to come up with a set of weights for any metrics you come up with to determine similarity. </p>\n\n<p>With the appropriate set of heuristics and weights, you'll have your comparison program quickly making the decisions that you would have made. </p>\n    "},{"t":"Good Java graph algorithm library? [closed]","l":"http://stackoverflow.com/questions/51574/good-java-graph-algorithm-library","q":"\n\n<p>Has anyone had good experiences with any Java libraries for Graph algorithms. I've tried <a href=\"http://www.jgraph.com/jgraph.html\">JGraph</a> and found it ok, and there are a lot of different ones in google. Are there any that people are actually using successfully in production code or would recommend?</p>\n\n<p>To clarify, I'm not looking for a library that produces graphs/charts, I'm looking for one that helps with Graph algorithms, eg minimum spanning tree, Kruskal's algorithm Nodes, Edges, etc. Ideally one with some good algorithms/data structures in a nice Java OO API.</p>\n    ","a":"\n<p>If you were using JGraph, you should give a try to <a href=\"http://jgrapht.org/\" rel=\"nofollow\">JGraphT</a> which is designed for algorithms.  One of its features is visualization using the JGraph library. It's still developed, but pretty stable. I analyzed the complexity of JGraphT algorithms some time ago.  Some of them aren't the quickest, but if you're going to implement them on your own and need to display your graph, then it might be the best choice.  I really liked using its API, when I quickly had to write an app that was working on graph and displaying it later.</p>\n    "},{"t":"What are the underlying data structures used for Redis?","l":"http://stackoverflow.com/questions/9625246/what-are-the-underlying-data-structures-used-for-redis","q":"\n\n<p>I'm trying to answer two questions in a definitive list:</p>\n\n<ol>\n<li>What are the underlying data structures used for Redis?</li>\n<li>And what are the main advantages/disadvantages/use cases for each type?</li>\n</ol>\n\n<p>So, I've read the Redis lists are actually implemented with linked lists. But for other types, I'm not able to dig up any information. Also, if someone were to stumble upon this question and not have a high level summary of the pros and cons of modifying or accessing different data structures, they'd have a complete list of <strong>when to best use specific types</strong> to reference as well.</p>\n\n<p>Specifically, I'm looking to outline all types: string, list, set, zset and hash.</p>\n\n<p>Oh, I've looked at these article, among others, so far:</p>\n\n<ul>\n<li><a href=\"http://redis.io/topics/data-types\">http://redis.io/topics/data-types</a></li>\n<li><a href=\"http://redis.io/topics/data-types-intro\">http://redis.io/topics/data-types-intro</a></li>\n<li><a href=\"http://redis.io/topics/faq\">http://redis.io/topics/faq</a></li>\n</ul>\n    ","a":"\n<p>I'll try to answer your question, but I'll start with something that may look strange at first: if you are not interested in Redis internals you <strong>should not care</strong> about how data types are implemented internally. This is for a simple reason: for every Redis operation you'll find the time complexity in the documentation and, if you have the set of operations and the time complexity, the only other thing you need is some clue about memory usage (and because we do many optimizations that may vary depending on data, the best way to get these latter figures are doing a few trivial real world tests).</p>\n\n<p>But since you asked, here is the underlying implementation of every Redis data type.</p>\n\n<ul>\n<li><strong>Strings</strong> are implemented using a C dynamic string library so that we don't pay (asymptotically speaking) for allocations in append operations. This way we have O(N) appends, for instance, instead of having quadratic behavior.</li>\n<li><strong>Lists</strong> are implemented with linked lists.</li>\n<li><strong>Sets</strong> and <strong>Hashes</strong> are implemented with hash tables.</li>\n<li><strong>Sorted sets</strong> are implemented with <a href=\"http://www.catonmat.net/blog/mit-introduction-to-algorithms-part-eight/\">skip lists</a> (a peculiar type of balanced trees).</li>\n</ul>\n\n<p>But when lists, sets, and sorted sets are small in number of items and size of the largest values, a different, much more compact encoding is used. This encoding differs for different types, but has the feature that it is a compact blob of data that often forces an O(N) scan for every operation. Since we use this format only for small objects this is not an issue; scanning a small O(N) blob is <em>cache oblivious</em> so practically speaking it is very fast, and when there are too many elements the encoding is automatically switched to the native encoding (linked list, hash, and so forth).</p>\n\n<p>But your question was not really just about internals, your point was <em>What type to use to accomplish what?</em>.</p>\n\n<h2>Strings</h2>\n\n<p>This is the base type of all the types. It's one of the four types but is also the base type of the complex types, because a List is a list of strings, a Set is a set of strings, and so forth.</p>\n\n<p>A Redis string is a good idea in all the obvious scenarios where you want to store an HTML page, but also when you want to avoid converting your already encoded data. So for instance, if you have JSON or MessagePack you may just store objects as strings. In Redis 2.6 you can even manipulate this kind of object server side using Lua scripts.</p>\n\n<p>Another interesting usage of strings is bitmaps, and in general random access arrays of bytes, since Redis exports commands to access random ranges of bytes, or even single bits. For instance check <a href=\"http://blog.getspool.com/2011/11/29/fast-easy-realtime-metrics-using-redis-bitmaps/\">this good blog post: Fast Easy real time metrics using Redis</a>.</p>\n\n<h2>Lists</h2>\n\n<p>Lists are good when you are likely to touch only the extremes of the list: near tail, or near head. Lists are not very good to paginate stuff, because random access is slow, O(N).\nSo good uses of lists are plain queues and stacks, or processing items in a loop using RPOPLPUSH with same source and destination to \"rotate\" a ring of items.</p>\n\n<p>Lists are also good when we want just to create a capped collection of N items where <em>usually</em> we access just the top or bottom items, or when N is small.</p>\n\n<h2>Sets</h2>\n\n<p>Sets are an unordered data collection, so they are good every time you have a collection of items and it is very important to check for existence or size of the collection in a very fast way. Another cool thing about sets is support for peeking or popping random elements (SRANDMEMBER and SPOP commands).</p>\n\n<p>Sets are also good to represent relations, e.g., \"What are friends of user X?\" and so forth. But other good data structures for this kind of stuff are sorted sets as we'll see.</p>\n\n<p>Sets support complex operations like intersections, unions, and so forth, so this is a good data structure for using Redis in a \"computational\" manner, when you have data and you want to perform transformations on that data to obtain some output.</p>\n\n<p>Small sets are encoded in a very efficient way.</p>\n\n<h2>Hashes</h2>\n\n<p>Hashes are the perfect data structure to represent objects, composed of fields and values. Fields of hashes can also be atomically incremented using HINCRBY. When you have objects such as users, blog posts, or some other kind of <em>item</em>, hashes are likely the way to go if you don't want to use your own encoding like JSON or similar.</p>\n\n<p>However, keep in mind that small hashes are encoded very efficiently by Redis, and you can ask Redis to atomically GET, SET or increment individual fields in a very fast fashion.</p>\n\n<p>Hashes can also be used to represent linked data structures, using references. For instance check the lamernews.com implementation of comments.</p>\n\n<h2>Sorted Sets</h2>\n\n<p>Sorted sets are the <em>only other data structures, besides lists, to maintain ordered elements</em>. You can do a number of cool stuff with sorted sets. For instance, you can have all kinds of <strong>Top Something</strong> lists in your web application. Top users by score, top posts by pageviews, top whatever, but a single Redis instance will support tons of insertion and get-top-elements operations per second.</p>\n\n<p>Sorted sets, like regular sets, can be used to describe relations, but they also allow you to paginate the list of items and to remember the ordering. For instance, if I remember friends of user X with a sorted set I can easily remember them in order of accepted friendship.</p>\n\n<p>Sorted sets are good for priority queues.</p>\n\n<p>Sorted sets are like more powerful lists where inserting, removing, or getting ranges from the the middle of the list is always fast. But they use more memory, and are O(log(N)) data structures.</p>\n\n<h2>Conclusion</h2>\n\n<p>I hope that I provided some info in this post, but it is far better to download the source code of lamernews from <a href=\"http://github.com/antirez/lamernews\">http://github.com/antirez/lamernews</a> and understand how it works. Many data structures from Redis are used inside Lamer News, and there are many clues about what to use to solve a given task.</p>\n\n<p>Sorry for grammar typos, it's midnight here and too tired to review the post ;)</p>\n    "},{"t":"Are loops really faster in reverse…?","l":"http://stackoverflow.com/questions/13136724/why-is-i-faster-than-i-in-loops","q":"\n\n<p>I've heard this quite a few times. Are JavaScript loops really faster when counting backward? If so, why? I've seen a few test suite examples showing that reversed loops are quicker, but I can't find any explanation as to why!</p>\n\n<p>I'm assuming it's because the loop no longer has to evaluate a property each time it checks to see if it's finished and it just checks against the final numeric value.</p>\n\n<p>I.e. </p>\n\n<pre><code>for (var i = count - 1; i &gt;= 0; i--)\n{\n  // count is only evaluated once and then the comparison is always on 0.\n}\n</code></pre>\n    ","a":"\n<p>It's not that <code>i--</code> is faster than <code>i++</code>. Actually, they're both as fast.</p>\n\n<p>What takes time in ascending loops, is to evaluate, for each <code>i</code>, the size of your array. In this loop:</p>\n\n<pre><code>for(var i = array.length; i--; )\n</code></pre>\n\n<p>You evaluate <code>.length</code> only once, when you declare <code>i</code>, whereas for this loop</p>\n\n<pre><code>for(var i = 1; i &lt;= array.length; i++ )\n</code></pre>\n\n<p>you evaluate <code>.length</code> each time you increment <code>i</code>, when you check if <code>i &lt;= array.length</code>.</p>\n\n<p>In most cases you <strong>shouldn't even worry about this kind of optimization</strong>.</p>\n    "},{"t":"Get a random number focused on center","l":"http://stackoverflow.com/questions/30492259/get-a-random-number-focused-on-center","q":"\n\n<p>Is it possible to get a random number between 1-100 and keep the results mainly within the 40-60 range? I mean, it will go out of that range rarely, but I want it to be mainly within that range... Is it possible with JavaScript/jQuery?</p>\n\n<p>Right now I'm just using the basic <code>Math.random() * 100 + 1</code>.</p>\n    ","a":"\n<p>The simplest way would be to generate two random numbers from 0-50 and add them together.</p>\n\n<p>This gives a distribution biased towards 50, in the same way rolling two dice biases towards 7.</p>\n\n<p>By using different settings (<code>bellFactor</code>) as @Falco suggests, you can easily choose from even distribution or triangle distribution or different bell curves:</p>\n\n<pre><code>function weightedRandom(max, bellFactor) {\n    var num = 0;\n    for (var i = 0; i &lt; bellFactor; i++) {\n        num += Math.random() * (max/bellFactor);\n    }    \n    return num;\n}\n</code></pre>\n\n<p><img src=\"http://i.stack.imgur.com/WskvS.png\" alt=\"Weighted random numbers\"></p>\n\n<p>JSFiddle: <a href=\"http://jsfiddle.net/797qhcza/1/\">http://jsfiddle.net/797qhcza/1/</a></p>\n    "},{"t":"Constant Amortized Time","l":"http://stackoverflow.com/questions/200384/constant-amortized-time","q":"\n\n<p>What is meant by \"Constant Amortized Time\" when talking about time complexity of an algorithm?</p>\n    ","a":"\n<p>Amortised time explained in simple terms:</p>\n\n<p>If you do an operation say a million times, you don't really care about the worst-case or the best-case of that operation - what you care about is how much time is taken in total when you repeat the operation a million times.</p>\n\n<p>So it doesn't matter if the operation is very slow once in a while, as long as \"once in a while\" is rare enough for the slowness to be diluted away. Essentially amortised time means \"average time taken per operation, if you do many operations\". Amortised time doesn't have to be constant; you can have linear and logarithmic amortised time or whatever else.</p>\n\n<p>Let's take mats' example of a dynamic array, to which you repeatedly add new items. Normally adding an item takes constant time (that is, <code>O(1)</code>). But each time the array is full, you allocate twice as much space, copy your data into the new region, and free the old space. Assuming allocates and frees run in constant time, this enlargement process takes <code>O(n)</code> time where n is the current size of the array.</p>\n\n<p>So each time you enlarge, you take about twice as much time as the last enlarge. But you've also waited twice as long before doing it! The cost of each enlargement can thus be \"spread out\" among the insertions. This means that in the long term, the total time taken for adding <em>m</em> items to the array is <code>O(m)</code>, and so the amortised time (i.e. time per insertion) is <code>O(1)</code>.</p>\n    "},{"t":"Image comparison - fast algorithm","l":"http://stackoverflow.com/questions/843972/image-comparison-fast-algorithm","q":"\n\n<p>I'm looking to create a base table of images and then compare any new images against that to determine if the new image is an exact (or close) duplicate of the base.</p>\n\n<p>For example: if you want to reduce storage of the same image 100's of times, you could store one copy of it and provide reference links to it.  When a new image is entered you want to compare to an existing image to make sure it's not a duplicate ... ideas?</p>\n\n<p>One idea of mine was to reduce to a small thumbnail and then randomly pick 100 pixel locations and compare.</p>\n    ","a":"\n<p>Below are three approaches to solving this problem (and there are many others).  </p>\n\n<ul>\n<li><p>The first is a standard approach in computer vision, keypoint matching.  This may require some background knowledge to implement, and can be slow.</p></li>\n<li><p>The second method uses only elementary image processing, and is potentially faster than the first approach, and is straightforward to implement.  However, what it gains in understandability, it lacks in robustness -- matching fails on scaled, rotated, or discolored images.</p></li>\n<li><p>The third method is both fast and robust, but is potentially the hardest to implement.</p></li>\n</ul>\n\n<p><strong>Keypoint Matching</strong></p>\n\n<p>Better than picking 100 random points is picking 100 <em>important</em> points.  Certain parts of an image have more information than others (particularly at edges and corners), and these are the ones you'll want to use for smart image matching.  Google \"<a href=\"http://www.google.com/search?q=keypoint+extraction\">keypoint extraction</a>\" and \"<a href=\"https://www.google.com/search?q=keypoint+matching\">keypoint matching</a>\" and you'll find quite a few academic papers on the subject.  These days, <a href=\"http://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">SIFT keypoints</a> are arguably the most popular, since they can match images under different scales, rotations, and lighting.  Some SIFT implementations can be found <a href=\"http://people.csail.mit.edu/albert/ladypack/wiki/index.php/Known_implementations_of_SIFT\">here</a>.</p>\n\n<p>One downside to keypoint matching is the running time of a naive implementation: O(n^2m), where n is the number of keypoints in each image, and m is the number of images in the database.  Some clever algorithms might find the closest match faster, like quadtrees or binary space partitioning.</p>\n\n<hr>\n\n<p><strong>Alternative solution: Histogram method</strong></p>\n\n<p>Another less robust but potentially faster solution is to build feature histograms for each image, and choose the image with the histogram closest to the input image's histogram.  I implemented this as an undergrad, and we used 3 color histograms (red, green, and blue), and two texture histograms, direction and scale. I'll give the details below, but I should note that this only worked well for matching images VERY similar to the database images.  Re-scaled, rotated, or discolored images can fail with this method, but small changes like cropping won't break the algorithm </p>\n\n<p>Computing the color histograms is straightforward -- just pick the range for your histogram buckets, and for each range, tally the number of pixels with a color in that range.  For example, consider the \"green\" histogram, and suppose we choose 4 buckets for our histogram: 0-63, 64-127, 128-191, and 192-255.  Then for each pixel, we look at the green value, and add a tally to the appropriate bucket.  When we're done tallying, we divide each bucket total by the number of pixels in the entire image to get a normalized histogram for the green channel.</p>\n\n<p>For the texture direction histogram, we started by performing edge detection on the image.  Each edge point has a normal vector pointing in the direction perpendicular to the edge.   We quantized the normal vector's angle into one of 6 buckets between 0 and PI (since edges have 180-degree symmetry, we converted angles between -PI and 0 to be between 0 and PI).  After tallying up the number of edge points in each direction, we have an un-normalized histogram representing texture direction, which we normalized by dividing each bucket by the total number of edge points in the image.  </p>\n\n<p>To compute the texture scale histogram, for each edge point, we measured the distance to the next-closest edge point with the same direction.  For example, if edge point A has a direction of 45 degrees, the algorithm walks in that direction until it finds another edge point with a direction of 45 degrees (or within a reasonable deviation).  After computing this distance for each edge point, we dump those values into a histogram and normalize it by dividing by the total number of edge points.</p>\n\n<p>Now you have 5 histograms for each image.  To compare two images, you take the absolute value of the difference between each histogram bucket, and then sum these values.  For example, to compare images A and B, we would compute </p>\n\n<pre><code>|A.green_histogram.bucket_1 - B.green_histogram.bucket_1| \n</code></pre>\n\n<p>for each bucket in the green histogram, and repeat for the other histograms, and then sum up all the results.  The smaller the result, the better the match.  Repeat for all images in the database, and the match with the smallest result wins.  You'd probably want to have a threshold, above which the algorithm concludes that no match was found.</p>\n\n<hr>\n\n<p><strong>Third Choice - Keypoints + Decision Trees</strong></p>\n\n<p>A third approach that is probably much faster than the other two is using <a href=\"http://jamie.shotton.org/work/publications/cvpr08.pdf\">semantic texton forests</a> (PDF).  This involves extracting simple keypoints and using a collection decision trees to classify the image.   This is faster than simple SIFT keypoint matching, because it avoids the costly matching process, and keypoints are much simpler than SIFT, so keypoint extraction is much faster.  However, it preserves the SIFT method's invariance to rotation, scale, and lighting, an important feature that the histogram method lacked.</p>\n\n<p><strong>Update</strong>:</p>\n\n<p>My mistake -- the Semantic Texton Forests paper isn't specifically about image matching, but rather region labeling. The original paper that does matching is this one:  <a href=\"http://cvlab.epfl.ch/publications/publications/2006/LepetitF06.pdf\">Keypoint Recognition using Randomized Trees</a>.  Also, the papers below continue to develop the ideas and represent the state of the art (c. 2010):</p>\n\n<ul>\n<li><a href=\"http://cvlab.epfl.ch/publications/publications/2010/OzuysalCLF10.pdf\">Fast Keypoint Recognition using Random Ferns</a> - faster and more scalable than Lepetit 06</li>\n<li><a href=\"http://cvlab.epfl.ch/publications/publications/2010/LepetitF10.pdf\">BRIEF: Binary Robust Independent Elementary Features</a> - less robust but very fast -- I think the goal here is real-time matching on smart phones and other handhelds</li>\n</ul>\n    "},{"t":"What is an NP-complete in computer science?","l":"http://stackoverflow.com/questions/210829/what-is-an-np-complete-in-computer-science","q":"\n\n<p>What is an NP-complete problem? Why is it such an important topic in computer science?</p>\n    ","a":"\n<p><strong>NP</strong> stands for <strong><em>Non-deterministic</em> Polynomial</strong> time.</p>\n\n<p>This means that the problem can be solved in Polynomial time using a Non-deterministic Turing machine (like a regular Turing machine but also including a non-deterministic \"choice\" function). Basically, a solution has to be <i>testable</i> in poly time. If that's the case, and a known NP problem can be solved using the given problem with modified input (an NP problem can be <i>reduced</i> to the given problem) then the problem is NP complete.</p>\n\n<p>The main thing to take away from an NP-complete problem is that it cannot be solved in polynomial time in any known way. NP-Hard/NP-Complete is a way of showing that certain classes of problems are not solvable in realistic time.</p>\n\n<p>Edit: As others have noted, there are often approximation solutions for NP-Complete problems. In this case, the approximation solution usually gives a approximation bound using special notation which tells us how close the approximation is.</p>\n    "},{"t":"How to detect a loop in a linked list?","l":"http://stackoverflow.com/questions/2663115/how-to-detect-a-loop-in-a-linked-list","q":"\n\n<p>Say you have a linked list structure in Java.  It's made up of Nodes:</p>\n\n<pre><code>class Node {\n    Node next;\n    // some user data\n}\n</code></pre>\n\n<p>and each Node points to the next node, except for the last Node, which has null for next.  Say there is a possibility that the list can contain a loop - i.e. the final Node, instead of having a null, has a reference to one of the nodes in the list which came before it.</p>\n\n<p>What's the best way of writing</p>\n\n<pre><code>boolean hasLoop(Node first)\n</code></pre>\n\n<p>which would return <code>true</code> if the given Node is the first of a list with a loop, and <code>false</code> otherwise?  How could you write so that it takes a constant amount of space and a reasonable amount of time?</p>\n\n<p>Here's a picture of what a list with a loop looks like:</p>\n\n<p><img src=\"http://i.stack.imgur.com/irw1S.jpg\" alt=\"alt text\"></p>\n    ","a":"\n<p>You can make use of <a href=\"http://en.wikipedia.org/wiki/Cycle_detection#Tortoise_and_hare\"><strong>Floyd's cycle-finding algorithm</strong></a>, also know as <em>tortoise and hare algorithm</em>.<br><br>\n The idea is to have two references to the list and move them at <strong>different speeds</strong>. Move one forward by <code>1</code> node and the other by <code>2</code> nodes. </p>\n\n<ul>\n<li>If the linked list has a loop they\nwill <em>definitely</em> meet.</li>\n<li>Else either of\nthe two references(or their <code>next</code>)\nwill become <code>null</code>.</li>\n</ul>\n\n<p>Java function implementing the algorithm:</p>\n\n<pre><code>boolean hasLoop(Node first) {\n\n    if(first == null) // list does not exist..so no loop either.\n        return false;\n\n    Node slow, fast; // create two references.\n\n    slow = fast = first; // make both refer to the start of the list.\n\n    while(true) {\n\n        slow = slow.next;          // 1 hop.\n\n        if(fast.next != null)\n            fast = fast.next.next; // 2 hops.\n        else\n            return false;          // next node null =&gt; no loop.\n\n        if(slow == null || fast == null) // if either hits null..no loop.\n            return false;\n\n        if(slow == fast) // if the two ever meet...we must have a loop.\n            return true;\n    }\n}\n</code></pre>\n    "},{"t":"Why does Java's hashCode() in String use 31 as a multiplier?","l":"http://stackoverflow.com/questions/299304/why-does-javas-hashcode-in-string-use-31-as-a-multiplier","q":"\n\n<p>In Java, the <a href=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html#hashCode()\">hash code</a> for a <code>String</code> object is computed as</p>\n\n<p><code>s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]</code></p>\n\n<p>using <code>int</code> arithmetic, where <code>s[i]</code> is the <code>i</code><sup>th</sup> character of the string, <code>n</code> is the length of the string, and <code>^</code> indicates exponentiation.</p>\n\n<p>Why is 31 used as a multiplier?</p>\n\n<p>I understand that the multiplier should be a relatively large prime number. So why not 29, or 37, or even 97?</p>\n    ","a":"\n<p>According to Joshua Bloch's <a href=\"http://rads.stackoverflow.com/amzn/click/0321356683\">Effective Java</a> (a book that can't be recommended enough, and which I bought thanks to continual mentions on stackoverflow):</p>\n\n<blockquote>\n  <p>The value 31 was chosen because it is an odd prime. If it were even and the multiplication overflowed, information would be lost, as multiplication by 2 is equivalent to shifting. The advantage of using a prime is less clear, but it is traditional. A nice property of 31 is that the multiplication can be replaced by a shift and a subtraction for better performance: <code>31 * i == (i &lt;&lt; 5) - i</code>. Modern VMs do this sort of optimization automatically.</p>\n</blockquote>\n\n<p><em>(from Chapter 3, Item 9: Always override hashcode when you override equals, page 48)</em></p>\n    "},{"t":"Bomb dropping algorithm","l":"http://stackoverflow.com/questions/15300149/bomb-dropping-algorithm","q":"\n\n<p>I have an <code>n x m</code> matrix consisting of non-negative integers. For example:</p>\n\n<pre><code>2 3 4 7 1\n1 5 2 6 2\n4 3 4 2 1\n2 1 2 4 1\n3 1 3 4 1\n2 1 4 3 2\n6 9 1 6 4\n</code></pre>\n\n<p>\"Dropping a bomb\" decreases by one the number of the target cell and all eight of its neighbours, to a minimum of zero.</p>\n\n<pre><code>x x x \nx X x\nx x x\n</code></pre>\n\n<p>What is an algorithm that would determine the minimum number of bombs required to reduce all the cells to zero?</p>\n\n<p><strong>B Option (Due to me not being a careful reader)</strong></p>\n\n<p>Actually the first version of problem is not the one I'm seeking answer for. I didn't carefully read whole task, there's additional constraints, let us say:</p>\n\n<p>What about simple problem, when sequence in row must be non-increasing:</p>\n\n<p><code>8 7 6 6 5</code> is possible input sequence</p>\n\n<p><code>7 8 5 5 2</code> is not possible since 7 -&gt; 8 growing in a sequence.</p>\n\n<p>Maybe finding answer for \"easier\" case would help in finding solution for harder one.</p>\n\n<p><strong>PS:</strong> I believe that when we have several same situations require minimum bombs to clear upper line, we choose one that use most bombs on \"left side\" of the row. Still any proof that might be correct?</p>\n    ","a":"\n<p>There is a way to reduce this to a simple sub-problem.</p>\n\n<p>There are 2 parts to the explanation, the algorithm, and the reason the algorithm \nprovides an optimal solution.  The first won't make sense without the second, so I'll \nstart with the why.</p>\n\n<p>If you think of bombing the rectangle (assume a big rectangle - no edge cases yet) \nyou can see that the only way to reduce the hollow rectangle of squares on the\nperimeter to 0 is to bomb either the perimeter or to bomb the hollow rectangle of \nsquares just inside the perimeter.  I'll call the perimeter layer 1, and the rectangle inside it layer 2.</p>\n\n<p>An important insight is that there is no point bombing layer 1, because the \n\"blast radius\" you get from doing so is always contained within the blast radius of \nanother square from layer 2.  You should be able to easily convince yourself of this.</p>\n\n<p>So, we can reduce the problem to finding an optimal way to bomb away the perimeter, then we can repeat that until all squares are 0.</p>\n\n<p>But of course, that won't always find an optimal solution if it's possible to bomb \naway the perimeter in a less than optimal fashion, but by using X extra bombs make \nthe problem of reducing the inner layer simpler by &gt;X bombs.  So, if we call \nthe permiter layer one, if we place an extra X bombs somewhere in layer 2 (just \ninside layer 1), can we reduce the effort of later bombing away layer 2 by more than \nX?  In other words, we have to prove we can be greedy in reducing the outer \nperimeter.</p>\n\n<p>But, we do know we can be greedy.  Because no bomb in layer 2 can ever be more \nefficient in reducing layer 2 to 0 than a strategically placed bomb in layer 3.  And \nfor the same reason as before - there is always a bomb we can place in layer 3 that \nwill affect every square of layer 2 that a bomb placed in layer 2 can.  So, it can \nnever harm us to be greedy (in this sense of greedy).</p>\n\n<p>So, all we have to do is find the optimal way to reduce the permiter to 0 by bombing \nthe next inner layer.  </p>\n\n<p>We are never hurt by first bombing the corner to 0, because only the corner of the inner layer can reach it, so we really have no choice (and, any bomb on the perimeter that can reach the corner has a blast radius contained in the blast radius from the corner of the inner layer).</p>\n\n<p>Once we have done so, the squares on the perimeter adjacent to the 0 corner can only be reached by 2 squares from the inner layer:</p>\n\n<pre><code>0       A       B\n\nC       X       Y\n\nD       Z\n</code></pre>\n\n<p>At this point the perimeter is effectively a closed 1 dimensional loop, because any bomb will reduce 3 adjacent squares.  Except for some weirdness near the corners - X can \"hit\" A,B,C,and D.</p>\n\n<p>Now we can't use any blast radius tricks - the situation of each square is symmetric, except for the weird corners, and even there no blast radius is a subset of another.  Note that if this were a line (as Colonel Panic discusses) instead of a closed loop  the solution is trivial.  The end points must be reduced to 0, and it never harms you to bomb the points adjacent to the end points, again because the blast radius is a superset.  Once you have made your endpoint 0, you still have a new endpoint, so repeat (until the line is all 0).  </p>\n\n<p>So, if we can optimally reduce a single square in the layer to 0 we have an algorithm (because we have cut the loop and now have a straight line with endpoints).  I believe bombing adjacent to the square with the lowest value (giving you 2 options) such that the highest value within 2 squares of that lowest value is the minimum possible (you may have to split your bombing to manage this) will be optimal but I don't (yet?) have a proof.  </p>\n    "},{"t":"How to implement classic sorting algorithms in modern C++?","l":"http://stackoverflow.com/questions/24650626/how-to-implement-classic-sorting-algorithms-in-modern-c","q":"\n\n<p>The <code>std::sort</code> algorithm (and its cousins <code>std::partial_sort</code> and <code>std::nth_element</code>) from the C++ Standard Library is in most implementations <a href=\"http://stackoverflow.com/q/22339240/819272\">a complicated and hybrid amalgation of more elementary sorting algorithms</a>, such as selection sort, instertion sort, quick sort, merge sort, or heap sort.</p>\n\n<p>There are many questions here and on sister sites such as <a href=\"http://codereview.stackexchange.com/\">http://codereview.stackexchange.com/</a> related to bugs, complexity and other aspects of implementations of these classic sorting algorithms. Most of the offered implementations consist of raw loops, use index manipulation and concrete types, and are generally non-trivial to analyze in terms of correctness and efficiency.</p>\n\n<p><strong>Question</strong>: how can the above mentioned classic sorting algorithms be implemented using modern C++?</p>\n\n<ul>\n<li><strong>no raw loops</strong>, but combining the Standard Library's algorithmic building blocks from <code>&lt;algorithm&gt;</code></li>\n<li><strong>iterator interface</strong> and use of <strong>templates</strong> instead of index manipulation and concrete types</li>\n<li><strong>C++14 style</strong>, including the full Standard Library, as well as syntactic noise reducers such as <code>auto</code>, template aliases, transparant comparators and polymorphic lambdas.</li>\n</ul>\n\n<p><strong>Notes</strong>: </p>\n\n<ul>\n<li>for further references on implementations of sorting algorithms see <a href=\"http://en.wikipedia.org/wiki/Sorting_algorithm\">Wikipedia</a>, <a href=\"http://rosettacode.org/wiki/Sorting_algorithms\">Rosetta Code</a> or <a href=\"http://www.sorting-algorithms.com/\">http://www.sorting-algorithms.com/</a> </li>\n<li>according to <a href=\"https://github.com/sean-parent/sean-parent.github.com/wiki/presentations/2013-09-11-cpp-seasoning/cpp-seasoning.pdf\"><strong>Sean Parent's conventions</strong></a> (slide 39), a raw loop is a <code>for</code>-loop longer than composition of two functions with an operator. So <code>f(g(x));</code> or <code>f(x); g(x);</code> or <code>f(x) + g(x);</code> are not raw loops, and neither are the loops in <code>selection_sort</code> and <code>insertion_sort</code> below.</li>\n<li>I follow Scott Meyers's terminology to denote the current C++1y already as C++14, and to denote C++98 and C++03 both as C++98, so don't flame me for that.</li>\n<li>As suggested in the comments by @Mehrdad, I provide four implementations as a Live Example at the end of the answer: C++14, C++11, C++98 and Boost and C++98. </li>\n<li>The answer itself is presented in terms of C++14 only. Where relevant, I denote the syntactic and library differences where the various language versions differ.</li>\n</ul>\n    ","a":"\n<h2>Algorithmic building blocks</h2>\n\n<p>We begin by assembling the algorithmic building blocks from the Standard Library:</p>\n\n<pre><code>#include &lt;algorithm&gt;    // min_element, iter_swap, \n                        // upper_bound, rotate, \n                        // nth_element, partition, \n                        // inplace_merge,\n                        // make_heap, sort_heap, push_heap, pop_heap,\n                        // is_heap, is_sorted\n#include &lt;cassert&gt;      // assert \n#include &lt;functional&gt;   // less\n#include &lt;iterator&gt;     // distance, begin, end, next\n</code></pre>\n\n<ul>\n<li>the iterator tools such as non-member <code>std::begin()</code> / <code>std::end()</code> as well as with <code>std::next()</code> are only available as of C++11 and beyond. For C++98, one needs to write these oneself. There are substitutes from Boost.Range in <code>boost::begin()</code> / <code>boost::end()</code>, and from Boost.Utility in <code>boost::next()</code>. </li>\n<li>the <code>std::is_sorted</code> algorithm is only available for C++11 and beyond. For C++98, this can be implemented in terms of <code>std::adjacent_find</code> and a hand-written function object. Boost.Algorithm also provides a <code>boost::algorithm::is_sorted</code> as a substitute.</li>\n<li>the <code>std::xxx_heap</code> related algorithms are only available for C++11 and beyond. For C++98, this excludes <code>heap_sort</code> from being written in terms of standard algorithms. Boost.Range provides <code>boost::xxx_heap</code> and <code>boost::make_iterator_range</code> as substitutes.</li>\n</ul>\n\n<h2>Syntactical goodies</h2>\n\n<p>C++14 provides <a href=\"http://stackoverflow.com/questions/20317413/what-are-transparent-comparators\"><strong>transparant comparators</strong></a> of the form <code>std::less&lt;&gt;</code> that act polymorphically on their arguments. This avoids having to provide an iterator's type. This can be used in combination with C++11's <a href=\"http://stackoverflow.com/q/2447458/819272\"><strong>default function template arguments</strong></a> to create <strong>a single overload</strong> for sorting algorithms that take <code>&lt;</code> as comparison and those that have a user-defined comparison function object.</p>\n\n<pre><code>template&lt;class It, class Compare = std::less&lt;&gt;&gt;\nvoid xxx_sort(It first, It last, Compare cmp = Compare{});\n</code></pre>\n\n<p>In C++11, one can define a reusable <a href=\"http://stackoverflow.com/q/2795023/819272\"><strong>template alias</strong></a> to extract an iterator's value type which adds minor clutter to the sort algorithms' signatures:</p>\n\n<pre><code>template&lt;class It&gt;\nusing value_type_t = typename std::iterator_traits&lt;It&gt;::value_type;\n\ntemplate&lt;class It, class Compare = std::less&lt;value_type_t&lt;It&gt;&gt;&gt;\nvoid xxx_sort(It first, It last, Compare cmp = Compare{});\n</code></pre>\n\n<p>In C++98, one needs to write two overloads and use the verbose <code>typename xxx&lt;yyy&gt;::type</code> syntax</p>\n\n<pre><code>template&lt;class It, class Compare&gt;\nvoid xxx_sort(It first, It last, Compare cmp); // general implementation\n\ntemplate&lt;class It&gt;\nvoid xxx_sort(It first, It last)\n{\n    xxx_sort(first, last, std::less&lt;typename std::iterator_traits&lt;It&gt;::value_type&gt;());\n}\n</code></pre>\n\n<ul>\n<li>Another syntactical nicety is that C++14 facilitates wrapping user-defined comparators through <strong>polymorphic lambdas</strong> (with <code>auto</code> parameters that are deduced like function template arguments).     </li>\n<li>C++11 only has monomorphic lambdas, that require the use of the above template alias <code>value_type_t</code>. </li>\n<li>In C++98, one either needs to write a standalone function object or resort to the verbose <code>std::bind1st</code> / <code>std::bind2nd</code> / <code>std::not1</code> type of syntax. </li>\n<li>Boost.Bind improves this with <code>boost::bind</code> and <code>_1</code> / <code>_2</code> placeholder syntax.</li>\n<li>C++11 and beyond also have <code>std::find_if_not</code>, whereas C++98 needs <code>std::find_if</code> with a <code>std::not1</code> around a function object.</li>\n</ul>\n\n<h2>C++ Style</h2>\n\n<p>There is no generally acceptable C++14 style yet. For better of for worse, I closely follow Scott Meyers's <a href=\"http://my.safaribooksonline.com/book/programming/cplusplus/9781491908419\"><strong>draft Effective Modern C++</strong></a> and Herb Sutter's <a href=\"http://herbsutter.com/gotw/\"><strong>revamped GotW</strong></a>. I use the following style recommendations:</p>\n\n<ul>\n<li>Herb Sutter's <a href=\"http://herbsutter.com/2013/08/12/gotw-94-solution-aaa-style-almost-always-auto/\"><strong>\"Almost Always Auto\"</strong></a> and Scott Meyers's <a href=\"http://my.safaribooksonline.com/book/programming/cplusplus/9781491908419/chapter-2-auto/item_5__prefer_auto_to_explici\"><strong>\"Prefer auto to specific type declarations\"</strong></a> recommendation, for which the brevity is unsurpassed, although its clarity is sometimes <a href=\"http://www.reddit.com/r/cpp/comments/2a4tv0/dont_use_auto_unless_you_mean_it_a_unified/\"><strong>disputed</strong></a>.</li>\n<li>Scott Meyers's <a href=\"http://my.safaribooksonline.com/book/programming/cplusplus/9781491908419/chapter-3-from-cplusplus98-to-cplusplus11-and-cplusplus14/item_7_distinguish__and__when\"><strong>\"Distinguish <code>()</code> and <code>{}</code> when creating objects\"</strong></a> and consistently choose braced-initialization <code>{}</code> instead of the good old parenthesized initialization <code>()</code> (in order to side-step all most-vexing-parse issues in generic code).</li>\n<li>Scott Meyers's <a href=\"http://my.safaribooksonline.com/book/programming/cplusplus/9781491908419/chapter-3-from-cplusplus98-to-cplusplus11-and-cplusplus14/item_9_prefer_alias_declaratio\"><strong>\"Prefer alias declarations to typedefs\"</strong></a>. For templates this is a must anyway, and using it everywhere instead of <code>typedef</code> saves time and adds consistency.</li>\n<li>I use a <code>for (auto it = first; it != last; ++it)</code> pattern in some places, in order to allow for loop invariant checking for already sorted sub-ranges. In production code, the use of <code>while (first != last)</code> and a <code>++first</code> somewhere inside the loop might be slightly better.</li>\n</ul>\n\n<h2>Selection sort</h2>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Selection_sort\"><strong>Selection sort</strong></a> does not adapt to the data in any way, so its runtime is always <code>O(N^2)</code>. However, selection sort has the property of <strong>minimizing the number of swaps</strong>. In applications where the cost of swapping items is high, selection sort very well may be the algorithm of choice.</p>\n\n<p>To implement it using the Standard Library, repeatedly use <code>std::min_element</code> to find the remaining minimum element, and <code>iter_swap</code> to swap it into place:</p>\n\n<pre><code>template&lt;class FwdIt, class Compare = std::less&lt;&gt;&gt;\nvoid selection_sort(FwdIt first, FwdIt last, Compare cmp = Compare{})\n{\n    for (auto it = first; it != last; ++it) {\n        auto const selection = std::min_element(it, last, cmp);\n        std::iter_swap(selection, it); \n        assert(std::is_sorted(first, std::next(it), cmp));\n    }\n}\n</code></pre>\n\n<p>Note that <code>selection_sort</code> has the already processed range <code>[first, it)</code> sorted as its loop invariant. The minimal requirements are <strong>forward iterators</strong>, compared to <code>std::sort</code>'s random access iterators.</p>\n\n<p><strong>Details omitted</strong>:</p>\n\n<ul>\n<li>selection sort can be optimized with an early test <code>if (std::distance(first, last) &lt;= 1) return;</code> (or for forward / bidirectional iterators: <code>if (first == last || std::next(first) == last) return;</code>).</li>\n<li>for <strong>bidirectional iterators</strong>, the above test can be combined with a loop over the interval <code>[first, std::prev(last))</code>, because the last element is guaranteed to be the minimal remaining element and doesn't require a swap.</li>\n</ul>\n\n<h2>Insertion sort</h2>\n\n<p>Although it is one of the elementary sorting algorithms with <code>O(N^2)</code> worst-case time, <a href=\"http://en.wikipedia.org/wiki/Insertion_sort\"><strong>insertion sort</strong></a> is the algorithm of choice either when the data is nearly sorted (because it is <strong>adaptive</strong>) or when the problem size is small (because it has low overhead). For these reasons, and because it is also <strong>stable</strong>, insertion sort is often used as the recursive base case (when the problem size is small) for higher overhead divide-and-conquer sorting algorithms, such as merge sort or quick sort.</p>\n\n<p>To implement <code>insertion_sort</code> with the Standard Library, repeatedly use <code>std::upper_bound</code> to find the location where the current element needs to go, and use <code>std::rotate</code> to shift the remaining elements upward in the input range:</p>\n\n<pre><code>template&lt;class FwdIt, class Compare = std::less&lt;&gt;&gt;\nvoid insertion_sort(FwdIt first, FwdIt last, Compare cmp = Compare{})\n{\n    for (auto it = first; it != last; ++it) {\n        auto const insertion = std::upper_bound(first, it, *it, cmp);\n        std::rotate(insertion, it, std::next(it)); \n        assert(std::is_sorted(first, std::next(it), cmp));\n    }\n}\n</code></pre>\n\n<p>Note that <code>insertion_sort</code> has the already processed range <code>[first, it)</code> sorted as its loop invariant. Insertion sort also works with forward iterators.</p>\n\n<p><strong>Details omitted</strong>:</p>\n\n<ul>\n<li>insertion sort can be optimized with an early test <code>if (std::distance(first, last) &lt;= 1) return;</code> (or for forward / bidirectional iterators: <code>if (first == last || std::next(first) == last) return;</code>) and a loop over the interval <code>[std::next(first), last)</code>, because the first element is guaranteed to be in place and doesn't require a rotate.</li>\n<li>for <strong>bidirectional iterators</strong>, the binary search to find the insertion point can be replaced with a <strong>reverse linear search</strong> using the Standard Library's <code>std::find_if_not</code> algorithm. </li>\n</ul>\n\n<p>Four <strong>Live Examples</strong> (<a href=\"http://coliru.stacked-crooked.com/a/024be20937f2f04d\"><strong>C++14</strong></a>, <a href=\"http://coliru.stacked-crooked.com/a/8f5d919f7615fe6b\"><strong>C++11</strong></a>, <a href=\"http://coliru.stacked-crooked.com/a/bcc5a2c688c6d154\"><strong>C++98 and Boost</strong></a>, <a href=\"http://coliru.stacked-crooked.com/a/2f9ad1b555fd7812\"><strong>C++98</strong></a>) for the fragment below:</p>\n\n<pre><code>using RevIt = std::reverse_iterator&lt;BiDirIt&gt;;\nauto const insertion = std::find_if_not(RevIt(it), RevIt(first), \n    [=](auto const&amp; elem){ return cmp(*it, elem); }\n).base();\n</code></pre>\n\n<ul>\n<li>For random inputs this gives <code>O(N^2)</code> comparisons, but this improves to <code>O(N)</code> comparisons for almost sorted inputs. The binary search always uses <code>O(N log N)</code> comparisons. </li>\n<li>For small input ranges, the better memory locality (cache, prefetching) of a linear search might also dominate a binary search (one should test this, of course).</li>\n</ul>\n\n<h2>Quick sort</h2>\n\n<p>When carefully implemented, <a href=\"http://en.wikipedia.org/wiki/Quicksort\"><strong>quick sort</strong></a> is robust and has low overhead. When a stable sort is not needed, quick sort is an excellent general-purpose sort. </p>\n\n<p>The <em>simplest version</em> of <code>quick_sort</code> is straightforward to implement using the Standard Library: use a few iterator utilities to locate the middle of the input range <code>[first, last)</code> as the pivot, use <code>std::nth_element</code> to separate the input range into element that are either smaller or equal to or larger than the selected pivot. Finally the two segments are recursively sorted:</p>\n\n<pre><code>template&lt;class FwdIt, class Compare = std::less&lt;&gt;&gt;\nvoid quick_sort(FwdIt first, FwdIt last, Compare cmp = Compare{})\n{\n    auto const N = std::distance(first, last);\n    if (N &lt;= 1) return;\n    auto const pivot = std::next(first, N / 2);\n    std::nth_element(first, pivot, last, cmp);\n    quick_sort(first, pivot, cmp); // assert(std::is_sorted(first, pivot, cmp));\n    quick_sort(pivot, last, cmp);  // assert(std::is_sorted(pivot, last, cmp));\n}\n</code></pre>\n\n<p>Note that although quick sort in general works with forward iterators, the above implementation cheats on this promise by using <code>std::nth_element</code>, which requires random access iterators (it can be remedied by choosing the pivot differently, see below).</p>\n\n<p>However, quick sort is rather tricky to get correct and efficient, as each of the above steps has to be carefully checked and optimized for production level code. </p>\n\n<p><strong>Details omitted</strong>:</p>\n\n<ul>\n<li>the above implementation is particularly vulnerable to special inputs, e.g. it has <code>O(N^2)</code> complexity for the \"<strong>organ pipe</strong>\" input <code>1, 2, 3, ..., N/2, ... 3, 2, 1</code></li>\n<li><a href=\"http://stackoverflow.com/q/12937732/819272\"><strong>median-of-3</strong></a> pivot selection from <a href=\"http://stackoverflow.com/q/6942273/819272\"><strong>randomly chosen elements</strong></a> from the input range in combination with <strong><code>std::partition</code></strong> to segment the input range around the pivot. </li>\n<li>This guards against almost sorted inputs for which the complexity would otherwise deteriorate to <code>O(N^2)</code> and  would also weaken the requirements to the advertised <strong>forward iterators</strong> (since C++11, before that it was bidirectional iterators) instead of random access iterators that are imposed by <code>std::nth_element</code>.</li>\n<li><a href=\"http://www.stepanovpapers.com/notes.pdf\"><strong>3-way partitioning</strong></a> (separating elements smaller than, equal to and larger than the pivot) version should be used instead of the 2-way partitioning code (keeping elements equal to and larger than the pivot together). The latter exhibits poor locality, and, critically, exhibits <code>O(N^2)</code> time when there are few unique keys. </li>\n</ul>\n\n<h2>Merge sort</h2>\n\n<p>If using <code>O(N)</code> extra space is of no concern, then <a href=\"http://en.wikipedia.org/wiki/Merge_sort\"><strong>merge sort</strong></a> is an excellent choice: it is the only <strong>stable</strong> <code>O(N log N)</code> sorting algorithm. </p>\n\n<p>It is simple to implement using Standard algorithms: use a few iterator utilities to locate the middle of the input range <code>[first, last)</code> and combine two recursively sorted segments with a <code>std::inplace_merge</code>:</p>\n\n<pre><code>template&lt;class BiDirIt, class Compare = std::less&lt;&gt;&gt;\nvoid merge_sort(BiDirIt first, BiDirIt last, Compare cmp = Compare{})\n{\n    auto const N = std::distance(first, last);\n    if (N &lt;= 1) return;                   \n    auto const middle = std::next(first, N / 2);\n    merge_sort(first, middle, cmp); // assert(std::is_sorted(first, middle, cmp));\n    merge_sort(middle, last, cmp);  // assert(std::is_sorted(middle, last, cmp));\n    std::inplace_merge(first, middle, last, cmp); // assert(std::is_sorted(first, last, cmp));\n}\n</code></pre>\n\n<p>Merge sort requires bidirectional iterators, the bottleneck being the <code>std::inplace_merge</code>. Note that when sorting linked lists, merge sort requires only <code>O(log N)</code> extra space (for recursion). The latter algorithm is implemented by <code>std::list&lt;T&gt;::sort</code> in the Standard Library. </p>\n\n<h2>Heap sort</h2>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Heapsort\"><strong>Heap sort</strong></a> is simple to implement, performs an <code>O(N log N)</code> in-place sort, but is not stable.</p>\n\n<p>The first loop, <code>O(N)</code> \"heapify\" phase, puts the array into heap order. The second loop, the <code>O(N log N</code>) \"sortdown\" phase, repeatedly extracts the maximum and restores heap order. The Standard Library makes this extremely straightforward:</p>\n\n<pre><code>template&lt;class RandomIt, class Compare = std::less&lt;&gt;&gt;\nvoid heap_sort(RandomIt first, RandomIt last, Compare cmp = Compare{})\n{\n    lib::make_heap(first, last, cmp); // assert(std::is_heap(first, last, cmp));\n    lib::sort_heap(first, last, cmp); // assert(std::is_sorted(first, last, cmp));\n}\n</code></pre>\n\n<p>In case you consider it \"cheating\" to use <code>std::make_heap</code> and <code>std::sort_heap</code>, you can go one level deeper and write those functions yourself in terms of <code>std::push_heap</code> and <code>std::pop_heap</code>, respectively:</p>\n\n<pre><code>namespace lib {\n\n// NOTE: is O(N log N), not O(N) as std::make_heap\ntemplate&lt;class RandomIt, class Compare = std::less&lt;&gt;&gt;\nvoid make_heap(RandomIt first, RandomIt last, Compare cmp = Compare{})\n{\n    for (auto it = first; it != last;) {\n        std::push_heap(first, ++it, cmp); \n        assert(std::is_heap(first, it, cmp));           \n    }\n}\n\ntemplate&lt;class RandomIt, class Compare = std::less&lt;&gt;&gt;\nvoid sort_heap(RandomIt first, RandomIt last, Compare cmp = Compare{})\n{\n    for (auto it = last; it != first;) {\n        std::pop_heap(first, it--, cmp);\n        assert(std::is_heap(first, it, cmp));           \n    } \n}\n\n}   // namespace lib\n</code></pre>\n\n<p>The Standard Library specifies both <code>push_heap</code> and <code>pop_heap</code> as complexity <code>O(log N)</code>. Note however that the outer loop over the range <code>[first, last)</code> results in <code>O(N log N)</code> complexity for <code>make_heap</code>, whereas <code>std::make_heap</code> has only <code>O(N)</code> complexity. For the overall <code>O(N log N)</code> complexity of <code>heap_sort</code> it doesn't matter.</p>\n\n<p><strong>Details omitted</strong>: <a href=\"http://stackoverflow.com/q/6299859/819272\"><strong><code>O(N)</code> implementation of <code>make_heap</code></strong></a></p>\n\n<h2>Testing</h2>\n\n<p>Here is are four <strong>Live Examples</strong> (<a href=\"http://coliru.stacked-crooked.com/a/23824290a0a450e2\"><strong>C++14</strong></a>, <a href=\"http://coliru.stacked-crooked.com/a/cccf2b841a452c85\"><strong>C++11</strong></a>, <a href=\"http://coliru.stacked-crooked.com/a/4182baaf37736575\"><strong>C++98 and Boost</strong></a>, <a href=\"http://coliru.stacked-crooked.com/a/89d11c21a76d8351\"><strong>C++98</strong></a>) testing all five algorithms on a variety of inputs (not meant to be exhaustive or rigorous). Just note the huge differences in the LOC: C++11/C++14 need around 120 LOC, C++98 and Boost 180 (+50%) and C++98 more than +100% (note that heap sort could not even be done in terms of standard algorithms).</p>\n    "},{"t":"How to generate all permutations of a list in Python","l":"http://stackoverflow.com/questions/104420/how-to-generate-all-permutations-of-a-list-in-python","q":"\n\n<p>How do you generate all the permutations of a list in Python, independently of the type of elements in that list?</p>\n\n<p>For example:</p>\n\n<pre><code>permutations ([])\n[]\n\npermutations ([1,])\n[1]\n\npermutations ([1,2])\n[1, 2]\n[2, 1]\n\npermutations ([1,2,3])\n[1, 2, 3]\n[1, 3, 2]\n[2, 1, 3]\n[2, 3, 1]\n[3, 1, 2]\n[3, 2, 1]\n</code></pre>\n\n<p>EDIT:\nEliben pointed to a solution that's similar to mine although simpler, so I'm choosing it as the accepted answer, although Python 2.6+ has a builtin solution in the <strong>itertools</strong> module:</p>\n\n<pre><code>import itertools\nitertools.permutations([1,2,3])\n</code></pre>\n    ","a":"\n<p><strong>Starting with Python 2.6</strong> (and if you're on Python 3) you have a <strong>standard-library</strong> tool for this: <a href=\"https://docs.python.org/2/library/itertools.html#itertools.permutations\"><code>itertools.permutations</code></a>.</p>\n\n<hr>\n\n<p>If you're using an <strong>older Python (&lt;2.6)</strong> for some reason or are just curious to know how it works, here's one nice approach, taken from  <a href=\"http://code.activestate.com/recipes/252178/\">http://code.activestate.com/recipes/252178/</a>:</p>\n\n<pre><code>def all_perms(elements):\n    if len(elements) &lt;=1:\n        yield elements\n    else:\n        for perm in all_perms(elements[1:]):\n            for i in range(len(elements)):\n                # nb elements[0:1] works in both string and list contexts\n                yield perm[:i] + elements[0:1] + perm[i:]\n</code></pre>\n\n<p>A couple of alternative approaches are listed in the documentation of <code>itertools.permutations</code>. Here's one:</p>\n\n<pre><code>def permutations(iterable, r=None):\n    # permutations('ABCD', 2) --&gt; AB AC AD BA BC BD CA CB CD DA DB DC\n    # permutations(range(3)) --&gt; 012 021 102 120 201 210\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    if r &gt; n:\n        return\n    indices = range(n)\n    cycles = range(n, n-r, -1)\n    yield tuple(pool[i] for i in indices[:r])\n    while n:\n        for i in reversed(range(r)):\n            cycles[i] -= 1\n            if cycles[i] == 0:\n                indices[i:] = indices[i+1:] + indices[i:i+1]\n                cycles[i] = n - i\n            else:\n                j = cycles[i]\n                indices[i], indices[-j] = indices[-j], indices[i]\n                yield tuple(pool[i] for i in indices[:r])\n                break\n        else:\n            return\n</code></pre>\n\n<p>And another, based on <code>itertools.product</code>:</p>\n\n<pre><code>def permutations(iterable, r=None):\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    for indices in product(range(n), repeat=r):\n        if len(set(indices)) == r:\n            yield tuple(pool[i] for i in indices)\n</code></pre>\n    "},{"t":"Best way to reverse a string","l":"http://stackoverflow.com/questions/228038/best-way-to-reverse-a-string","q":"\n\n<p>I've just had to write a string reverse function in C# 2.0 (i.e. LINQ not available) and came up with this:</p>\n\n<pre><code>public string Reverse(string text)\n{\n    char[] cArray = text.ToCharArray();\n    string reverse = String.Empty;\n    for (int i = cArray.Length - 1; i &gt; -1; i--)\n    {\n        reverse += cArray[i];\n    }\n    return reverse;\n}\n</code></pre>\n\n<p>Personally I'm not crazy about the function and am convinced that there's a better way to do it. Is there?</p>\n    ","a":"\n<pre><code>public static string Reverse( string s )\n{\n    char[] charArray = s.ToCharArray();\n    Array.Reverse( charArray );\n    return new string( charArray );\n}\n</code></pre>\n\n<p>I think the above works not tested, although the stringbuilder class may also have a reverse function I haven't checked that though.</p>\n    "},{"t":"Best algorithm for detecting cycles in a directed graph","l":"http://stackoverflow.com/questions/261573/best-algorithm-for-detecting-cycles-in-a-directed-graph","q":"\n\n<p>What is the most efficient algorithm for detecting all cycles within a directed graph?</p>\n\n<p>I have a directed graph representing a schedule of jobs that need to be executed, a job being a node and a dependency being an edge. I need to detect the error case of a cycle within this graph leading to cyclic dependencies.</p>\n    ","a":"\n<p><a href=\"http://en.wikipedia.org/wiki/Tarjan%E2%80%99s_strongly_connected_components_algorithm\" rel=\"nofollow\">Tarjan's strongly connected components algorithm</a> has <code>O(|E| + |V|)</code> time complexity.</p>\n\n<p>For other algorithms, see <a href=\"http://en.wikipedia.org/wiki/Strongly_connected_components\" rel=\"nofollow\">Strongly connected components</a> on Wikipedia.</p>\n    "},{"t":"Determine if two rectangles overlap each other?","l":"http://stackoverflow.com/questions/306316/determine-if-two-rectangles-overlap-each-other","q":"\n\n<p>I am trying to write a C++ program that takes the following inputs from the user to construct rectangles (between 2 and 5): height, width, x-pos, y-pos. All of these rectangles will exist parallel to the x and the y axis, that is all of their edges will have slopes of 0 or infinity.</p>\n\n<p>I've tried to implement what is mentioned in <a href=\"http://stackoverflow.com/questions/115426\">this</a> question but I am not having very much luck.</p>\n\n<p>My current implementation does the following:</p>\n\n<pre><code>// Gets all the vertices for Rectangle 1 and stores them in an array -&gt; arrRect1\n// point 1 x: arrRect1[0], point 1 y: arrRect1[1] and so on...\n// Gets all the vertices for Rectangle 2 and stores them in an array -&gt; arrRect2\n\n// rotated edge of point a, rect 1\nint rot_x, rot_y;\nrot_x = -arrRect1[3];\nrot_y = arrRect1[2];\n// point on rotated edge\nint pnt_x, pnt_y;\npnt_x = arrRect1[2]; \npnt_y = arrRect1[3];\n// test point, a from rect 2\nint tst_x, tst_y;\ntst_x = arrRect2[0];\ntst_y = arrRect2[1];\n\nint value;\nvalue = (rot_x * (tst_x - pnt_x)) + (rot_y * (tst_y - pnt_y));\ncout &lt;&lt; \"Value: \" &lt;&lt; value;\n</code></pre>\n\n<p>However I'm not quite sure if (a) I've implemented the algorithm I linked to correctly, or if I did exactly how to interpret this?</p>\n\n<p>Any suggestions?</p>\n    ","a":"\n<pre><code>if (RectA.X1 &lt; RectB.X2 &amp;&amp; RectA.X2 &gt; RectB.X1 &amp;&amp;\n    RectA.Y1 &lt; RectB.Y2 &amp;&amp; RectA.Y2 &gt; RectB.Y1) \n</code></pre>\n\n<p>Say you have Rect A, and Rect B. \nProof is by contradiction. Any one of four conditions guarantees that <strong>no overlap can exist</strong>:</p>\n\n<ul>\n<li>Cond1.  If A's left edge is to the right of the B's right edge,\n       -  then A is Totally to right Of B</li>\n<li>Cond2.  If A's right edge is to the left of the B's left edge,\n       -  then A is Totally to left Of B</li>\n<li>Cond3.  If A's top edge is below B's bottom  edge,\n       -  then A is Totally below B</li>\n<li>Cond4.  If A's bottom edge is above B's top edge,\n       -  then A is Totally above B</li>\n</ul>\n\n<p>So condition for Non-Overlap is </p>\n\n<pre>Cond1 Or Cond2 Or Cond3 Or Cond4</pre>\n\n<p>Therefore, a sufficient condition for Overlap is the opposite (De Morgan)</p>\n\n<pre>Not Cond1 And Not Cond2 And Not Cond3 And Not Cond4</pre>\n\n<p>This is equivalent to:</p>\n\n<ul>\n<li>A's Left Edge to left of B's right edge, and</li>\n<li>A's right edge to right of B's left edge, and</li>\n<li>A's top above B's bottom, and</li>\n<li>A's bottom below B's Top</li>\n</ul>\n\n<p><strong>Note 1</strong>:  It is fairly obvious this same principle can be extended to any number of dimensions.<br>\n<strong>Note 2</strong>:  It should also be fairly obvious to count overlaps of just one pixel, change the <code>&lt;</code> and/or the <code>&gt;</code> on that boundary to a <code>&lt;=</code> or a <code>&gt;=</code>.</p>\n    "},{"t":"What is the fastest way to get the value of π?","l":"http://stackoverflow.com/questions/19/what-is-the-fastest-way-to-get-the-value-of-%cf%80","q":"\n\n<p>Solutions are welcome in any language. :-) I'm looking for the fastest way to obtain the value of π, as a personal challenge. More specifically I'm using ways that don't involve using <code>#define</code>d constants like <code>M_PI</code>, or hard-coding the number in.</p>\n\n<p>The program below tests the various ways I know of. The inline assembly version is, in theory, the fastest option, though clearly not portable; I've included it as a baseline to compare the other versions against. In my tests, with built-ins, the <code>4 * atan(1)</code> version is fastest on GCC 4.2, because it auto-folds the <code>atan(1)</code> into a constant. With <code>-fno-builtin</code> specified, the <code>atan2(0, -1)</code> version is fastest.</p>\n\n<p>Here's the main testing program (<code>pitimes.c</code>):</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n\n#define ITERS 10000000\n#define TESTWITH(x) {                                                       \\\n    diff = 0.0;                                                             \\\n    time1 = clock();                                                        \\\n    for (i = 0; i &lt; ITERS; ++i)                                             \\\n        diff += (x) - M_PI;                                                 \\\n    time2 = clock();                                                        \\\n    printf(\"%s\\t=&gt; %e, time =&gt; %f\\n\", #x, diff, diffclock(time2, time1));   \\\n}\n\nstatic inline double\ndiffclock(clock_t time1, clock_t time0)\n{\n    return (double) (time1 - time0) / CLOCKS_PER_SEC;\n}\n\nint\nmain()\n{\n    int i;\n    clock_t time1, time2;\n    double diff;\n\n    /* Warmup. The atan2 case catches GCC's atan folding (which would\n     * optimise the ``4 * atan(1) - M_PI'' to a no-op), if -fno-builtin\n     * is not used. */\n    TESTWITH(4 * atan(1))\n    TESTWITH(4 * atan2(1, 1))\n\n#if defined(__GNUC__) &amp;&amp; (defined(__i386__) || defined(__amd64__))\n    extern double fldpi();\n    TESTWITH(fldpi())\n#endif\n\n    /* Actual tests start here. */\n    TESTWITH(atan2(0, -1))\n    TESTWITH(acos(-1))\n    TESTWITH(2 * asin(1))\n    TESTWITH(4 * atan2(1, 1))\n    TESTWITH(4 * atan(1))\n\n    return 0;\n}\n</code></pre>\n\n<p>And the inline assembly stuff (<code>fldpi.c</code>), noting that it will only work for x86 and x64 systems:</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>double\nfldpi()\n{\n    double pi;\n    asm(\"fldpi\" : \"=t\" (pi));\n    return pi;\n}\n</code></pre>\n\n<p>And a build script that builds all the configurations I'm testing (<code>build.sh</code>):</p>\n\n<pre><code>#!/bin/sh\ngcc -O3 -Wall -c           -m32 -o fldpi-32.o fldpi.c\ngcc -O3 -Wall -c           -m64 -o fldpi-64.o fldpi.c\n\ngcc -O3 -Wall -ffast-math  -m32 -o pitimes1-32 pitimes.c fldpi-32.o\ngcc -O3 -Wall              -m32 -o pitimes2-32 pitimes.c fldpi-32.o -lm\ngcc -O3 -Wall -fno-builtin -m32 -o pitimes3-32 pitimes.c fldpi-32.o -lm\ngcc -O3 -Wall -ffast-math  -m64 -o pitimes1-64 pitimes.c fldpi-64.o -lm\ngcc -O3 -Wall              -m64 -o pitimes2-64 pitimes.c fldpi-64.o -lm\ngcc -O3 -Wall -fno-builtin -m64 -o pitimes3-64 pitimes.c fldpi-64.o -lm\n</code></pre>\n\n<p>Apart from testing between various compiler flags (I've compared 32-bit against 64-bit too, because the optimisations are different), I've also tried switching the order of the tests around. The <code>atan2(0, -1)</code> version still comes out top every time, though.</p>\n    ","a":"\n<p>The <a href=\"http://en.wikipedia.org/wiki/Monte_Carlo_method\">Monte Carlo method</a>, as mentioned, applies some great concepts but it is, clearly, not the fastest --not by a long shot, not by any reasonable usefulness. Also, it all depends on what kind of accuracy you are looking for. The fastest pi I know of is the digits hard coded. Looking at <a href=\"http://functions.wolfram.com/Constants/Pi/\">Pi</a> and <a href=\"http://functions.wolfram.com/PDF/Pi.pdf\">Pi[PDF]</a>, there are a lot of formulas.</p>\n\n<p>Here is a method that converges quickly (~14digits per iteration). The current fastest application, <a href=\"http://numbers.computation.free.fr/Constants/PiProgram/pifast.html\">PiFast</a>, uses this formula with the <a href=\"http://www.ele.uri.edu/~hansenj/projects/ele436/fft.pdf\">FFT</a>. I'll just write the formula, since the code is straight forward. This formula was almost found by <a href=\"http://numbers.computation.free.fr/Constants/Pi/piramanujan.html\">Ramanujan and discovered by Chudnovsky</a>. It is actually how he calculated several billion digits of the number --so it isn't a method to disregard. The formula will overflow quickly since we are dividing factorials, it would be advantageous then to delay such calculating to remove terms.</p>\n\n<p><img src=\"http://i.stack.imgur.com/aQMkk.gif\" alt=\"enter image description here\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/2y2l9.gif\" alt=\"enter image description here\"></p>\n\n<p>where,</p>\n\n<p><img src=\"http://i.stack.imgur.com/QqVnB.gif\" alt=\"enter image description here\"></p>\n\n<p>Below is the <a href=\"http://mathworld.wolfram.com/Brent-SalaminFormula.html\">Brent–Salamin algorithm</a>. Wikipedia mentions that when a and b are 'close enough' then (a+b)^2/4t will be an approximation of pi. I'm not sure what 'close enough' means, but from my tests, one iteration got 2digits, two got 7, and three had 15, of course this is with doubles, so it might have error based on it's representation and the 'true' calculation could be more accurate.</p>\n\n<pre><code>let pi_2 iters =\n    let rec loop_ a b t p i =\n        if i = 0 then a,b,t,p\n        else\n            let a_n = (a +. b) /. 2.0 \n            and b_n = sqrt (a*.b)\n            and p_n = 2.0 *. p in\n            let t_n = t -. (p *. (a -. a_n) *. (a -. a_n)) in\n            loop_ a_n b_n t_n p_n (i - 1)\n    in \n    let a,b,t,p = loop_ (1.0) (1.0 /. (sqrt 2.0)) (1.0/.4.0) (1.0) iters in\n    (a +. b) *. (a +. b) /. (4.0 *. t)\n</code></pre>\n\n<p>Lastly, how about some pi golf (800 digits)? 160 characters!</p>\n\n<pre><code>int a=10000,b,c=2800,d,e,f[2801],g;main(){for(;b-c;)f[b++]=a/5;for(;d=0,g=c*2;c-=14,printf(\"%.4d\",e+d/a),e=d%a)for(b=c;d+=f[b]*a,f[b]=d%--g,d/=g--,--b;d*=b);}\n</code></pre>\n    "},{"t":"Equation for testing if a point is inside a circle","l":"http://stackoverflow.com/questions/481144/equation-for-testing-if-a-point-is-inside-a-circle","q":"\n\n<p>If you have  a circle with center <code>(center_x, center_y)</code> and radius <code>radius</code>, how do you test if a given point with coordinates <code>(x, y)</code> is inside the circle?</p>\n    ","a":"\n<p>In general, <code>x</code> and <code>y</code> must satisfy <code>(x - center_x)^2 + (y - center_y)^2 &lt; radius^2</code>.</p>\n\n<p>Please note that points that satisfy the above equation with <code>&lt;</code> replaced by <code>==</code> are considered the points <em>on</em> the circle, and the points that satisfy the above equation with <code>&lt;</code> replaced by <code>&gt;</code> are considered the <em>outside</em> the circle.</p>\n    "},{"t":"Algorithm to randomly generate an aesthetically-pleasing color palette","l":"http://stackoverflow.com/questions/43044/algorithm-to-randomly-generate-an-aesthetically-pleasing-color-palette","q":"\n\n<p>I'm looking for a simple algorithm to generate a large number of random, aesthetically pleasing colors. So no crazy neon colors, colors reminiscent of feces, etc. </p>\n\n<p>I've found solutions to this problem but they rely on alternative color palettes than RGB.\nI would rather just use straight RGB than mapping back and forth. These other solutions also can at most generate only 32 or so pleasing random colors. </p>\n\n<p>Any ideas would be great.</p>\n    ","a":"\n<p>You could average the RGB values of random colors with those of a constant color:</p>\n\n<p><em>(example in Java)</em></p>\n\n<pre><code>public Color generateRandomColor(Color mix) {\n    Random random = new Random();\n    int red = random.nextInt(256);\n    int green = random.nextInt(256);\n    int blue = random.nextInt(256);\n\n    // mix the color\n    if (mix != null) {\n        red = (red + mix.getRed()) / 2;\n        green = (green + mix.getGreen()) / 2;\n        blue = (blue + mix.getBlue()) / 2;\n    }\n\n    Color color = new Color(red, green, blue);\n    return color;\n}\n</code></pre>\n\n<p><br>\nMixing random colors with white (255, 255, 255) creates neutral pastels by increasing the lightness while keeping the hue of the original color. These randomly generated pastels usually go well together, especially in large numbers.</p>\n\n<p>Here are some pastel colors generated using the above method:</p>\n\n<p><img src=\"http://i.stack.imgur.com/8jKGx.jpg\" alt=\"First\"></p>\n\n<p><br>\nYou could also mix the random color with a constant pastel, which results in a tinted set of neutral colors. For example, using a light blue creates colors like these:</p>\n\n<p><img src=\"http://i.stack.imgur.com/zI406.jpg\" alt=\"Second\"></p>\n\n<p><br>\nGoing further, you could add heuristics to your generator that take into account complementary colors or levels of shading, but it all depends on the impression you want to achieve with your random colors.</p>\n\n<p>Some additional resources:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Color_theory\">http://en.wikipedia.org/wiki/Color_theory</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Complementary_color\">http://en.wikipedia.org/wiki/Complementary_color</a></li>\n</ul>\n    "},{"t":"Given a number, find the next higher number which has the exact same set of digits as the original number","l":"http://stackoverflow.com/questions/9368205/given-a-number-find-the-next-higher-number-which-has-the-exact-same-set-of-digi","q":"\n\n<p>I just bombed an interview and made pretty much zero progress on my interview question. Can anyone let me know how to do this? I tried searching online but couldn't find anything:</p>\n\n<blockquote>\n  <p>Given a number, find the next higher number which has the exact same\n  set of digits as the original number. For example: given 38276 return\n  38627</p>\n</blockquote>\n\n<p>I wanted to begin by finding the index of the first digit (from the right) that was less than the ones digit. Then I would rotate the last digits in the subset such that it was the next biggest number comprised of the same digits, but got stuck.</p>\n\n<p>The interviewer also suggested trying to swap digits one at a time, but I couldn't figure out the algorithm and just stared at a screen for like 20-30 minutes. Needless to say, I think I'm going to have to continue the job hunt.</p>\n\n<p>edit: for what its worth, I was invited to the next round of interviews</p>\n    ","a":"\n<p>You can do it in <code>O(n)</code> (where <code>n</code> is the number of digits) like this:</p>\n\n<p>Starting from the right, you find the first pair-of-digits such that the left-digit is smaller than the right-digit.  Let's refer to the left-digit by \"digit-x\".  Find the smallest number larger than digit-x to the right of digit-x, and place it immediately left of digit-x.  Finally, sort the remaining digits in ascending order - since they were already in <em>descending</em> order, all you need to do is reverse them <em>(save for digit-x, which can be placed in the correct place in <code>O(n)</code>)</em>.</p>\n\n<p>An example will make this more clear:</p>\n\n<pre>123456784987654321\nstart with a number\n\n123456784 987654321\n         ^the first place where the left-digit is less-than the right-digit is here.  \n         Digit \"x\" is 4\n\n123456784 987654321\n              ^find the smallest digit larger than 4 to the right\n\n123456785 4 98764321\n        ^place it to the left of 4\n\n123456785 4 12346789\n123456785123446789\n         ^sort the digits to the right of 5.  Since all of them except \n         the '4' were already in descending order, all we need to do is \n         reverse their order, and find the correct place for the '4'\n</pre>\n    "},{"t":"Throwing the fattest people off of an overloaded airplane.","l":"http://stackoverflow.com/questions/7746648/throwing-the-fattest-people-off-of-an-overloaded-airplane","q":"\n\n<p>Let's say you've got an airplane, and it is low on fuel.  Unless the plane drops 3000 pounds of passenger weight, it will not be able to reach the next airport.  To save the maximum number of lives, we would like to throw the heaviest people off of the plane first.  </p>\n\n<p>And oh yeah, there are millions of people on the airplane, and we would like an optimal algorithm to find the heaviest passengers, without necessarily sorting the entire list.</p>\n\n<p>This is a proxy problem for something I'm trying to code in C++.  I would like to do a \"partial_sort\" on the passenger manifest by weight, but I don't know how many elements I'm going to need.  I could implement my own \"partial_sort\" algorithm (\"partial_sort_accumulate_until\"), but I'm wondering if there's any easier way to do this using standard STL.</p>\n    ","a":"\n<p>One way would be to use a <a href=\"http://en.wikipedia.org/wiki/Binary_heap\">min heap</a> (<a href=\"http://en.cppreference.com/w/cpp/container/priority_queue\"><code>std::priority_queue</code></a> in C++). Here's how you'd do it, assuming you had a <code>MinHeap</code> class.  (Yes, my example is in C#. I think you get the idea.)</p>\n\n<pre><code>int targetTotal = 3000;\nint totalWeight = 0;\n// this creates an empty heap!\nvar myHeap = new MinHeap&lt;Passenger&gt;(/* need comparer here to order by weight */);\nforeach (var pass in passengers)\n{\n    if (totalWeight &lt; targetTotal)\n    {\n        // unconditionally add this passenger\n        myHeap.Add(pass);\n        totalWeight += pass.Weight;\n    }\n    else if (pass.Weight &gt; myHeap.Peek().Weight)\n    {\n        // If this passenger is heavier than the lightest\n        // passenger already on the heap,\n        // then remove the lightest passenger and add this one\n        var oldPass = myHeap.RemoveFirst();\n        totalWeight -= oldPass.Weight;\n        myHeap.Add(pass);\n        totalWeight += pass.Weight;\n    }\n}\n\n// At this point, the heaviest people are on the heap,\n// but there might be too many of them.\n// Remove the lighter people until we have the minimum necessary\nwhile ((totalWeight - myHeap.Peek().Weight) &gt; targetTotal)\n{\n    var oldPass = myHeap.RemoveFirst();\n    totalWeight -= oldPass.Weight; \n}\n// The heap now contains the passengers who will be thrown overboard.\n</code></pre>\n\n<p>According to the standard references, running time should be proportional to <code>n log k</code>, where <code>n</code> is the number of passengers and <code>k</code> is the maximum number of items on the heap. If we assume that passengers' weights will typically be 100 lbs or more, then it's unlikely that the heap will contain more than 30 items at any time.</p>\n\n<p>The worst case would be if the passengers are presented in order from lowest weight to highest. That would require that every passenger be added to the heap, and every passenger be removed from the heap. Still, with a million passengers and assuming that the lightest weighs 100 lbs, the <code>n log k</code> works out to a reasonably small number.</p>\n\n<p>If you get the passengers' weights randomly, performance is much better. I use something quite like this for a recommendation engine (I select the top 200 items from a list of several million). I typically end up with only 50,000 or 70,000 items actually added to the heap.</p>\n\n<p>I suspect that you'll see something quite similar: the majority of your candidates will be rejected because they're lighter than the lightest person already on the heap. And <code>Peek</code> is an <code>O(1)</code> operation.</p>\n\n<p>For a more information about the performance of heap select and quick select, see <a href=\"http://blog.mischel.com/2011/10/25/when-theory-meets-practice/\">When theory meets practice</a>. Short version: if you're selecting fewer than 1% of the total number of items, then heap select is a clear winner over quick select. More than 1%, then use quick select or a variant like <a href=\"http://en.wikipedia.org/wiki/Selection_algorithm#Introselect\">Introselect</a>.</p>\n    "},{"t":"List of Big-O for PHP functions","l":"http://stackoverflow.com/questions/2473989/list-of-big-o-for-php-functions","q":"\n\n<p>After using PHP for a while now, I've noticed that not all PHP built in functions as fast as expected. Consider the below two possible implementations of a function that finds if a number is prime using a cached array of primes.</p>\n\n<pre><code>//very slow for large $prime_array\n$prime_array = array( 2, 3, 5, 7, 11, 13, .... 104729, ... );\n$result_array = array();\nforeach( $array_of_number =&gt; $number ) {\n    $result_array[$number] = in_array( $number, $large_prime_array );\n}\n\n//speed is much less dependent on size of $prime_array, and runs much faster.\n$prime_array =&gt; array( 2 =&gt; NULL, 3 =&gt; NULL, 5 =&gt; NULL, 7 =&gt; NULL,\n                       11 =&gt; NULL, 13 =&gt; NULL, .... 104729 =&gt; NULL, ... );\nforeach( $array_of_number =&gt; $number ) {\n    $result_array[$number] = array_key_exists( $number, $large_prime_array );\n}\n</code></pre>\n\n<p>This is because in_array is implemented with a linear search O(n) which will linearly slow down as <code>$prime_array</code> grows. Where the <code>array_key_exists</code> function is implemented with a hash lookup O(1) which will not slow down unless the hash table gets extremely populated (in which case it's only O(n)).</p>\n\n<p>So far I've had to discover the big-O's via trial and error, and occasionally <a href=\"http://stackoverflow.com/questions/2350361/how-is-the-php-array-implemented-on-the-c-level\">looking at the source code</a>. Now for the question...</p>\n\n<p><strong>Is there was a list of the theoretical (or practical) big O times for all* the PHP built in functions?</strong></p>\n\n<p>*or at least the interesting ones</p>\n\n<p>For example find it very hard to predict what the big O of functions listed because the possible implementation depends on unknown core data structures of PHP: array_merge, array_merge_recursive, array_reverse, array_intersect, array_combine, str_replace (with array inputs), etc.</p>\n    ","a":"\n<p>Since it doesn't seem like anyone has done this before I thought it'd be good idea to have it for reference somewhere. I've gone though and either via benchmark or code-skimming to characterize the <code>array_*</code> functions. I've tried to put the more interesting Big-O near the top. This list is not complete.</p>\n\n<p>Note: All the Big-O where calculated assuming a hash lookup is O(1) even though it's really O(n). The coefficient of the n is so low, the ram overhead of storing a large enough array would hurt you before the characteristics of lookup Big-O would start taking effect. For example the difference between a call to <code>array_key_exists</code> at N=1 and N=1,000,000 is ~50% time increase.</p>\n\n<p><strong>Interesting Points</strong>:</p>\n\n<ol>\n<li><code>isset</code>/<code>array_key_exists</code> is much faster than <code>in_array</code> and <code>array_search</code></li>\n<li><code>+</code>(union) is a bit faster than <code>array_merge</code> (and looks nicer). But it does work differently so keep that in mind.</li>\n<li><code>shuffle</code> is on the same Big-O tier as <code>array_rand</code></li>\n<li><code>array_pop</code>/<code>array_push</code> is faster than <code>array_shift</code>/<code>array_unshift</code> due to re-index penalty</li>\n</ol>\n\n<p><strong>Lookups</strong>:</p>\n\n<p><code>array_key_exists</code> O(n) but really close to O(1) - this is because of linear polling in collisions, but because the chance of collisions is very small, the coefficient is also very small. I find you treat hash lookups as O(1) to give a more realistic big-O. For example the different between N=1000 and N=100000 is only about 50% slow down.</p>\n\n<p><code>isset( $array[$index] )</code> O(n) but really close to O(1) - it uses the same lookup as array_key_exists. Since it's language construct, will cache the lookup if the key is hardcoded, resulting in speed up in cases where the same key is used repeatedly. </p>\n\n<p><code>in_array</code> O(n) - this is because it does a linear search though the array until it finds the value.</p>\n\n<p><code>array_search</code> O(n) - it uses the same core function as in_array but returns value.</p>\n\n<p><strong>Queue functions</strong>:</p>\n\n<p><code>array_push</code> O(∑ var_i, for all i)</p>\n\n<p><code>array_pop</code> O(1) </p>\n\n<p><code>array_shift</code> O(n) - it has to reindex all the keys</p>\n\n<p><code>array_unshift</code> O(n + ∑ var_i, for all i) - it has to reindex all the keys</p>\n\n<p><strong>Array Intersection, Union, Subtraction</strong>:</p>\n\n<p><code>array_intersect_key</code> if intersection 100% do O(Max(param_i_size)*∑param_i_count, for all i), if intersection 0% intersect O(∑param_i_size, for all i)</p>\n\n<p><code>array_intersect</code> if intersection 100% do O(n^2*∑param_i_count, for all i), if intersection 0% intersect O(n^2)</p>\n\n<p><code>array_intersect_assoc</code> if intersection 100% do O(Max(param_i_size)*∑param_i_count, for all i), if intersection 0% intersect O(∑param_i_size, for all i)</p>\n\n<p><code>array_diff</code> O(π param_i_size, for all i) - That's product of all the param_sizes</p>\n\n<p><code>array_diff_key</code> O(∑ param_i_size, for i != 1) - this is because we don't need to iterate over the first array.</p>\n\n<p><code>array_merge</code> O( ∑ array_i, i != 1 ) - doesn't need to iterate over the first array</p>\n\n<p><code>+</code> (union) O(n), where n is size of the 2nd array (ie array_first + array_second) - less overhead than array_merge since it doesn't have to renumber</p>\n\n<p><code>array_replace</code> O( ∑ array_i, for all i )</p>\n\n<p><strong>Random</strong>:</p>\n\n<p><code>shuffle</code> O(n)</p>\n\n<p><code>array_rand</code> O(n) - Requires a linear poll.</p>\n\n<p><strong>Obvious Big-O</strong>:</p>\n\n<p><code>array_fill</code> O(n)</p>\n\n<p><code>array_fill_keys</code> O(n)</p>\n\n<p><code>range</code> O(n)</p>\n\n<p><code>array_splice</code> O(offset + length)</p>\n\n<p><code>array_slice</code> O(offset + length) or O(n) if length = NULL</p>\n\n<p><code>array_keys</code> O(n)</p>\n\n<p><code>array_values</code> O(n)</p>\n\n<p><code>array_reverse</code> O(n)</p>\n\n<p><code>array_pad</code> O(pad_size)</p>\n\n<p><code>array_flip</code> O(n)</p>\n\n<p><code>array_sum</code> O(n)</p>\n\n<p><code>array_product</code> O(n)</p>\n\n<p><code>array_reduce</code> O(n)</p>\n\n<p><code>array_filter</code> O(n)</p>\n\n<p><code>array_map</code> O(n)</p>\n\n<p><code>array_chunk</code> O(n)   </p>\n\n<p><code>array_combine</code> O(n)</p>\n\n<p>I'd like to thank <a href=\"http://ccsl.mae.cornell.edu/eureqa\">Eureqa</a> for making it easy to find the Big-O of the functions. It's an amazing <em>free</em> program that can find the best fitting function for arbitrary data.</p>\n\n<p>EDIT:</p>\n\n<p>For those who doubt that PHP array lookups are <code>O(N)</code>, I've written a benchmark to test that (they are still effectively <code>O(1)</code> for most realistic values).</p>\n\n<p><img src=\"http://i.stack.imgur.com/PgRQd.png\" alt=\"php array lookup graph\"></p>\n\n<pre><code>$tests = 1000000;\n$max = 5000001;\n\n\nfor( $i = 1; $i &lt;= $max; $i += 10000 ) {\n    //create lookup array\n    $array = array_fill( 0, $i, NULL );\n\n    //build test indexes\n    $test_indexes = array();\n    for( $j = 0; $j &lt; $tests; $j++ ) {\n        $test_indexes[] = rand( 0, $i-1 );\n    }\n\n    //benchmark array lookups\n    $start = microtime( TRUE );\n    foreach( $test_indexes as $test_index ) {\n        $value = $array[ $test_index ];\n        unset( $value );\n    }\n    $stop = microtime( TRUE );\n    unset( $array, $test_indexes, $test_index );\n\n    printf( \"%d,%1.15f\\n\", $i, $stop - $start ); //time per 1mil lookups\n    unset( $stop, $start );\n}\n</code></pre>\n    "},{"t":"A simple explanation of Naive Bayes Classification","l":"http://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification","q":"\n\n<p>I am finding it hard to understand the process of Naive Bayes, and I was wondering if someone could explained it with a simple step by step process in English. I understand it takes comparisons by times occurred as a probability, but I have no idea how the training data is related to the actual dataset.</p>\n\n<p>Please give me an explanation of what role the training set plays. I am giving a very simple example for fruits here, like banana for example</p>\n\n<pre><code>training set---\nround-red\nround-orange\noblong-yellow\nround-red\n\ndataset----\nround-red\nround-orange\nround-red\nround-orange\noblong-yellow\nround-red\nround-orange\noblong-yellow\noblong-yellow\nround-red\n</code></pre>\n    ","a":"\n<p>Your question as I understand is divided in two parts. One being you need more understanding for Naive Bayes classifier &amp; second being the confusion surrounding Training set. </p>\n\n<p>In general all of Machine Learning Algorithms need to be trained for supervised learning tasks like classification, prediction etc. or for unsupervised learning tasks like clustering.</p>\n\n<p>By training it means to train them on particular inputs so that later on we may test them for unknown inputs (which they have never seen before) for which they may classify or predict etc (in case of supervised learning) based on their learning. This is what most of the Machine Learning techniques like Neural Networks, SVM, Bayesian etc. are based upon.</p>\n\n<p>So in a general Machine Learning project basically you have to divide your input set to a Development Set (Training Set + Dev-Test Set) &amp; a Test Set (or Evaluation set). Remember your basic objective would be that your system learns and classifies new inputs which they have never seen before in either Dev set or test set.</p>\n\n<p>The test set typically has the same format as the training set. However, it is very important that the test set be distinct from the training corpus: if we simply\nreused the training set as the test set, then a model that simply memorized its input, without learning how to generalize to new examples, would receive misleadingly high scores.</p>\n\n<p>In general, for an example, 70% can be training set cases. Also remember randomly partitioning the training and test sets.</p>\n\n<p>Now I come to your other question about Naive Bayes.</p>\n\n<p><strong>Source for example below</strong>: <a href=\"http://www.statsoft.com/textbook/naive-bayes-classifier\">http://www.statsoft.com/textbook/naive-bayes-classifier</a></p>\n\n<p>To demonstrate the concept of Naïve Bayes Classification, consider the example given below:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Eh6HI.gif\" alt=\"enter image description here\"></p>\n\n<p>As indicated, the objects can be classified as either <code>GREEN</code> or <code>RED</code>. Our task is to classify new cases as they arrive, i.e., decide to which class label they belong, based on the currently exiting objects.</p>\n\n<p>Since there are twice as many <code>GREEN</code> objects as <code>RED</code>, it is reasonable to believe that a new case (which hasn't been observed yet) is twice as likely to have membership <code>GREEN</code> rather than <code>RED</code>. In the Bayesian analysis, this belief is known as the prior probability. Prior probabilities are based on previous experience, in this case the percentage of <code>GREEN</code> and <code>RED</code> objects, and often used to predict outcomes before they actually happen.</p>\n\n<p>Thus, we can write:</p>\n\n<p><strong>Prior Probability of <code>GREEN</code></strong>: <code>number of GREEN objects / total number of objects</code></p>\n\n<p><strong>Prior Probability of <code>RED</code></strong>: <code>number of RED objects / total number of objects</code></p>\n\n<p>Since there is a total of <code>60</code> objects, <code>40</code> of which are <code>GREEN</code> and 20 <code>RED</code>, our prior probabilities for class membership are:</p>\n\n<p><strong>Prior Probability for <code>GREEN</code></strong>: <code>40 / 60</code></p>\n\n<p><strong>Prior Probability for <code>RED</code></strong>: <code>20 / 60</code></p>\n\n<p>Having formulated our prior probability, we are now ready to classify a new object (<code>WHITE</code> circle in the diagram below). Since the objects are well clustered, it is reasonable to assume that the more <code>GREEN</code> (or <code>RED</code>) objects in the vicinity of X, the more likely that the new cases belong to that particular color. To measure this likelihood, we draw a circle around X which encompasses a number (to be chosen a priori) of points irrespective of their class labels. Then we calculate the number of points in the circle belonging to each class label. From this we calculate the likelihood:</p>\n\n<p><img src=\"http://i.stack.imgur.com/gVpJF.gif\" alt=\"enter image description here\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/sh1zX.gif\" alt=\"enter image description here\"></p>\n\n<p>From the illustration above, it is clear that Likelihood of <code>X</code> given <code>GREEN</code> is smaller than Likelihood of <code>X</code> given <code>RED</code>, since the circle encompasses <code>1</code> <code>GREEN</code> object and <code>3</code> <code>RED</code> ones. Thus:</p>\n\n<p><img src=\"http://i.stack.imgur.com/DLCqA.gif\" alt=\"enter image description here\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/cJzst.gif\" alt=\"enter image description here\"></p>\n\n<p>Although the prior probabilities indicate that <code>X</code> may belong to <code>GREEN</code> (given that there are twice as many <code>GREEN</code> compared to <code>RED</code>) the likelihood indicates otherwise; that the class membership of <code>X</code> is <code>RED</code> (given that there are more <code>RED</code> objects in the vicinity of <code>X</code> than <code>GREEN</code>). In the Bayesian analysis, the final classification is produced by combining both sources of information, i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes' rule (named after Rev. Thomas Bayes 1702-1761).</p>\n\n<p><img src=\"http://i.stack.imgur.com/N8MPA.gif\" alt=\"enter image description here\"></p>\n\n<p>Finally, we classify X as <code>RED</code> since its class membership achieves the largest posterior probability.</p>\n    "},{"t":"How to implement a queue using two stacks?","l":"http://stackoverflow.com/questions/69192/how-to-implement-a-queue-using-two-stacks","q":"\n\n<p>Suppose we have two stacks and no other temporary variable.</p>\n\n<p>Is to possible to \"construct\" a queue data structure using only the two stacks?</p>\n    ","a":"\n<p>Keep 2 stacks, let's call them <code>inbox</code> and <code>outbox</code>.</p>\n\n<p><strong>Queue</strong>:<br>\n - Push the new element onto <code>inbox</code></p>\n\n<p><strong>Dequeue</strong>:<br>\n - If <code>outbox</code> is empty, refill it by popping each element from <code>inbox</code> and pushing it onto <code>outbox</code><br>\n - Pop and return the top element from <code>outbox</code></p>\n\n<p>Using this method, each element will be in each stack exactly once - meaning each element will be pushed twice and popped twice, giving amortized constant time operations.</p>\n\n<p>Here's an implementation in Java:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>public class Queue&lt;E&gt;\n{\n\n    private Stack&lt;E&gt; inbox = new Stack&lt;E&gt;();\n    private Stack&lt;E&gt; outbox = new Stack&lt;E&gt;();\n\n    public void queue(E item) {\n        inbox.push(item);\n    }\n\n    public E dequeue() {\n        if (outbox.isEmpty()) {\n            while (!inbox.isEmpty()) {\n               outbox.push(inbox.pop());\n            }\n        }\n        return outbox.pop();\n    }\n\n}\n</code></pre>\n    "},{"t":"What are good examples of genetic algorithms/genetic programming solutions? [closed]","l":"http://stackoverflow.com/questions/1538235/what-are-good-examples-of-genetic-algorithms-genetic-programming-solutions","q":"\n\n<p><a href=\"http://en.wikipedia.org/wiki/Genetic_algorithm\">Genetic algorithms</a> (GA) and <a href=\"http://en.wikipedia.org/wiki/Genetic_programming\">genetic programming</a> (GP) are interesting areas of research. </p>\n\n<p>I'd like to know about specific problems you have solved using GA/GP and what libraries/frameworks you used if you didn't roll your own.</p>\n\n<p>Questions:</p>\n\n<ul>\n<li>What problems have you used GA/GP to solve?</li>\n<li>What libraries/frameworks did you use?</li>\n</ul>\n\n<p>I'm looking for first-hand experiences, so please do not answer unless you have that.</p>\n    ","a":"\n<p><em>Not</em> homework.</p>\n\n<p>My first job as a professional programmer (1995) was writing a genetic-algorithm based automated trading system for S&amp;P500 futures.  The application was written in Visual Basic 3 [!] and I have no idea how I did anything back then, since VB3 didn't even have classes.</p>\n\n<p>The application started with a population of randomly-generated fixed-length strings (the \"gene\" part), each of which corresponded to a specific shape in the minute-by-minute price data of the S&amp;P500 futures, as well as a specific order (buy or sell) and stop-loss and stop-profit amounts.  Each string (or \"gene\") had its profit performance evaluated by a run through 3 years of historical data; whenever the specified \"shape\" matched the historical data, I assumed the corresponding buy or sell order and evaluated the trade's result.  I added the caveat that each gene started with a fixed amount of money and could thus potentially go broke and be removed from the gene pool entirely.</p>\n\n<p>After each evaluation of a population, the survivors were cross-bred randomly (by just mixing bits from two parents), with the likelihood of a gene being selected as a parent being proportional to the profit it produced.  I also added the possibility of point mutations to spice things up a bit.  After a few hundred generations of this, I ended up with a population of genes that could turn $5000 into an average of about $10000 with no chance of death/brokeness (on the historical data, of course).</p>\n\n<p>Unfortunately, I never got the chance to use this system live, since my boss lost close to $100,000 in less than 3 months trading the traditional way, and he lost his willingness to continue with the project.  In retrospect, I think the system would have made huge profits - not because I was necessarily doing anything right, but because the population of genes that I produced happened to be biased towards buy orders (as opposed to sell orders) by about a 5:1 ratio.  And as we know with our 20/20 hindsight, the market went up a bit after 1995.</p>\n    "},{"t":"What's the best way to model recurring events in a calendar application?","l":"http://stackoverflow.com/questions/85699/whats-the-best-way-to-model-recurring-events-in-a-calendar-application","q":"\n\n<p>I'm building a group calendar application that needs to support recurring events, but all the solutions I've come up with to handle these events seem like a hack. I can limit how far ahead one can look, and then generate all the events at once. Or I can store the events as repeating and dynamically display them when one looks ahead on the calendar, but I'll have to convert them to a normal event if someone wants to change the details on a particular instance of the event.</p>\n\n<p>I'm sure there's a better way to do this, but I haven't found it yet. What's the best way to model recurring events, where you can change details of or delete particular event instances?</p>\n\n<p>(I'm using Ruby, but please don't let that constrain your answer. If there's a Ruby-specific library or something, though, that's good to know.)</p>\n    ","a":"\n<p>I would use a 'link' concept for all future recurring events. They are dynamically displayed in the calendar and link back to a single reference object. When events have taken place the link is broken and the event becomes a standalone instance. If you attempt to edit a recurring event then prompt to change all future items (i.e. change single linked reference) or change just that instance (in which case convert this to a standalone instance and then make change). The latter cased is slightly problematic as you need to keep track in your recurring list of all future events that were converted to single instance. But, this is entirely do-able.</p>\n\n<p>So, in essence, have 2 classes of events - single instances and recurring events.</p>\n    "},{"t":"Representing and solving a maze given an image","l":"http://stackoverflow.com/questions/12995434/representing-and-solving-a-maze-given-an-image","q":"\n\n<p>What is the best way to represent and solve a maze given an image?</p>\n\n<p><img src=\"http://i.stack.imgur.com/TqKCM.jpg\" alt=\"The cover image of The Scope Issue 134\"></p>\n\n<p>Given an JPEG image (as seen above), what's the best way to read it in, parse it into some data structure and solve the maze? My first instinct is to read the image in pixel by pixel and store it in a list (array) of boolean values: <code>True</code> for a white pixel, and <code>False</code> for a non-white pixel (the colours can be discarded). The issue with this method, is that the image may not be \"pixel perfect\". By that I simply mean that if there is a white pixel somewhere on a wall it may create an unintended path.</p>\n\n<p>Another method (which came to me after a bit of thought) is to convert the image to an SVG file - which is a list of paths drawn on a canvas. This way, the paths could be read into the same sort of list (boolean values) where <code>True</code> indicates a path or wall, <code>False</code> indicating a travel-able space. An issue with this method arises if the conversion is not 100% accurate, and does not fully connect all of the walls, creating gaps.</p>\n\n<p>Also an issue with converting to SVG is that the lines are not \"perfectly\" straight. This results in the paths being cubic bezier curves. With a list (array) of boolean values indexed by integers, the curves would not transfer easily, and all the points that line on the curve would have to be calculated, but won't exactly match to list indices.</p>\n\n<p>I assume that while one of these methods may work (though probably not) that they are woefully inefficient given such a large image, and that there exists a better way. How is this best (most efficiently and/or with the least complexity) done? Is there even a best way?</p>\n\n<p>Then comes the solving of the maze. If I use either of the first two methods, I will essentially end up with a matrix. According to <a href=\"http://stackoverflow.com/a/3097677/1267663\">this answer</a>, a good way to represent a maze is using a tree, and a good way to solve it is using the <a href=\"http://en.wikipedia.org/wiki/A%2a_search_algorithm\">A* algorithm</a>. How would one create a tree from the image? Any ideas?</p>\n\n<p><strong>TL;DR</strong><br>\nBest way to parse? Into what data structure? How would said structure help/hinder solving?</p>\n\n<p><strong>UPDATE</strong><br>\nI've tried my hand at implementing what @Mikhail has written in Python, using <code>numpy</code>, as @Thomas recommended. I feel that the algorithm is correct, but it's not working as hoped. (Code below.) The PNG library is <a href=\"https://raw.github.com/drj11/pypng/master/code/png.py\">PyPNG</a>.</p>\n\n<pre><code>import png, numpy, Queue, operator, itertools\n\ndef is_white(coord, image):\n  \"\"\" Returns whether (x, y) is approx. a white pixel.\"\"\"\n  a = True\n  for i in xrange(3):\n    if not a: break\n    a = image[coord[1]][coord[0] * 3 + i] &gt; 240\n  return a\n\ndef bfs(s, e, i, visited):\n  \"\"\" Perform a breadth-first search. \"\"\"\n  frontier = Queue.Queue()\n  while s != e:\n    for d in [(-1, 0), (0, -1), (1, 0), (0, 1)]:\n      np = tuple(map(operator.add, s, d))\n      if is_white(np, i) and np not in visited:\n        frontier.put(np)\n    visited.append(s)\n    s = frontier.get()\n  return visited\n\ndef main():\n  r = png.Reader(filename = \"thescope-134.png\")\n  rows, cols, pixels, meta = r.asDirect()\n  assert meta['planes'] == 3 # ensure the file is RGB\n  image2d = numpy.vstack(itertools.imap(numpy.uint8, pixels))\n  start, end = (402, 985), (398, 27)\n  print bfs(start, end, image2d, [])\n</code></pre>\n    ","a":"\n<p>Here is a solution.</p>\n\n<ol>\n<li>Convert image to grayscale (not yet binary), adjusting weights for the colors so that final grayscale image is approximately uniform. You can do it simply by controlling sliders in Photoshop in Image -&gt; Adjustments -&gt; Black &amp; White.</li>\n<li>Convert image to binary by setting appropriate threshold in Photoshop in Image -&gt; Adjustments -&gt; Threshold.</li>\n<li>Make sure threshold is selected right. Use the Magic Wand Tool with 0 tolerance, point sample, contiguous, no anti-aliasing. Check that edges at which selection breaks are not false edges introduced by wrong threshold. In fact, all interior points of this maze are accessible from the start.</li>\n<li>Add artificial borders on the maze to make sure virtual traveler will not walk around it :)</li>\n<li>Implement <a href=\"http://en.wikipedia.org/wiki/Breadth-first_search\">breadth-first search</a> (BFS) in your favorite language and run it from the start. I prefer <a href=\"http://en.wikipedia.org/wiki/MATLAB\">MATLAB</a> for this task. As @Thomas already mentioned, there is no need to mess with regular representation of graphs. You can work with binarized image directly.</li>\n</ol>\n\n<p>Here is the MATLAB code for BFS:</p>\n\n<pre><code>function path = solve_maze(img_file)\n  %% Init data\n  img = imread(img_file);\n  img = rgb2gray(img);\n  maze = img &gt; 0;\n  start = [985 398];\n  finish = [26 399];\n\n  %% Init BFS\n  n = numel(maze);\n  Q = zeros(n, 2);\n  M = zeros([size(maze) 2]);\n  front = 0;\n  back = 1;\n\n  function push(p, d)\n    q = p + d;\n    if maze(q(1), q(2)) &amp;&amp; M(q(1), q(2), 1) == 0\n      front = front + 1;\n      Q(front, :) = q;\n      M(q(1), q(2), :) = reshape(p, [1 1 2]);\n    end\n  end\n\n  push(start, [0 0]);\n\n  d = [0 1; 0 -1; 1 0; -1 0];\n\n  %% Run BFS\n  while back &lt;= front\n    p = Q(back, :);\n    back = back + 1;\n    for i = 1:4\n      push(p, d(i, :));\n    end\n  end\n\n  %% Extracting path\n  path = finish;\n  while true\n    q = path(end, :);\n    p = reshape(M(q(1), q(2), :), 1, 2);\n    path(end + 1, :) = p;\n    if isequal(p, start) \n      break;\n    end\n  end\nend\n</code></pre>\n\n<p>It is really very simple and standard, there should not be difficulties on implementing this in <a href=\"http://en.wikipedia.org/wiki/Python_%28programming_language%29\">Python</a> or whatever.</p>\n\n<p>And here is the answer:</p>\n\n<p><a href=\"http://i.stack.imgur.com/gQ7nV.png\"><img src=\"http://i.stack.imgur.com/gQ7nV.png\" alt=\"Enter image description here\"></a></p>\n    "},{"t":"Is Disney's FastPass Valid and/or Useful Queue Theory","l":"http://stackoverflow.com/questions/643032/is-disneys-fastpass-valid-and-or-useful-queue-theory","q":"\n\n<p>At Disney World, they use a system called <a href=\"http://en.wikipedia.org/wiki/FASTPASS\">Fastpass</a> to create a second, shorter line for popular rides.  The idea is that you can wait in the standard line, often with a wait longer than an hour, or you can get a FastPass which allows you to come back during a specified time block (usually a couple hours later) and only wait for 10 minutes or less.  You can only be \"waiting\" for one ride at a time with a FastPass.</p>\n\n<p>I have been trying to figure out the queue theory behind this concept, but the only explanation I have found is that it is designed to get people out of the lines and doing things that will bring in additional revenue (shopping, eating, etc).  </p>\n\n<p>Is this why FastPass was implemented, or is there a real visitor efficiency problem that it solving?  Are there software applications that have applied similar logic?  Are there software applications that <em>should</em> apply similar logic?</p>\n\n<p>Part of the problem I see with implementing something similar in software is that it is based on users choosing their queue.  Do to the faster wait cycles in software, I think a good application of this theory would require the application to be smart enough to know what queues to place people in based on their needs without requiring end-user choice.</p>\n    ","a":"\n<p>The fast pass line is obviously not going to increase total throughput on a given ride queue, but it does help in resource scheduling and resource assignment where people and rides are the resources.</p>\n\n<p>Like I said, you aren't going to create any more total throughput for said ride, but there may be rides being underutilized elsewhere. If you are now able to ride these rides as well as the rides you have to wait on, then you can increase the overall efficiency of the park. What I mean by that is minimizing the amount of rides that are running below passenger capacity.</p>\n\n<p>If you have computer resources sitting idle, waiting to perform a task that might take a long time, it makes sense to utilize this resource for something else in the meantime right? It's simple from that perspective.</p>\n    "},{"t":"Is this a “good enough” random algorithm; why isn't it used if it's faster?","l":"http://stackoverflow.com/questions/14491966/is-this-a-good-enough-random-algorithm-why-isnt-it-used-if-its-faster","q":"\n\n<p>I made a class called <code>QuickRandom</code>, and its job is to produce random numbers quickly. It's really simple: just take the old value, multiply by a <code>double</code>, and take the decimal part.</p>\n\n<p>Here is my <code>QuickRandom</code> class in its entirety:</p>\n\n<pre><code>public class QuickRandom {\n    private double prevNum;\n    private double magicNumber;\n\n    public QuickRandom(double seed1, double seed2) {\n        if (seed1 &gt;= 1 || seed1 &lt; 0) throw new IllegalArgumentException(\"Seed 1 must be &gt;= 0 and &lt; 1, not \" + seed1);\n        prevNum = seed1;\n        if (seed2 &lt;= 1 || seed2 &gt; 10) throw new IllegalArgumentException(\"Seed 2 must be &gt; 1 and &lt;= 10, not \" + seed2);\n        magicNumber = seed2;\n    }\n\n    public QuickRandom() {\n        this(Math.random(), Math.random() * 10);\n    }\n\n    public double random() {\n        return prevNum = (prevNum*magicNumber)%1;\n    }\n\n}\n</code></pre>\n\n<p>And here is the code I wrote to test it:</p>\n\n<pre><code>public static void main(String[] args) {\n        QuickRandom qr = new QuickRandom();\n\n        /*for (int i = 0; i &lt; 20; i ++) {\n            System.out.println(qr.random());\n        }*/\n\n        //Warm up\n        for (int i = 0; i &lt; 10000000; i ++) {\n            Math.random();\n            qr.random();\n            System.nanoTime();\n        }\n\n        long oldTime;\n\n        oldTime = System.nanoTime();\n        for (int i = 0; i &lt; 100000000; i ++) {\n            Math.random();\n        }\n        System.out.println(System.nanoTime() - oldTime);\n\n        oldTime = System.nanoTime();\n        for (int i = 0; i &lt; 100000000; i ++) {\n            qr.random();\n        }\n        System.out.println(System.nanoTime() - oldTime);\n}\n</code></pre>\n\n<p>It is a very simple algorithm that simply multiplies the previous double by a \"magic number\" double. I threw it together pretty quickly, so I could probably make it better, but strangely, it seems to be working fine.</p>\n\n<p>This is sample output of the commented-out lines in the <code>main</code> method:</p>\n\n<pre><code>0.612201846732229\n0.5823974655091941\n0.31062451498865684\n0.8324473610354004\n0.5907187526770246\n0.38650264675748947\n0.5243464344127049\n0.7812828761272188\n0.12417247811074805\n0.1322738256858378\n0.20614642573072284\n0.8797579436677381\n0.022122999476108518\n0.2017298328387873\n0.8394849894162446\n0.6548917685640614\n0.971667953190428\n0.8602096647696964\n0.8438709031160894\n0.694884972852229\n</code></pre>\n\n<p>Hm. Pretty random. In fact, that would work for a random number generator in a game.</p>\n\n<p>Here is sample output of the non-commented out part:</p>\n\n<pre><code>5456313909\n1427223941\n</code></pre>\n\n<p>Wow! It performs almost 4 times faster than <code>Math.random</code>.</p>\n\n<p>I remember reading somewhere that <code>Math.random</code> used <code>System.nanoTime()</code> and tons of crazy modulus and division stuff. Is that really necessary? My algorithm performs a lot faster and it seems pretty random.</p>\n\n<p>I have two questions:</p>\n\n<ul>\n<li>Is my algorithm \"good enough\" (for, say, a game, where <strong>really</strong> random numbers aren't too important)?</li>\n<li>Why does <code>Math.random</code> do so much when it seems just simple multiplication and cutting out the decimal will suffice?</li>\n</ul>\n    ","a":"\n<p>Your <code>QuickRandom</code> implementation hasn't really an uniform distribution. The frequencies are generally higher at the lower values while <code>Math.random()</code> has a more uniform distribution. Here's a <a href=\"http://sscce.org\">SSCCE</a> which shows that:</p>\n\n<pre><code>package com.stackoverflow.q14491966;\n\nimport java.util.Arrays;\n\npublic class Test {\n\n    public static void main(String[] args) throws Exception {\n        QuickRandom qr = new QuickRandom();\n        int[] frequencies = new int[10];\n        for (int i = 0; i &lt; 100000; i++) {\n            frequencies[(int) (qr.random() * 10)]++;\n        }\n        printDistribution(\"QR\", frequencies);\n\n        frequencies = new int[10];\n        for (int i = 0; i &lt; 100000; i++) {\n            frequencies[(int) (Math.random() * 10)]++;\n        }\n        printDistribution(\"MR\", frequencies);\n    }\n\n    public static void printDistribution(String name, int[] frequencies) {\n        System.out.printf(\"%n%s distribution |8000     |9000     |10000    |11000    |12000%n\", name);\n        for (int i = 0; i &lt; 10; i++) {\n            char[] bar = \"                                                  \".toCharArray(); // 50 chars.\n            Arrays.fill(bar, 0, Math.max(0, Math.min(50, frequencies[i] / 100 - 80)), '#');\n            System.out.printf(\"0.%dxxx: %6d  :%s%n\", i, frequencies[i], new String(bar));\n        }\n    }\n\n}\n</code></pre>\n\n<p>The average result looks like this:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>QR distribution |8000     |9000     |10000    |11000    |12000\n0.0xxx:  11376  :#################################                 \n0.1xxx:  11178  :###############################                   \n0.2xxx:  11312  :#################################                 \n0.3xxx:  10809  :############################                      \n0.4xxx:  10242  :######################                            \n0.5xxx:   8860  :########                                          \n0.6xxx:   9004  :##########                                        \n0.7xxx:   8987  :#########                                         \n0.8xxx:   9075  :##########                                        \n0.9xxx:   9157  :###########                                       \n\nMR distribution |8000     |9000     |10000    |11000    |12000\n0.0xxx:  10097  :####################                              \n0.1xxx:   9901  :###################                               \n0.2xxx:  10018  :####################                              \n0.3xxx:   9956  :###################                               \n0.4xxx:   9974  :###################                               \n0.5xxx:  10007  :####################                              \n0.6xxx:  10136  :#####################                             \n0.7xxx:   9937  :###################                               \n0.8xxx:  10029  :####################                              \n0.9xxx:   9945  :###################    \n</code></pre>\n\n<p>If you repeat the test, you'll see that the QR distribution varies heavily, depending on the initial seeds, while the MR distribution is stable. Sometimes it reaches the desired uniform distribution, but more than often it doesn't. Here's one of the more extreme examples, it's even beyond the borders of the graph:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>QR distribution |8000     |9000     |10000    |11000    |12000\n0.0xxx:  41788  :##################################################\n0.1xxx:  17495  :##################################################\n0.2xxx:  10285  :######################                            \n0.3xxx:   7273  :                                                  \n0.4xxx:   5643  :                                                  \n0.5xxx:   4608  :                                                  \n0.6xxx:   3907  :                                                  \n0.7xxx:   3350  :                                                  \n0.8xxx:   2999  :                                                  \n0.9xxx:   2652  :                                                  \n</code></pre>\n    "},{"t":"O(nlogn) Algorithm - Find three evenly spaced ones within binary string","l":"http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string","q":"\n\n<p>I had this question on an Algorithms test yesterday, and I can't figure out the answer.  It is driving me absolutely crazy, because it was worth about 40 points.  I figure that most of the class didn't solve it correctly, because I haven't come up with a solution in the past 24 hours.</p>\n\n<p>Given a arbitrary binary string of length n, find three evenly spaced ones within the string if they exist.  Write an algorithm which solves this in O(n * log(n)) time.</p>\n\n<p>So strings like these have three ones that are \"evenly spaced\": 11100000, 0100100100</p>\n\n<p>edit: It is a random number, so it should be able to work for any number.  The examples I gave were to illustrate the \"evenly spaced\" property.  So 1001011 is a valid number. With 1, 4, and 7 being ones that are evenly spaced.</p>\n    ","a":"\n<p>Finally! Following up leads in <a href=\"http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string/1579165#1579165\">sdcvvc's answer</a>, we have it: the O(n log n) algorithm for the problem! It is simple too, after you understand it. Those who guessed FFT were right.</p>\n\n<p>The problem: we are given a binary string <code>S</code> of length <em>n</em>, and we want to find three evenly spaced 1s in it. For example, <code>S</code> may be <code>110110010</code>, where <em>n</em>=9. It has evenly spaced 1s at positions 2, 5, and 8.</p>\n\n<ol>\n<li><p>Scan <code>S</code> left to right, and make a list <code>L</code> of positions of 1. For the <code>S=110110010</code> above, we have the list L = [1, 2, 4, 5, 8]. This step is O(n). The problem is now to find an <strong>arithmetic progression of length 3</strong> in <code>L</code>, i.e. to find distinct <em>a, b, c</em> in <code>L</code> such that <em>b-a = c-b</em>, or equivalently <strong><em>a+c=2b</em></strong>. For the example above, we want to find the progression (2, 5, 8).</p></li>\n<li><p>Make a <strong>polynomial</strong> <code>p</code> with terms <em>x<sup>k</sup></em> for each <em>k</em> in <code>L</code>. For the example above, we make the polynomial <em>p(x) = (x + x<sup>2</sup> + x<sup>4</sup> + x<sup>5</sup>+x<sup>8</sup>)</em>. This step is O(n).</p></li>\n<li><p>Find the polynomial <code>q</code> = <em>p<sup>2</sup></em>, using the <a href=\"http://www.cs.iastate.edu/~cs577/handouts/polymultiply.pdf\">Fast Fourier Transform</a>. For the example above, we get the polynomial <em>q(x) = x<sup>16</sup> + 2x<sup>13</sup> + 2x<sup>12</sup> + 3x<sup>10</sup> + 4x<sup>9</sup> + x<sup>8</sup> + 2x<sup>7</sup> + 4x<sup>6</sup> + 2x<sup>5</sup> + x<sup>4</sup> + 2x<sup>3</sup> + x<sup>2</sup></em>. <strong>This step is O(n log n).</strong></p></li>\n<li><p>Ignore all terms except those corresponding to <em>x<sup>2k</sup></em> for some <em>k</em> in <code>L</code>. For the example above, we get the terms <em>x<sup>16</sup>, 3x<sup>10</sup>, x<sup>8</sup>, x<sup>4</sup>, x<sup>2</sup></em>. This step is O(n), if you choose to do it at all.</p></li>\n</ol>\n\n<p>Here's the crucial point: the coefficient of any <em>x<sup>2b</sup></em> for <em>b</em> in <code>L</code> is <em>precisely</em> the number of pairs <em>(a,c)</em> in <code>L</code> such that <em>a+c=2b</em>. [CLRS, Ex. 30.1-7] One such pair is <em>(b,b)</em> always (so the coefficient is at least 1), but if there exists any other pair <em>(a,c)</em>, then the coefficient is at least 3, from <em>(a,c)</em> and <em>(c,a)</em>. For the example above, we have the coefficient of <em>x<sup>10</sup></em> to be 3 precisely because of the AP (2,5,8). (These coefficients <em>x<sup>2b</sup></em> will always be odd numbers, for the reasons above. And all other coefficients in q will always be even.)</p>\n\n<p>So then, the algorithm is to look at the coefficients of these terms <em>x<sup>2b</sup></em>, and see if any of them is greater than 1. If there is none, then there are no evenly spaced 1s. If there <em>is</em> a <em>b</em> in <code>L</code> for which the coefficient of <em>x<sup>2b</sup></em> is greater than 1, then we know that there is some pair <em>(a,c)</em> — other than <em>(b,b)</em> — for which <em>a+c=2b</em>. To find the actual pair, we simply try each <em>a</em> in <code>L</code> (the corresponding <em>c</em> would be <em>2b-a</em>) and see if there is a 1 at position <em>2b-a</em> in <code>S</code>. This step is O(n).</p>\n\n<p>That's all, folks.</p>\n\n<p></p><hr><p></p>\n\n<p>One might ask: do we need to use FFT? Many answers, such as <a href=\"http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string/1561827#1561827\">beta's</a>, <a href=\"http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string/1572080#1572080\">flybywire's</a>, and <a href=\"http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string/1567324#1567324\">rsp's</a>, suggest that the approach that checks each pair of 1s and sees if there is a 1 at the \"third\" position, might work in O(n log n), based on the intuition that if there are too many 1s, we would find a triple easily, and if there are too few 1s, checking all pairs takes little time. Unfortunately, while this intuition is correct and the simple approach <em>is</em> better than O(n<sup>2</sup>), it is not significantly better. As in <a href=\"http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string/1579165#1579165\">sdcvvc's answer</a>, we can take the \"Cantor-like set\" of strings of length <em>n=3<sup>k</sup></em>, with 1s at the positions whose ternary representation has only 0s and 2s (no 1s) in it. Such a string has <em>2<sup>k</sup> = n<sup>(log 2)/(log 3)</sup> ≈ n<sup>0.63</sup></em> ones in it and no evenly spaced 1s, so checking all pairs would be of the order of the square of the number of 1s in it: that's <em>4<sup>k</sup> ≈ n<sup>1.26</sup></em> which unfortunately is asymptotically much larger than (n log n). In fact, the worst case is even worse: Leo Moser in 1953 <a href=\"http://books.google.com/books?id=Cvtwu5vVZF4C&amp;pg=PA245\">constructed</a> (effectively) such strings which have <em>n<sup>1-c/√(log n)</sup></em> 1s in them but no evenly spaced 1s, which means that on such strings, the simple approach would take <em>Θ(n<sup>2-2c/√(log n)</sup>)</em> — only a <strong><em>tiny</em></strong> bit better than <em>Θ(n<sup>2</sup>)</em>, surprisingly!</p>\n\n<p></p><hr><p></p>\n\n<p>About the maximum number of 1s in a string of length n with no 3 evenly spaced ones (which we saw above was at least <em>n<sup>0.63</sup></em> from the easy Cantor-like construction, and at least <em>n<sup>1-c/√(log n)</sup></em> with Moser's construction) — this is <a href=\"http://www.research.att.com/~njas/sequences/A003002\">OEIS A003002</a>. It can also be calculated directly from <a href=\"http://www.research.att.com/~njas/sequences/A065825\">OEIS A065825</a> as the k such that A065825(k) ≤ n &lt; A065825(k+1). I wrote a program to find these, and it turns out that the greedy algorithm does <em>not</em> give the longest such string. For example, for <em>n</em>=9, we can get 5 1s (110100011) but the greedy gives only 4 (110110000), for <em>n</em>=26 we can get 11 1s (11001010001000010110001101) but the greedy gives only 8 (11011000011011000000000000), and for <em>n</em>=74 we can get 22 1s (11000010110001000001011010001000000000000000010001011010000010001101000011) but the greedy gives only 16 (11011000011011000000000000011011000011011000000000000000000000000000000000). They do agree at quite a few places until 50 (e.g. all of 38 to 50), though. As the OEIS references say, it seems that Jaroslaw Wroblewski is interested in this question, and he maintains a website on these <a href=\"http://www.math.uni.wroc.pl/~jwr/non-ave.htm\">non-averaging sets</a>. The exact numbers are known only up to 194.</p>\n    "},{"t":"In-Place Radix Sort","l":"http://stackoverflow.com/questions/463105/in-place-radix-sort","q":"\n\n<p>This is a long text. Please bear with me. Boiled down, the question is: <strong>Is there a workable in-place radix sort algorithm</strong>?</p>\n\n<hr>\n\n<h2>Preliminary</h2>\n\n<p>I've got a huge number of <em>small fixed-length</em> strings that only use the letters “A”, “C”, “G” and “T” (yes, you've guessed it: <a href=\"https://en.wikipedia.org/wiki/DNA\">DNA</a>) that I want to sort.</p>\n\n<p>At the moment, I use <code>std::sort</code> which uses <a href=\"https://en.wikipedia.org/wiki/Introsort\">introsort</a> in all common implementations of the <a href=\"https://en.wikipedia.org/wiki/Standard_Template_Library\">STL</a>. This works quite well. However, I'm convinced that <a href=\"https://en.wikipedia.org/wiki/Radix_sort\">radix sort</a> fits my problem set perfectly and should work <strong>much</strong> better in practice.</p>\n\n<h2>Details</h2>\n\n<p>I've tested this assumption with a very naive implementation and for relatively small inputs (on the order of 10,000) this was true (well, at least more than twice as fast). However, runtime degrades abysmally when the problem size becomes larger (<em>N</em> &gt; 5,000,000).</p>\n\n<p>The reason is obvious: radix sort requires copying the whole data (more than once in my naive implementation, actually). This means that I've put ~ 4 GiB into my main memory which obviously kills performance. Even if it didn't, I can't afford to use this much memory since the problem sizes actually become even larger.</p>\n\n<h2>Use Cases</h2>\n\n<p>Ideally, this algorithm should work with any string length between 2 and 100, for DNA as well as DNA5 (which allows an additional wildcard character “N”), or even DNA with <a href=\"https://en.wikipedia.org/wiki/International_Union_of_Pure_and_Applied_Chemistry\">IUPAC</a> <a href=\"https://en.wikipedia.org/wiki/Nucleic_acid_notation\">ambiguity codes</a> (resulting in 16 distinct values). However, I realize that all these cases cannot be covered, so I'm happy with any speed improvement I get. The code can decide dynamically which algorithm to dispatch to.</p>\n\n<h2>Research</h2>\n\n<p>Unfortunately, the <a href=\"https://en.wikipedia.org/wiki/Radix_sort\">Wikipedia article on radix sort</a> is useless. The section about an in-place variant is complete rubbish. The <a href=\"http://xlinux.nist.gov/dads/HTML/radixsort.html\">NIST-DADS section on radix sort</a> is next to nonexistent. There's a promising-sounding paper called <a href=\"https://www.mii.vu.lt/informatica/pdf/INFO562.pdf\">Efficient Adaptive In-Place Radix Sorting</a> which describes the algorithm “MSL”. Unfortunately, this paper, too, is disappointing.</p>\n\n<p>In particular, there are the following things.</p>\n\n<p>First, the algorithm contains several mistakes and leaves a lot unexplained. In particular, it doesn’t detail the recursion call (I simply assume that it increments or reduces some pointer to calculate the current shift and mask values). Also, it uses the functions <code>dest_group</code> and <code>dest_address</code> without giving definitions. I fail to see how to implement these efficiently (that is, in O(1); at least <code>dest_address</code> isn’t trivial).</p>\n\n<p>Last but not least, the algorithm achieves in-place-ness by swapping array indices with elements inside the input array. This obviously only works on numerical arrays. I need to use it on strings. Of course, I could just screw strong typing and go ahead assuming that the memory will tolerate my storing an index where it doesn’t belong. But this only works as long as I can squeeze my strings into 32 bits of memory (assuming 32 bit integers). That's only 16 characters (let's ignore for the moment that 16 &gt; log(5,000,000)).</p>\n\n<p>Another paper by one of the authors gives no accurate description at all, but it gives MSL’s runtime as sub-linear which is flat out wrong.</p>\n\n<p><strong>To recap</strong>: Is there any hope of finding a working reference implementation or at least a good pseudocode/description of a working in-place radix sort that works on DNA strings?</p>\n    ","a":"\n<p>Well, here's a simple implementation of an MSD radix sort for DNA.  It's written in D because that's the language that I use most and therefore am least likely to make silly mistakes in, but it could easily be translated to some other language.  It's in-place but requires 2 * seq.length passes through the array.</p>\n\n<pre><code>void radixSort(string[] seqs, size_t base = 0) {\n    if(seqs.length == 0)\n        return;\n\n    size_t TPos = seqs.length, APos = 0;\n    size_t i = 0;\n    while(i &lt; TPos) {\n        if(seqs[i][base] == 'A') {\n             swap(seqs[i], seqs[APos++]);\n             i++;\n        }\n        else if(seqs[i][base] == 'T') {\n            swap(seqs[i], seqs[--TPos]);\n        } else i++;\n    }\n\n    i = APos;\n    size_t CPos = APos;\n    while(i &lt; TPos) {\n        if(seqs[i][base] == 'C') {\n            swap(seqs[i], seqs[CPos++]);\n        }\n        i++;\n    }\n    if(base &lt; seqs[0].length - 1) {\n        radixSort(seqs[0..APos], base + 1);\n        radixSort(seqs[APos..CPos], base + 1);\n        radixSort(seqs[CPos..TPos], base + 1);\n        radixSort(seqs[TPos..seqs.length], base + 1);\n   }\n}\n</code></pre>\n\n<p>Obviously, this is kind of specific to DNA, as opposed to being general, but it should be fast.</p>\n\n<p>Edit:  I got curious whether this code actually works, so I tested/debugged it while waiting for my own bioinformatics code to run.  The version above now is actually tested and works.  For 10 million sequences of 5 bases each, it's about 3x faster than an optimized introsort.</p>\n    "},{"t":"Generating all permutations of a given string","l":"http://stackoverflow.com/questions/4240080/generating-all-permutations-of-a-given-string","q":"\n\n<p>What is an elegant way to find all the permutations of a string. E.g. <code>ba</code>, would be <code>ba</code> and <code>ab</code>, but what about <code>abcdefgh</code>? Is there any example Java implementation?</p>\n    ","a":"\n<pre><code>public static void permutation(String str) { \n    permutation(\"\", str); \n}\n\nprivate static void permutation(String prefix, String str) {\n    int n = str.length();\n    if (n == 0) System.out.println(prefix);\n    else {\n        for (int i = 0; i &lt; n; i++)\n            permutation(prefix + str.charAt(i), str.substring(0, i) + str.substring(i+1, n));\n    }\n}\n</code></pre>\n\n<p>(via <a href=\"http://introcs.cs.princeton.edu/java/23recursion/Permutations.java.html\">Introduction to Programming in Java</a>)</p>\n\n<p>Explanation of how the above code works: <a href=\"http://www.ericleschinski.com/c/java_permutations_recursion/\">ericleschinski.com/c/java_permutations_recursion/</a></p>\n    "},{"t":"What are the mathematical/computational principles behind this game?","l":"http://stackoverflow.com/questions/6240113/what-are-the-mathematical-computational-principles-behind-this-game","q":"\n\n<p>My kids have this fun game called <a href=\"http://www.blueorangegames.com/spotit-tg.php\">Spot It!</a> The game constraints (as best I can describe) are:</p>\n\n<ul>\n<li>It is a deck of 55 cards</li>\n<li>On each card are 8 unique pictures (i.e. a card can't have 2 of the same picture)</li>\n<li><strong>Given any 2 cards chosen from the deck, there is 1 and only 1 matching picture</strong>. </li>\n<li>Matching pictures may be scaled differently on different cards but that is only to make the game harder (i.e. a small tree still matches a larger tree)</li>\n</ul>\n\n<p>The principle of the game is: flip over 2 cards and whoever first picks the matching picture gets a point. </p>\n\n<p>Here's a picture for clarification:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Lv4I1.jpg\" alt=\"spot it\"></p>\n\n<p>(Example: you can see from the bottom 2 cards above that the matching picture is the green dinosaur. Between the bottom-right and middle-right picture, it's a clown's head.)</p>\n\n<p>I'm trying to understand the following: </p>\n\n<ol>\n<li><p>What are the minimum number of different pictures required to meet these criteria and how would you determine this? </p></li>\n<li><p>Using pseudocode (or Ruby), how would you generate 55 game cards from an array of N pictures (where N is the minimum number from question 1)? </p></li>\n</ol>\n\n<p><strong>Update:</strong></p>\n\n<p>Pictures do occur more than twice per deck (contrary to what some have surmised). See this picture of 3 cards, each with a lightning bolt:<img src=\"http://i.stack.imgur.com/9Pk2v.jpg\" alt=\"3 cards\"></p>\n    ","a":"\n<p><strong>Finite  Projective Geometries</strong></p>\n\n<p>The <a href=\"https://en.wikipedia.org/wiki/Projective_geometry#Axioms_of_projective_geometry\">axioms</a> of <a href=\"https://en.wikipedia.org/wiki/Projective_geometry\">projective (plane) geometry</a> are slightly different than the Euclidean geometry:</p>\n\n<ul>\n<li>Every two points have exactly one line that passes through them (this is the same).</li>\n<li>Every two lines meet in exactly one point (this is a bit different from Euclid).</li>\n</ul>\n\n<p>Now, add <a href=\"https://en.wikipedia.org/wiki/Projective_plane#Finite_projective_planes\">\"finite\"</a> into the soup and you have the question:</p>\n\n<p>Can we have a geometry with just 2 points? With 3 points? With 4? With 7?</p>\n\n<p>There are still open questions regarding this problem but we do know this:</p>\n\n<ul>\n<li>If there are geometries with <code>Q</code> points, then <code>Q = n^2 + n + 1</code> and <code>n</code> is called the <code>order</code> of the geometry.</li>\n<li>There are <code>n+1</code> points in every line.</li>\n<li>From every point, pass exactly <code>n+1</code> lines.</li>\n<li><p>Total number of lines is also <code>Q</code>.</p></li>\n<li><p>And finally, if <code>n</code> is prime, then there does exists a geometry of order <code>n</code>.</p></li>\n</ul>\n\n<hr>\n\n<p>What does that have to do with the puzzle, one may ask.</p>\n\n<p>Put <code>card</code> instead of <code>point</code> and <code>picture</code> instead of <code>line</code> and the axioms become:</p>\n\n<ul>\n<li>Every two cards have exactly one picture in common.</li>\n<li>For every two pictures there is exactly one card that has both of them.</li>\n</ul>\n\n<p>Now, lets take <code>n=7</code> and we have the <code>order-7</code> finite geometry with <code>Q = 7^2 + 7 + 1</code> . That makes <code>Q=57</code> lines (pictures) and <code>Q=57</code> points (cards). I guess the puzzle makers decided that 55 is more round number than 57 and left 2 cards out.</p>\n\n<p>We also get <code>n+1 = 8</code>, so from every point (card), 8 lines pass (8 pictures appear) and every line (picture) has 8 points (appears in 8 cards). </p>\n\n<hr>\n\n<p>Here's a representation  of the most famous finite projective (order-2) plane (geometry) with 7 points, known as <a href=\"https://en.wikipedia.org/wiki/Fano_plane\"><strong>Fano Plane</strong></a>, copied from <a href=\"http://home.wlu.edu/~mcraea/Finite_Geometry/TableOfContents/table.html\">Noelle Evans - Finite Geometry Problem Page</a></p>\n\n<p><img src=\"http://i.stack.imgur.com/OUhVC.gif\" alt=\"enter image description here\"></p>\n\n<p>I was thinking of creating an image that explain how the above order-2 plane could be made a similar puzzle with 7 cards and 7 pictures, but then a link from the math.exchange twin question has exactly such a diagram: <strong><a href=\"http://images.math.cnrs.fr/Dobble-et-la-geometrie-finie.html\">Dobble-et-la-geometrie-finie</a></strong></p>\n\n<p><img src=\"http://i.stack.imgur.com/RPsD9.png\" width=\"400\" alt=\"Fano Plane\"></p>\n    "},{"t":"Why is quicksort better than mergesort?","l":"http://stackoverflow.com/questions/70402/why-is-quicksort-better-than-mergesort","q":"\n\n<p>I was asked this question during an interview. They're both O(nlogn) and yet most people use Quicksort instead of Mergesort. Why is that?</p>\n    ","a":"\n<p>Quicksort has O(<i>n</i><sup>2</sup>) worst-case runtime and O(<i>n</i>log<i>n</i>) average case runtime. However, it’s superior to merge sort in many scenarios because many factors influence an algorithm’s runtime, and, when taking them all together, quicksort wins out.</p>\n\n<p>In particular, the often-quoted runtime of sorting algorithms refers to the number of comparisons or the number of swaps necessary to perform to sort the data. This is indeed a good measure of performance, especially since it’s independent of the underlying hardware design. However, other things – such as locality of reference (i.e. do we read lots of elements which are probably in cache?) – also play an important role on current hardware. Quicksort in particular requires little additional space and exhibits good cache locality, and this makes it faster than merge sort in many cases.</p>\n\n<p>In addition, it’s very easy to avoid quicksort’s worst-case run time of O(<i>n</i><sup>2</sup>) almost entirely by using an appropriate choice of the pivot&nbsp;– such as picking it at random (this is an excellent strategy).</p>\n\n<p>In practice, many modern implementations of quicksort (in particular libstdc++’s <code>std::sort</code>) are actually <a href=\"http://en.wikipedia.org/wiki/Introsort\">introsort</a>, whose theoretical worst-case is O(<i>n</i>log<i>n</i>), same as merge sort. It achieves this by limiting the recursion depth, and switching to a different algorithm (<a href=\"http://en.wikipedia.org/wiki/Heapsort\">heapsort</a>) once it exceeds log<i>n</i>.</p>\n    "},{"t":"Best Algorithm for Bit Reversal ( from MSB->LSB to LSB->MSB) in C","l":"http://stackoverflow.com/questions/746171/best-algorithm-for-bit-reversal-from-msb-lsb-to-lsb-msb-in-c","q":"\n\n<p>What is the best algorithm to achieve the following:</p>\n\n<p><code>0010 0000 =&gt; 0000 0100</code></p>\n\n<p>The conversion is from MSB-&gt;LSB to LSB-&gt;MSB.  All bits must be reversed; that is, this is <em>not</em> endianness-swapping.</p>\n    ","a":"\n<p><strong>NOTE</strong>: All algorithms below are in C, but should be portable to your language of choice (just don't look at me when they're not as fast :)</p>\n\n<h2>Options</h2>\n\n<p><strong>Low Memory (32-bit <code>int</code>, 32-bit machine)(from <a href=\"http://aggregate.org/MAGIC/#Bit%20Reversal\">here</a>):</strong></p>\n\n<pre><code>unsigned int\nreverse(register unsigned int x)\n{\n    x = (((x &amp; 0xaaaaaaaa) &gt;&gt; 1) | ((x &amp; 0x55555555) &lt;&lt; 1));\n    x = (((x &amp; 0xcccccccc) &gt;&gt; 2) | ((x &amp; 0x33333333) &lt;&lt; 2));\n    x = (((x &amp; 0xf0f0f0f0) &gt;&gt; 4) | ((x &amp; 0x0f0f0f0f) &lt;&lt; 4));\n    x = (((x &amp; 0xff00ff00) &gt;&gt; 8) | ((x &amp; 0x00ff00ff) &lt;&lt; 8));\n    return((x &gt;&gt; 16) | (x &lt;&lt; 16));\n\n}\n</code></pre>\n\n<p>From the famous <a href=\"http://graphics.stanford.edu/~seander/bithacks.html\">Bit Twiddling Hacks page</a>:</p>\n\n<p><strong>Fastest (lookup table)</strong>:</p>\n\n<pre><code>static const unsigned char BitReverseTable256[] = \n{\n  0x00, 0x80, 0x40, 0xC0, 0x20, 0xA0, 0x60, 0xE0, 0x10, 0x90, 0x50, 0xD0, 0x30, 0xB0, 0x70, 0xF0, \n  0x08, 0x88, 0x48, 0xC8, 0x28, 0xA8, 0x68, 0xE8, 0x18, 0x98, 0x58, 0xD8, 0x38, 0xB8, 0x78, 0xF8, \n  0x04, 0x84, 0x44, 0xC4, 0x24, 0xA4, 0x64, 0xE4, 0x14, 0x94, 0x54, 0xD4, 0x34, 0xB4, 0x74, 0xF4, \n  0x0C, 0x8C, 0x4C, 0xCC, 0x2C, 0xAC, 0x6C, 0xEC, 0x1C, 0x9C, 0x5C, 0xDC, 0x3C, 0xBC, 0x7C, 0xFC, \n  0x02, 0x82, 0x42, 0xC2, 0x22, 0xA2, 0x62, 0xE2, 0x12, 0x92, 0x52, 0xD2, 0x32, 0xB2, 0x72, 0xF2, \n  0x0A, 0x8A, 0x4A, 0xCA, 0x2A, 0xAA, 0x6A, 0xEA, 0x1A, 0x9A, 0x5A, 0xDA, 0x3A, 0xBA, 0x7A, 0xFA,\n  0x06, 0x86, 0x46, 0xC6, 0x26, 0xA6, 0x66, 0xE6, 0x16, 0x96, 0x56, 0xD6, 0x36, 0xB6, 0x76, 0xF6, \n  0x0E, 0x8E, 0x4E, 0xCE, 0x2E, 0xAE, 0x6E, 0xEE, 0x1E, 0x9E, 0x5E, 0xDE, 0x3E, 0xBE, 0x7E, 0xFE,\n  0x01, 0x81, 0x41, 0xC1, 0x21, 0xA1, 0x61, 0xE1, 0x11, 0x91, 0x51, 0xD1, 0x31, 0xB1, 0x71, 0xF1,\n  0x09, 0x89, 0x49, 0xC9, 0x29, 0xA9, 0x69, 0xE9, 0x19, 0x99, 0x59, 0xD9, 0x39, 0xB9, 0x79, 0xF9, \n  0x05, 0x85, 0x45, 0xC5, 0x25, 0xA5, 0x65, 0xE5, 0x15, 0x95, 0x55, 0xD5, 0x35, 0xB5, 0x75, 0xF5,\n  0x0D, 0x8D, 0x4D, 0xCD, 0x2D, 0xAD, 0x6D, 0xED, 0x1D, 0x9D, 0x5D, 0xDD, 0x3D, 0xBD, 0x7D, 0xFD,\n  0x03, 0x83, 0x43, 0xC3, 0x23, 0xA3, 0x63, 0xE3, 0x13, 0x93, 0x53, 0xD3, 0x33, 0xB3, 0x73, 0xF3, \n  0x0B, 0x8B, 0x4B, 0xCB, 0x2B, 0xAB, 0x6B, 0xEB, 0x1B, 0x9B, 0x5B, 0xDB, 0x3B, 0xBB, 0x7B, 0xFB,\n  0x07, 0x87, 0x47, 0xC7, 0x27, 0xA7, 0x67, 0xE7, 0x17, 0x97, 0x57, 0xD7, 0x37, 0xB7, 0x77, 0xF7, \n  0x0F, 0x8F, 0x4F, 0xCF, 0x2F, 0xAF, 0x6F, 0xEF, 0x1F, 0x9F, 0x5F, 0xDF, 0x3F, 0xBF, 0x7F, 0xFF\n};\n\nunsigned int v; // reverse 32-bit value, 8 bits at time\nunsigned int c; // c will get v reversed\n\n// Option 1:\nc = (BitReverseTable256[v &amp; 0xff] &lt;&lt; 24) | \n    (BitReverseTable256[(v &gt;&gt; 8) &amp; 0xff] &lt;&lt; 16) | \n    (BitReverseTable256[(v &gt;&gt; 16) &amp; 0xff] &lt;&lt; 8) |\n    (BitReverseTable256[(v &gt;&gt; 24) &amp; 0xff]);\n\n// Option 2:\nunsigned char * p = (unsigned char *) &amp;v;\nunsigned char * q = (unsigned char *) &amp;c;\nq[3] = BitReverseTable256[p[0]]; \nq[2] = BitReverseTable256[p[1]]; \nq[1] = BitReverseTable256[p[2]]; \nq[0] = BitReverseTable256[p[3]];\n</code></pre>\n\n<p>You can extend this idea to 64-bit <code>int</code>s, or trade off memory for speed (assuming your L1 Data Cache is large enough), and reverse 16-bits at a time with a 64K-entry lookup table.</p>\n\n<hr>\n\n<h2>Others</h2>\n\n<p><strong>Simple</strong></p>\n\n<pre><code>unsigned int v;     // input bits to be reversed\nunsigned int r = v; // r will be reversed bits of v; first get LSB of v\nint s = sizeof(v) * CHAR_BIT - 1; // extra shift needed at end\n\nfor (v &gt;&gt;= 1; v; v &gt;&gt;= 1)\n{   \n  r &lt;&lt;= 1;\n  r |= v &amp; 1;\n  s--;\n}\nr &lt;&lt;= s; // shift when v's highest bits are zero\n</code></pre>\n\n<p><strong>Faster (32-bit processor)</strong></p>\n\n<pre><code>unsigned char b = x;\nb = ((b * 0x0802LU &amp; 0x22110LU) | (b * 0x8020LU &amp; 0x88440LU)) * 0x10101LU &gt;&gt; 16; \n</code></pre>\n\n<p><strong>Faster (64-bit processor)</strong></p>\n\n<pre><code>unsigned char b; // reverse this (8-bit) byte\nb = (b * 0x0202020202ULL &amp; 0x010884422010ULL) % 1023;\n</code></pre>\n\n<p>If you want to do this on a 32-bit <code>int</code>, just reverse the bits in each bytes, and reverse the order of the bytes.  That is:</p>\n\n<pre><code>unsigned int toReverse;\nunsigned int reversed;\nunsigned char inByte0 = (toReverse &amp; 0xFF);\nunsigned char inByte1 = (toReverse &amp; 0xFF00) &gt;&gt; 8;\nunsigned char inByte2 = (toReverse &amp; 0xFF0000) &gt;&gt; 16;\nunsigned char inByte3 = (toReverse &amp; 0xFF000000) &gt;&gt; 24;\nreversed = (reverseBits(inByte0) &lt;&lt; 24) | (reverseBits(inByte1) &lt;&lt; 16) | (reverseBits(inByte2) &lt;&lt; 8) | (reverseBits(inByte3);\n</code></pre>\n\n<hr>\n\n<h2>Results</h2>\n\n<p>I benchmarked the two most promising solutions, the lookup table, and bitwise-AND (the first one).  The test machine is a laptop w/ 4GB of DDR2-800 and a Core 2 Duo T7500 @ 2.4GHz, 4MB L2 Cache; YMMV.  I used <em>gcc</em> 4.3.2 on 64-bit Linux.  OpenMP (and the GCC bindings) were used for high-resolution timers.</p>\n\n<p><strong>reverse.c</strong></p>\n\n<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\n\nunsigned int\nreverse(register unsigned int x)\n{\n    x = (((x &amp; 0xaaaaaaaa) &gt;&gt; 1) | ((x &amp; 0x55555555) &lt;&lt; 1));\n    x = (((x &amp; 0xcccccccc) &gt;&gt; 2) | ((x &amp; 0x33333333) &lt;&lt; 2));\n    x = (((x &amp; 0xf0f0f0f0) &gt;&gt; 4) | ((x &amp; 0x0f0f0f0f) &lt;&lt; 4));\n    x = (((x &amp; 0xff00ff00) &gt;&gt; 8) | ((x &amp; 0x00ff00ff) &lt;&lt; 8));\n    return((x &gt;&gt; 16) | (x &lt;&lt; 16));\n\n}\n\nint main()\n{\n    unsigned int *ints = malloc(100000000*sizeof(unsigned int));\n    unsigned int *ints2 = malloc(100000000*sizeof(unsigned int));\n    for(unsigned int i = 0; i &lt; 100000000; i++)\n      ints[i] = rand();\n\n    unsigned int *inptr = ints;\n    unsigned int *outptr = ints2;\n    unsigned int *endptr = ints + 100000000;\n    // Starting the time measurement\n    double start = omp_get_wtime();\n    // Computations to be measured\n    while(inptr != endptr)\n    {\n      (*outptr) = reverse(*inptr);\n      inptr++;\n      outptr++;\n    }\n    // Measuring the elapsed time\n    double end = omp_get_wtime();\n    // Time calculation (in seconds)\n    printf(\"Time: %f seconds\\n\", end-start);\n\n    free(ints);\n    free(ints2);\n\n    return 0;\n}\n</code></pre>\n\n<p><strong>reverse_lookup.c</strong></p>\n\n<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\n\nstatic const unsigned char BitReverseTable256[] = \n{\n  0x00, 0x80, 0x40, 0xC0, 0x20, 0xA0, 0x60, 0xE0, 0x10, 0x90, 0x50, 0xD0, 0x30, 0xB0, 0x70, 0xF0, \n  0x08, 0x88, 0x48, 0xC8, 0x28, 0xA8, 0x68, 0xE8, 0x18, 0x98, 0x58, 0xD8, 0x38, 0xB8, 0x78, 0xF8, \n  0x04, 0x84, 0x44, 0xC4, 0x24, 0xA4, 0x64, 0xE4, 0x14, 0x94, 0x54, 0xD4, 0x34, 0xB4, 0x74, 0xF4, \n  0x0C, 0x8C, 0x4C, 0xCC, 0x2C, 0xAC, 0x6C, 0xEC, 0x1C, 0x9C, 0x5C, 0xDC, 0x3C, 0xBC, 0x7C, 0xFC, \n  0x02, 0x82, 0x42, 0xC2, 0x22, 0xA2, 0x62, 0xE2, 0x12, 0x92, 0x52, 0xD2, 0x32, 0xB2, 0x72, 0xF2, \n  0x0A, 0x8A, 0x4A, 0xCA, 0x2A, 0xAA, 0x6A, 0xEA, 0x1A, 0x9A, 0x5A, 0xDA, 0x3A, 0xBA, 0x7A, 0xFA,\n  0x06, 0x86, 0x46, 0xC6, 0x26, 0xA6, 0x66, 0xE6, 0x16, 0x96, 0x56, 0xD6, 0x36, 0xB6, 0x76, 0xF6, \n  0x0E, 0x8E, 0x4E, 0xCE, 0x2E, 0xAE, 0x6E, 0xEE, 0x1E, 0x9E, 0x5E, 0xDE, 0x3E, 0xBE, 0x7E, 0xFE,\n  0x01, 0x81, 0x41, 0xC1, 0x21, 0xA1, 0x61, 0xE1, 0x11, 0x91, 0x51, 0xD1, 0x31, 0xB1, 0x71, 0xF1,\n  0x09, 0x89, 0x49, 0xC9, 0x29, 0xA9, 0x69, 0xE9, 0x19, 0x99, 0x59, 0xD9, 0x39, 0xB9, 0x79, 0xF9, \n  0x05, 0x85, 0x45, 0xC5, 0x25, 0xA5, 0x65, 0xE5, 0x15, 0x95, 0x55, 0xD5, 0x35, 0xB5, 0x75, 0xF5,\n  0x0D, 0x8D, 0x4D, 0xCD, 0x2D, 0xAD, 0x6D, 0xED, 0x1D, 0x9D, 0x5D, 0xDD, 0x3D, 0xBD, 0x7D, 0xFD,\n  0x03, 0x83, 0x43, 0xC3, 0x23, 0xA3, 0x63, 0xE3, 0x13, 0x93, 0x53, 0xD3, 0x33, 0xB3, 0x73, 0xF3, \n  0x0B, 0x8B, 0x4B, 0xCB, 0x2B, 0xAB, 0x6B, 0xEB, 0x1B, 0x9B, 0x5B, 0xDB, 0x3B, 0xBB, 0x7B, 0xFB,\n  0x07, 0x87, 0x47, 0xC7, 0x27, 0xA7, 0x67, 0xE7, 0x17, 0x97, 0x57, 0xD7, 0x37, 0xB7, 0x77, 0xF7, \n  0x0F, 0x8F, 0x4F, 0xCF, 0x2F, 0xAF, 0x6F, 0xEF, 0x1F, 0x9F, 0x5F, 0xDF, 0x3F, 0xBF, 0x7F, 0xFF\n};\n\nint main()\n{\n    unsigned int *ints = malloc(100000000*sizeof(unsigned int));\n    unsigned int *ints2 = malloc(100000000*sizeof(unsigned int));\n    for(unsigned int i = 0; i &lt; 100000000; i++)\n      ints[i] = rand();\n\n    unsigned int *inptr = ints;\n    unsigned int *outptr = ints2;\n    unsigned int *endptr = ints + 100000000;\n    // Starting the time measurement\n    double start = omp_get_wtime();\n    // Computations to be measured\n    while(inptr != endptr)\n    {\n    unsigned int in = *inptr;  \n\n    // Option 1:\n    //*outptr = (BitReverseTable256[in &amp; 0xff] &lt;&lt; 24) | \n    //    (BitReverseTable256[(in &gt;&gt; 8) &amp; 0xff] &lt;&lt; 16) | \n    //    (BitReverseTable256[(in &gt;&gt; 16) &amp; 0xff] &lt;&lt; 8) |\n    //    (BitReverseTable256[(in &gt;&gt; 24) &amp; 0xff]);\n\n    // Option 2:\n    unsigned char * p = (unsigned char *) &amp;(*inptr);\n    unsigned char * q = (unsigned char *) &amp;(*outptr);\n    q[3] = BitReverseTable256[p[0]]; \n    q[2] = BitReverseTable256[p[1]]; \n    q[1] = BitReverseTable256[p[2]]; \n    q[0] = BitReverseTable256[p[3]];\n\n      inptr++;\n      outptr++;\n    }\n    // Measuring the elapsed time\n    double end = omp_get_wtime();\n    // Time calculation (in seconds)\n    printf(\"Time: %f seconds\\n\", end-start);\n\n    free(ints);\n    free(ints2);\n\n    return 0;\n}\n</code></pre>\n\n<p>I tried both approaches at several different optimizations, ran 3 trials at each level, and each trial reversed 100 million random unsigned ints.  For the lookup table option, I tried both schemes (options 1 and 2) given on the bitwise hacks page.  Results are shown below.</p>\n\n<p><strong>Bitwise AND</strong></p>\n\n<pre><code>mrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -o reverse reverse.c\nmrj10@mjlap:~/code$ ./reverse\nTime: 2.000593 seconds\nmrj10@mjlap:~/code$ ./reverse\nTime: 1.938893 seconds\nmrj10@mjlap:~/code$ ./reverse\nTime: 1.936365 seconds\nmrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -O2 -o reverse reverse.c\nmrj10@mjlap:~/code$ ./reverse\nTime: 0.942709 seconds\nmrj10@mjlap:~/code$ ./reverse\nTime: 0.991104 seconds\nmrj10@mjlap:~/code$ ./reverse\nTime: 0.947203 seconds\nmrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -O3 -o reverse reverse.c\nmrj10@mjlap:~/code$ ./reverse\nTime: 0.922639 seconds\nmrj10@mjlap:~/code$ ./reverse\nTime: 0.892372 seconds\nmrj10@mjlap:~/code$ ./reverse\nTime: 0.891688 seconds\n</code></pre>\n\n<p><strong>Lookup Table (option 1)</strong></p>\n\n<pre><code>mrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -o reverse_lookup reverse_lookup.c\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.201127 seconds              \nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.196129 seconds              \nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.235972 seconds              \nmrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -O2 -o reverse_lookup reverse_lookup.c\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 0.633042 seconds              \nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 0.655880 seconds              \nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 0.633390 seconds              \nmrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -O3 -o reverse_lookup reverse_lookup.c\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 0.652322 seconds              \nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 0.631739 seconds              \nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 0.652431 seconds  \n</code></pre>\n\n<p><strong>Lookup Table (option 2)</strong></p>\n\n<pre><code>mrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -o reverse_lookup reverse_lookup.c\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.671537 seconds\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.688173 seconds\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.664662 seconds\nmrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -O2 -o reverse_lookup reverse_lookup.c\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.049851 seconds\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.048403 seconds\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.085086 seconds\nmrj10@mjlap:~/code$ gcc -fopenmp -std=c99 -O3 -o reverse_lookup reverse_lookup.c\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.082223 seconds\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.053431 seconds\nmrj10@mjlap:~/code$ ./reverse_lookup\nTime: 1.081224 seconds\n</code></pre>\n\n<h2>Conclusion</h2>\n\n<p><strong>Use the lookup table, with option 1</strong> (byte addressing is unsurprisingly slow) if you're concerned about performance.  If you need to squeeze every last byte of memory out of your system (and you might, if you care about the performance of bit reversal), the optimized versions of the bitwise-AND approach aren't too shabby either.</p>\n\n<h2>Caveat</h2>\n\n<p>Yes, I know the benchmark code is a complete hack.  Suggestions on how to improve it are more than welcome.  Things I know about:</p>\n\n<ul>\n<li>I don't have access to ICC.  This may be faster (please respond in a comment if you can test this out).</li>\n<li>A 64K lookup table may do well on some modern microarchitectures with large L1D.</li>\n<li>-mtune=native didn't work for -O2/-O3 (<code>ld</code> blew up with some crazy symbol redefinition error), so I don't believe the generated code is tuned for my microarchitecture.</li>\n<li>There may be a way to do this slightly faster with SSE.  I have no idea how, but with fast replication, packed bitwise AND, and swizzling instructions, there's got to be something there.</li>\n<li>I know only enough x86 assembly to be dangerous; here's the code GCC generated on -O3 for option 1, so somebody more knowledgable than myself can check it out:</li>\n</ul>\n\n<p><strong>32-bit</strong></p>\n\n<pre><code>.L3:\nmovl    (%r12,%rsi), %ecx\nmovzbl  %cl, %eax\nmovzbl  BitReverseTable256(%rax), %edx\nmovl    %ecx, %eax\nshrl    $24, %eax\nmov     %eax, %eax\nmovzbl  BitReverseTable256(%rax), %eax\nsall    $24, %edx\norl     %eax, %edx\nmovzbl  %ch, %eax\nshrl    $16, %ecx\nmovzbl  BitReverseTable256(%rax), %eax\nmovzbl  %cl, %ecx\nsall    $16, %eax\norl     %eax, %edx\nmovzbl  BitReverseTable256(%rcx), %eax\nsall    $8, %eax\norl     %eax, %edx\nmovl    %edx, (%r13,%rsi)\naddq    $4, %rsi\ncmpq    $400000000, %rsi\njne     .L3\n</code></pre>\n\n<p>EDIT: I also tried using uint64_t's on my machine to see if there was any performance boost.  Performance was about 10% faster than 32-bit, and was nearly identical whether you were just using 64-bit types to reverse bits on two 32-bit ints at a time, or whether you were actually reversing bits in half as many 64-bit values.  The assembly code is shown below (for the former case, reversing bits for 2 32-bit ints at a time):</p>\n\n<pre><code>.L3:\nmovq    (%r12,%rsi), %rdx\nmovq    %rdx, %rax\nshrq    $24, %rax\nandl    $255, %eax\nmovzbl  BitReverseTable256(%rax), %ecx\nmovzbq  %dl,%rax\nmovzbl  BitReverseTable256(%rax), %eax\nsalq    $24, %rax\norq     %rax, %rcx\nmovq    %rdx, %rax\nshrq    $56, %rax\nmovzbl  BitReverseTable256(%rax), %eax\nsalq    $32, %rax\norq     %rax, %rcx\nmovzbl  %dh, %eax\nshrq    $16, %rdx\nmovzbl  BitReverseTable256(%rax), %eax\nsalq    $16, %rax\norq     %rax, %rcx\nmovzbq  %dl,%rax\nshrq    $16, %rdx\nmovzbl  BitReverseTable256(%rax), %eax\nsalq    $8, %rax\norq     %rax, %rcx\nmovzbq  %dl,%rax\nshrq    $8, %rdx\nmovzbl  BitReverseTable256(%rax), %eax\nsalq    $56, %rax\norq     %rax, %rcx\nmovzbq  %dl,%rax\nshrq    $8, %rdx\nmovzbl  BitReverseTable256(%rax), %eax\nandl    $255, %edx\nsalq    $48, %rax\norq     %rax, %rcx\nmovzbl  BitReverseTable256(%rdx), %eax\nsalq    $40, %rax\norq     %rax, %rcx\nmovq    %rcx, (%r13,%rsi)\naddq    $8, %rsi\ncmpq    $400000000, %rsi\njne     .L3\n</code></pre>\n    "},{"t":"Are 2^n and n*2^n in the same time complexity?","l":"http://stackoverflow.com/questions/21764861/are-2n-and-n2n-in-the-same-time-complexity","q":"\n\n<p>Resources I've found on time complexity are unclear about when it is okay to ignore terms in a time complexity equation, specifically with non-polynomial examples. </p>\n\n<p>It's clear to me that given something of the form n<sup>2</sup> + n + 1, the last two terms are insignificant. </p>\n\n<p>Specifically, given two categorizations, 2<sup>n</sup>, and n*(2<sup>n</sup>), is the second in the same order as the first? Does the additional n multiplication there matter? Usually resources just say x<sup>n</sup> is in an exponential and grows much faster... then move on.</p>\n\n<p>I can understand why it wouldn't since 2<sup>n</sup> will greatly outpace n, but because they're not being added together, it would matter greatly when comparing the two equations, in fact the difference between them will always be a factor of n, which seems important to say the least.</p>\n    ","a":"\n<p>You will have to go to the formal definition of the big O (<code>O</code>) in order to answer this question. </p>\n\n<p>The definition is that <code>f(x)</code> belongs to <code>O(g(x))</code> if and only if the limit <code>limsup<sub>x → ∞</sub> (f(x)/g(x))</code> exists i.e. is not infinity. In short this means that there exists a constant <code>M</code>, such that value of <code>f(x)/g(x)</code> is never greater than <code>M</code>. </p>\n\n<p>In the case of your question let <code>f(n) = n ⋅ 2<sup>n</sup></code> and let <code>g(n) = 2<sup>n</sup></code>. Then <code>f(n)/g(n)</code> is <code>n</code> which will still grow infinitely. Therefore <code>f(n)</code> does not belong to <code>O(g(n))</code>.</p>\n    "},{"t":"Is there a way to measure how sorted a list is?","l":"http://stackoverflow.com/questions/16994668/is-there-a-way-to-measure-how-sorted-a-list-is","q":"\n\n<p>Is there is a way to measure how sorted a list is?</p>\n\n<p>I mean, it's not about knowing if a list is sorted or not (boolean), but something like a ratio of \"sortness\", something like the coefficient of correlation in statistics.</p>\n\n<p>For example, </p>\n\n<ul>\n<li><p>If the items of a list are in ascending order, then its rate would be 1.0</p></li>\n<li><p>If list is sorted descending, its rate would be -1.0</p></li>\n<li><p>If list is almost sorted ascending, its rate would be 0.9 or some value close to 1.</p></li>\n<li><p>If the list is not sorted at all (random), its rate would be close to 0</p></li>\n</ul>\n\n<p>I'm writting a small library in Scala for practice. I think a sorting rate would be useful, but I don't find any information about something like that. Maybe I don't know adequate terms for the concept.</p>\n    ","a":"\n<p>You can simply count the number of inversions in the list.</p>\n\n<h1>Inversion</h1>\n\n<p>An inversion in a sequence of elements of type <code>T</code> is a pair of sequence elements that appear out of order according to some ordering <code>&lt;</code> on the set of <code>T</code>'s.</p>\n\n<p><em>From <a href=\"http://en.wikipedia.org/wiki/Inversion_%28discrete_mathematics%29\">Wikipedia</a>:</em></p>\n\n<blockquote>\n  <p>Formally, let <code>A(1), A(2), ..., A(n)</code> be a sequence of <code>n</code> numbers.<br>If <code>i &lt; j</code> and <code>A(i) &gt; A(j)</code>, then the pair <code>(i,j)</code> is called an <strong>inversion</strong> of <code>A</code>.</p>\n  \n  <p>The <strong>inversion number</strong> of a sequence is one common measure of its sortedness.<br>Formally, the inversion number is defined to be the number of inversions, that is,</p>\n  \n  <p><img src=\"http://i.stack.imgur.com/KSkHx.png\" alt=\"definition\"></p>\n</blockquote>\n\n<p>To make these definitions clearer, consider the example sequence <code>9, 5, 7, 6</code>. This sequence has the <strong>inversions</strong> <code>(0,1), (0,2), (0,3), (2,3)</code> and the <strong>inversion number</strong> <code>4</code>.</p>\n\n<p>If you want a value between <code>0</code> and <code>1</code>, you can divide the inversion number by <code>N choose 2</code>.</p>\n\n<p>To actually create an algorithm to compute this score for how sorted a list is, you have two approaches:</p>\n\n<h1>Approach 1 (Deterministic)</h1>\n\n<p>Modify your favorite sorting algorithm to keep track of how many inversions it is correcting as it runs. Though this is nontrivial and has varying implementations depending on the sorting algorithm you choose, you will end up with an algorithm that is no more expensive (in terms of complexity) than the sorting algorithm you started with.</p>\n\n<p>If you take this route, be aware that it's not as simple as counting \"swaps.\" Mergesort, for example, is worst case <code>O(N log N)</code>, yet if it is run on a list sorted in descending order, it will correct all <code>N choose 2</code> inversions.  That's <code>O(N^2)</code> inversions corrected in <code>O(N log N)</code> operations.  So some operations must inevitably be correcting more than one inversion at a time. You have to be careful with your implementation. <strong>Note: you can do this with <code>O(N log N)</code> complexity, it's just tricky.</strong></p>\n\n<p>Related: <a href=\"http://stackoverflow.com/questions/6523712/calculating-the-number-of-inversions-in-a-permutation\">calculating the number of “inversions” in a permutation</a></p>\n\n<h1>Approach 2 (Stochastic)</h1>\n\n<ul>\n<li>Randomly sample pairs <code>(i,j)</code>, where <code>i != j</code></li>\n<li>For each pair, determine whether <code>list[min(i,j)] &lt; list[max(i,j)]</code> (0 or 1)</li>\n<li>Compute the average of these comparisons and then normalize by <code>N choose 2</code></li>\n</ul>\n\n<hr>\n\n<p>I would personally go with the stochastic approach unless you have a requirement of exactness - if only because it's so easy to implement.</p>\n\n<hr>\n\n<p>If what you really want is a value (<code>z'</code>) between <code>-1</code> (sorted descending) to <code>1</code> (sorted ascending), you can simply map the value above (<code>z</code>), which is between <code>0</code> (sorted ascending) and <code>1</code> (sorted descending), to this range using this formula:</p>\n\n<pre><code>z' = -2 * z + 1\n</code></pre>\n    "},{"t":"Is 161803398 A 'Special' Number? Inside of Math.Random()","l":"http://stackoverflow.com/questions/23688168/is-161803398-a-special-number-inside-of-math-random","q":"\n\n<p><em>I suspect the answer is '<strong>Because of Math</strong>', but I was hoping someone could give a little more insight at a basic level...</em></p>\n\n<p>I was poking around in the BCL source code today, having a look at how some of the classes I've used before were actually implemented.  I'd never thought about how to generate (pseudo) random numbers before, so I decided to see how it was done.</p>\n\n<p>Full source here: <a href=\"http://referencesource.microsoft.com/#mscorlib/system/random.cs#29\">http://referencesource.microsoft.com/#mscorlib/system/random.cs#29</a></p>\n\n<pre><code>private const int MSEED = 161803398; \n</code></pre>\n\n<p>This MSEED value is used every time a Random() class is seeded.</p>\n\n<p>Anyway, I saw this 'magic number' - 161803398 - and I don't have the foggiest idea of why that number was selected.  It's not a prime number or a power of 2.  It's not 'half way' to a number that seemed more significant.  I looked at it in binary and hex and well, it just looked like a number to me.</p>\n\n<p>I tried searching for the number in Google, but I found nothing. </p>\n    ","a":"\n<p>No, but it's based on Phi (the \"golden ratio\").</p>\n\n<pre><code>161803398 = 1.61803398 * 10^8 ≈ φ * 10^8\n</code></pre>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Golden_ratio\">More about the golden ratio here</a>.</p>\n\n<p><a href=\"http://books.google.com/books/about/The_Golden_Ratio.html?id=bUARfgWRH14C\">And a <em>really</em> good read for the casual mathematician here</a>.</p>\n\n<p>And I found <a href=\"http://portal.idc.ac.il/en/schools/cs/research/documents/sinai_2011.pdf\">a research paper on random number generators</a> that agrees with this assertion.  (See page 53.)</p>\n    "},{"t":"Need for predictable random generator","l":"http://stackoverflow.com/questions/910215/need-for-predictable-random-generator","q":"\n\n<p>I'm a web-game developer and I got a problem with random numbers. Let's say that a player has 20% chance to get a critical hit with his sword. That means, 1 out of 5 hits should be critical. The problem is I got very bad real life results -- sometimes players get 3 crits in 5 hits, sometimes none in 15 hits. Battles are rather short (3-10 hits) so it's important to get good random distribution.</p>\n\n<p>Currently I use PHP mt_rand(), but we are just moving our code to C++, so I want to solve this problem in our game's new engine.</p>\n\n<p>I don't know if the solution is some uniform random generator, or maybe to remember previous random states to force proper distribution.</p>\n    ","a":"\n<p>I agree with the earlier answers that real randomness in small runs of some games is undesirable -- it does seem too unfair for some use cases.  </p>\n\n<p>I wrote a simple Shuffle Bag like implementation in Ruby and did some testing.  The implementation did this:</p>\n\n<ul>\n<li>If it still seems fair or we haven't reached a threshold of minimum rolls, it returns a fair hit based on the normal probability.</li>\n<li>If the observed probability from past rolls makes it seem unfair, it returns a \"fair-ifying\" hit.</li>\n</ul>\n\n<p>It is deemed unfair based on boundary probabilities.  For instance, for a probability of 20%, you could set 10% as a lower bound and 40% as an upper bound.</p>\n\n<p>Using those bounds, I found that with runs of 10 hits, <strong>14.2% of the time the true pseudorandom implementation produced results that were out of those bounds</strong>.  About 11% of the time, 0 critical hits were scored in 10 tries.  3.3% of the time, 5 or more critical hits were landed out of 10.  Naturally, using this algorithm (with a minimum roll count of 5), a much smaller amount (0.03%) of the \"Fairish\" runs were out of bounds.  Even if the below implementation is unsuitable (more clever things can be done, certainly), it is worth noting that noticably often your users will feel that it's unfair with a real pseudorandom solution.</p>\n\n<p>Here is the meat of my <code>FairishBag</code> written in Ruby.  The whole implementation and quick Monte Carlo simulation <a href=\"http://gist.github.com/118194\">is available here (gist)</a>.</p>\n\n<pre class=\"lang-ruby prettyprint-override\"><code>def fire!\n  hit = if @rolls &gt;= @min_rolls &amp;&amp; observed_probability &gt; @unfair_high\n    false\n  elsif @rolls &gt;= @min_rolls &amp;&amp; observed_probability &lt; @unfair_low\n    true\n  else\n    rand &lt;= @probability\n  end\n  @hits += 1 if hit\n  @rolls += 1\n  return hit\nend\n\ndef observed_probability\n  @hits.to_f / @rolls\nend\n</code></pre>\n\n<p><strong>Update:</strong> Using this method does increase the overall probability of getting a critical hit, to about 22% using the bounds above.  You can offset this by setting its \"real\" probability a little bit lower.  A probability of 17.5% with the fairish modification yields an observed long term probability of about 20%, and keeps the short term runs feeling fair.</p>\n    "},{"t":"Map Tiling Algorithm","l":"http://stackoverflow.com/questions/8901987/map-tiling-algorithm","q":"\n\n<h2>The Map</h2>\n\n<p>I'm making a tile based RPG with Javascript, using perlin noise heightmaps, then assigning a tile type based on the height of the noise.</p>\n\n<p>The maps end up looking something like this (in the minimap view).</p>\n\n<p><img src=\"http://i.stack.imgur.com/boOia.png\" alt=\"enter image description here\"></p>\n\n<p>I have a fairly simple algorithm which extracts the color value from each pixel on the image and converts it into a integer (0-5) depending on its postion between (0-255) which corresponds to a tile in tile dictionary. This 200x200 array is then passed to the client.</p>\n\n<p>The engine then determines the tiles from the values in the array and draws them to the canvas. So, I end up with interesting worlds that have realistic looking features: mountains, seas etc.</p>\n\n<p>Now the next thing I wanted to do was to apply some kind of blending algorithm that would cause tiles to seamlessly blend into their neighbours, <em>if</em> the neighbour is not of the same type. The example map above is what the player sees in their minimap. Onscreen they see a rendered version of the section marked by the white rectangle; where the tiles are rendered with their images rather than as single color pixels.</p>\n\n<p>This is an example of what the user would see in the map but <em>it is not the same location as the viewport above shows!</em></p>\n\n<p><img src=\"http://i.stack.imgur.com/F0f9E.png\" alt=\"enter image description here\"></p>\n\n<p>It is in this view that I want the transitioning to occur.</p>\n\n<h2>The Algorithm</h2>\n\n<p>I came up with a simple algorithm that would traverse the map within the viewport and render another image over the top of each tile, providing it was next to a tile of different type. (Not changing the map! Just rendering some extra images.) The idea of the algorithm was to profile the current tile's neighbors:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Cetyi.png\" alt=\"An example of a tile profile\"></p>\n\n<p>This is an example scenario of what the engine might have to render, with the current tile being the one marked with the X.</p>\n\n<p>A 3x3 array is created and the values around it are read in. So for this example the array would look like.</p>\n\n<pre><code>[\n    [1,2,2]\n    [1,2,2]\n    [1,1,2]\n];\n</code></pre>\n\n<p>My idea was then to work out a series of cases for the possible tile configurations. On a very simple level:</p>\n\n<pre><code>if(profile[0][1] != profile[1][1]){\n     //draw a tile which is half sand and half transparent\n     //Over the current tile -&gt; profile[1][1]\n     ...\n}\n</code></pre>\n\n<p>Which gives this result:</p>\n\n<p><img src=\"http://i.stack.imgur.com/R4UPM.png\" alt=\"Result\"></p>\n\n<p>Which works as a transition from <code>[0][1]</code> to <code>[1][1]</code>, but not from <code>[1][1]</code> to <code>[2][1]</code>, where a hard edge remains. So I figured that in that instance a corner tile would have to be used. I created two 3x3 sprite sheets that I thought would hold all the possible combinations of tiles that could be needed. Then I replicated this for all of the tiles that there are in the game (The white areas are transparent). This ends up being 16 tiles for each type of tile (The center tiles on each spritesheet are not used.) </p>\n\n<p><img src=\"http://i.stack.imgur.com/4QXGR.png\" alt=\"Sand\"><img src=\"http://i.stack.imgur.com/qKUwD.png\" alt=\"Sand2\"></p>\n\n<h2>The Ideal Outcome</h2>\n\n<p>So, with these new tiles and the correct algorithm, the example section would look like this:</p>\n\n<p><img src=\"http://i.stack.imgur.com/SYNqQ.png\" alt=\"Correct\"></p>\n\n<p>Every attempt I have made has failed though, there is always some flaw in the algorithm and the patterns end up strange. I can't seem to get all the cases right and overall it seems like a poor way of doing it.</p>\n\n<h2>A Solution?</h2>\n\n<p>So, if anyone could provide an alternative solution as to how I could create this effect, or what direction to go for writing the profiling algorithm, then I would be very grateful!</p>\n    ","a":"\n<p>The basic idea of this algorithm is to use a pre-processing step to find all edges and then select the correct smoothing tile according to the shape of the edge.</p>\n\n<p>The first step would be to find all edges. In the example below the <em>edge tiles</em> marked with an X are all green tiles with a tan tile as one or more of their eight neighbouring tiles. With different types of terrain this condition could translate to a tile being an edge tile if it has neighbours of lower terrain-number.</p>\n\n<p><img src=\"http://i.stack.imgur.com/X4Kyx.png\" alt=\"Edge tiles.\"></p>\n\n<p>Once all edge tiles are detected the next thing to do is to select the right smoothing tile for each edge tile. Here is my representation of your smoothing tiles.</p>\n\n<p><img src=\"http://i.stack.imgur.com/0VXxS.png\" alt=\"Smoothing tiles.\"></p>\n\n<p>Note that there are actually not that many different types of tiles. We need the eight outer tiles from one of the 3x3 squares but only the four corner squares from the other since the straight-edge tiles are already found in the first square. This means that there in total are 12 different cases we must distinguish between.</p>\n\n<p>Now, looking at one edge tile we can determine which way the boundary turns by looking at its four closest neighbour tiles. Marking an edge tile with X just as above we have the following six different cases.</p>\n\n<p><img src=\"http://i.stack.imgur.com/BRgLf.png\" alt=\"Six cases.\"></p>\n\n<p>These cases are used to determine the corresponding smoothing tile and we can number the smoothing tiles accordingly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/qGuxN.png\" alt=\"Smoothed tiles with numbers.\"></p>\n\n<p>There is still a choice of a or b for each case. This depends on which side the grass is on. One way to determine this could be to keep track of the orientation of the boundary but probably the simplest way to do it is to pick one tile next to the edge and see what colour it has. The image below shows the two cases 5a) and 5b) which can be distinguished between by for example checking the colour of the top right tile.</p>\n\n<p><img src=\"http://i.stack.imgur.com/3sgIZ.png\" alt=\"Choosing 5a or 5b.\"> </p>\n\n<p>The final enumeration for the original example would then look like this.</p>\n\n<p><img src=\"http://i.stack.imgur.com/oniTS.png\" alt=\"Final enumeration.\"></p>\n\n<p>And after selecting the corresponding edge tile the border would look something like this.</p>\n\n<p><img src=\"http://i.stack.imgur.com/fYBDR.png\" alt=\"Final result.\"></p>\n\n<p>As a final note I might say that this would work as long as the boundary is somewhat regular. More precisely, edge tiles that do not have exactly two edge tiles as their neighbours will have to be treated separately. This will occur for edge tiles on the edge of the map which will have a single edge neighbour and for very narrow pieces of terrain where the number of neighbouring edge tiles could be three or even four.</p>\n    "},{"t":"How do you rotate a two dimensional array?","l":"http://stackoverflow.com/questions/42519/how-do-you-rotate-a-two-dimensional-array","q":"\n\n<p>Inspired by <a href=\"http://blogs.msdn.com/oldnewthing/archive/2008/09/02/8918130.aspx\">Raymond Chen's post</a>, say you have a 4x4 two dimensional array, write a function that rotates it 90 degrees. Raymond links to a solution in pseudo code, but I'd like to see some real world stuff.</p>\n\n<pre><code>[1][2][3][4]\n[5][6][7][8]\n[9][0][1][2]\n[3][4][5][6]\n</code></pre>\n\n<p>Becomes:</p>\n\n<pre><code>[3][9][5][1]\n[4][0][6][2]\n[5][1][7][3]\n[6][2][8][4]\n</code></pre>\n\n<p><strong>Update</strong>: Nick's answer is the most straightforward, but is there a way to do it better than n^2? What if the matrix was 10000x10000?</p>\n    ","a":"\n<p>Here it is in C#</p>\n\n<pre class=\"lang-csharp prettyprint-override\"><code>int[,] array = new int[4,4] {\n    { 1,2,3,4 },\n    { 5,6,7,8 },\n    { 9,0,1,2 },\n    { 3,4,5,6 }\n};\n\nint[,] rotated = RotateMatrix(array, 4);\n\nstatic int[,] RotateMatrix(int[,] matrix, int n) {\n    int[,] ret = new int[n, n];\n\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; n; ++j) {\n            ret[i, j] = matrix[n - j - 1, i];\n        }\n    }\n\n    return ret;\n}\n</code></pre>\n    "},{"t":"How to find the kth largest element in an unsorted array of length n in O(n)?","l":"http://stackoverflow.com/questions/251781/how-to-find-the-kth-largest-element-in-an-unsorted-array-of-length-n-in-on","q":"\n\n<p>I believe there's a way to find the kth largest element in an unsorted array of length n in O(n).  Or perhaps it's \"expected\" O(n) or something.  How can we do this?</p>\n    ","a":"\n<p>This is called finding the <strong>k-th order statistic</strong>. There's a very simple randomized algorithm (called <em>quickselect</em>) taking <code>O(n)</code> average time, and a pretty complicated non-randomized algorithm taking <code>O(n)</code> worst case time. There's some info on <a href=\"http://en.wikipedia.org/wiki/Selection_algorithm\">Wikipedia</a>, but it's not very good.</p>\n\n<p>Everything you need is in <a href=\"http://c3p0demo.googlecode.com/svn/trunk/scalaDemo/script/Order_statistics.ppt\">these powerpoint slides</a>. Just to extract the basic algorithm of the <code>O(n)</code> worst-case algorithm:</p>\n\n<pre><code>Select(A,n,i):\n    Divide input into ⌈n/5⌉ groups of size 5.\n\n    /* Partition on median-of-medians */\n    medians = array of each group’s median.\n    pivot = Select(medians, ⌈n/5⌉, ⌈n/10⌉)\n    Left Array L and Right Array G = partition(A, pivot)\n\n    /* Find ith element in L, pivot, or G */\n    k = |L| + 1\n    If i = k, return pivot\n    If i &lt; k, return Select(L, k-1, i)\n    If i &gt; k, return Select(G, n-k, i-k)\n</code></pre>\n\n<p>It's also very nicely detailed in the Introduction to Algorithms book by Cormen et al.</p>\n    "},{"t":"Tricky Google interview question","l":"http://stackoverflow.com/questions/5505894/tricky-google-interview-question","q":"\n\n<p>A friend of mine is interviewing for a job. One of the interview questions got me thinking, just wanted some feedback.</p>\n\n<p>There are 2 non-negative integers: i and j. Given the following equation, find an (optimal) solution to iterate over i and j in such a way that the output is sorted.</p>\n\n<pre><code>2^i * 5^j\n</code></pre>\n\n<p>So the first few rounds would look like this:</p>\n\n<pre><code>2^0 * 5^0 = 1\n2^1 * 5^0 = 2\n2^2 * 5^0 = 4\n2^0 * 5^1 = 5\n2^3 * 5^0 = 8\n2^1 * 5^1 = 10\n2^4 * 5^0 = 16\n2^2 * 5^1 = 20\n2^0 * 5^2 = 25\n</code></pre>\n\n<p>Try as I might, I can't see a pattern. Your thoughts?</p>\n    ","a":"\n<p>Dijkstra derives an eloquent solution in \"A Discipline of Programming\". He attributes the problem to Hamming.\nHere is my implementation of Dijkstra’s solution.</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>int main()\n{\n    const int n = 20;       // Generate the first n numbers\n\n    std::vector&lt;int&gt; v(n);\n    v[0] = 1;\n\n    int i2 = 0;             // Index for 2\n    int i5 = 0;             // Index for 5\n\n    int x2 = 2 * v[i2];     // Next two candidates\n    int x5 = 5 * v[i5];\n\n    for (int i = 1; i != n; ++i)\n    {\n        int m = std::min(x2, x5);\n        std::cout &lt;&lt; m &lt;&lt; \" \";\n        v[i] = m;\n\n        if (x2 == m)\n        {\n            ++i2;\n            x2 = 2 * v[i2];\n        }\n        if (x5 == m)\n        {\n            ++i5;\n            x5 = 5 * v[i5];\n        }\n    }\n\n    std::cout &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre>\n    "},{"t":"Throwing cats out of windows","l":"http://stackoverflow.com/questions/3974077/throwing-cats-out-of-windows","q":"\n\n<p>Imagine you're in a tall building with a cat. The cat can survive a fall out of a low story window, but will die if thrown from a high floor. How can you figure out the longest drop that the cat can survive, using the least number of attempts?</p>\n\n<p>Obviously, if you only have one cat, then you can only search linearly. First throw the cat from the first floor. If it survives, throw it from the second. Eventually, after being thrown from floor f, the cat will die. You then know that floor f-1 was the maximal safe floor.</p>\n\n<p>But what if you have more than one cat? You can now try some sort of logarithmic search. Let's say that the build has 100 floors and you have two identical cats. If you throw the first cat out of the 50th floor and it dies, then you only have to search 50 floors linearly. You can do even better if you choose a lower floor for your first attempt. Let's say that you choose to tackle the problem 20 floors at a time and that the first fatal floor is #50. In that case, your first cat will survive flights from floors 20 and 40 before dying from floor 60. You just have to check floors 41 through 49 individually. That's a total of 12 attempts, which is much better than the 50 you would need had you attempted to use binary elimination.</p>\n\n<p><strong>In general, what's the best strategy and it's worst-case complexity for an n-storied building with 2 cats? What about for n floors and m cats?</strong></p>\n\n<p>Assume that all cats are equivalent: they will all survive or die from a fall from a given window. Also, every attempt is independent: if a cat survives a fall, it is completely unharmed.</p>\n\n<p>This isn't homework, although I may have solved it for school assignment once. It's just a whimsical problem that popped into my head today and I don't remember the solution. Bonus points if anyone knows the name of this problem or of the solution algorithm.</p>\n    ","a":"\n<p>You can easily write a little DP (dynamic programming) for the general case of n floors and m cats.  </p>\n\n<p>The main formula, <code>a[n][m] = min(max(a[k - 1][m - 1], a[n - k][m]) + 1) : for each k in 1..n</code>, should be self-explanatory:</p>\n\n<ul>\n<li>If first cat is thrown from k-th floor and dies, we now have <code>k - 1</code> floors to check (all below <code>k</code>) and <code>m - 1</code> cats (<code>a[k - 1][m - 1]</code>).  </li>\n<li>If cat survives, there're <code>n - k</code> floors left (all floors above <code>k</code>) and still <code>m</code> cats. </li>\n<li>The worst case of two should be chosen, hence <code>max</code>.</li>\n<li><code>+ 1</code> comes from the fact that we've just used one attempt (regardless of whether cat has survived or not).</li>\n<li>We try every possible floor to find the best result, hence <code>min(f(k)) : for k in 1..n</code>.</li>\n</ul>\n\n<p>It agrees with Google result from <strong>Gaurav Saxena</strong>'s link for (100, 2).</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>int n = 100; // number of floors\nint m = 20; // number of cats\nint INFINITY = 1000000;\n\nint[][] a = new int[n + 1][m + 1];\nfor (int i = 1; i &lt;= n; ++i) {\n    // no cats - no game\n    a[i][0] = INFINITY;\n}\n\nfor (int i = 1; i &lt;= n; ++i) {\n    for (int j = 1; j &lt;= m; ++j) {\n        // i floors, j cats\n        a[i][j] = INFINITY;\n\n        for (int k = 1; k &lt;= i; ++k) {\n            // try throw first cat from k-th floor\n            int result = Math.max(a[k - 1][j - 1], a[i - k][j]) + 1;\n            a[i][j] = Math.min(a[i][j], result);\n        }\n    }\n}\n\nSystem.out.println(a[n][m]);\n</code></pre>\n\n<p>You can easily find strategy (how to throw first cat), if you save best <code>k</code> in another array.  </p>\n\n<p>There's also a faster solution, not involving O(n^3) computations, but I'm a bit sleepy already.  </p>\n\n<p><strong>edit</strong><br>\nOh yeah, <a href=\"http://acm.timus.ru/problem.aspx?space=1&amp;num=1223\">I remember where I saw this problem before</a>.</p>\n    "},{"t":"What is the difference between a Generative and Discriminative Algorithm? [closed]","l":"http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm","q":"\n\n<p>Please help me understand the difference between a Generative and Discriminative Algorithm keeping in mind that I am just a beginner.</p>\n    ","a":"\n<p>Let's say you have input data x and you want to classify the data into labels y. A generative model learns the <strong>joint</strong> probability distribution <code>p(x,y)</code> and a discriminative model learns the <strong>conditional</strong> probability distribution <code>p(y|x)</code> - which you should read as <em>\"the probability of y given x\"</em>.</p>\n\n<p>Here's a really simple example. Suppose you have the following data in the form (x,y):</p>\n\n<p><code>(1,0), (1,0), (2,0), (2, 1)</code></p>\n\n<p><code>p(x,y)</code> is</p>\n\n<pre><code>      y=0   y=1\n     -----------\nx=1 | 1/2   0\nx=2 | 1/4   1/4\n</code></pre>\n\n<p><code>p(y|x)</code> is</p>\n\n<pre><code>      y=0   y=1\n     -----------\nx=1 | 1     0\nx=2 | 1/2   1/2\n</code></pre>\n\n<p>If you take a few minutes to stare at those two matrices, you will understand the difference between the two probability distributions.</p>\n\n<p>The distribution <code>p(y|x)</code> is the natural distribution for classifying a given example <code>x</code> into a class <code>y</code>, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model <code>p(x,y)</code>, which can be tranformed into <code>p(y|x)</code> by applying Bayes rule and then used for classification. However, the distribution <code>p(x,y)</code> can also be used for other purposes. For example you could use <code>p(x,y)</code> to <em>generate</em> likely <code>(x,y)</code> pairs.</p>\n\n<p>From the description above you might be thinking that generative models are more generally useful and therefore better, but it's not as simple as that. <a href=\"http://robotics.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf\">This paper</a> is a very popular reference on the subject of discriminative vs. generative classifiers, but it's pretty heavy going. The overall gist is that discriminative models generally outperform generative models in classification tasks.</p>\n    "},{"t":"How to check whether two lists are circularly identical in Python","l":"http://stackoverflow.com/questions/26924836/how-to-check-whether-two-lists-are-circularly-identical-in-python","q":"\n\n<p>For instance, I have lists:</p>\n\n<pre><code>a[0] = [1, 1, 1, 0, 0]\na[1] = [1, 1, 0, 0, 1]\na[2] = [0, 1, 1, 1, 0]\n# and so on\n</code></pre>\n\n<p>They seem to be different, but if it is supposed that the start and the end are connected, then they are <em>circularly</em> identical.</p>\n\n<p>The problem is, each list which I have has a length of 55 and contains only three ones and 52 zeros in it. Without circular condition, there are 26,235 (55 choose 3) lists. However, if the condition 'circular' exists, there are a huge number of circularly identical lists</p>\n\n<p>Currently I check circularly identity by following:</p>\n\n<pre><code>def is_dup(a, b):\n    for i in range(len(a)):\n        if a == list(numpy.roll(b, i)): # shift b circularly by i\n            return True\n    return False\n</code></pre>\n\n<p>This function requires 55 cyclic shift operations at the worst case. And there are 26,235 lists to be compared with each other. In short, I need 55 * 26,235 * (26,235 - 1) / 2 = 18,926,847,225 computations. It's about nearly 20 Giga!</p>\n\n<p>Is there any good way to do it with less computations? Or any data types that supports <em>circular</em>?</p>\n    ","a":"\n<p>First off, this can be done in <code>O(n)</code> in terms of the length of the list\nYou can notice that if you will duplicate your list 2 times (<code>[1, 2, 3]</code>) will be <code>[1, 2, 3, 1, 2, 3]</code> then your new list will definitely hold all possible cyclic lists.</p>\n\n<p>So all you need is to check whether the list you are searching is inside a 2 times of your starting list. In python you can achieve this in the following way (assuming that the lengths are the same).</p>\n\n<pre><code>list1 = [1, 1, 1, 0, 0]\nlist2 = [1, 1, 0, 0, 1]\nprint ' '.join(map(str, list2)) in ' '.join(map(str, list1 * 2))\n</code></pre>\n\n<p>Some explanation about my oneliner:\n<code>list * 2</code> will combine a list with itself, <code>map(str, [1, 2])</code> convert all numbers to string and  <code>' '.join()</code> will convert array <code>['1', '2', '111']</code> into a string <code>'1 2 111'</code>.</p>\n\n<p>As pointed by some people in the comments, oneliner can potentially give some false positives, so to cover all the possible edge cases:</p>\n\n<pre><code>def isCircular(arr1, arr2):\n    if len(arr1) != len(arr2):\n        return False\n\n    str1 = ' '.join(map(str, arr1))\n    str2 = ' '.join(map(str, arr2))\n    if len(str1) != len(str2):\n        return False\n\n    return str1 in str2 + ' ' + str2\n</code></pre>\n\n<p><strong>P.S.1</strong> when speaking about time complexity, it is worth noticing that <code>O(n)</code> will be achieved if substring can be found in <code>O(n)</code> time. It is not always so and depends on the implementation in your language (<a href=\"http://en.wikipedia.org/wiki/String_searching_algorithm\">although potentially it can be done in linear</a> time KMP for example).</p>\n\n<p><strong>P.S.2</strong> for people who are afraid strings operation and due to this fact think that the answer is not good. What important is complexity and speed. This algorithm potentially runs in <code>O(n)</code> time and <code>O(n)</code> space which makes it much better than anything in <code>O(n^2)</code> domain. To see this by yourself, you can run a small benchmark (creates a random list pops the first element and appends it to the end thus creating a cyclic list. You are free to do your own manipulations)</p>\n\n<pre><code>from random import random\nbigList = [int(1000 * random()) for i in xrange(10**6)]\nbigList2 = bigList[:]\nbigList2.append(bigList2.pop(0))\n\n# then test how much time will it take to come up with an answer\nfrom datetime import datetime\nstartTime = datetime.now()\nprint isCircular(bigList, bigList2)\nprint datetime.now() - startTime    # please fill free to use timeit, but it will give similar results\n</code></pre>\n\n<p>0.3 seconds on my machine. Not really long. Now try to compare this with <code>O(n^2)</code> solutions. While it is comparing it, you can travel from US to Australia (most probably by a cruise ship)</p>\n    "},{"t":"How to sort in-place using the merge sort algorithm?","l":"http://stackoverflow.com/questions/2571049/how-to-sort-in-place-using-the-merge-sort-algorithm","q":"\n\n<p>I know the question is not too specific. </p>\n\n<p>All I want is someone to tell me how to convert a normal merge sort into an in-place merge sort (or a merge sort with constant extra space overhead). </p>\n\n<p>All I can find (on the net) is pages saying \"it is too complex\" or \"out of scope of this text\". </p>\n\n<blockquote>\n  <p>The only known ways to merge in-place (without any extra space) are too complex to be reduced to practical program.  (taken <a href=\"http://www.personal.kent.edu/~rmuhamma/Algorithms/MyAlgorithms/Sorting/mergeSort.htm\">from here</a>)</p>\n</blockquote>\n\n<p>Even if it is too complex, <strong>what is the basic concept of how to make the merge sort in-place?</strong></p>\n    ","a":"\n<p>Including its \"big result\", this paper describes a couple of variants of in-place merge sort (PDF):</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.5514&amp;rep=rep1&amp;type=pdf\">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.5514&amp;rep=rep1&amp;type=pdf</a></p>\n\n<p><em>In-place sorting with fewer moves</em></p>\n\n<p>Jyrki Katajainen, Tomi A. Pasanen </p>\n\n<blockquote>\n  <p>It is shown that an array of n\n  elements can be sorted using O(1)\n  extra space, O(n log n / log log n)\n  element moves, and n log 2 n+O(n log\n  log n) comparisons. This is the first\n  in-place sorting algorithm requiring\n  o(n log n) moves in the worst case\n  while guaranteeing O(n log n)\n  comparisons, but due to the constant\n  factors involved the algorithm is\n  predominantly of theoretical interest.</p>\n</blockquote>\n\n<p>I think this is relevant too. I have a printout of it lying around, passed on to me by a colleague, but I haven't read it. It seems to cover basic theory, but I'm not familiar enough with the topic to judge how comprehensively:</p>\n\n<p><a href=\"http://comjnl.oxfordjournals.org/cgi/content/abstract/38/8/681\">http://comjnl.oxfordjournals.org/cgi/content/abstract/38/8/681</a></p>\n\n<p><em>Optimal Stable Merging</em></p>\n\n<p>Antonios Symvonis</p>\n\n<blockquote>\n  <p>This paper shows how to stably merge\n  two sequences A and B  of sizes m and\n  n, m ≤ n, respectively, with O(m+n)\n  assignments, O(mlog(n/m+1))\n  comparisons and using only a constant\n  amount of additional space. This\n  result matches all known lower bounds...</p>\n</blockquote>\n    "},{"t":"Algorithm needed for packing rectangles in a fairly optimal way","l":"http://stackoverflow.com/questions/1213394/algorithm-needed-for-packing-rectangles-in-a-fairly-optimal-way","q":"\n\n<p>Ive got a bunch of rectangular objects which I need to pack into the smallest space possible (the dimensions of this space should be powers of two).</p>\n\n<p>I'm aware of various packing algorithms that will pack the items as well as possible into a given space, however in this case I need the algorithm to work out how large that space should be as well.</p>\n\n<p>Eg say Ive got the following rectangles</p>\n\n<ul>\n<li>128*32</li>\n<li>128*64</li>\n<li>64*32</li>\n<li>64*32</li>\n</ul>\n\n<p>They can be packed into a 128*128 space</p>\n\n<pre> _________________\n|128*32          |\n|________________|\n|128*64          |\n|                |\n|                |\n|________________|\n|64*32  |64*32   |\n|_______|________|\n</pre>\n\n<p>However if there was also a 160*32 and a 64*64 one it would need a 256*128 space</p>\n\n<pre> ________________________________\n|128*32          |64*64  |64*32  |\n|________________|       |_______|\n|128*64          |       |64*32  |\n|                |_______|_______|\n|                |               |\n|________________|___            |\n|160*32              |           |\n|____________________|___________|\n</pre>\n\n<p>What algorithms are there that are able to pack a bunch of rectangles and determine the required size for the container (to a power of 2, and within a given maximum size for each dimension)?</p>\n    ","a":"\n<p>The quick and dirty first pass solution is always a great one to start with, as a comparison if nothing else.</p>\n\n<p>Greedy placement from large to small.</p>\n\n<p>Put the largest rectangle remaining into your packed area. If it can't fit anywhere, place it in a place that extends the pack region as little as possible. Repeat until you finish with the smallest rectangle.</p>\n\n<p>It's not perfect at all but it's easy and a nice baseline. It would still pack your original example perfectly, and give you an equivalent answer for the second as well.</p>\n    "},{"t":"Understanding recursion [closed]","l":"http://stackoverflow.com/questions/717725/understanding-recursion","q":"\n\n<p>I'm having major trouble understanding <em>recursion</em> at school. Whenever the professor is talking about it, I seem to get it but as soon as I try it on my own it completely blows my brains. </p>\n\n<p>I was trying to solve <em>Towers of Hanoi</em> all night and completely blew my mind. My textbook has only about 30 pages in recursion so it is not too useful. Does anyone know of books or resources that can help clarify this topic?</p>\n    ","a":"\n<p>How do you empty a vase containing five flowers?</p>\n\n<p>Answer: if the vase is not empty, you take out one flower\n and then you empty a vase containing four flowers.</p>\n\n<p>How do you empty a vase containing four flowers?</p>\n\n<p>Answer: if the vase is not empty, you take out one flower\n and then you empty a vase containing three flowers.</p>\n\n<p>How do you empty a vase containing three flowers?</p>\n\n<p>Answer: if the vase is not empty, you take out one flower\n and then you empty a vase containing two flowers.</p>\n\n<p>How do you empty a vase containing two flowers?</p>\n\n<p>Answer: if the vase is not empty, you take out one flower\n and then you empty a vase containing one flower.</p>\n\n<p>How do you empty a vase containing one flower?</p>\n\n<p>Answer: if the vase is not empty, you take out one flower\n and then you empty a vase containing no flowers.</p>\n\n<p>How do you empty a vase containing no flowers?</p>\n\n<p>Answer: if the vase is not empty, you take out one flower\n  but the vase is empty so you're done.</p>\n\n<p>That's repetitive. Let's generalize it:</p>\n\n<p>How do you empty a vase containing <em>N</em> flowers?</p>\n\n<p>Answer: if the vase is not empty, you take out one flower\n  and then you empty a vase containing <em>N-1</em> flowers.</p>\n\n<p>Hmm, can we see that in code?</p>\n\n<pre><code>void emptyVase( int flowersInVase ) {\n  if( flowersInVase &gt; 0 ) {\n   // take one flower and\n    emptyVase( flowersInVase - 1 ) ;\n\n  } else {\n   // the vase is empty, nothing to do\n  }\n}\n</code></pre>\n\n<p>Hmm, couldn't we have just done that in a for loop?</p>\n\n<p>Why yes, recursion can be replaced with iteration, but often recursion is more elegant.</p>\n\n<p>Let's talk about trees. In computer science, a <em>tree</em> is a structure made up of <em>nodes</em>, where each node has some number of children that are also nodes, or null. A <em>binary tree</em> is a tree made of nodes that have exactly <em>two</em> children, typically called \"left\" and \"right\"; again the children can be nodes, or null. A <em>root</em> is a node that is not the child of any other node.</p>\n\n<p>Imagine that a node, in addition to its children, has a value, a number, and imagine that we wish to sum all the values in some tree.</p>\n\n<p>To sum value in any one node, we would add the value of node itself to the value of its left child, if any, and the value of its right child, if any. Now recall that the children, if they're not null, are also nodes. </p>\n\n<p>So to sum the left child, we would add the value of child node itself to the value of its left child, if any, and the value of its right child, if any.</p>\n\n<p>So to sum the value of the left child's left child, we would add the value of child node itself to the value of its left child, if any, and the value of its right child, if any.</p>\n\n<p>Perhaps you've anticipated where I'm going with this, and would like to see some code? OK:</p>\n\n<pre><code>struct node {\n  node* left;\n  node* right;\n  int value;\n} ;\n\nint sumNode( node* root ) {\n  // if there is no tree, its sum is zero\n  if( root == null ) {\n    return 0 ;\n\n  } else { // there is a tree\n    return root-&gt;value + sumNode( root-&gt;left ) + sumNode( root-&gt;right ) ;\n  }\n}\n</code></pre>\n\n<p>Notice that instead of explicitly testing the children to see if they're null or nodes, we just make the recursive function return zero for a null node.</p>\n\n<p>So say we have a tree that looks like this (the numbers are values, the slashes point to children, and @ means the pointer points to null):</p>\n\n<pre><code>     5\n    / \\\n   4   3\n  /\\   /\\\n 2  1 @  @\n/\\  /\\\n@@  @@\n</code></pre>\n\n<p>If we call sumNode on the root (the node with value 5), we will return:</p>\n\n<pre><code>return root-&gt;value + sumNode( root-&gt;left ) + sumNode( root-&gt;right ) ;\nreturn 5 + sumNode( node-with-value-4 ) + sumNode( node-with-value-3 ) ;\n</code></pre>\n\n<p>Let's expand that in place. Everywhere we see sumNode, we'll replace it with the expansion of the return statement: </p>\n\n<pre><code>sumNode( node-with-value-5);\nreturn root-&gt;value + sumNode( root-&gt;left ) + sumNode( root-&gt;right ) ;\nreturn 5 + sumNode( node-with-value-4 ) + sumNode( node-with-value-3 ) ;\n\nreturn 5 + 4 + sumNode( node-with-value-2 ) + sumNode( node-with-value-1 ) \n + sumNode( node-with-value-3 ) ;  \n\nreturn 5 + 4 \n + 2 + sumNode(null ) + sumNode( null )\n + sumNode( node-with-value-1 ) \n + sumNode( node-with-value-3 ) ;  \n\nreturn 5 + 4 \n + 2 + 0 + 0\n + sumNode( node-with-value-1 ) \n + sumNode( node-with-value-3 ) ; \n\nreturn 5 + 4 \n + 2 + 0 + 0\n + 1 + sumNode(null ) + sumNode( null )\n + sumNode( node-with-value-3 ) ; \n\nreturn 5 + 4 \n + 2 + 0 + 0\n + 1 + 0 + 0\n + sumNode( node-with-value-3 ) ; \n\nreturn 5 + 4 \n + 2 + 0 + 0\n + 1 + 0 + 0\n + 3 + sumNode(null ) + sumNode( null ) ; \n\nreturn 5 + 4 \n + 2 + 0 + 0\n + 1 + 0 + 0\n + 3 + 0 + 0 ;\n\nreturn 5 + 4 \n + 2 + 0 + 0\n + 1 + 0 + 0\n + 3 ;\n\nreturn 5 + 4 \n + 2 + 0 + 0\n + 1 \n + 3  ;\n\nreturn 5 + 4 \n + 2 \n + 1 \n + 3  ;\n\nreturn 5 + 4 \n + 3\n + 3  ;\n\nreturn 5 + 7\n + 3  ;\n\nreturn 5 + 10 ;\n\nreturn 15 ;\n</code></pre>\n\n<p>Now see how we conquered a structure of arbitrary depth and \"branchiness\", by considering  it as the repeated application of a composite template? each time through our sumNode function, we dealt with only a single node, using a singe if/then branch, and two simple return statements that almost wrote themsleves, directly from our specification?</p>\n\n<pre><code>How to sum a node:\n If a node is null \n   its sum is zero\n otherwise \n   its sum is its value \n   plus the sum of its left child node\n   plus the sum of its right child node\n</code></pre>\n\n<p><em>That's</em> the power of recursion.</p>\n\n<hr>\n\n<p>The vase example above is an example of <em>tail recursion</em>. All that <em>tail recursion</em> means is that in the recursive function, if we recursed (that is, if we called the function again), that was the last thing we did.</p>\n\n<p>The tree example was not tail recursive, because even though that last thing we did was to recurse the right child, before we did that we recursed the left child.</p>\n\n<p>In fact, the order in which we called the children, and added the current node's value didn't matter at all, because addition is commutative.</p>\n\n<p>Now let's look at an operation where order does matter. We'll use a binary tree of nodes, but this time the value held will be a character, not a number.</p>\n\n<p>Our tree will have a special property, that for any node, its character comes <em>after</em> (in alphabetical order) the character held by its left child and <em>before</em> (in alphabetical order) the character held by its right child.</p>\n\n<p>What we want to do is print the tree is alphabetical order. That's easy to do, given the tree special property. We just print the left child, then the node's character, then right child.</p>\n\n<p>We don't just want to print willy-nilly, so we'll pass our function something to print on. This will be an object with a print( char ) function; we don't need to worry about how it works, just that when print is called, it'll print something, somewhere.</p>\n\n<p>Let's see that in code:</p>\n\n<pre><code>struct node {\n  node* left;\n  node* right;\n  char value;\n} ;\n\n// don't worry about this code\nclass Printer {\n  private ostream&amp; out;\n  Printer( ostream&amp; o ) :out(o) {}\n  void print( char c ) { out &lt;&lt; c; }\n}\n\n// worry about this code\nint printNode( node* root, Printer&amp; printer ) {\n  // if there is no tree, do nothing\n  if( root == null ) {\n    return ;\n\n  } else { // there is a tree\n    printNode( root-&gt;left, printer );\n    printer.print( value );\n    printNode( root-&gt;right, printer );\n}\n\nPrinter printer( std::cout ) ;\nnode* root = makeTree() ; // this function returns a tree, somehow\nprintNode( root, printer );\n</code></pre>\n\n<p>In addition to the order of operations now mattering, this example illustrates that we can pass things into a recursive function. The only thing we have to do is make sure that on each recursive call, we continue to pass it along. We passed in a node pointer and a printer to the function, and on each recursive call, we passed them \"down\".</p>\n\n<p>Now if our tree looks like this:</p>\n\n<pre><code>         k\n        / \\\n       h   n\n      /\\   /\\\n     a  j @  @\n    /\\ /\\\n    @@ i@\n       /\\\n       @@\n</code></pre>\n\n<p>What will we print?</p>\n\n<pre><code>From k, we go left to\n  h, where we go left to\n    a, where we go left to \n      null, where we do nothing and so\n    we return to a, where we print 'a' and then go right to\n      null, where we do nothing and so\n    we return to a and are done, so\n  we return to h, where we print 'h' and then go right to\n    j, where we go left to\n      i, where we go left to \n        null, where we do nothing and so\n      we return to i, where we print 'i' and then go right to\n        null, where we do nothing and so\n      we return to i and are done, so\n    we return to j, where we print 'j' and then go right to\n      null, where we do nothing and so\n    we return to j and are done, so\n  we return to h and are done, so\nwe return to k, where we print 'k' and then go right to\n  n where we go left to \n    null, where we do nothing and so\n  we return to n, where we print 'n' and then go right to\n    null, where we do nothing and so\n  we return to n and are done, so \nwe return to k and are done, so we return to the caller\n</code></pre>\n\n<p>So if we just look at the lines were we printed:</p>\n\n<pre><code>    we return to a, where we print 'a' and then go right to\n  we return to h, where we print 'h' and then go right to\n      we return to i, where we print 'i' and then go right to\n    we return to j, where we print 'j' and then go right to\nwe return to k, where we print 'k' and then go right to\n  we return to n, where we print 'n' and then go right to\n</code></pre>\n\n<p>We see we printed \"ahijkn\", which is indeed in alphabetical order.</p>\n\n<p>We manage to print an entire tree, in alphabetical order, just by knowing how to print a single node in alphabetical order. Which was just (because our tree had the special property of ordering values to the left of alphabetically later values) knowing to print the left child before printing the node's value, and tto print the right child after  printing the node's value.</p>\n\n<p>And <em>that's</em> the power of recursion: being able to do whole things by knowing only how to do a part of the whole (and knowing when to stop recursing).</p>\n\n<p>Recalling that in most languages, operator || (\"or\") short-circuits when its first operand is true, the general recursive function is:</p>\n\n<pre><code>void recurse() { doWeStop() || recurse(); } \n</code></pre>\n\n<p>Luc M comments:</p>\n\n<blockquote>\n  <p>SO should create a badge for this kind of answer. Congratulations!</p>\n</blockquote>\n\n<p>Thanks, Luc! But, actually, because I edited this answer more than four times (to add the last example, but mostly to correct typos and polish it -- typing on a tiny netbook keyboard is hard), I can't get any more points for it. Which somewhat discourages me from putting as much effort into future answers.</p>\n\n<p>See my comment here on that: <a href=\"http://stackoverflow.com/questions/128434/what-are-community-wiki-posts-in-stackoverflow/718699#718699\">http://stackoverflow.com/questions/128434/what-are-community-wiki-posts-in-stackoverflow/718699#718699</a></p>\n    "},{"t":"How to implement a queue with three stacks?","l":"http://stackoverflow.com/questions/5538192/how-to-implement-a-queue-with-three-stacks","q":"\n\n<p>I came across this question in an algorithms book (<a href=\"http://rads.stackoverflow.com/amzn/click/032157351X\">Algorithms, 4th Edition</a> by Robert Sedgewick and Kevin Wayne).  </p>\n\n<blockquote>\n  <p>Queue with three stacks. Implement a queue with three stacks so that each queue operation takes a constant (worst-case) number of stack operations. Warning : high degree of difficulty.</p>\n</blockquote>\n\n<p>I know how to make a queue with 2 stacks but I can't find the solution with 3 stacks. Any idea ?  </p>\n\n<p>(oh and, this is not homework :) )</p>\n    ","a":"\n<p>SUMMARY</p>\n\n<ul>\n<li>O(1) algorithm is known for 6 stacks</li>\n<li>O(1) algorithm is known for 3 stacks, but using lazy evaluation which in practice corresponds to having extra internal data structures, so it does not constitute a solution</li>\n<li>People near Sedgewick have confirmed they are not aware of a 3-stack solution within all the constraints of the original question</li>\n</ul>\n\n<p>DETAILS</p>\n\n<p>There are two implementations behind this link: <a href=\"http://www.eecs.usma.edu/webs/people/okasaki/jfp95/index.html\">http://www.eecs.usma.edu/webs/people/okasaki/jfp95/index.html</a></p>\n\n<p>One of them is O(1) with three stacks BUT it uses lazy execution, which in practice creates extra intermediate data structures (closures).</p>\n\n<p>Another of them is O(1) but uses SIX stacks. However, it works without lazy execution.</p>\n\n<p>UPDATE: Okasaki's paper is here: <a href=\"http://www.eecs.usma.edu/webs/people/okasaki/jfp95.ps\">http://www.eecs.usma.edu/webs/people/okasaki/jfp95.ps</a> and it seems that he actually uses only 2 stacks for the O(1) version that has lazy evaluation. The problem is that it's really based on lazy evaluation. The question is if it can be translated to a 3-stack algorithm without lazy evaluation.</p>\n\n<p>UPDATE: Another related algorithm is described in paper \"Stacks versus Deques\" by Holger Petersen, published in 7th Annual Conference on Computing and Combinatorics. You can find the article from Google Books. Check pages 225-226. But the algorithm is not actually real-time simulation, it's linear-time simulation of a double-ended queue on three stacks.</p>\n\n<p>gusbro: \"As @Leonel said some days ago, I thought it would be fair to check with Prof. Sedgewick if he knew a solution or there was some mistake. So I did write to him. I just received a response (albeit not from himself but from a colleague at Princeton) so I like to share with all of you.He basically said that he knew of no algorithms using three stacks AND the other constraints imposed (like not using lazy evaluation). He did know of an algorithm using 6 stacks as we already know looking at the answers here. So I guess the question is still open to find an algorithm (or prove one cannot be found).\"</p>\n    "},{"t":"Algorithm to detect intersection of two rectangles?","l":"http://stackoverflow.com/questions/115426/algorithm-to-detect-intersection-of-two-rectangles","q":"\n\n<p>I'm looking for an algorithm to detect if two rectangles intersect (one at an arbitrary angle, the other with only vertical/horizontal lines).</p>\n\n<p>Testing if a corner of one is in the other ALMOST works.  It fails if the rectangles form a cross-like shape.</p>\n\n<p>It seems like a good idea to avoid using slopes of the lines, which would require special cases for vertical lines.</p>\n    ","a":"\n<p>The standard method would be to do the <strong>separating axis test</strong> (do a google search on that).</p>\n\n<p>In short:</p>\n\n<ul>\n<li>Two objects don't intersect if you can find a line that separates the two objects. e.g. the objects / all points of an object are on different sides of the line.</li>\n</ul>\n\n<p>The fun thing is, that it's sufficient to just check all edges of the two rectangles. If the rectangles don't overlap one of the edges will be the separating axis.</p>\n\n<p>In 2D you can do this without using slopes. An edge is simply defined as the difference between two vertices, e.g.</p>\n\n<pre><code>  edge = v(n) - v(n-1)\n</code></pre>\n\n<p>You can get a perpendicular to this by rotating it by 90°. In 2D this is easy as:</p>\n\n<pre><code>  rotated.x = -unrotated.y\n  rotated.y =  unrotated.x\n</code></pre>\n\n<p>So no trigonometry or slopes involved. Normalizing the vector to unit-length is not required either.</p>\n\n<p>If you want to test if a point is on one or another side of the line you can just use the dot-product. the sign will tell you which side you're on:</p>\n\n<pre><code>  // rotated: your rotated edge\n  // v(n-1) any point from the edge.\n  // testpoint: the point you want to find out which side it's on.\n\n  side = sign (rotated.x * (testpoint.x - v(n-1).x) + \n               rotated.y * (testpoint.y - v(n-1).y);\n</code></pre>\n\n<p>Now test all points of rectangle A against the edges of rectangle B and vice versa. If you find a separating edge the objects don't intersect (providing all other points in B are on the other side of the edge being tested for - see drawing below). If you find no separating edge either the rectangles are intersecting or one rectangle is contained in the other.</p>\n\n<p>The test works with any convex polygons btw.. </p>\n\n<p><strong>Amendment:</strong> To identify a separating edge, it is not enough to test all points of one rectangle against each edge of the other. The candidate-edge E (below) would as such be identified as a separating edge, as all points in A are in the same half-plane of E. However, it isn't a separating edge because the vertices Vb1 and Vb2 of B are also in that half-plane. It would only have been a separating edge if that had not been the case\n<img src=\"http://www.iassess.com/collision.png\"></p>\n    "},{"t":"Set every cell in matrix to 0 if that row or column contains a 0","l":"http://stackoverflow.com/questions/339262/set-every-cell-in-matrix-to-0-if-that-row-or-column-contains-a-0","q":"\n\n<p>Given a NxN matrix with 0s and 1s. Set every row that contains a <code>0</code> to all <code>0</code>s and set every column that contains a <code>0</code> to all <code>0</code>s.</p>\n\n<p>For example</p>\n\n<pre><code>1 0 1 1 0\n0 1 1 1 0\n1 1 1 1 1\n1 0 1 1 1\n1 1 1 1 1\n</code></pre>\n\n<p>results in</p>\n\n<pre><code>0 0 0 0 0\n0 0 0 0 0\n0 0 1 1 0\n0 0 0 0 0\n0 0 1 1 0\n</code></pre>\n\n<p>A Microsoft Engineer told me that there is a solution that involves no extra memory, just two boolean variables and one pass, so I'm looking for that answer.</p>\n\n<p>BTW, imagine it is a bit matrix, therefore just 1s and 0s are allow to be in the matrix.</p>\n    ","a":"\n<p>Ok, so I'm tired as it's 3AM here, but I have a first try inplace with exactly 2 passes on each number in the matrix, so in O(NxN) and it is linear in the size of the matrix.</p>\n\n<p>I use 1rst column and first row as markers to know where are rows/cols with only 1's. Then, there are 2 variables l and c to remember if 1rst row/column are all 1's also.\nSo the first pass sets the markers and resets the rest to 0's.</p>\n\n<p>The second pass sets 1 in places where rows and cols where marked to be 1, and resets 1st line/col depending on l and c. </p>\n\n<p>I doubt strongly that I can be done in 1 pass as squares in the beginning depend on squares in the end. Maybe my 2nd pass can be made more efficient...</p>\n\n<pre><code>import pprint\n\nm = [[1, 0, 1, 1, 0],\n     [0, 1, 1, 1, 0],\n     [1, 1, 1, 1, 1],\n     [1, 0, 1, 1, 1],\n     [1, 1, 1, 1, 1]]\n\n\n\nN = len(m)\n\n### pass 1\n\n# 1 rst line/column\nc = 1\nfor i in range(N):\n    c &amp;= m[i][0]\n\nl = 1\nfor i in range(1,N):\n    l &amp;= m[0][i]\n\n\n# other line/cols\n# use line1, col1 to keep only those with 1\nfor i in range(1,N):\n    for j in range(1,N):\n        if m[i][j] == 0:\n            m[0][j] = 0\n            m[i][0] = 0\n        else:\n            m[i][j] = 0\n\n### pass 2\n\n# if line1 and col1 are ones: it is 1\nfor i in range(1,N):\n    for j in range(1,N):\n        if m[i][0] &amp; m[0][j]:\n            m[i][j] = 1\n\n# 1rst row and col: reset if 0\nif l == 0:\n    for i in range(N):\n        m [i][0] = 0\n\nif c == 0:\n    for j in range(1,N):\n        m [0][j] = 0\n\n\npprint.pprint(m)\n</code></pre>\n    "},{"t":"Algorithm to calculate the number of divisors of a given number","l":"http://stackoverflow.com/questions/110344/algorithm-to-calculate-the-number-of-divisors-of-a-given-number","q":"\n\n<p>What would be the most optimal algorithm (performance-wise) to calculate the number of divisors of a given number?</p>\n\n<p>It'll be great if you could provide pseudocode or a link to some example.</p>\n\n<p>EDIT: All the answers have been very helpful, thank you. I'm implementing the Sieve of Atkin and then I'm going to use something similar to what Jonathan Leffler indicated. The link posted by Justin Bozonier has further information on what I wanted.</p>\n    ","a":"\n<p>Dmitriy is right that you'll want the Sieve of Atkin to generate the prime list but I don't believe that takes care of the whole issue. Now that you have a list of primes you'll need to see how many of those primes act as a divisor (and how often).</p>\n\n<p><strike><a href=\"http://mail.python.org/pipermail/python-list/2005-March/315250.html\">Here's some python for the algo</a></strike> <a href=\"http://mail.python.org/pipermail/python-list/2005-March.txt\">Look here</a> and search for \"Subject: math - need divisors algorithm\". Just count the number of items in the list instead of returning them however.</p>\n\n<p><a href=\"http://mathforum.org/library/drmath/view/55843.html\">Here's a Dr. Math</a> that explains what exactly it is you need to do mathematically.</p>\n\n<p>Essentially it boils down to if your number <code>n</code> is:<br>\n <code>n = a^x * b^y * c^z</code><br>\n(where a, b, and c are n's prime divisors and x, y, and z are the number of times that divisor is repeated) \nthen the total count for all of the divisors is:<br>\n<code>(x + 1) * (y + 1) * (z + 1)</code>.</p>\n\n<p>Edit: BTW, to find a,b,c,etc you'll want to do what amounts to a greedy algo if I'm understanding this correctly. Start with your largest prime divisor and multiply it by itself until a further multiplication would exceed the number n. Then move to the next lowest factor and times the previous prime ^ number of times it was multiplied by the current prime and keep multiplying by the prime until the next will exceed n... etc. Keep track of the number of times you multiply the divisors together and apply those numbers into the formula above.</p>\n\n<p>Not 100% sure about my algo description but if that isn't it it's something similar .</p>\n    "},{"t":"The most efficient way to implement an integer based power function pow(int, int)","l":"http://stackoverflow.com/questions/101439/the-most-efficient-way-to-implement-an-integer-based-power-function-powint-int","q":"\n\n<p>What is the most efficient way given to raise an integer to the power of another integer in C?</p>\n\n<pre><code>// 2^3\npow(2,3) == 8\n\n// 5^5\npow(5,5) == 3125\n</code></pre>\n    ","a":"\n<p>Exponentiation by squaring.</p>\n\n<pre><code>int ipow(int base, int exp)\n{\n    int result = 1;\n    while (exp)\n    {\n        if (exp &amp; 1)\n            result *= base;\n        exp &gt;&gt;= 1;\n        base *= base;\n    }\n\n    return result;\n}\n</code></pre>\n\n<p>This is the standard method for doing modular exponentiation for huge numbers in asymmetric cryptography.</p>\n    "},{"t":"Has anyone actually implemented a Fibonacci-Heap efficiently?","l":"http://stackoverflow.com/questions/504823/has-anyone-actually-implemented-a-fibonacci-heap-efficiently","q":"\n\n<p>Has anyone of you ever implemented a <a href=\"http://en.wikipedia.org/wiki/Fibonacci_heap\">Fibonacci-Heap</a>? I did so a few years back, but it was several orders of magnitude slower than using array-based BinHeaps.</p>\n\n<p>Back then, I thought of it as a valuable lesson in how research is not always as good as it claims to be. However, a lot of research papers claim the running times of their algorithms based on using a Fibonacci-Heap. </p>\n\n<p>Did you ever manage to produce an efficient implementation? Or did you work with data-sets so large that the Fibonacci-Heap was more efficient? If so, some details would be appreciated.</p>\n    ","a":"\n<p><strike>The <a href=\"http://www.boost.org/\">Boost C++ libraries</a> include an implementation of Fibonacci heaps in <code>boost/pending/fibonacci_heap.hpp</code>.  This file has apparently been in <code>pending/</code> for years and by my projections will never be accepted.  Also, there have been bugs in that implementation, which were fixed by my acquaintance and all-around cool guy Aaron Windsor.  Unfortunately, most of the versions of that file that I could find online (and the one in Ubuntu's libboost-dev package) still had the bugs; I had to pull <a href=\"http://svn.boost.org/svn/boost/trunk/boost/pending/fibonacci_heap.hpp\">a clean version</a> from the Subversion repository. </strike></p>\n\n<p>Since version <a href=\"http://www.boost.org/doc/libs/1_49_0/doc/html/heap.html\">1.49</a> <a href=\"http://www.boost.org/\">Boost C++ libraries</a> added a lot of new heaps structs including fibonacci heap.</p>\n\n<p>I was able to compile <a href=\"http://svn.boost.org/svn/boost/trunk/libs/graph/test/dijkstra_heap_performance.cpp\">dijkstra_heap_performance.cpp</a> against a modified version of <a href=\"http://svn.boost.org/svn/boost/trunk/boost/graph/dijkstra_shortest_paths.hpp\">dijkstra_shortest_paths.hpp</a> to compare Fibonacci heaps and binary heaps.  (In the line <code>typedef relaxed_heap&lt;Vertex, IndirectCmp, IndexMap&gt; MutableQueue</code>, change <code>relaxed</code> to <code>fibonacci</code>.)  I first forgot to compile with optimizations, in which case Fibonacci and binary heaps perform about the same, with Fibonacci heaps usually outperforming by an insignificant amount.  After I compiled with very strong optimizations, binary heaps got an enormous boost.  In my tests, Fibonacci heaps only outperformed binary heaps when the graph was incredibly large and dense, e.g.:</p>\n\n<pre><code>Generating graph...10000 vertices, 20000000 edges.\nRunning Dijkstra's with binary heap...1.46 seconds.\nRunning Dijkstra's with Fibonacci heap...1.31 seconds.\nSpeedup = 1.1145.\n</code></pre>\n\n<p>As far as I understand, this touches at the fundamental differences between Fibonacci heaps and binary heaps.  The only real theoretical difference between the two data structures is that Fibonacci heaps support decrease-key in (amortized) constant time.  On the other hand, binary heaps get a great deal of performance from their implementation as an array; using an explicit pointer structure means Fibonacci heaps suffer a huge performance hit.</p>\n\n<p>Therefore, to benefit from Fibonacci heaps <em>in practice</em>, you have to use them in an application where decrease_keys are incredibly frequent.  In terms of Dijkstra, this means that the underlying graph is dense.  Some applications could be intrinsically decrease_key-intense; I wanted to try <a href=\"http://www.cs.amherst.edu/ccm/challenge5/p_queue/index.html\">the Nagomochi-Ibaraki minimum-cut algorithm</a> because apparently it generates lots of decrease_keys, but it was too much effort to get a timing comparison working.</p>\n\n<p><em>Warning</em>:  I may have done something wrong.  You may wish to try reproducing these results yourself.</p>\n\n<p><em>Theoretical note</em>: The improved performance of Fibonacci heaps on decrease_key is important for theoretical applications, such as Dijkstra's runtime.  Fibonacci heaps also outperform binary heaps on insertion and merging (both amortized constant-time for Fibonacci heaps).  Insertion is essentially irrelevant, because it doesn't affect Dijkstra's runtime, and it's fairly easy to modify binary heaps to also have insert in amortized constant time.  Merge in constant time is fantastic, but not relevant to this application.</p>\n\n<p><em>Personal note</em>:  A friend of mine and I once wrote a paper explaining a new priority queue, which attempted to replicate the (theoretical) running time of Fibonacci heaps without their complexity.  The paper was never published, but my coauthor did implement binary heaps, Fibonacci heaps, and our own priority queue to compare the data structures.  The graphs of the experimental results indicate that Fibonacci heaps slightly out-performed binary heaps in terms of total comparisons, suggesting that Fibonacci heaps would perform better in a situation where comparison cost exceeds overhead.  Unfortunately, I do not have the code available, and presumably in your situation comparison is cheap, so these comments are relevant but not directly applicable.</p>\n\n<p>Incidentally, I highly recommend trying to match the runtime of Fibonacci heaps with your own data structure.  I found that I simply reinvented Fibonacci heaps myself.  Before I thought that all of the complexities of Fibonacci heaps were some random ideas, but afterward I realized that they were all natural and fairly forced.</p>\n    "},{"t":"How to find the lowest common ancestor of two nodes in any binary tree?","l":"http://stackoverflow.com/questions/1484473/how-to-find-the-lowest-common-ancestor-of-two-nodes-in-any-binary-tree","q":"\n\n<p>The Binary Tree here is may not necessarily be a Binary Search Tree.<br>\nThe structure could be taken as -</p>\n\n<pre><code>struct node {\n    int data;\n    struct node *left;\n    struct node *right;\n};\n</code></pre>\n\n<p>The maximum solution I could work out with a friend was something of this sort -<br>\nConsider <a href=\"http://lcm.csa.iisc.ernet.in/dsa/node87.html\">this binary tree</a> :</p>\n\n<p><img src=\"http://lcm.csa.iisc.ernet.in/dsa/img151.gif\" alt=\"Binary Tree\"></p>\n\n<p>The inorder traversal yields - 8, 4, 9, 2, 5, 1, 6, 3, 7</p>\n\n<p>And the postorder traversal yields - 8, 9, 4, 5, 2, 6, 7, 3, 1</p>\n\n<p>So for instance, if we want to find the common ancestor of nodes 8 and 5, then we make a list of all the nodes which are between 8 and 5 in the inorder tree traversal, which in this case happens to be [4, 9, 2]. Then we check which node in this list appears last in the postorder traversal, which is 2. Hence the common ancestor for 8 and 5 is 2.</p>\n\n<p>The complexity for this algorithm, I believe is O(n) (O(n) for inorder/postorder traversals, the rest of the steps again being O(n) since they are nothing more than simple iterations in arrays). But there is a strong chance that this is wrong. :-)</p>\n\n<p>But this is a very crude approach, and I'm not sure if it breaks down for some case. Is there any other (possibly more optimal) solution to this problem?</p>\n    ","a":"\n<p>Nick Johnson is correct. But keep in mind that if your nodes have parent pointers, a slight variation on his algorithm is possible.\nFor both nodes in question construct a list containing the path from root to the node by starting at the node, and front inserting the parent.</p>\n\n<p>So for 8 in your example, you get (showing steps): {4}, {2, 4}, {1,  2, 4}</p>\n\n<p>Do the same for your other node in question, resulting in (steps not shown): {1, 2}</p>\n\n<p>Now compare the two lists you made looking for the first element where the list differ, or the last element of one of the lists, whichever comes first.</p>\n\n<p>This algorithm requires O(h) where h is the height of the tree. If the tree is balanced, that is O(log(n)).</p>\n\n<p></p><hr><p></p>\n\n<p>Regardless of how the tree is constructed, if this will be an operation you perform many times on the tree without changing it in between, there are other algorithms you can use that require O(n) [linear] time preparation, but then finding any pair takes only O(1) [constant] time. For references to these algorithms, see the the lowest common ancestor problem page on <a href=\"http://en.wikipedia.org/wiki/Lowest%5Fcommon%5Fancestor\">Wikipedia</a>. (Credit to Jason for originally posting this link)</p>\n    "},{"t":"Find running median from a stream of integers [duplicate]","l":"http://stackoverflow.com/questions/10657503/find-running-median-from-a-stream-of-integers","q":"\n\n<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1309263/rolling-median-algorithm-in-c\">Rolling median algorithm in C</a>  </p>\n</blockquote>\n\n\n\n<blockquote>\n  <p>Given that integers are read from a data stream. Find median of elements read so far in efficient way. </p>\n</blockquote>\n\n<p>Solution I have read: We can use a max heap on left side to represent elements that are less than the effective median, and a min heap on right side to represent elements that are greater than the effective median.</p>\n\n<p>After processing an incoming element, the number of elements in heaps differ at most by 1 element. When both heaps contain the same number of elements, we find the average of heap's root data as effective median. When the heaps are not balanced, we select the effective median from the root of heap containing more elements.</p>\n\n<p>But how would we construct a max heap and min heap i.e. how would we know the effective median here? I think that we would insert 1 element in max-heap and then the next 1 element in min-heap, and so on for all the elements. Correct me If I am wrong here.</p>\n    ","a":"\n<p>There are a number of different solutions for finding running median from streamed data, I will briefly talk about them at the very end of the answer. </p>\n\n<p>The question is about the details of the a specific solution (max heap/min heap solution), and how heap based solution works is explained below:</p>\n\n<p>For the first two elements add smaller one to the maxHeap on the left, and bigger one to the minHeap on the right. Then process stream data one by one, </p>\n\n<pre><code>Step 1: Add next item to one of the heaps\n\n   if next item is smaller than maxHeap root add it to maxHeap,\n   else add it to minHeap\n\nStep 2: Balance the heaps (after this step heaps will be either balanced or\n   one of them will contain 1 more item)\n\n   if number of elements in one of the heaps is greater than the other by\n   more than 1, remove the root element from the one containing more elements and\n   add to the other one\n</code></pre>\n\n<p>Then at any given time you can calculate median like this:</p>\n\n<pre><code>   If the heaps contain equal elements;\n     median = (root of maxHeap + root of minHeap)/2\n   Else\n     median = root of the heap with more elements\n</code></pre>\n\n<p>Now I will talk about the problem in general as promised in the beginning of the answer. Finding running median from a stream of data is a tough problem, and finding an <strong>exact solution</strong> with memory constraints efficiently is probably impossible for the general case. On the other hand, if the data has some characteristics we can exploit, we can develop efficient specialized solutions. For example, if we know that the data is an integral type, then we can use <a href=\"http://en.wikipedia.org/wiki/Counting_sort\">counting sort</a>, which can give you a constant memory constant time algorithm. Heap based solution is a more general solution because it can be used for other data types (doubles) as well. And finally, if the exact median is not required and an approximation is enough, you can just try to estimate a probability density function for the data and estimate median using that.</p>\n    "},{"t":"Detecting endianness programmatically in a C++ program","l":"http://stackoverflow.com/questions/1001307/detecting-endianness-programmatically-in-a-c-program","q":"\n\n<p>Is there a programmatic way to detect whether or not you are on a big-endian or little-endian architecture?  I need to be able to write code that will execute on an Intel or PPC system and use exactly the same code (i.e. no conditional compilation).</p>\n    ","a":"\n<p>I don't like the method based on type punning - it will often be warned against by compiler. That's exactly what unions are for !</p>\n\n<pre><code>int is_big_endian(void)\n{\n    union {\n        uint32_t i;\n        char c[4];\n    } bint = {0x01020304};\n\n    return bint.c[0] == 1; \n}\n</code></pre>\n\n<p>The principle is equivalent to the type case as suggested by others, but this is clearer - and according to C99, is guaranteed to be correct. gcc prefers this compared to the direct pointer cast.</p>\n\n<p>This is also much better than fixing the endianness at compile time - for OS which support multi-architecture (fat binary on Mac os x for example), this will work for both ppc/i386, whereas it is very easy to mess things up otherwise.</p>\n    "},{"t":"Circle line-segment collision detection algorithm?","l":"http://stackoverflow.com/questions/1073336/circle-line-segment-collision-detection-algorithm","q":"\n\n<p>I have a line from A to B and a circle positioned at C with the radius R. </p>\n\n<p><img src=\"http://i.stack.imgur.com/QpgMZ.png\" alt=\"Image\"></p>\n\n<p>What is a good algorithm to use to check whether the line intersects the circle? And at what coordinate along the circles edge it occurred?</p>\n    ","a":"\n<p>Taking </p>\n\n<ol>\n<li><strong>E</strong> is the starting point of the ray,  </li>\n<li><strong>L</strong> is the end point of the ray,  </li>\n<li><strong>C</strong> is the center of sphere you're testing against  </li>\n<li><strong>r</strong> is the radius of that sphere</li>\n</ol>\n\n<p>Compute:<br>\n<strong>d</strong> = L - E ( Direction vector of ray, from start to end )<br>\n<strong>f</strong> = E - C ( Vector from center sphere to ray start )  </p>\n\n<p>Then the intersection is found by..<br>\nPlugging:<br>\n<strong>P = E + t * d</strong><br>\nThis is a parametric equation:<br>\nP<sub>x</sub> = E<sub>x</sub> + td<sub>x</sub><br>\nP<sub>y</sub> = E<sub>y</sub> + td<sub>y</sub><br>\ninto<br>\n<strong>(x - h)<sup>2</sup> + (y - k)<sup>2</sup> = r<sup>2</sup></strong><br>\n(h,k) = center of circle.  </p>\n\n<blockquote>\n  <p>Note: We've simplified the problem to 2D here, the solution we get applies also in 3D</p>\n</blockquote>\n\n<p><strong>to get:</strong></p>\n\n<ol>\n<li><strong>Expand</strong><br>\nx<sup>2</sup> - 2xh + h<sup>2</sup> + y<sup>2</sup> - 2yk + k<sup>2</sup> - r<sup>2</sup> = 0 </li>\n<li><strong>Plug</strong><br>\nx = e<sub>x</sub> + td<sub>x</sub><br>\ny = e<sub>y</sub> + td<sub>y</sub><br>\n( e<sub>x</sub> + td<sub>x</sub> )<sup>2</sup> - 2( e<sub>x</sub> + td<sub>x</sub> )h + h<sup>2</sup> +\n( e<sub>y</sub> + td<sub>y</sub> )<sup>2</sup> - 2( e<sub>y</sub> + td<sub>y</sub> )k + k<sup>2</sup> - r<sup>2</sup> = 0</li>\n<li>Explode<br>\ne<sub>x</sub><sup>2</sup> + 2e<sub>x</sub>td<sub>x</sub> + t<sup>2</sup>d<sub>x</sub><sup>2</sup> - 2e<sub>x</sub>h - 2td<sub>x</sub>h + h<sup>2</sup> +\ne<sub>y</sub><sup>2</sup> + 2e<sub>y</sub>td<sub>y</sub> + t<sup>2</sup>d<sub>y</sub><sup>2</sup> - 2e<sub>y</sub>k - 2td<sub>y</sub>k + k<sup>2</sup> - r<sup>2</sup> = 0</li>\n<li><strong>Group</strong><br>\nt<sup>2</sup>( d<sub>x</sub><sup>2</sup> + d<sub>y</sub><sup>2</sup> ) +\n2t( e<sub>x</sub>d<sub>x</sub> + e<sub>y</sub>d<sub>y</sub> - d<sub>x</sub>h - d<sub>y</sub>k ) +\ne<sub>x</sub><sup>2</sup> + e<sub>y</sub><sup>2</sup> -\n2e<sub>x</sub>h - 2e<sub>y</sub>k + h<sup>2</sup> + k<sup>2</sup> - r<sup>2</sup> = 0</li>\n<li><strong>Finally,</strong><br>\nt<sup>2</sup>( _d * _d ) + 2t( _e * _d - _d * _c ) + _e * _e - 2( _e*_c ) + _c * _c - r<sup>2</sup> = 0<br>\n*Where _d is the vector d and * is the dot product.*</li>\n<li><strong>And then,</strong><br>\nt<sup>2</sup>( _d * _d ) + 2t( _d * ( _e - _c ) ) + ( _e - _c ) * ( _e - _c ) - r<sup>2</sup> = 0 </li>\n<li><strong>Letting _f = _e - _c</strong><br>\nt<sup>2</sup>( _d * _d ) + 2t( _d * _f ) + _f * _f - r<sup>2</sup> = 0</li>\n</ol>\n\n<p>So we get:<br>\n<strong>t<sup>2</sup> * (d DOT d) + 2t*( f DOT d ) + ( f DOT f - r<sup>2</sup> ) = 0</strong><br>\nSo solving the quadratic equation:</p>\n\n<pre><code>float a = d.Dot( d ) ;\nfloat b = 2*f.Dot( d ) ;\nfloat c = f.Dot( f ) - r*r ;\n\nfloat discriminant = b*b-4*a*c;\nif( discriminant &lt; 0 )\n{\n  // no intersection\n}\nelse\n{\n  // ray didn't totally miss sphere,\n  // so there is a solution to\n  // the equation.\n\n  discriminant = sqrt( discriminant );\n\n  // either solution may be on or off the ray so need to test both\n  // t1 is always the smaller value, because BOTH discriminant and\n  // a are nonnegative.\n  float t1 = (-b - discriminant)/(2*a);\n  float t2 = (-b + discriminant)/(2*a);\n\n  // 3x HIT cases:\n  //          -o-&gt;             --|--&gt;  |            |  --|-&gt;\n  // Impale(t1 hit,t2 hit), Poke(t1 hit,t2&gt;1), ExitWound(t1&lt;0, t2 hit), \n\n  // 3x MISS cases:\n  //       -&gt;  o                     o -&gt;              | -&gt; |\n  // FallShort (t1&gt;1,t2&gt;1), Past (t1&lt;0,t2&lt;0), CompletelyInside(t1&lt;0, t2&gt;1)\n\n  if( t1 &gt;= 0 &amp;&amp; t1 &lt;= 1 )\n  {\n    // t1 is the intersection, and it's closer than t2\n    // (since t1 uses -b - discriminant)\n    // Impale, Poke\n    return true ;\n  }\n\n  // here t1 didn't intersect so we are either started\n  // inside the sphere or completely past it\n  if( t2 &gt;= 0 &amp;&amp; t2 &lt;= 1 )\n  {\n    // ExitWound\n    return true ;\n  }\n\n  // no intn: FallShort, Past, CompletelyInside\n  return false ;\n}\n</code></pre>\n    "},{"t":"Algorithm for classifying words for hangman difficulty levels as “Easy”,“Medium”, or “Hard”","l":"http://stackoverflow.com/questions/16223305/algorithm-for-classifying-words-for-hangman-difficulty-levels-as-easy-medium","q":"\n\n<p>What is a good algorithm to determine the \"difficulty\" of a word for a hangman game, so that the game can select words to match a specified difficulty level?</p>\n\n<p>Difficulty would seem related to the number of guesses required, the relative frequency of usage of letters (e.g. words with many uncommon letters may be harder to guess), and potentially the length of the word. </p>\n\n<p>There are also some subjective factors to (attempt to) compensate for, such as the likelihood a word is in the vocabulary of the player, and can be recognised, allowing moving from a guessing strategy based on letter frequencies alone to guessing based on list of known matching words.</p>\n\n<p>My attempt for now is below in ruby. Any suggestions on how to improve the categorisation?</p>\n\n<pre><code>def classify_word(w)\n  n = w.chars.to_a.uniq.length # Num. unique chars in w\n  if n &lt; 5 and w.length &gt; 4\n    return WordDifficulty::Easy\n  end\n  if n &gt; w.length / 2\n    return WordDifficulty::Hard\n  else\n    return WordDifficulty::Medium\n  end\nend\n</code></pre>\n\n<p>I am writing a hangman game I would like my children to play; I am rather too old to be attempting \"homework\", which may be why the question is receiving so many down votes... \nWords are drawn randomly from large word databases, which include many obscure words, and are being filtered by the difficulty level determined for the word.</p>\n    ","a":"\n<h3>1. Introduction</h3>\n\n<p>Here's a way to approach this problem systematically: if you have an algorithm that plays hangman well, then you can take the difficulty of each word to be the number of wrong guesses that your program would take if guessing that word.</p>\n\n<h3>2. Aside on hangman strategy</h3>\n\n<p>There's an idea that's implicit in some the other answers and comments, that the optimal strategy for the solver would be to base their decisions on the frequency of letters in English, or on the frequency of words in some corpus. This is a seductive idea, but it's not quite right. The solver does best if it <em>accurately models the distribution of words chosen by the setter</em>, and a human setter may well be choosing words based on their rarity or avoidance of frequently used letters. For example, although <code>E</code> is the most frequently used letter in English, if the setter always chooses from the words <code>JUGFUL</code>, <code>RHYTHM</code>, <code>SYZYGY</code>, and <code>ZYTHUM</code>, then a perfect solver does not start by guessing <code>E</code>!</p>\n\n<p>The best approach to modelling the setter depends on the context, but I guess that some kind of Bayesian inductive inference would work well in a context where the solver plays many games against the same setter, or against a group of similar setters.</p>\n\n<h3>3. A hangman algorithm</h3>\n\n<p>Here I'll outline a solver that is pretty good (but far from perfect). It models the setter as choosing words uniformly from a fixed dictionary. It's a <a href=\"http://en.wikipedia.org/wiki/Greedy_algorithm\" rel=\"nofollow\">greedy algorithm</a>: at each stage it guesses the letter that minimizes the number of misses, that is, words that do not contain the guess. For example, if no guesses have been made so far, and the possible words are <code>DEED</code>, <code>DEAD</code> and <code>DARE</code>, then:</p>\n\n<ul>\n<li>if you guess <code>D</code> or <code>E</code>, there are no misses;</li>\n<li>if you guess <code>A</code>, there's one miss (<code>DEED</code>);</li>\n<li>if you guess <code>R</code>, there are two misses (<code>DEED</code> and <code>DEAD</code>);</li>\n<li>if you guess any other letter, there are three misses.</li>\n</ul>\n\n<p>So either <code>D</code> or <code>E</code> is a good guess in this situation.</p>\n\n<p>(Thanks to <a href=\"http://stackoverflow.com/questions/16223305/algorithm-for-classifying-words-for-hangman-difficulty-levels-as-easy-medium/16225534#comment23208206_16225534\">Colonel Panic in comments</a> for pointing out that correct guesses are free in hangman—I totally forgot this in my first attempt!)</p>\n\n<h3>4. Implementation</h3>\n\n<p>Here's an implementation of this algorithm in Python:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from collections import defaultdict\nfrom string import ascii_lowercase\n\ndef partition(guess, words):\n    \"\"\"Apply the single letter 'guess' to the sequence 'words' and return\n    a dictionary mapping the pattern of occurrences of 'guess' in a\n    word to the list of words with that pattern.\n\n    &gt;&gt;&gt; words = 'deed even eyes mews peep star'.split()\n    &gt;&gt;&gt; sorted(list(partition('e', words).items()))\n    [(0, ['star']), (2, ['mews']), (5, ['even', 'eyes']), (6, ['deed', 'peep'])]\n\n    \"\"\"\n    result = defaultdict(list)\n    for word in words:\n        key = sum(1 &lt;&lt; i for i, letter in enumerate(word) if letter == guess)\n        result[key].append(word)\n    return result\n\ndef guess_cost(guess, words):\n    \"\"\"Return the cost of a guess, namely the number of words that don't\n    contain the guess.\n\n    &gt;&gt;&gt; words = 'deed even eyes mews peep star'.split()\n    &gt;&gt;&gt; guess_cost('e', words)\n    1\n    &gt;&gt;&gt; guess_cost('s', words)\n    3\n\n    \"\"\"\n    return sum(guess not in word for word in words)\n\ndef word_guesses(words, wrong = 0, letters = ''):\n    \"\"\"Given the collection 'words' that match all letters guessed so far,\n    generate tuples (wrong, nguesses, word, guesses) where\n    'word' is the word that was guessed;\n    'guesses' is the sequence of letters guessed;\n    'wrong' is the number of these guesses that were wrong;\n    'nguesses' is len(guesses).\n\n    &gt;&gt;&gt; words = 'deed even eyes heel mere peep star'.split()\n    &gt;&gt;&gt; from pprint import pprint\n    &gt;&gt;&gt; pprint(sorted(word_guesses(words)))\n    [(0, 1, 'mere', 'e'),\n     (0, 2, 'deed', 'ed'),\n     (0, 2, 'even', 'en'),\n     (1, 1, 'star', 'e'),\n     (1, 2, 'eyes', 'en'),\n     (1, 3, 'heel', 'edh'),\n     (2, 3, 'peep', 'edh')]\n\n    \"\"\"\n    if len(words) == 1:\n        yield wrong, len(letters), words[0], letters\n        return\n    best_guess = min((g for g in ascii_lowercase if g not in letters),\n                     key = lambda g:guess_cost(g, words))\n    best_partition = partition(best_guess, words)\n    letters += best_guess\n    for pattern, words in best_partition.items():\n        for guess in word_guesses(words, wrong + (pattern == 0), letters):\n            yield guess\n</code></pre>\n\n<h3>5. Example results</h3>\n\n<p>Using this strategy it's possible to evaluate the difficulty of guessing each word in a collection. Here I consider the six-letter words in my system dictionary:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>&gt;&gt;&gt; words = [w.strip() for w in open('/usr/share/dict/words') if w.lower() == w]\n&gt;&gt;&gt; six_letter_words = set(w for w in words if len(w) == 6)\n&gt;&gt;&gt; len(six_letter_words)\n15066\n&gt;&gt;&gt; results = sorted(word_guesses(six_letter_words))\n</code></pre>\n\n<p>The easiest words to guess in this dictionary (together with the sequence of guesses needed for the solver to guess them) are as follows:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; pprint(results[:10])\n[(0, 1, 'eelery', 'e'),\n (0, 2, 'coneen', 'en'),\n (0, 2, 'earlet', 'er'),\n (0, 2, 'earner', 'er'),\n (0, 2, 'edgrew', 'er'),\n (0, 2, 'eerily', 'el'),\n (0, 2, 'egence', 'eg'),\n (0, 2, 'eleven', 'el'),\n (0, 2, 'enaena', 'en'),\n (0, 2, 'ennead', 'en')]\n</code></pre>\n\n<p>and the hardest words are these:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>&gt;&gt;&gt; pprint(results[-10:])\n[(12, 16, 'buzzer', 'eraoiutlnsmdbcfg'),\n (12, 16, 'cuffer', 'eraoiutlnsmdbpgc'),\n (12, 16, 'jugger', 'eraoiutlnsmdbpgh'),\n (12, 16, 'pugger', 'eraoiutlnsmdbpcf'),\n (12, 16, 'suddle', 'eaioulbrdcfghmnp'),\n (12, 16, 'yucker', 'eraoiutlnsmdbpgc'),\n (12, 16, 'zipper', 'eraoinltsdgcbpjk'),\n (12, 17, 'tuzzle', 'eaioulbrdcgszmnpt'),\n (13, 16, 'wuzzer', 'eraoiutlnsmdbpgc'),\n (13, 17, 'wuzzle', 'eaioulbrdcgszmnpt')]\n</code></pre>\n\n<p>The reason that these are hard is because after you've guessed <code>-UZZLE</code>, you still have seven possibilities left:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>&gt;&gt;&gt; ' '.join(sorted(w for w in six_letter_words if w.endswith('uzzle')))\n'buzzle guzzle muzzle nuzzle puzzle tuzzle wuzzle'\n</code></pre>\n\n<h3>6. Choice of wordlist</h3>\n\n<p>Of course when preparing wordlists for your children you wouldn't start with your computer's system dictionary, you'd start with a list of words that you think they are likely to know. For example, you might have a look at <a href=\"http://en.wiktionary.org/wiki/Wiktionary%3aFrequency_lists#English\" rel=\"nofollow\">Wiktionary's lists of the most frequently used words</a> in various English corpora.</p>\n\n<p>For example, among the 1,700 six-letter words in the <a href=\"http://en.wiktionary.org/wiki/Wiktionary%3aFrequency_lists/PG/2006/04/1-10000\" rel=\"nofollow\">10,000 most common words in Project Gutenberg as of 2006</a>, the most difficult ten are these:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>[(6, 10, 'losing', 'eaoignvwch'),\n (6, 10, 'monkey', 'erdstaoync'),\n (6, 10, 'pulled', 'erdaioupfh'),\n (6, 10, 'slaves', 'erdsacthkl'),\n (6, 10, 'supper', 'eriaoubsfm'),\n (6, 11, 'hunter', 'eriaoubshng'),\n (6, 11, 'nought', 'eaoiustghbf'),\n (6, 11, 'wounds', 'eaoiusdnhpr'),\n (6, 11, 'wright', 'eaoithglrbf'),\n (7, 10, 'soames', 'erdsacthkl')]\n</code></pre>\n\n<p>(Soames Forsyte is a character in the <a href=\"http://www.gutenberg.org/ebooks/4397\" rel=\"nofollow\">Forsyte Saga by John Galsworthy</a>; the wordlist has been converted to lower-case so it wasn't possible for me to quickly remove proper names.)</p>\n    "},{"t":"What is the optimal Jewish toenail cutting algorithm?","l":"http://stackoverflow.com/questions/7769032/what-is-the-optimal-jewish-toenail-cutting-algorithm","q":"\n\n<p>I am working on the software for a machine that will automatically trim toenails, so that users can simply put their feet in it and run it instead of having to manually do it by biting them or using nail clippers.</p>\n\n<p>A sizeable percentage of our potential user base will likely be Jewish, and, evidently, there is a <a href=\"http://judaism.stackexchange.com/questions/1301/trimming-toenails-in-sequence\">tradition about not trimming toenails</a> (<a href=\"http://judaism.stackexchange.com/questions/1118/rules-for-cutting-nails\">or fingernails</a>) in sequential order </p>\n\n<p>There seems to be dissenting opinion on the precise application of this tradition, but we think that the following rules are sufficient to accomodate people whose religious practices prohibit cutting toenails in order:</p>\n\n<ul>\n<li>No two adjacent toenails should be cut consecutively</li>\n<li>The cutting sequence on the left foot should not match the sequence on the right foot</li>\n<li>The cutting sequence on two consecutive runs should not be the same. The sequences shouldn't be easily predictable, so hardcoding an alternating sequence does not work.</li>\n</ul>\n\n<p>This is how we have decided to number the toes:</p>\n\n<pre><code>5 4 3 2 1  1 2 3 4 5\nLeft foot  Right foot\n</code></pre>\n\n<p>I have written code to solve the problem, but the algorithm used is sub-optimal: in fact, the worst case performance is <strong>O(∞)</strong>. The way it works is comparable to <a href=\"http://en.wikipedia.org/wiki/Bogosort\">bogosort</a>. Here is a pseudocode simplification of the actual code used:</p>\n\n<pre class=\"lang-default prettyprint-override\"><code>function GenerateRandomSequence\n   sequence = Array[5]\n   foreach (item in sequence)\n       item = RandomNumberBetween(1,5)\n   return sequence\n\nfunction GetToenailCuttingOrder\n   while (true)\n      sequence = GenerateRandomSequence()\n      if (!AllItemsAreUnique(sequence))\n         continue\n      if (NoTwoAdjacentItemsHaveConsecutiveNumbers(sequence))\n         return sequence\n\ndo\n    leftFootSequence = GetToenailCuttingOrder()\n    rightFootSequence = GetToenailCuttingOrder()\nuntil (leftFootSequence != rightFootSequence &amp;&amp;\n       leftFootSequence != leftFootSequenceFromLastRun &amp;&amp;\n       rightFootSequence != rightFootSequenceFromLastRun)\n</code></pre>\n\n<p>Basically, it generates random sequences and checks if they meet the criteria. If it doesn't meet the criteria, it starts over. It doesn't take a ridiculously long amount of time, but it is very unpredictable.</p>\n\n<p>I realize that the way I am currently doing it is pretty terrible, but I'm having trouble coming up with a better way. Can any of you suggest a more elegant and performant algorithm?</p>\n    ","a":"\n<p>You could generate all possible toenail cutting sequences with no restrictions, and then filter out all sequences that violate the jewish rule. Luckily, humans only have five toes per foot*, so there are only 5! = 120 unrestricted sequences. </p>\n\n<p>Python example:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#seq is only valid when consecutive elements in the list differ by at least two.\ndef isValid(seq):\n    for i in range(len(seq)-1):\n        a = seq[i]\n        b = seq[i+1]\n        if abs(a-b) == 1:\n            return False\n    return True\n\n\nfrom itertools import ifilter, permutations\nvalidseqs = ifilter(isValid, permutations([1,2,3,4,5]))\nfor i in validseqs:\n    print i\n\n(1, 3, 5, 2, 4)\n(1, 4, 2, 5, 3)\n(2, 4, 1, 3, 5)\n(2, 4, 1, 5, 3)\n(2, 5, 3, 1, 4)\n(3, 1, 4, 2, 5)\n(3, 1, 5, 2, 4)\n(3, 5, 1, 4, 2)\n(3, 5, 2, 4, 1)\n(4, 1, 3, 5, 2)\n(4, 2, 5, 1, 3)\n(4, 2, 5, 3, 1)\n(5, 2, 4, 1, 3)\n(5, 3, 1, 4, 2)\n</code></pre>\n\n<p>To enforce your \"no repeats of the same sequence\" rule, you can just choose four of the above sequences, and use them alternately. The only catch here is that if you count the two big toes as \"consecutive\", then you can't choose two sequences that end and begin with 1, respectively.</p>\n\n<p>*You may want to make a numberOfToesPerFoot variable, so you can easily change it later if any of your clients turn out to have less toes than you expect, or more.</p>\n    "},{"t":"Algorithm to compare two images","l":"http://stackoverflow.com/questions/23931/algorithm-to-compare-two-images","q":"\n\n<p>Given two different image files (in whatever format I choose), I need to write a program to predict the chance if one being the illegal copy of another. The author of the copy may do stuff like rotating, making negative, or adding trivial details (as well as changing the dimension of the image).</p>\n\n<p>Do you know any algorithm to do this kind of job?</p>\n    ","a":"\n<p>These are simply ideas I've had thinking about the problem, never tried it but I like thinking about problems like this!</p>\n\n<p><strong>Before you begin</strong></p>\n\n<p>Consider normalising the pictures, if one is a higher resolution than the other, consider the option that one of them is a compressed version of the other, therefore scaling the resolution down might provide more accurate results.</p>\n\n<p>Consider scanning various prospective areas of the image that could represent zoomed portions of the image and various positions and rotations.  It starts getting tricky if one of the images are a skewed version of another, these are the sort of limitations you should identify and compromise on.</p>\n\n<p><a href=\"http://www.mathworks.com/\">Matlab</a> is an excellent tool for testing and evaluating images.</p>\n\n<p><strong>Testing the algorithms</strong></p>\n\n<p>You should test (at the minimum) a large human analysed set of test data where matches are known beforehand.  If for example in your test data you have 1,000 images where 5% of them match, you now have a reasonably reliable benchmark.  An algorithm that finds 10% positives is not as good as one that finds 4% of positives in our test data.  However, one algorithm may find all the matches, but also have a large 20% false positive rate, so there are several ways to rate your algorithms.</p>\n\n<p>The test data should attempt to be designed to cover as many types of dynamics as possible that you would expect to find in the real world.</p>\n\n<p>It is important to note that each algorithm to be useful must perform better than random guessing, otherwise it is useless to us!</p>\n\n<p>You can then apply your software into the real world in a controlled way and start to analyse the results it produces.  This is the sort of software project which can go on for infinitum, there are always tweaks and improvements you can make, it is important to bear that in mind when designing it as it is easy to fall into the trap of the never ending project.</p>\n\n<p><strong>Colour Buckets</strong></p>\n\n<p>With two pictures, scan each pixel and count the colours.  For example you might have the 'buckets':</p>\n\n<pre><code>white\nred\nblue\ngreen\nblack\n</code></pre>\n\n<p>(Obviously you would have a higher resolution of counters).  Every time you find a 'red' pixel, you increment the red counter.  Each bucket can be representative of spectrum of colours, the higher resolution the more accurate but you should experiment with an acceptable difference rate.</p>\n\n<p>Once you have your totals, compare it to the totals for a second image.  You might find that each image has a fairly unique footprint, enough to identify matches.</p>\n\n<p><strong>Edge detection</strong></p>\n\n<p>How about using <a href=\"http://en.wikipedia.org/wiki/Edge_detection\">Edge Detection</a>.\n<img src=\"http://upload.wikimedia.org/wikipedia/en/thumb/8/8e/EdgeDetectionMathematica.png/500px-EdgeDetectionMathematica.png\" alt=\"alt text\"></p>\n\n<p>With two similar pictures edge detection should provide you with a usable and fairly reliable unique footprint.</p>\n\n<p>Take both pictures, and apply edge detection.  Maybe measure the average thickness of the edges and then calculate the probability the image could be scaled, and rescale if necessary.  Below is an example of an applied <a href=\"http://en.wikipedia.org/wiki/Gabor_filter\">Gabor Filter</a> (a type of edge detection) in various rotations.</p>\n\n<p><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Gabor-ocr.png/220px-Gabor-ocr.png\" alt=\"alt text\"></p>\n\n<p>Compare the pictures pixel for pixel, count the matches and the non matches.  If they are within a certain threshold of error, you have a match.  Otherwise, you could try reducing the resolution up to a certain point and see if the probability of a match improves.  </p>\n\n<p><strong>Regions of Interest</strong></p>\n\n<p>Some images may have distinctive segments/regions of interest.  These regions probably contrast highly with the rest of the image, and are a good item to search for in your other images to find matches.  Take this image for example:</p>\n\n<p><img src=\"http://meetthegimp.org/wp-content/uploads/2009/04/97.jpg\" alt=\"alt text\"></p>\n\n<p>The construction worker in blue is a region of interest and can be used as a search object.  There are probably several ways you could extract properties/data from this region of interest and use them to search your data set.</p>\n\n<p>If you have more than 2 regions of interest, you can measure the distances between them.  Take this simplified example:</p>\n\n<p><img src=\"http://www.per2000.eu/assets/images/3_dots_black_03.jpg\" alt=\"alt text\"></p>\n\n<p>We have 3 clear regions of interest.  The distance between region 1 and 2 may be 200 pixels, between 1 and 3 400 pixels, and 2 and 3 200 pixels.</p>\n\n<p>Search other images for similar regions of interest, normalise the distance values and see if you have potential matches.  This technique could work well for rotated and scaled images.  The more regions of interest you have, the probability of a match increases as each distance measurement matches.</p>\n\n<p>It is important to think about the context of your data set.  If for example your data set is modern art, then regions of interest would work quite well, as regions of interest were probably <em>designed</em> to be a fundamental part of the final image.  If however you are dealing with images of construction sites, regions of interest may be interpreted by the illegal copier as ugly and may be cropped/edited out liberally.  Keep in mind common features of your dataset, and attempt to exploit that knowledge.</p>\n\n<p><strong>Morphing</strong></p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Morphing\">Morphing</a> two images is the process of turning one image into the other through a set of steps:</p>\n\n<p><img src=\"http://t0.gstatic.com/images?q=tbn%3aff9g8dy_q1Nr4M%3ahttp://blog.ekoventure.com/data//1498/original/morph.jpg&amp;t=1\" alt=\"alt text\"></p>\n\n<p>Note, this is different to fading one image into another!</p>\n\n<p>There are many software packages that can morph images.  It's traditionaly used as a transitional effect, two images don't morph into something halfway usually, one extreme morphs into the other extreme as the final result.</p>\n\n<p>Why could this be useful?  Dependant on the morphing algorithm you use, there may be a relationship between similarity of images, and some parameters of the morphing algorithm.</p>\n\n<p>In a grossly over simplified example, one algorithm might execute faster when there are less changes to be made.  We then know there is a higher probability that these two images share properties with each other.</p>\n\n<p>This technique <em>could</em> work well for rotated, distorted, skewed, zoomed, all types of copied images.  Again this is just an idea I have had, it's not based on any researched academia as far as I am aware (I haven't look hard though), so it may be a lot of work for you with limited/no results.</p>\n\n<p><strong>Zipping</strong></p>\n\n<p>Ow's answer in this question is excellent, I remember reading about these sort of techniques studying AI. It is quite effective at comparing corpus lexicons.</p>\n\n<p>One interesting optimisation when comparing corpuses is that you can remove words considered to be too common, for example 'The', 'A', 'And' etc.  These words dilute our result, we want to work out how different the two corpus are so these can be removed before processing.  Perhaps there are similar common signals in images that could be stripped before compression?  It might be worth looking into.</p>\n\n<p>Compression ratio is a very quick and reasonably effective way of determining how similar two sets of data are.  Reading up about <a href=\"http://www.howstuffworks.com/file-compression.htm\">how compression works</a> will give you a good idea why this could be so effective.  For a fast to release algorithm this would probably be a good starting point.</p>\n\n<p><strong>Transparency</strong></p>\n\n<p>Again I am unsure how transparency data is stored for certain image types, gif png etc, but this will be extractable and would serve as an effective simplified cut out to compare with your data sets transparency.</p>\n\n<p><strong>Inverting Signals</strong></p>\n\n<p>An image is just a signal.  If you play a noise from a speaker, and you play the opposite noise in another speaker in perfect sync at the exact same volume, they cancel each other out.</p>\n\n<p><img src=\"http://www.themotorreport.com.au/wp-content/uploads/2008/07/noise-cancellation.gif\" alt=\"alt text\"></p>\n\n<p>Invert on of the images, and add it onto your other image.  Scale it/loop positions repetitively until you find a resulting image where enough of the pixels are white (or black?  I'll refer to it as a neutral canvas) to provide you with a positive match, or partial match.</p>\n\n<p>However, consider two images that are equal, except one of them has a brighten effect applied to it:</p>\n\n<p><img src=\"http://www.mcburrz.com/images/photo/brighten.jpg\" alt=\"alt text\"></p>\n\n<p>Inverting one of them, then adding it to the other will not result in a neutral canvas which is what we are aiming for.  However, when comparing the pixels from both original images, we can definatly see a clear relationship between the two.</p>\n\n<p>I haven't studied colour for some years now, and am unsure if the colour spectrum is on a linear scale, but if you determined the average factor of colour difference between both pictures, you can use this value to normalise the data before processing with this technique.</p>\n\n<p><strong>Tree Data structures</strong></p>\n\n<p>At first these don't seem to fit for the problem, but I think they could work.</p>\n\n<p>You could think about extracting certain properties of an image (for example colour bins) and generate a <a href=\"http://en.wikipedia.org/wiki/Huffman_coding\">huffman tree</a> or similar data structure.  You might be able to compare two trees for similarity.  This wouldn't work well for photographic data for example with a large spectrum of colour, but cartoons or other reduced colour set images this might work.</p>\n\n<p>This probably wouldn't work, but it's an idea.  The <a href=\"http://en.wikipedia.org/wiki/Trie\">trie datastructure</a> is great at storing lexicons, for example a dictionarty.  It's a prefix tree.  Perhaps it's possible to build an image equivalent of a lexicon, (again I can only think of colours) to construct a trie.  If you reduced say a 300x300 image into 5x5 squares, then decompose each 5x5 square into a sequence of colours you could construct a trie from the resulting data.  If a 2x2 square contains:</p>\n\n<pre><code>FFFFFF|000000|FDFD44|FFFFFF\n</code></pre>\n\n<p>We have a fairly unique trie code that extends 24 levels, increasing/decreasing the levels (IE reducing/increasing the size of our sub square) may yield more accurate results.</p>\n\n<p>Comparing trie trees should be reasonably easy, and could possible provide effective results. </p>\n\n<p><strong>More ideas</strong></p>\n\n<p>I stumbled accross an interesting paper breif about <a href=\"http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=387577\">classification of satellite imagery</a>, it outlines:</p>\n\n<blockquote>\n  <p>Texture measures considered are: cooccurrence matrices, gray-level differences, texture-tone analysis, features derived from the Fourier spectrum, and Gabor filters. Some Fourier features and some Gabor filters were found to be good choices, in particular when a single frequency band was used for classification.</p>\n</blockquote>\n\n<p>It may be worth investigating those measurements in more detail, although some of them may not be relevant to your data set.</p>\n\n<p><strong>Other things to consider</strong></p>\n\n<p>There are probably a lot of papers on this sort of thing, so reading some of them should help although they can be very technical.  It is an extremely difficult area in computing, with many fruitless hours of work spent by many people attempting to do similar things.  Keeping it simple and building upon those ideas would be the best way to go.  It should be a reasonably difficult challenge to create an algorithm with a better than random match rate, and to start improving on that really does start to get quite hard to achieve.</p>\n\n<p>Each method would probably need to be tested and tweaked thoroughly, if you have any information about the type of picture you will be checking as well, this would be useful.  For example advertisements, many of them would have text in them, so doing text recognition would be an easy and probably very reliable way of finding matches especially when combined with other solutions.  As mentioned earlier, attempt to exploit common properties of your data set.</p>\n\n<p>Combining alternative measurements and techniques each that can have a weighted vote (dependant on their effectiveness) would be one way you could create a system that generates more accurate results.</p>\n\n<p>If employing multiple algorithms, as mentioned at the begining of this answer, one may find all the positives but have a false positive rate of 20%, it would be of interest to study the properties/strengths/weaknesses of other algorithms as another algorithm may be effective in eliminating false positives returned from another.</p>\n\n<p>Be careful to not fall into attempting to complete the never ending project, good luck!</p>\n    "},{"t":"How to determine a point in a triangle?","l":"http://stackoverflow.com/questions/2049582/how-to-determine-a-point-in-a-triangle","q":"\n\n<p>Is there an easy way to determine if a point inside a triangle? It's 2D not 3D.</p>\n    ","a":"\n<p>In general, the simplest (and quite optimal) algorithm is checking on which side of the half-plane created by the edges the point is.</p>\n\n<p>Here's some high quality info in this <a href=\"http://www.gamedev.net/community/forums/topic.asp?topic_id=295943\">topic on GameDev</a>, including performance issues.</p>\n\n<p>And here's some code to get you started:</p>\n\n<pre class=\"lang-c++ prettyprint-override\"><code>float sign (fPoint p1, fPoint p2, fPoint p3)\n{\n    return (p1.x - p3.x) * (p2.y - p3.y) - (p2.x - p3.x) * (p1.y - p3.y);\n}\n\nbool PointInTriangle (fPoint pt, fPoint v1, fPoint v2, fPoint v3)\n{\n    bool b1, b2, b3;\n\n    b1 = sign(pt, v1, v2) &lt; 0.0f;\n    b2 = sign(pt, v2, v3) &lt; 0.0f;\n    b3 = sign(pt, v3, v1) &lt; 0.0f;\n\n    return ((b1 == b2) &amp;&amp; (b2 == b3));\n}\n</code></pre>\n    "},{"t":"Bubble Sort Homework","l":"http://stackoverflow.com/questions/895371/bubble-sort-homework","q":"\n\n<p>In class we are doing sorting algorithms and, although I understand them fine when talking about them and writing pseudocode, I am having problems writing actual code for them.</p>\n\n<p>This is my attempt in Python:</p>\n\n<pre><code>mylist = [12, 5, 13, 8, 9, 65]\n\ndef bubble(badList):\n    length = len(badList) - 1\n    unsorted = True\n\n    while unsorted:\n    \tfor element in range(0,length):\n\t\t    unsorted = False\n\t\t    if badList[element] &gt; badList[element + 1]:\n\t\t\t    hold = badList[element + 1]\n\t\t\t    badList[element + 1] = badList[element]\n\t\t\t    badList[element] = hold\n\t\t\t    print badList\n\t\t    else:\n\t\t\t    unsorted = True\n\nprint bubble(mylist)\n</code></pre>\n\n<p>Now, this (as far as I can tell) sorts correctly, but once it finishes it just loops indefinitely.</p>\n\n<p>How can this code be fixed so the function finishes properly and correctly sorts a list of any (reasonable) size?</p>\n\n<p>P.S. I know I should not really have prints in a function and I should have a return, but I just have not done that yet as my code does not really work yet.</p>\n    ","a":"\n<p>To explain why your script isn't working right now, I'll rename the variable <code>unsorted</code> to <code>sorted</code>.</p>\n\n<p>At first, your list isn't yet sorted. Of course, we set <code>sorted</code> to <code>False</code>.</p>\n\n<p>As soon as we start the <code>while</code> loop, we assume that the list is already sorted. The idea is this: as soon as we find two elements that are not in the right order, we set <code>sorted</code> back to <code>False</code>. <code>sorted</code> will remain <code>True</code> <em>only if there were no elements in the wrong order</em>.</p>\n\n<pre><code>sorted = False  # We haven't started sorting yet\n\nwhile not sorted:\n    sorted = True  # Assume the list is now sorted\n    for element in range(0, length):\n        if badList[element] &gt; badList[element + 1]:\n            sorted = False  # We found two elements in the wrong order\n            hold = badList[element + 1]\n            badList[element + 1] = badList[element]\n            badList[element] = hold\n    # We went through the whole list. At this point, if there were no elements\n    # in the wrong order, sorted is still True. Otherwise, it's false, and the\n    # while loop executes again.\n</code></pre>\n\n<p>There are also minor little issues that would help the code be more efficient or readable.</p>\n\n<ul>\n<li><p>In the <code>for</code> loop, you use the variable <code>element</code>. Technically, <code>element</code> is not an element; it's a number representing a list index. Also, it's quite long. In these cases, just use a temporary variable name, like <code>i</code> for \"index\".</p>\n\n<pre><code>for i in range(0, length):\n</code></pre></li>\n<li><p>The <code>range</code> command can also take just one argument (named <code>stop</code>). In that case, you get a list of all the integers from 0 to that argument.</p>\n\n<pre><code>for i in range(length):\n</code></pre></li>\n<li><p>The <a href=\"http://www.python.org/dev/peps/pep-0008/\">Python Style Guide</a> recommends that variables be named in lowercase with underscores. This is a very minor nitpick for a little script like this; it's more to get you accustomed to what Python code most often resembles.</p>\n\n<pre><code>def bubble(bad_list):\n</code></pre></li>\n<li><p>To swap the values of two variables, write them as a tuple assignment. The right hand side gets evaluated as a tuple (say, <code>(badList[i+1], badList[i])</code> is <code>(3, 5)</code>) and then gets assigned to the two variables on the left hand side (<code>(badList[i], badList[i+1])</code>).</p>\n\n<pre><code>bad_list[i], bad_list[i+1] = bad_list[i+1], bad_list[i]\n</code></pre></li>\n</ul>\n\n<p>Put it all together, and you get this:</p>\n\n<pre><code>my_list = [12, 5, 13, 8, 9, 65]\n\ndef bubble(bad_list):\n    length = len(bad_list) - 1\n    sorted = False\n\n    while not sorted:\n        sorted = True\n        for i in range(length):\n            if bad_list[i] &gt; bad_list[i+1]:\n                sorted = False\n                bad_list[i], bad_list[i+1] = bad_list[i+1], bad_list[i]\n\nbubble(my_list)\nprint my_list\n</code></pre>\n\n<p>(I removed your print statement too, by the way.)</p>\n    "},{"t":"Why do we check upto the square root of a prime number to determine if it is prime?","l":"http://stackoverflow.com/questions/5811151/why-do-we-check-upto-the-square-root-of-a-prime-number-to-determine-if-it-is-pri","q":"\n\n<p>To test whether a number is prime or not why do we have to test whether it is divisible only upto the square root of that number ? </p>\n    ","a":"\n<p>If a number <code>n</code> is not a prime, it can be factored into two factors <code>a</code> and <code>b</code>:</p>\n\n<pre><code>n = a*b\n</code></pre>\n\n<p>If both <code>a</code> and <code>b</code> were greater than the square root of <code>n</code>, <code>a*b</code> would be greater than <code>n</code>.  So at least one of those factors must be less or equal to the square root of <code>n</code>, and to check if <code>n</code> is prime, we only need to test for factors less than or equal to the square root.</p>\n    "},{"t":"Effective method to hide email from spam bots","l":"http://stackoverflow.com/questions/483212/effective-method-to-hide-email-from-spam-bots","q":"\n\n<p>On my homepage, I'm using this method to hide my email from spam bots:</p>\n\n<pre><code>&lt;a href=\"admin [at] example.com\"\n   rel=\"nofollow\"\n   onclick=\"this.href='mailto:' + 'admin' + '@' + 'example.com'\"&gt;Contact me&lt;/a&gt;\n</code></pre>\n\n<p>What do you think about it? Is it effective? What other methods do you know or use?</p>\n    ","a":"\n<p>This is the method I used, with a server-side include, e.g. <code>&lt;!--#include file=\"emailObfuscator.include\" --&gt;</code> where <code>emailObfuscator.include</code> contains the following:</p>\n\n<pre><code>&lt;!-- // http://lists.evolt.org/archive/Week-of-Mon-20040202/154813.html --&gt;\n&lt;script type=\"text/javascript\"&gt;&lt;!--\nfunction gen_mail_to_link(lhs,rhs,subject)\n{\ndocument.write(\"&lt;A HREF=\\\"mailto\");\ndocument.write(\":\" + lhs + \"@\");\ndocument.write(rhs + \"?subject=\" + subject + \"\\\"&gt;\" + lhs + \"@\" + rhs + \"&lt;\\/A&gt;\"); } \n// --&gt; &lt;/SCRIPT&gt;\n</code></pre>\n\n<p>To include an address, I use JavaScript:</p>\n\n<pre><code>&lt;SCRIPT LANGUAGE=\"JavaScript\" type=\"text/javascript\"&gt;&lt;!-- \n  gen_mail_to_link('john.doe','example.com','Feedback about your site...')\n// --&gt; &lt;/SCRIPT&gt;\n&lt;NOSCRIPT&gt;\n  &lt;em&gt;Email address protected by JavaScript. Activate javascript to see the email.&lt;/em&gt;\n&lt;/NOSCRIPT&gt;\n</code></pre>\n\n<p>Because I get mail via Gmail since 2005, spam is pretty much a non-issue. So, I can't speak of how effective this method is. You might want to read <a href=\"http://techblog.tilllate.com/2008/07/20/ten-methods-to-obfuscate-e-mail-addresses-compared/\">this study</a> (although it's old) that produced this graph: </p>\n\n<p><img src=\"http://i.stack.imgur.com/To13I.png\" alt=\"enter image description here\"></p>\n    "},{"t":"Finding all cycles in graph","l":"http://stackoverflow.com/questions/546655/finding-all-cycles-in-graph","q":"\n\n<p>How can I find (iterate over) ALL the cycles in a directed graph from/to a given node?</p>\n\n<p>For example, I want something like this:</p>\n\n<pre><code>A-&gt;B-&gt;A\nA-&gt;B-&gt;C-&gt;A\n</code></pre>\n\n<p>but not:\n    B-&gt;C-&gt;B</p>\n    ","a":"\n<p>As far as I know, the best way to solve this would be with Tarjans(or Gabows or Kosaraju's --see Wikipedia link below) algorithm for finding strongly connected components of a graph. Strongly connected components and cycles are synonymous (but not exactly the same).</p>\n\n<p>To get a better idea, please see the following links:</p>\n\n<ol>\n<li><p>Wikipedia on Tarjans algorithm:\n<a href=\"http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm\">http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm</a></p></li>\n<li><p>A rigorous explanation: \n<a href=\"http://www.ics.uci.edu/~eppstein/161/960220.html\">http://www.ics.uci.edu/~eppstein/161/960220.html</a></p></li>\n<li><p>Other interesting links:<br>\n<a href=\"http://discuss.joelonsoftware.com/default.asp?design.4.249152.10\">http://discuss.joelonsoftware.com/default.asp?design.4.249152.10</a><br>\n<a href=\"http://forums.sun.com/thread.jspa?threadID=597673\">http://forums.sun.com/thread.jspa?threadID=597673</a><br>\n<a href=\"http://coding.derkeiler.com/Archive/General/comp.theory/2004-02/0468.html\">http://coding.derkeiler.com/Archive/General/comp.theory/2004-02/0468.html</a>  </p></li>\n<li><p>Similar question on SO:\n<a href=\"http://stackoverflow.com/questions/261573/best-algorithm-for-detecting-cycles-in-a-directed-graph\">Best algorithm for detecting cycles in a directed graph</a></p></li>\n</ol>\n\n<p>Now, that I've given the links, let me proceed to explain (after all its good answers and not links that really make stackoverflow such a great place).</p>\n\n<p><strong>Some points to remember</strong> (Taken from link 1):<br>\n1.Two vertices, A and B, are strongly connected if there's a path from A to B and a path from B to A. </p>\n\n<p>2.The set of all vertices that are strongly connected to a given vertex forms a strongly connected component of the graph. </p>\n\n<p>3.Any strongly connected component with more than one vertex in it contains at least one cycle, except components with a <a href=\"http://mathworld.wolfram.com/GraphLoop.html\">self-loop</a>. (Thanks for the help Jens Schauder, bcorso)</p>\n\n<p>4.We want to somehow collapse all the vertices in a cycle into a single node in a 'tree' (See links). Any future cycle involving vertices we've already visited gets folded into the same node. What we end up with is a tree where each node is a strongly connected component. </p>\n\n<p>5.To do this is to store two extra bits of information on each node. The number of steps the depth-first search takes to reach that node and the minimum number of steps the depth-first search takes to reach any node in that node's strongly connected component (from the nodes we've seen so far).</p>\n\n<p>6.As we perform a depth-first search on the main graph, we use the secondary data structure to help us test whether two nodes are \"the same\" (in the same strongly connected component, as it turns out) and add the current node to that secondary structure correctly.  </p>\n\n<p><strong>Algorithm</strong><br>\nThe question you have isn't trivial to solve. Here's how Tarjans algorithm works-  </p>\n\n<p>1.The first thing to know is that you have to do a DFS. I am assuming that a stack is  used to implement it. The DFS has to cover <strong><em>all</em></strong> vertices in the graph.</p>\n\n<p>2.Each vertex v, has to be labeled with two values, the index and the lowval. The index is simply the order in which DFS visits the node. The lowval is the minimum of the v's index and the index of the vertex that is nearest to v in the DFS. This vertex is then pushed onto the stack.</p>\n\n<p>3.For each vertex accessible from v, recurse if it isn't already in the stack.  </p>\n\n<p>4.For a vertex v, whose lowval == index, pop off all elements on the stack upto v itself and print them as one strongly connected component (cycle). </p>\n\n<p>I am going to try and implement this algorithm. I'll post it here if I succeed and if you want it at that time.</p>\n\n<p><strong>Edit</strong><br>\nThis question is still puzzling me -\nThis algorithm is linear in V+E. However, the number of cycles may be exponential in V. I am quite puzzled as to how this can be possible? I haven't been able to figure it out myself.<br>\nSee this link: <a href=\"http://www.me.utexas.edu/~bard/IP/Handouts/cycles.pdf\">http://www.me.utexas.edu/~bard/IP/Handouts/cycles.pdf</a> given by ShuggyCoUk and unknown(yahoo) for more details about the no. of cycles.</p>\n    "},{"t":"Unique (non-repeating) random numbers in O(1)?","l":"http://stackoverflow.com/questions/196017/unique-non-repeating-random-numbers-in-o1","q":"\n\n<p>I'd like to generate unique random numbers between 0 and 1000 that never repeat (i.e. 6 doesn't show up twice), but that doesn't resort to something like an O(N) search of previous values to do it. Is this possible?</p>\n    ","a":"\n<p>Initialize an array of 1001 integers with the values 0-1000 and set a variable, max, to the current max index of the array (starting with 1000).  Pick a random number, r,  between 0 and max, swap the number at the position r with the number at position max and return the number now at position max.  Decrement max by 1 and continue.  When max is 0, set max back to the size of the array - 1 and start again without the need to reinitialize the array.</p>\n\n<p><strong>Update:</strong>\nAlthough I came up with this method on my own when I answered the question, after some research I realize this is a modified version of <a href=\"http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle\">Fisher-Yates</a> known as Durstenfeld-Fisher-Yates or Knuth-Fisher-Yates.  Since the description may be a little difficult to follow, I have provided an example below (using 11 elements instead of 1001):</p>\n\n<p>Array starts off with 11 elements initialized to array[n] = n, max starts off at 10:</p>\n\n<pre><code>+--+--+--+--+--+--+--+--+--+--+--+\n| 0| 1| 2| 3| 4| 5| 6| 7| 8| 9|10|\n+--+--+--+--+--+--+--+--+--+--+--+\n                                ^\n                               max    \n</code></pre>\n\n<p>At each iteration, a random number r is selected between 0 and max, array[r] and array[max] are swapped, the new array[max] is returned, and max is decremented:</p>\n\n<pre><code>max = 10, r = 3\n           +--------------------+\n           v                    v\n+--+--+--+--+--+--+--+--+--+--+--+\n| 0| 1| 2|10| 4| 5| 6| 7| 8| 9| 3|\n+--+--+--+--+--+--+--+--+--+--+--+\n\nmax = 9, r = 7\n                       +-----+\n                       v     v\n+--+--+--+--+--+--+--+--+--+--+--+\n| 0| 1| 2|10| 4| 5| 6| 9| 8| 7: 3|\n+--+--+--+--+--+--+--+--+--+--+--+\n\nmax = 8, r = 1\n     +--------------------+\n     v                    v\n+--+--+--+--+--+--+--+--+--+--+--+\n| 0| 8| 2|10| 4| 5| 6| 9| 1: 7| 3|\n+--+--+--+--+--+--+--+--+--+--+--+\n\nmax = 7, r = 5\n                 +-----+\n                 v     v\n+--+--+--+--+--+--+--+--+--+--+--+\n| 0| 8| 2|10| 4| 9| 6| 5: 1| 7| 3|\n+--+--+--+--+--+--+--+--+--+--+--+\n\n...\n</code></pre>\n\n<p>After 11 iterations, all numbers in the array have been selected, max == 0, and the array elements are shuffled:</p>\n\n<pre><code>+--+--+--+--+--+--+--+--+--+--+--+\n| 4|10| 8| 6| 2| 0| 9| 5| 1| 7| 3|\n+--+--+--+--+--+--+--+--+--+--+--+\n</code></pre>\n\n<p>At this point, max can be reset to 10 and the process can continue.</p>\n    "},{"t":"Skip List vs. Binary Tree","l":"http://stackoverflow.com/questions/256511/skip-list-vs-binary-tree","q":"\n\n<p>I recently came across the data structure known as a <a href=\"http://en.wikipedia.org/wiki/Skip_list\">Skip list</a>. They seem to have very similar behavior to a binary search tree... my question is - why would you ever want to use a skip list over a binary search tree? </p>\n    ","a":"\n<p>Skip lists are more amenable to concurrent access/modification.  Herb Sutter wrote an <a href=\"http://www.ddj.com/hpc-high-performance-computing/208801371\">article</a> about data structure in concurrent environments.  It has more indepth information.</p>\n\n<p>The most frequently used implementation of a binary search tree is a <a href=\"http://en.wikipedia.org/wiki/Red-black_tree\">red-black tree</a>.  The concurrent problems come in when the tree is modified it often needs to rebalance.  The rebalance operation can affect large portions of the tree, which would require a mutex lock on many of the tree nodes.  Inserting a node into a skip list is far more localized, only nodes directly linked to the affected node need to be locked.</p>\n\n<hr>\n\n<p>Update from Jon Harrops comments</p>\n\n<p>I read Fraser and Harris's latest paper <a href=\"http://www.cl.cam.ac.uk/netos/papers/2007-cpwl.pdf\">Concurrent programming without locks</a>.  Really good stuff if you're interested in lock-free data structures.  The paper focuses on <a href=\"http://en.wikipedia.org/wiki/Transactional_memory\">Transactional Memory</a> and a theoretical operation multiword-compare-and-swap MCAS.  Both of these are simulated in software as no hardware supports them yet.  I'm fairly impressed that they were able to build MCAS in software at all.</p>\n\n<p>I didn't find the transactional memory stuff particularly compelling as it requires a garbage collector.  Also <a href=\"http://en.wikipedia.org/wiki/Software_transactional_memory\">software transactional memory</a> is plagued with performance issues.  However, I'd be very excited if hardware transactional memory ever becomes common.  In the end it's still research and won't be of use for production code for another decade or so.</p>\n\n<p>In section 8.2 they compare the performance of several concurrent tree implementations.  I'll summarize their findings.  It's worth it to download the pdf as it has some very informative graphs on pages 50, 53, and 54.</p>\n\n<ul>\n<li><strong>Locking skip lists</strong> are insanely fast.  They scale incredibly well with the number of concurrent accesses.  This is what makes skip lists special, other lock based data structures tend to croak under pressure.</li>\n<li><strong>Lock-free skip lists</strong> are consistently faster than locking skip lists but only barely.</li>\n<li><strong>transactional skip lists</strong> are consistently 2-3 times slower than the locking and non-locking versions.</li>\n<li><strong>locking red-black trees</strong> croak under concurrent access.  Their performance degrades linearly with each new concurrent user.  Of the two known locking red-black tree implementations, one essentially has a global lock during tree rebalancing.  The other uses fancy (and complicated) lock escalation but still doesn't significantly out perform the global lock version.</li>\n<li><strong>lock-free red-black trees</strong> don't exist (no longer true, see Update).</li>\n<li><strong>transactional red-black trees</strong> are comparable with transactional skip-lists.  That was very surprising and very promising.  Transactional memory, though slower if far easier to write.  It can be as easy as quick search and replace on the non-concurrent version.</li>\n</ul>\n\n<hr>\n\n<p>Update<br>\nHere is paper about lock-free trees: <a href=\"http://www.cs.umanitoba.ca/~hacamero/Research/RBTreesKim.pdf\">Lock-Free Red-Black Trees Using CAS</a>.<br>\nI haven't looked into it deeply, but on the surface it seems solid.</p>\n    "},{"t":"An algorithm for inflating/deflating (offsetting, buffering) polygons","l":"http://stackoverflow.com/questions/1109536/an-algorithm-for-inflating-deflating-offsetting-buffering-polygons","q":"\n\n<p><strong>UPDATE</strong>: the math term for what I'm looking for is actually <strong>inward/outward polygon offseting</strong>. +1 to balint for pointing this out. The alternative naming is <strong>polygon buffering</strong>.</p>\n\n<p><strong>UPDATE 2</strong> (02.11.2011): check out the newly accepted answer - Clipper library by Angus Johnson.</p>\n\n<p>Before I start developing my own solution from scratch, does anyone know of any good source for an algorithm that can inflate a polygon, something similar to this: </p>\n\n<p><img src=\"http://www.delphiforfun.org/programs/images/Inflated%20Polygon.gif\" alt=\"alt text\"></p>\n\n<p>The requirement is that the new (inflated) polygon's edges/points are all at the same constant distance from the old (original) polygon's (on the example pic. they are not, since then it would have to use arcs for inflated vertices, but let's forget about that for now ;) ). </p>\n\n<p><strong>Results of my search:</strong></p>\n\n<p>Here are some links:</p>\n\n<ul>\n<li><a href=\"http://fcacciola.50webs.com/Offseting%20Methods.htm\">A Survey of Polygon Offseting Strategies</a></li>\n<li><a href=\"http://groups.google.si/group/comp.graphics/browse_thread/thread/4799ab44d71f434e/e3ec0b40c231706c?hl=en&amp;ie=UTF-8&amp;q=offset+polygon&amp;pli=1\">Polygon offset, PROBLEM</a></li>\n<li><a href=\"http://www.sli.unimelb.edu.au/gisweb/BuffersModule/Buff_poly.htm\">Buffering Polygon Data</a></li>\n</ul>\n    ","a":"\n<p>I understand that this question was asked with an answer provided and accepted some time ago. Nevertheless I thought I might briefly mention my own <strong>polygon clipping and offsetting library</strong> - <a href=\"http://www.angusj.com/delphi/clipper.php\" rel=\"nofollow\"><strong>Clipper</strong></a> - in case others are still searching for a solution to this problem.</p>\n\n<p>While <a href=\"http://www.angusj.com/delphi/clipper.php\" rel=\"nofollow\">Clipper</a> is primarily designed for polygon clipping operations, it does polygon offsetting too. The library is <strong>open source freeware</strong> written in <strong>Delphi, C++ and C#</strong>. It has a very unencumbered <a href=\"http://www.boost.org/LICENSE_1_0.txt\" rel=\"nofollow\">Boost</a> license allowing it to be used in both freeware and commercial applications without charge. </p>\n\n<p>Polygon offsetting can be performed using one of three offset styles - squared, round and mitered.</p>\n\n<p><img src=\"http://www.angusj.com/delphi/clipper/documentation/Images/jointypes.png\" alt=\"Polygon offsetting styles\"></p>\n    "},{"t":"Is using Random and OrderBy a good shuffle algorithm?","l":"http://stackoverflow.com/questions/1287567/is-using-random-and-orderby-a-good-shuffle-algorithm","q":"\n\n<p>I have read <a href=\"http://www.codinghorror.com/blog/archives/001015.html\">an article</a> about various shuffle algorithms over at <a href=\"http://www.codinghorror.com/\">Coding Horror</a>. I have seen that somewhere people have done this to shuffle a list:</p>\n\n<pre><code>var r = new Random();\nvar shuffled = ordered.OrderBy(x =&gt; r.Next());\n</code></pre>\n\n<p>Is this a good shuffle algorithm? How does it work exactly? Is it an acceptable way of doing this?</p>\n    ","a":"\n<p>It's not a way of shuffling that I like, mostly on the grounds that it's O(n log n) for no good reason when it's easy to implement an O(n) shuffle. The code in the question \"works\" by basically giving a random (hopefully unique!) number to each element, then ordering the elements according to that number.</p>\n\n<p>I prefer Durstenfield's variant of the <a href=\"http://en.wikipedia.org/wiki/Fisher-Yates_shuffle\">Fisher-Yates shuffle</a> which swaps elements.</p>\n\n<p>Implementing a simple <code>Shuffle</code> extension method would basically consist of calling <code>ToList</code> or <code>ToArray</code> on the input then using an existing implementation of Fisher-Yates. (Pass in the <code>Random</code> as a parameter to make life generally nicer.) There are plenty of implementations around... I've probably got one in an answer somewhere.</p>\n\n<p>The nice thing about such an extension method is that it would then be very clear to the reader what you're actually trying to do.</p>\n\n<p>EDIT: Here's a simple implementation (no error checking!):</p>\n\n<pre><code>public static IEnumerable&lt;T&gt; Shuffle&lt;T&gt;(this IEnumerable&lt;T&gt; source, Random rng)\n{\n    T[] elements = source.ToArray();\n    // Note i &gt; 0 to avoid final pointless iteration\n    for (int i = elements.Length-1; i &gt; 0; i--)\n    {\n        // Swap element \"i\" with a random earlier element it (or itself)\n        int swapIndex = rng.Next(i + 1);\n        T tmp = elements[i];\n        elements[i] = elements[swapIndex];\n        elements[swapIndex] = tmp;\n    }\n    // Lazily yield (avoiding aliasing issues etc)\n    foreach (T element in elements)\n    {\n        yield return element;\n    }\n}\n</code></pre>\n\n<p>EDIT: Comments on performance below reminded me that we can actually return the elements as we shuffle them:</p>\n\n<pre><code>public static IEnumerable&lt;T&gt; Shuffle&lt;T&gt;(this IEnumerable&lt;T&gt; source, Random rng)\n{\n&nbsp; &nbsp; T[] elements = source.ToArray();\n&nbsp; &nbsp; for (int i = elements.Length - 1; i &gt;= 0; i--)\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; // Swap element \"i\" with a random earlier element it (or itself)\n        // ... except we don't really need to swap it fully, as we can\n        // return it immediately, and afterwards it's irrelevant.\n&nbsp; &nbsp; &nbsp; &nbsp; int swapIndex = rng.Next(i + 1);\n        yield return elements[swapIndex];\n&nbsp; &nbsp; &nbsp; &nbsp; elements[swapIndex] = elements[i];\n&nbsp; &nbsp; }\n}\n</code></pre>\n\n<p>This will now only do as much work as it needs to.</p>\n\n<p>Note that in both cases, you need to be careful about the instance of <code>Random</code> you use as:</p>\n\n<ul>\n<li>Creating two instances of <code>Random</code> at roughly the same time will yield the same sequence of random numbers (when used in the same way)</li>\n<li><code>Random</code> isn't thread-safe.</li>\n</ul>\n\n<p>I have <a href=\"http://csharpindepth.com/Articles/Chapter12/Random.aspx\">an article on <code>Random</code></a> which goes into more detail on these issues and provides solutions.</p>\n    "},{"t":"How do I calculate a point on a circle’s circumference? [duplicate]","l":"http://stackoverflow.com/questions/839899/how-do-i-calculate-a-point-on-a-circle-s-circumference","q":"\n\n<div class=\"question-status question-originals-of-duplicate\">\n    <p>This question already has an answer here:</p>\n    <ul>\n        <li>\n            <a href=\"/questions/674225/calculating-point-on-a-circles-circumference-from-angle-in-c\" dir=\"ltr\">Calculating point on a circle's circumference from angle in C#?</a>\n                <span class=\"question-originals-answer-count\">\n                    6 answers\n                </span>\n        </li>\n    </ul>\n</div>\n<p>How can the following function be implemented in various languages?</p>\n\n<p>Calculate the <code>(x,y)</code> point on the circumference of a circle, given input values of:</p>\n\n<ul>\n<li>Radius</li>\n<li>Angle</li>\n<li>Origin (optional parameter, if supported by the language)</li>\n</ul>\n    ","a":"\n<p>The <a href=\"http://en.wikipedia.org/wiki/Circle#Equations\">parametric equation for a circle</a> is</p>\n\n<pre><code>x = cx + r * cos(a)\ny = cy + r * sin(a)\n</code></pre>\n\n<p>Where <em>r</em> is the radius, <em>cx,cy</em> the origin, and <em>a</em> the angle from 0..2PI radians or 0..360 degrees.</p>\n\n<p>That's pretty easy to adapt into any language with basic trig functions.</p>\n    "},{"t":"Combined area of overlapping circles","l":"http://stackoverflow.com/questions/1667310/combined-area-of-overlapping-circles","q":"\n\n<p>I recently came across a problem where I had four circles (midpoints and radius) and had to calculate the area of the union of these circles.</p>\n\n<p>Example image:</p>\n\n<p><img src=\"http://i.stack.imgur.com/LxEIF.png\"></p>\n\n<p>For two circles it's quite easy,</p>\n\n<p><img src=\"http://i.stack.imgur.com/L9WCg.png\"></p>\n\n<p>I can just calculate the fraction of the each circles area that is not within the triangles and then calculate the area of the triangles.</p>\n\n<p>But is there a clever algorithm I can use when there is more than two circles?</p>\n    ","a":"\n<p>Find all circle intersections on the outer perimeter (e.g. B,D,F,H on the following diagram). Connect them together with the centres of the corresponding circles to form a polygon. The area of the union of the circles is the area of the polygon + the area of the circle slices defined by consecutive intersection points and the circle center in between them. You'll need to also account for any holes.</p>\n\n<p><img src=\"http://i.stack.imgur.com/VTW7e.png\" alt=\"circle overlap\"></p>\n    "},{"t":"Picking a random element from a set","l":"http://stackoverflow.com/questions/124671/picking-a-random-element-from-a-set","q":"\n\n<p>How do I pick a random element from a set?\nI'm particularly interested in picking a random element from a\nHashSet or a LinkedHashSet, in Java.\nSolutions for other languages are also welcome. </p>\n    ","a":"\n<pre><code>int size = myHashSet.size();\nint item = new Random().nextInt(size); // In real life, the Random object should be rather more shared than this\nint i = 0;\nfor(Object obj : myhashSet)\n{\n    if (i == item)\n        return obj;\n    i = i + 1;\n}\n</code></pre>\n    "},{"t":"Finding three elements in an array whose sum is closest to an given number","l":"http://stackoverflow.com/questions/2070359/finding-three-elements-in-an-array-whose-sum-is-closest-to-an-given-number","q":"\n\n<p>Given an array of integers, A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>, including negatives and positives, and another integer S. Now we need to find three different integers in the array, whose sum is closest to the given integer S. If there exists more than one solution, any of them is ok.</p>\n\n<p>You can assume all the integers are within int32_t range, and no arithmetic overflow will occur with calculating the sum. S is nothing special but a randomly picked number.</p>\n\n<p>Is there any efficient algorithm other than brute force search to find the three integers?</p>\n    ","a":"\n<blockquote>\n  <p>Is there any efficient algorithm other than brute force search to find the three integers?</p>\n</blockquote>\n\n<p>Yep; we can solve this in O(n<sup>2</sup>) time! First, consider that your problem <code>P</code> can be phrased equivalently in a slightly different way that eliminates the need for a \"target value\":</p>\n\n<blockquote>\n  <p><strong>original problem <code>P</code>:</strong> Given an array <code>A</code> of <code>n</code> integers and a target value <code>S</code>, does there exist a 3-tuple from <code>A</code> that sums to <code>S</code>?</p>\n  \n  <p><strong>modified problem <code>P'</code>:</strong> Given an array <code>A</code> of <code>n</code> integers, does there exist a 3-tuple from <code>A</code> that sums to zero?</p>\n</blockquote>\n\n<p>Notice that you can go from this version of the problem <code>P'</code> from <code>P</code> by subtracting your S/3 from each element in <code>A</code>, but now you don't need the target value anymore.</p>\n\n<p>Clearly, if we simply test all possible 3-tuples, we'd solve the problem in O(n<sup>3</sup>) -- that's the brute-force baseline. Is it possible to do better? What if we pick the tuples in a somewhat smarter way?</p>\n\n<p>First, we invest some time to sort the array, which costs us an initial penalty of O(n log n). Now we execute this algorithm:</p>\n\n<pre><code>for (i in 1..n-2) {\n  j = i+1  // Start right after i.\n  k = n    // Start at the end of the array.\n\n  while (k &gt;= j) {\n    // We got a match! All done.\n    if (A[i] + A[j] + A[k] == 0) return (A[i], A[j], A[k])\n\n    // We didn't match. Let's try to get a little closer:\n    //   If the sum was too big, decrement k.\n    //   If the sum was too small, increment j.\n    (A[i] + A[j] + A[k] &gt; 0) ? k-- : j++\n  }\n  // When the while-loop finishes, j and k have passed each other and there's\n  // no more useful combinations that we can try with this i.\n}\n</code></pre>\n\n<p>This algorithm works by placing three pointers, <code>i</code>, <code>j</code>, and <code>k</code> at various points in the array. <code>i</code> starts off at the beginning and slowly works its way to the end. <code>k</code> points to the very last element. <code>j</code> points to where <code>i</code> has started at. We iteratively try to sum the elements at their respective indices, and each time one of the following happens:</p>\n\n<ul>\n<li>The sum is exactly right! We've found the answer.</li>\n<li>The sum was too small. Move <code>j</code> closer to the end to select the next biggest number.</li>\n<li>The sum was too big. Move <code>k</code> closer to the beginning to select the next smallest number.</li>\n</ul>\n\n<p>For each <code>i</code>, the pointers of <code>j</code> and <code>k</code> will gradually get closer to each other. Eventually they will pass each other, and at that point we don't need to try anything else for that <code>i</code>, since we'd be summing the same elements, just in a different order. After that point, we try the next <code>i</code> and repeat.</p>\n\n<p>Eventually, we'll either exhaust the useful possibilities, or we'll find the solution. You can see that this is O(n<sup>2</sup>) since we execute the outer loop O(n) times and we execute the inner loop O(n) times. It's possible to do this sub-quadratically if you get really fancy, by representing each integer as a bit vector and performing a fast Fourier transform, but that's beyond the scope of this answer.</p>\n\n<hr>\n\n<p><strong>Note:</strong> Because this is an interview question, I've cheated a little bit here: this algorithm allows the selection of the same element multiple times. That is, (-1, -1, 2) would be a valid solution, as would (0, 0, 0). It also finds only the <em>exact</em> answers, not the closest answer, as the title mentions. As an exercise to the reader, I'll let you figure out how to make it work with distinct elements only (but it's a very simple change) and exact answers (which is also a simple change).</p>\n    "},{"t":"Recursion or Iteration?","l":"http://stackoverflow.com/questions/72209/recursion-or-iteration","q":"\n\n<p>Is there a performance hit if we use loop instead of recursion or vice versa in algorithms where both can serve the same purpose? Eg : Check if given string is palindrome.\nI have seen many programmers using recursion as a means to show off when a simple iteration algorithm can fit the bill.\nDoes the compiler play a vital role in deciding what to use?</p>\n    ","a":"\n<p>It is possible that recursion will be more expensive, depending on if the recursive function is <a href=\"http://en.wikipedia.org/wiki/Tail_recursion\">tail recursive</a> (last line is recursive call).  Tail recursion <em>should</em> be recognized by the compiler and optimized to its iterative counterpart (while maintaining the concise, clear implementation you have in your code).</p>\n\n<p>I would write the algorithm in the way that makes the most sense and is the most clear for the poor sucker (be it yourself or someone else) that has to maintain the code in a few months or years.  If you run into performance issues, then profile your code, and then and only then look into optimizing by moving over to an iterative implementation.  You may want to look into <a href=\"http://en.wikipedia.org/wiki/Memoization\">memoization</a> and <a href=\"http://en.wikipedia.org/wiki/Dynamic_programming\">dynamic programming</a>.</p>\n    "},{"t":"What is a loop invariant?","l":"http://stackoverflow.com/questions/3221577/what-is-a-loop-invariant","q":"\n\n<p>I'm reading \"Introduction to Algorithm\" CLRS. and the authors are talking about loop invariants, in chapter 2 (Insertion Sort). I don't have any idea of what it means.</p>\n    ","a":"\n<p>In simple words, a loop invariant is some predicate (condition) that holds for every iteration of the loop. For example, let's look at a simple <code>for</code> loop that looks like this:</p>\n\n<pre><code>int j = 9;\nfor(int i=0; i&lt;10; i++)  \n  j--;\n</code></pre>\n\n<p>In this example it is true (for every iteration) that <code>i + j == 9</code>. A weaker invariant that is also true is that<br>\n<code>i &gt;= 0 &amp;&amp; i &lt; 10</code> (because this is the termination condition!) or that <code>j &lt;= 9 &amp;&amp; j &gt;= 0</code>.</p>\n    "},{"t":"Difference between Big-O and Little-O Notation","l":"http://stackoverflow.com/questions/1364444/difference-between-big-o-and-little-o-notation","q":"\n\n<p>What is the difference between <strong>Big-O</strong> notation <code>O(n)</code> and <strong>Little-O</strong> notation <code>o(n)</code>?</p>\n    ","a":"\n<p>f ∈ O(g) says, essentially</p>\n\n<blockquote>\n  <p>For <strong>at least one</strong> choice of a constant <em>k</em> &gt; 0, you can find a constant <em>a</em> such that the inequality f(x) &lt; k g(x) holds for all x &gt; a. </p>\n</blockquote>\n\n<p>Note that O(g) is the set of all functions for which this condition holds.</p>\n\n<p>f ∈ o(g) says, essentially</p>\n\n<blockquote>\n  <p>For <strong>every</strong> choice of a constant <em>k</em> &gt; 0, you can find a constant <em>a</em> such that the inequality f(x) &lt; k g(x) holds for all x &gt; a.</p>\n</blockquote>\n\n<p>Once again, note that o(g) is a set.</p>\n\n<p>In Big-O, it is only necessary that you find a particular multiplier <em>k</em> for which the inequality holds beyond some minimum <em>x</em>. </p>\n\n<p>In Little-o, it must be that there is a minimum <em>x</em> after which the inequality holds no matter how small you make <em>k</em>, as long as it is not negative or zero.</p>\n\n<p>These both describe upper bounds, although somewhat counter-intuitively, Little-o is the stronger statement. There is a much larger gap between the growth rates of f and g if f ∈ o(g) than if f ∈ O(g). </p>\n\n<p>One illustration of the disparity is this: f ∈ O(f) is true, but f ∈ o(f) is false. Therefore, Big-O can be read as \"f ∈ O(g) means that f's asymptotic growth is no faster than g's\", whereas \"f ∈ o(g) means that f's asymptotic growth is strictly slower than g's\". It's like <code>&lt;=</code> versus <code>&lt;</code>.</p>\n\n<p>More specifically, if the value of g(x) is a constant multiple of the value of f(x), then f ∈ O(g) is true. This is why you can drop constants when working with big-O notation.</p>\n\n<p>However, for f ∈ o(g) to be true, then g must include a higher <em>power</em> of x in its formula, and so the relative separation between f(x) and g(x) must actually get larger as x gets larger.</p>\n\n<p>To use purely math examples (rather than referring to algorithms):</p>\n\n<p>The following are true for Big-O, but would not be true if you used little-o:</p>\n\n<ul>\n<li>x^2 ∈ O(x^2) </li>\n<li>x^2 ∈ O(x^2 + x)</li>\n<li>x^2 ∈ O(200 * x^2)</li>\n</ul>\n\n<p>The following are true for little-o:</p>\n\n<ul>\n<li>x^2 ∈ o(x^3)</li>\n<li>x^2 ∈ o(x!)</li>\n<li>ln(x) ∈ o(x)</li>\n</ul>\n\n<p>Note that if f ∈ o(g), this implies f ∈ O(g). e.g. x^2 ∈ o(x^3) so it is also true that x^2 ∈ O(x^3), (again, think of O as <code>&lt;=</code> and o as <code>&lt;</code>)</p>\n    "},{"t":"Finding duplicates in O(n) time and O(1) space","l":"http://stackoverflow.com/questions/5739024/finding-duplicates-in-on-time-and-o1-space","q":"\n\n<p>Input: Given an array of n elements which contains elements from 0 to n-1, with any of these numbers appearing any number of times. </p>\n\n<p>Goal : To find these repeating numbers in O(n) and using only constant memory space.</p>\n\n<p>For example, let n be 7 and array be {1, 2, 3, 1, 3, 0, 6}, the answer should be 1 &amp; 3.\nI checked similar questions here but the answers used some data structures like <code>HashSet</code> etc. </p>\n\n<p>Any efficient algorithm for the same? </p>\n    ","a":"\n<p>This is what I came up with, which doesn't require the additional sign bit:</p>\n\n<pre><code>for i := 0 to n - 1\n    while A[A[i]] != A[i] \n        swap(A[i], A[A[i]])\n    end while\nend for\n\nfor i := 0 to n - 1\n    if A[i] != i then \n        print A[i]\n    end if\nend for\n</code></pre>\n\n<p>The first loop permutes the array so that if element <code>x</code> is present at least once, then one of those entries will be at position <code>A[x]</code>.</p>\n\n<p>Note that it may not look O(n) at first blush, but it is - although it has a nested loop, it still runs in <code>O(N)</code> time.  A swap only occurs if there is an <code>i</code> such that <code>A[i] != i</code>, and each swap sets at least one element such that <code>A[i] == i</code>, where that wasn't true before.  This means that the total number of swaps (and thus the total number of executions of the <code>while</code> loop body) is at most <code>N-1</code>.</p>\n\n<p>The second loop prints the values of <code>x</code> for which <code>A[x]</code> doesn't equal <code>x</code> - since the first loop guarantees that if <code>x</code> exists at least once in the array, one of those instances will be at <code>A[x]</code>, this means that it prints those values of <code>x</code> which are not present in the array.</p>\n\n<p><a href=\"http://ideone.com/kb81h\">(Ideone link so you can play with it)</a></p>\n    "},{"t":"Data structure for loaded dice?","l":"http://stackoverflow.com/questions/5027757/data-structure-for-loaded-dice","q":"\n\n<p>Suppose that I have an n-sided loaded die where each side k has some probability p<sub>k</sub> of coming up when I roll it.  I'm curious if there is good algorithm for storing this information statically (i.e. for a fixed set of probabilities) so that I can efficiently simulate a random roll of the die.</p>\n\n<p>Currently, I have an O(lg n) solution for this problem.  The idea is to store a table of the cumulative probability of the first k sides for all k, them to generate a random real number in the range [0, 1) and perform a binary search over the table to get the largest index whose cumulative value is no greater than the chosen value.  I rather like this solution, but it seems odd that the runtime doesn't take the probabilities into account.  In particular, in the extremal cases of one side always coming up or the values being uniformly distributed, it's possible to generate the result of the roll in O(1) using a naive approach, though my solution will still take logarithmicallh many steps.</p>\n\n<p>Does anyone have any suggestions for how to solve this problem in a way that is somehow \"adaptive\" in it's runtime?</p>\n\n<p>Thanks so much! </p>\n\n<p><strong>EDIT</strong>: Based on the answers to this question, I have written up <strong><a href=\"http://www.keithschwarz.com/darts-dice-coins/\">an article describing many approaches to this problem</a></strong>, along with their analyses.  It looks like Vose's implementation of the alias method gives Θ(n) preprocessing time and O(1) time per die roll, which is truly impressive.  Hopefully this is a useful addition to the information contained in the answers!</p>\n    ","a":"\n<p>You are looking for the <a href=\"http://en.wikipedia.org/wiki/Alias_method\">alias method</a> which provides a <strong>O(1)</strong> method for generating a fixed discrete probability distribution (assuming you can access entries in an array of length n in constant time) with a one-time O(n) set-up. You can find it documented in <a href=\"http://luc.devroye.org/chapter_three.pdf\">chapter 3 (PDF)</a> of <a href=\"http://luc.devroye.org/rnbookindex.html\">\"Non-Uniform Random Variate Generation\"</a> by Luc Devroye.</p>\n\n<p>The idea is to take your array of probabilities p<sub>k</sub> and produce three new n-element arrays, q<sub>k</sub>, a<sub>k</sub>, and b<sub>k</sub>. Each q<sub>k</sub> is a probability between 0 and 1, and each a<sub>k</sub> and b<sub>k</sub> is an integer between 1 and n. </p>\n\n<p>We generate random numbers between 1 and n by generating two random numbers, r and s, between 0 and 1. Let i = floor(r*N)+1. If q<sub>i</sub> &lt; s then return a<sub>i</sub> else return b<sub>i</sub>. The work in the alias method is in figuring out how to produce q<sub>k</sub>, a<sub>k</sub> and b<sub>k</sub>.</p>\n    "},{"t":"Are there any worse sorting algorithms than Bogosort (a.k.a Monkey Sort)? [closed]","l":"http://stackoverflow.com/questions/2609857/are-there-any-worse-sorting-algorithms-than-bogosort-a-k-a-monkey-sort","q":"\n\n<p>My co-workers took me back in time to my University days with a discussion of sorting algorithms this morning.  We reminisced about our favorites like <a href=\"http://en.wikipedia.org/wiki/Gnome_sort\">StupidSort</a>, and one of us was sure we had seen a sort algorithm that was <code>O(n!)</code>.  That got me started looking around for the \"worst\" sorting algorithms I could find.</p>\n\n<p>We postulated that a completely random sort would be pretty bad (i.e. randomize the elements - is it in order?  no?  randomize again), and I looked around and found out that it's apparently called <a href=\"http://en.wikipedia.org/wiki/Bogosort\">BogoSort, or Monkey Sort, or sometimes just Random Sort</a>.</p>\n\n<p>Monkey Sort appears to have a worst case performance of <code>O(∞)</code>, a best case performance of <code>O(n)</code>, and an average performance of <code>O(n·n!)</code>.</p>\n\n<p>Are there any named algorithms that have worse average performance than <code>O(n·n!)</code>?  Or are just sillier than Monkey Sort in general?</p>\n    ","a":"\n<p>From <a href=\"http://www.dangermouse.net/esoteric/\">David Morgan-Mar</a>'s Esoteric Algorithms page: <strong><a href=\"http://www.dangermouse.net/esoteric/intelligentdesignsort.html\">Intelligent Design Sort</a></strong></p>\n\n<blockquote>\n  <p><strong>Introduction</strong></p>\n  \n  <p>Intelligent design sort is a sorting algorithm based on the theory of\n  intelligent design.</p>\n  \n  <p><strong>Algorithm Description</strong></p>\n  \n  <p>The probability of the original input list being in the exact order\n  it's in is 1/(n!). There is such a small likelihood of this that it's\n  clearly absurd to say that this happened by chance, so it must have\n  been consciously put in that order by an intelligent Sorter. Therefore\n  it's safe to assume that it's already optimally Sorted in some way\n  that transcends our naïve mortal understanding of \"ascending order\".\n  Any attempt to change that order to conform to our own preconceptions\n  would actually make it less sorted.</p>\n  \n  <p><strong>Analysis</strong></p>\n  \n  <p>This algorithm is constant in time, and sorts the list in-place,\n  requiring no additional memory at all. In fact, it doesn't even\n  require any of that suspicious technological computer stuff. Praise\n  the Sorter!</p>\n  \n  <p><strong>Feedback</strong></p>\n  \n  <p>Gary Rogers writes:</p>\n  \n  <blockquote>\n    <p>Making the sort constant in time\n    denies the power of The Sorter. The\n    Sorter exists outside of time, thus\n    the sort is timeless. To require time\n    to validate the sort dimishes the role\n    of the Sorter. Thus... this particular\n    sort is flawed, and can not be\n    attributed to 'The Sorter'.</p>\n  </blockquote>\n  \n  <p>Heresy!</p>\n</blockquote>\n    "},{"t":"Largest prime factor of a number","l":"http://stackoverflow.com/questions/23287/largest-prime-factor-of-a-number","q":"\n\n<p>What is the best approach to calculating the largest prime factor of a number?</p>\n\n<p>I'm thinking the most efficient would be the following:</p>\n\n<ol>\n<li>Find lowest prime number that divides cleanly</li>\n<li>Check if result of division is prime</li>\n<li>If not, find next lowest</li>\n<li>Go to 2.</li>\n</ol>\n\n<p>I'm basing this assumption on it being easier to calculate the small prime factors. Is this about right? What other approaches should I look into?</p>\n\n<p>Edit: I've now realised that my approach is futile if there are more than 2 prime factors in play, since step 2 fails when the result is a product of two other primes, therefore a recursive algorithm is needed.</p>\n\n<p>Edit again: And now I've realised that this does still work, because the last found prime number has to be the highest one, therefore any further testing of the non-prime result from step 2 would result in a smaller prime.</p>\n    ","a":"\n<p>Actually there are several more efficient ways to find factors of big numbers (for smaller ones trial division works reasonably well). </p>\n\n<p>One method which is very fast if the input number has two factors very close to its square root is known as <a href=\"http://en.wikipedia.org/wiki/Fermat's_factorization_method\">Fermat factorisation</a>. It makes use of the identity N = (a + b)(a - b) = a^2 - b^2 and is easy to understand and implement. Unfortunately it's not very fast in general.</p>\n\n<p>The best known method for factoring numbers up to 100 digits long is the <a href=\"http://en.wikipedia.org/wiki/Quadratic_sieve\">Quadratic sieve</a>. As a bonus, part of the algorithm is easily done with parallel processing.</p>\n\n<p>Yet another algorithm I've heard of is <a href=\"http://en.wikipedia.org/wiki/Pollard's_rho_algorithm\">Pollard's Rho algorithm</a>. It's not as efficient as the Quadratic Sieve in general but seems to be easier to implement.</p>\n\n<hr>\n\n<p>Once you've decided on how to split a number into two factors, here is the fastest algorithm I can think of to find the largest prime factor of a number:</p>\n\n<p>Create a priority queue which initially stores the number itself. Each iteration, you remove the highest number from the queue, and attempt to split it into two factors (not allowing 1 to be one of those factors, of course). If this step fails, the number is prime and you have your answer! Otherwise you add the two factors into the queue and repeat.</p>\n    "},{"t":"Maximum number of characters using keystrokes A, Ctrl+A, Ctrl+C and Ctrl+V","l":"http://stackoverflow.com/questions/4606984/maximum-number-of-characters-using-keystrokes-a-ctrla-ctrlc-and-ctrlv","q":"\n\n<p>This is an interview question from google. I am not able to solve it by myself. Can somebody shed some light?</p>\n\n<p>Write a program to print the sequence of keystrokes such that it generates the maximum number of character 'A's. You are allowed to use only 4 keys: <kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>C</kbd> and <kbd>Ctrl</kbd>+<kbd>V</kbd>. Only N keystrokes are allowed. All <kbd>Ctrl</kbd>+ characters are considered as one keystroke, so <kbd>Ctrl</kbd>+<kbd>A</kbd> is one keystroke.</p>\n\n<p>For example, the sequence <kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>C</kbd>, <kbd>Ctrl</kbd>+<kbd>V</kbd> generates two A's in 4 keystrokes.</p>\n\n<ul>\n<li>Ctrl+A is Select All</li>\n<li>Ctrl+C is Copy</li>\n<li>Ctrl+V is Paste</li>\n</ul>\n\n<p>I did some mathematics. For any N, using x numbers of A's , one <kbd>Ctrl</kbd>+<kbd>A</kbd>, one <kbd>Ctrl</kbd>+<kbd>C</kbd> and y <kbd>Ctrl</kbd>+<kbd>V</kbd>, we can generate max ((N-1)/2)<sup>2</sup> number of A's. For some N &gt; M, it is better to use as many <kbd>Ctrl</kbd>+<kbd>A</kbd>'s, <kbd>Ctrl</kbd>+<kbd>C</kbd> and <kbd>Ctrl</kbd>+<kbd>V</kbd> sequences as it doubles the number of A's.</p>\n\n<p>The sequence <kbd>Ctrl</kbd>+<kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>V</kbd>, <kbd>Ctrl</kbd>+<kbd>C</kbd> will not overwrite the existing selection. It will append the copied selection to selected one.</p>\n    ","a":"\n<p>There's a dynamic programming solution. We start off knowing 0 keys can make us 0 A's. Then we iterate through for <code>i</code> up to <code>n</code>, doing two things: pressing A once and pressing select all + copy followed by paste <code>j</code> times (actually <code>j-i-1</code> below; note the trick here: the contents are still in the clipboard, so we can paste it multiple times without copying each time). We only have to consider up to 4 consecutive pastes, since select, copy, paste x 5 is equivalent to select, copy, paste, select, copy, paste and the latter is better since it leaves us with more in the clipboard. Once we've reached <code>n</code>, we have the desired result.</p>\n\n<p>The complexity might appear to be O(N), but since the numbers grow at an exponential rate it is actually O(N<sup>2</sup>) due to the complexity of multiplying the large numbers. Below is a Python implementation. It takes about 0.5 seconds to calculate for N=50,000.</p>\n\n<pre><code>def max_chars(n):\n  dp = [0] * (n+1)\n  for i in xrange(n):\n    dp[i+1] = max(dp[i+1], dp[i]+1) # press a\n    for j in xrange(i+3, min(i+7, n+1)):\n      dp[j] = max(dp[j], dp[i]*(j-i-1)) # press select all, copy, paste x (j-i-1)\n  return dp[n]\n</code></pre>\n\n<p>In the code, <code>j</code> represents the total number of keys pressed after our new sequence of keypresses. We already have <code>i</code> keypresses at this stage, and 2 new keypresses go to select-all and copy. Therefore we're hitting paste <code>j-i-2</code> times. Since pasting adds to the existing sequence of <code>dp[i]</code> <code>A</code>'s, we need to add <code>1</code> making it <code>j-i-1</code>. This explains the <code>j-i-1</code> in the 2nd-last line.</p>\n\n<p>Here are some results (<code>n</code> =&gt; number of A's):</p>\n\n<ul>\n<li>7 =&gt; 9</li>\n<li>8 =&gt; 12</li>\n<li>9 =&gt; 16</li>\n<li>10 =&gt; 20</li>\n<li>100 =&gt; 1391569403904</li>\n<li>1,000 =&gt; 3268160001953743683783272702066311903448533894049486008426303248121757146615064636953144900245\n174442911064952028008546304</li>\n<li>50,000 =&gt; <a href=\"http://pastebin.com/FPJU3VP2\">a very large number!</a></li>\n</ul>\n\n<p>I agree with @SB that you should always state your assumptions: Mine is that you don't need to paste twice to double the number of characters. This gets the answer for 7, so unless my solution is wrong the assumption must be right.</p>\n\n<p>In case someone wonders why I'm not checking sequences of the form <kbd>Ctrl</kbd>+<kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>C</kbd>, <kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>V</kbd>: The end result will always be the same as <kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>A</kbd>, <kbd>Ctrl</kbd>+<kbd>C</kbd>, <kbd>Ctrl</kbd>+<kbd>V</kbd> which I <em>do</em> consider.</p>\n    "},{"t":"Tetris-ing an array","l":"http://stackoverflow.com/questions/3275258/tetris-ing-an-array","q":"\n\n<p>Consider the following array:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>/www/htdocs/1/sites/lib/abcdedd\n/www/htdocs/1/sites/conf/xyz\n/www/htdocs/1/sites/conf/abc/def\n/www/htdocs/1/sites/htdocs/xyz\n/www/htdocs/1/sites/lib2/abcdedd\n</code></pre>\n\n<p>what is the shortest and most elegant way of detecting the <em>common base path</em> - in this case</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>/www/htdocs/1/sites/\n</code></pre>\n\n<p>and removing it from all elements in the array?</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>lib/abcdedd\nconf/xyz\nconf/abc/def\nhtdocs/xyz\nlib2/abcdedd\n</code></pre>\n    ","a":"\n<p>Write a function <code>longest_common_prefix</code> that takes two strings as input. Then apply it to the strings in any order to reduce them to their common prefix. Since it is associative and commutative the order doesn't matter for the result.</p>\n\n<p>This is the same as for other binary operations like for example addition or greatest common divisor.</p>\n    "},{"t":"Is there a perfect algorithm for chess?","l":"http://stackoverflow.com/questions/297577/is-there-a-perfect-algorithm-for-chess","q":"\n\n<p>I was recently in a discussion with a non-coder person on the possibilities of chess computers.  I'm not well versed in theory, but think I know enough.</p>\n\n<p>I argued that there could not exist a deterministic Turing machine that always won or stalemated at chess.  I think that, even if you search the entire space of all combinations of player1/2 moves, the single move that the computer decides upon at each step is based on a heuristic.  Being based on a heuristic, it does not necessarily beat ALL of the moves that the opponent could do.</p>\n\n<p>My friend thought, to the contrary, that a computer would always win or tie if it never made a \"mistake\" move (however do you define that?).  However, being a programmer who has taken CS, I know that even your good choices - given a wise opponent - can force you to make \"mistake\" moves in the end.  Even if you know everything, your next move is greedy in matching a heuristic.</p>\n\n<p>Most chess computers try to match a possible end game to the game in progress, which is essentially a dynamic programming traceback.  Again, the endgame in question is avoidable though.</p>\n\n<p>Edit: Hmm... looks like I ruffled some feathers here.  That's good.</p>\n\n<p>Thinking about it again, it seems like there is no theoretical problem with solving a finite game like chess.  I would argue that chess is a bit more complicated than checkers in that a win is not necessarily by numerical exhaustion of pieces, but by a mate.  My original assertion is probably wrong, but then again I think I've pointed out something that is not yet satisfactorily proven (formally).</p>\n\n<p>I guess my thought experiment was that whenever a branch in the tree is taken, then the algorithm (or memorized paths) must find a path to a mate (without getting mated) for any possible branch on the opponent moves. After the discussion, I will buy that given more memory than we can possibly dream of, all these paths could be found.</p>\n    ","a":"\n<p>\"I argued that there could not exist a deterministic Turing machine that always won or stalemated at chess.\"  </p>\n\n<p>You're not quite right.  There can be such a machine.  The issue is the hugeness of the state space that it would have to search.  It's finite, it's just <strong>REALLY</strong> big.</p>\n\n<p>That's why chess falls back on heuristics -- the state space is too huge (but finite).  To even enumerate -- much less search for every perfect move along every course of every possible game -- would be a very, very big search problem.</p>\n\n<p>Openings are scripted to get you to a mid-game that gives you a \"strong\" position.  Not a known outcome.  Even end games -- when there are fewer pieces -- are hard to enumerate to determine a best next move.  Technically they're finite.  But the number of alternatives is huge.  Even a 2 rooks + king has something like 22 possible next moves.  And if it takes 6 moves to mate, you're looking at 12,855,002,631,049,216 moves.</p>\n\n<p>Do the math on opening moves.  While there's only about 20 opening moves, there are something like 30 or so second moves, so by the third move we're looking at 360,000 alternative game states.</p>\n\n<p>But chess games are (technically) finite.  Huge, but finite.  There's perfect information.  There are defined start and end-states, There are no coin-tosses or dice rolls.  </p>\n    "},{"t":"How exactly does tail recursion work?","l":"http://stackoverflow.com/questions/15518882/how-exactly-does-tail-recursion-work","q":"\n\n<p>I almost understand how tail recursion works and the difference between it and a normal recursion. I <strong>only</strong> don't understand why it <strong>doesn't</strong> require stack to remember its return address.</p>\n\n<pre><code>// tail recursion\nint fac_times (int n, int acc) {\n    if (n == 0) return acc;\n    else return fac_times(n - 1, acc * n);\n}\n\nint factorial (int n) {\n    return fac_times (n, 1);\n}\n\n// normal recursion\nint factorial (int n) {\n    if (n == 0) return 1;\n    else return n * factorial(n - 1);\n}\n</code></pre>\n\n<p>There is nothing to do after calling a function itself in a tail recursion function but it doesn't make sense for me.</p>\n    ","a":"\n<p>The compiler is simply able to transform this</p>\n\n<pre><code>int fac_times (int n, int acc) {\n    if (n == 0) return acc;\n    else return fac_times(n - 1, acc * n);\n}\n</code></pre>\n\n<p>into something like this:</p>\n\n<pre><code>int fac_times (int n, int acc) {\nlabel:\n    if (n == 0) return acc;\n    acc *= n--;\n    goto label;\n}\n</code></pre>\n    "},{"t":"Given an array of numbers, return array of products of all other numbers (no division)","l":"http://stackoverflow.com/questions/2680548/given-an-array-of-numbers-return-array-of-products-of-all-other-numbers-no-div","q":"\n\n<p>I was asked this question in a job interview, and I'd like to know how others would solve it. I'm most comfortable with Java, but solutions in other languages are welcome.</p>\n\n<blockquote>\n  <p>Given an array of numbers, <code>nums</code>, return an array of numbers <code>products</code>, where <code>products[i]</code> is the product of all <code>nums[j], j != i</code>.</p>\n\n<pre><code>Input : [1, 2, 3, 4, 5]\nOutput: [(2*3*4*5), (1*3*4*5), (1*2*4*5), (1*2*3*5), (1*2*3*4)]\n      = [120, 60, 40, 30, 24]\n</code></pre>\n  \n  <p>You must do this in <code>O(N)</code> without using division.</p>\n</blockquote>\n    ","a":"\n\n\n<p>An explanation of <a href=\"http://stackoverflow.com/users/276101/polygenelubricants\">polygenelubricants</a> method is:\nThe trick is to construct the arrays (in the case for 4 elements)</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>{              1,         a[0],    a[0]*a[1],    a[0]*a[1]*a[2],  }\n{ a[1]*a[2]*a[3],    a[2]*a[3],         a[3],                 1,  }\n</code></pre>\n\n<p>Both of which can be done in O(n) by starting at the left and right edges respectively.</p>\n\n<p>Then multiplying the two arrays element by element gives the required result</p>\n\n<p>My code would look something like this:</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>int a[N] // This is the input\nint products_below[N];\np=1;\nfor(int i=0;i&lt;N;++i) {\n  products_below[i]=p;\n  p*=a[i];\n}\n\nint products_above[N];\np=1;\nfor(int i=N-1;i&gt;=0;--i) {\n  products_above[i]=p;\n  p*=a[i];\n}\n\nint products[N]; // This is the result\nfor(int i=0;i&lt;N;++i) {\n  products[i]=products_below[i]*products_above[i];\n}\n</code></pre>\n\n<p>If you need to be O(1) in space too you can do this (which is less clear IMHO)</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>int a[N] // This is the input\nint products[N];\n\n// Get the products below the current index\np=1;\nfor(int i=0;i&lt;N;++i) {\n  products[i]=p;\n  p*=a[i];\n}\n\n// Get the products above the curent index\np=1;\nfor(int i=N-1;i&gt;=0;--i) {\n  products[i]*=p;\n  p*=a[i];\n}\n</code></pre>\n    "},{"t":"What is the method for converting radians to degrees? [closed]","l":"http://stackoverflow.com/questions/135909/what-is-the-method-for-converting-radians-to-degrees","q":"\n\n<p>I run into this occasionally and always forget how to do it.</p>\n\n<p>One of those things that pop up ever so often.</p>\n\n<p>Also, what's the formula to convert angles expressed in radians to degrees and back again?</p>\n    ","a":"\n<pre><code>radians = degrees * (pi/180)\n\ndegrees = radians * (180/pi)\n</code></pre>\n\n<p>As for implementation, the main question is how precise you want to be about the value of pi.  There is some related discussion <a href=\"http://stackoverflow.com/questions/19/fastest-way-to-get-value-of-pi\">here</a></p>\n    "},{"t":"What is the best way to compute trending topics or tags?","l":"http://stackoverflow.com/questions/787496/what-is-the-best-way-to-compute-trending-topics-or-tags","q":"\n\n<p>Many sites offer some statistics like \"The hottest topics in the last 24h\". For example, Topix.com shows this in its section \"News Trends\". There, you can see the topics which have the fastest growing number of mentions.</p>\n\n<p>I want to compute such a \"buzz\" for a topic, too. How could I do this? The algorithm should weight the topics which are always hot less. The topics which normally (almost) noone mentions should be the hottest ones.</p>\n\n<p>Google offers \"Hot Trends\", topix.com shows \"Hot Topics\", fav.or.it shows \"Keyword Trends\" - all these services have one thing in common: They only show you upcoming trends which are abnormally hot at the moment.</p>\n\n<p>Terms like \"Britney Spears\", \"weather\" or \"Paris Hilton\" won't appear in these lists because they're always hot and frequent. <a href=\"http://www.americanscientist.org/issues/pub/the-britney-spears-problem\">This article calls this \"The Britney Spears Problem\".</a></p>\n\n<p>My question: How can you code an algorithm or use an existing one to solve this problem? Having a list with the keywords searched in the last 24h, the algorithm should show you the 10 (for example) hottest ones.</p>\n\n<p>I know, in the article above, there is some kind of algorithm mentioned. <a href=\"http://paste.bradleygill.com/index.php?paste_id=9117\">I've tried to code it in PHP</a> but I don't think that it'll work. It just finds the majority, doesn't it?</p>\n\n<p>I hope you can help me (coding examples would be great).</p>\n    ","a":"\n<p>You need an algorithm that measures the velocity of a topic - or in other words, if you graph it you want to show those that are going up at an incredible rate.</p>\n\n<p>This is the first derivative of the trend line, and it is not difficult to incorporate as a weighted factor of your overall calculation.</p>\n\n<p><strong>Normalize</strong></p>\n\n<p>One technique you'll need to do is to normalize all your data.  For each topic you are following, keep a very low pass filter that defines that topic's baseline.  Now every data point that comes in about that topic should be normalized - subtract its baseline and you'll get ALL of your topics near 0, with spikes above and below the line.  You may instead want to divide the signal by its baseline magnitude, which will bring the signal to around 1.0 - this not only brings all signals in line with each other (normalizes the baseline), but also normalizes the spikes.  A britney spike is going to be magnitudes larger than someone else's spike, but that doesn't mean you should pay attention to it - the spike may be very small relative to her baseline.</p>\n\n<p><strong>Derive</strong></p>\n\n<p>Once you've normalized everything, figure out the slope of each topic.  Take two consecutive points, and measure the difference.  A positive difference is trending up, a negative difference is trending down.  Then you can compare the normalized differences, and find out what topics are shooting upward in popularity compared to other topics - with each topic scaled appropriate to it's own 'normal' which may be magnitudes of order different from other topics.</p>\n\n<p>This is really a first-pass at the problem.  There are more advanced techniques which you'll need to use (mostly a combination of the above with other algorithms, weighted to suit your needs) but it should be enough to get you started.</p>\n\n<p><strong>Regarding the article</strong></p>\n\n<p>The article is about topic trending, but it's not about how to calculate what's hot and what's not, it's about how to process the huge amount of information that such an algorithm must process at places like Lycos and Google.  The space and time required to give each topic a counter, and find each topic's counter when a search on it goes through is huge.  This article is about the challenges one faces when attempting such a task.  It does mention the Brittney effect, but it doesn't talk about how to overcome it.</p>\n\n<p>As <a href=\"http://stackoverflow.com/questions/826330/britney-spears-problem-how-to-solve/826509#826509\">Nixuz points out</a> this is also referred to as a Z or <a href=\"http://en.wikipedia.org/wiki/Standard%5Fscore\">Standard Score</a>.</p>\n    "},{"t":"What is the difference between LL and LR parsing?","l":"http://stackoverflow.com/questions/5975741/what-is-the-difference-between-ll-and-lr-parsing","q":"\n\n<p>Can anyone give me a simple example of LL parsing versus LR parsing?</p>\n    ","a":"\n<p>At a high level, the difference between LL parsing and LR parsing is that LL parsers begin at the start symbol and try to apply productions to arrive at the target string, whereas LR parsers begin at the target string and try to arrive back at the start symbol.</p>\n\n<p>An LL parse is a left-to-right, leftmost derivation.  That is, we consider the input symbols from the left to the right and attempt to construct a leftmost derivation.  This is done by beginning at the start symbol and repeatedly expanding out the leftmost nonterminal until we arrive at the target string.  An LR parse is a left-to-right, rightmost derivation, meaning that we scan from the left to right and attempt to construct a rightmost derivation.  The parser continuously picks a substring of the input and attempts to reverse it back to a nonterminal.</p>\n\n<p>During an LL parse, the parser continuously chooses between two actions:</p>\n\n<ol>\n<li><strong>Predict</strong>: Based on the leftmost nonterminal and some number of lookahead tokens, choose which production ought to be applied to get closer to the input string.</li>\n<li><strong>Match</strong>: Match the leftmost guessed terminal symbol with the leftmost unconsumed symbol of input.</li>\n</ol>\n\n<p>As an example, given this grammar:</p>\n\n<ul>\n<li>S → E</li>\n<li>E → T + E</li>\n<li>E → T</li>\n<li>T → <code>int</code></li>\n</ul>\n\n<p>Then given the string <code>int + int + int</code>, an LL(2) parser (which uses two tokens of lookahead) would parse the string as follows:</p>\n\n<pre><code>Production       Input              Action\n---------------------------------------------------------\nS                int + int + int    Predict S -&gt; E\nE                int + int + int    Predict E -&gt; T + E\nT + E            int + int + int    Predict T -&gt; int\nint + E          int + int + int    Match int\n+ E              + int + int        Match +\nE                int + int          Predict E -&gt; T + E\nT + E            int + int          Predict T -&gt; int\nint + E          int + int          Match int\n+ E              + int              Match +\nE                int                Predict E -&gt; T\nT                int                Predict T -&gt; int\nint              int                Match int\n                                    Accept\n</code></pre>\n\n<p>Notice that in each step we look at the leftmost symbol in our production.  If it's a terminal, we match it, and if it's a nonterminal, we predict what it's going to be by choosing one of the rules.</p>\n\n<p>In an LR parser, there are two actions:</p>\n\n<ol>\n<li><strong>Shift</strong>: Add the next token of input to a buffer for consideration.</li>\n<li><strong>Reduce</strong>: Reduce a collection of terminals and nonterminals in this buffer back to some nonterminal by reversing a production.</li>\n</ol>\n\n<p>As an example, an LR(1) parser (with one token of lookahead) might parse that same string as follows:</p>\n\n<pre><code>Workspace        Input              Action\n---------------------------------------------------------\n                 int + int + int    Shift\nint              + int + int        Reduce T -&gt; int\nT                + int + int        Shift\nT +              int + int          Shift\nT + int          + int              Reduce T -&gt; int\nT + T            + int              Shift\nT + T +          int                Shift\nT + T + int                         Reduce T -&gt; int\nT + T + T                           Reduce E -&gt; T\nT + T + E                           Reduce E -&gt; T + E\nT + E                               Reduce E -&gt; T + E\nE                                   Reduce S -&gt; E\nS                                   Accept\n</code></pre>\n\n<p>The two parsing algorithms you mentioned (LL and LR) are known to have different characteristics.  LL parsers tend to be easier to write by hand, but they are less powerful than LR parsers and accept a much smaller set of grammars than LR parsers do.  LR parsers come in many flavors (LR(0), SLR(1), LALR(1), LR(1), IELR(1), GLR(0), etc.) and are far more powerful.  They also tend to have much more complex and are almost always generated by tools like <code>yacc</code> or <code>bison</code>.  LL parsers also come in many flavors (including LL(*), which is used by the <a href=\"http://www.antlr.org/\"><code>ANTLR</code></a> tool), though in practice LL(1) is the most-widely used.</p>\n\n<p>As a shameless plug, if you'd like to learn more about LL and LR parsing, I just finished teaching a compilers course and have <a href=\"http://www.Stanford.edu/class/archive/cs/cs143/cs143.1128/\">some handouts and lecture slides on parsing</a> on the course website.  I'd be glad to elaborate on any of them if you think it would be useful.</p>\n\n<p>Hope this helps!</p>\n    "},{"t":"How to determine the longest increasing subsequence using dynamic programming?","l":"http://stackoverflow.com/questions/2631726/how-to-determine-the-longest-increasing-subsequence-using-dynamic-programming","q":"\n\n<p>Let's say I have a set of integers.  I want to find the longest increasing subsequence of that set using dynamic programming.  This is simply out of practice, reviewing my old notes from my algorithms course, and I don't seem to understand how this works.</p>\n    ","a":"\n<p>OK, I will describe first the simplest solution which is O(N^2), where N is the size of the set. There also exists a O(N log N) solution, which I will describe also. Look <a href=\"http://en.wikipedia.org/wiki/Longest_increasing_subsequence\">here</a> for it at the section Efficient algorithms.</p>\n\n<p>I will assume the indices of the array are from 0 to N-1. So let's define DP[i] to be the length of the LIS(Longest increasing subsequence) which is ending at element with index i. To compute DP[i] we look at all indices j &lt; i and check both if DP[j] + 1 &gt; DP[i] and array[j] &lt; array[i](we want it to be increasing). If this is true we can update the current optimum for DP[i]. To find the global optimum for the array you can take the maximum value from DP[0..N-1].</p>\n\n<pre><code>int maxLength = 1, bestEnd = 0;\nDP[0] = 1;\nprev[0] = -1;\n\nfor (int i = 1; i &lt; N; i++)\n{\n   DP[i] = 1;\n   prev[i] = -1;\n\n   for (int j = i - 1; j &gt;= 0; j--)\n      if (DP[j] + 1 &gt; DP[i] &amp;&amp; array[j] &lt; array[i])\n      {\n         DP[i] = DP[j] + 1;\n         prev[i] = j;\n      }\n\n   if (DP[i] &gt; maxLength)\n   {\n      bestEnd = i;\n      maxLength = DP[i];\n   }\n}\n</code></pre>\n\n<p>I use the array <code>prev</code> to be able later to find the actual sequence not only its length. Just go back recursively from bestEnd in a loop using prev[bestEnd]. The -1 value is a sign to stop.</p>\n\n<p><strong>OK, now to the more efficient <code>O(N log N)</code> solution:</strong></p>\n\n<p>Let <code>S[pos]</code> be defined as the smallest integer that ends an increasing sequence of length <code>pos</code>.</p>\n\n<p>Now iterate through every integer <code>X</code> of the input set and do the following:</p>\n\n<ol>\n<li><p>If <code>X</code> &gt; last element in <code>S</code>, then append <code>X</code> to the end of <code>S</code>. This essentialy means we have found a new largest <code>LIS</code>.</p></li>\n<li><p>Otherwise find the smallest element in <code>S</code>, which is <code>&gt;=</code> than <code>X</code>, and change it to <code>X</code>. \nBecause <code>S</code> is sorted at any time, the element can be found using binary search in <code>log(N)</code>.</p></li>\n</ol>\n\n<p>Total runtime - <code>N</code> integers and a binary search for each of them - N * log(N) = O(N log N)</p>\n\n<p>Now let's do a real example:</p>\n\n<p>Set of integers:\n<code>2 6 3 4 1 2 9 5 8</code></p>\n\n<p>Steps:</p>\n\n<pre><code>0. S = {} - Initialize S to the empty set\n1. S = {2} - New largest LIS\n2. S = {2, 6} - New largest LIS\n3. S = {2, 3} - Changed 6 to 3\n4. S = {2, 3, 4} - New largest LIS\n5. S = {1, 3, 4} - Changed 2 to 1\n6. S = {1, 2, 4} - Changed 3 to 2\n7. S = {1, 2, 4, 9} - New largest LIS\n8. S = {1, 2, 4, 5} - Changed 9 to 5\n9. S = {1, 2, 4, 5, 8} - New largest LIS\n</code></pre>\n\n<p>So the length of the LIS is <code>5</code> (the size of S).</p>\n\n<p>To reconstruct the actual <code>LIS</code> we will again use a parent array.\nLet <code>parent[i]</code> be the predecessor of element with index <code>i</code> in the <code>LIS</code> ending at element with index <code>i</code>.</p>\n\n<p>To make things simpler, we can keep in the array <code>S</code>, not the actual integers, but their indices(positions) in the set. We do not keep <code>{1, 2, 4, 5, 8}</code>, but keep <code>{4, 5, 3, 7, 8}</code>. </p>\n\n<p>That is input[4] = <strong>1</strong>, input[5] = <strong>2</strong>, input[3] = <strong>4</strong>, input[7] = <strong>5</strong>, input[8] = <strong>8</strong>.</p>\n\n<p>If we update properly the parent array, the actual LIS is:</p>\n\n<pre><code>input[S[lastElementOfS]], \ninput[parent[S[lastElementOfS]]],\ninput[parent[parent[S[lastElementOfS]]]],\n........................................\n</code></pre>\n\n<p>Now to the important thing - how do we update the parent array? There are two options:</p>\n\n<ol>\n<li><p>If <code>X</code> &gt; last element in <code>S</code>, then <code>parent[indexX] = indexLastElement</code>. This means the parent of the newest element is the last element. We just prepend <code>X</code> to the end of <code>S</code>.</p></li>\n<li><p>Otherwise find the index of the smallest element in <code>S</code>, which is <code>&gt;=</code> than <code>X</code>, and change it to <code>X</code>. Here <code>parent[indexX] = S[index - 1]</code>.</p></li>\n</ol>\n    "},{"t":"Is log(n!) = Θ(n·log(n))?","l":"http://stackoverflow.com/questions/2095395/is-logn-%ce%98n-logn","q":"\n\n<p>This is a homework question.  I'm not expecting an answer, just some guidance, possibly :)  I am to show that <strong>log(<em>n</em>!) = Θ(<em>n</em>·log(<em>n</em>))</strong>.</p>\n\n<p>A hint was given that I should show the upper bound with <strong><em>n</em><sup><em>n</em></sup></strong> and show the lower bound with <strong>(<em>n</em>/2)<sup>(<em>n</em>/2)</sup></strong>.  This does not seem all that intuitive to me.  Why would that be the case?  I can definitely see how to convert <strong><em>n</em><sup><em>n</em></sup></strong> to <strong><em>n</em>·log(<em>n</em>)</strong> [log both sides of an equation], but that's kind of working backwards.  What would be the correct approach to tackle this problem?  Should I draw the recursion tree?  There is nothing recursive about this, so that doesn't seem like a likely approach..</p>\n    ","a":"\n<p>Remember that </p>\n\n<pre><code>log(n!) = log(1) + log(2) + ... + log(n-1) + log(n)\n</code></pre>\n\n<p>You can get the upper bound by </p>\n\n<pre><code>log(1) + log(2) + ... + log(n) &lt;= log(n) + log(n) + ... + log(n)\n                                = n*log(n)\n</code></pre>\n\n<p>And you can get the lower bound by doing a similar thing after throwing away the first half of the sum:</p>\n\n<pre><code>log(1) + ... + log(n/2) + ... + log(n) &gt;= log(n/2) + ... + log(n) \n                                       &gt;= log(n/2) + ... + log(n/2)\n                                        = n/2 * log(n/2) \n</code></pre>\n    "},{"t":"Which sort algorithm works best on mostly sorted data? [closed]","l":"http://stackoverflow.com/questions/220044/which-sort-algorithm-works-best-on-mostly-sorted-data","q":"\n\n<p>Which sort algorithm works best on mostly sorted data?</p>\n    ","a":"\n<p>Based on the highly scientific method of watching <a href=\"http://www.sorting-algorithms.com/\">animated gifs</a> I would say Insertion and Bubble sorts are good candidates.  </p>\n    "},{"t":"Kruskal vs Prim","l":"http://stackoverflow.com/questions/1195872/kruskal-vs-prim","q":"\n\n<p>I was wondering when one should use <a href=\"http://en.wikipedia.org/wiki/Prim%27s%5Falgorithm\">Prim's algorithm</a> and when <a href=\"http://en.wikipedia.org/wiki/Kruskal%27s%5Falgorithm\">Kruskal's</a> to find the minimum spanning tree? They both have easy logics, same worst cases, and only difference is implementation which might involve a bit different data structures. So what is the deciding factor?</p>\n    ","a":"\n<p>Use Prim's algorithm when you have a graph with lots of edges.</p>\n\n<p>For a graph with <strong>V</strong> vertices <strong>E</strong> edges, Kruskal's algorithm runs in <strong>O(E log V)</strong> time and Prim's algorithm can run in <strong>O(E + V log V)</strong> amortized time, if you use a <a href=\"http://en.wikipedia.org/wiki/Fibonacci%5Fheap\">Fibonacci Heap</a>.</p>\n\n<p>Prim's algorithm is significantly faster in the limit when you've got a really dense graph with many more edges than vertices.  Kruskal performs better in typical situations (sparse graphs) because it uses simpler data structures.</p>\n    "},{"t":"How might I find the largest number contained in a JavaScript array?","l":"http://stackoverflow.com/questions/1379553/how-might-i-find-the-largest-number-contained-in-a-javascript-array","q":"\n\n<p>I have a simple JavaScript Array object containing a few numbers.</p>\n\n<pre><code>[267, 306, 108]\n</code></pre>\n\n<p>Is there a function that would find the largest number in this array?</p>\n    ","a":"\n<p><a href=\"http://ejohn.org/blog/fast-javascript-maxmin/\">Resig to the rescue:</a></p>\n\n<pre><code>Array.max = function( array ){\n    return Math.max.apply( Math, array );\n};\n</code></pre>\n    "},{"t":"Build heap complexity","l":"http://stackoverflow.com/questions/9755721/build-heap-complexity","q":"\n\n<p>Can someone help explain how can building a heap be O(n) complexity? Inserting an item into a heap is O(logN), and the insert is repeated n/2 times (the remainder are leaves and can't violate the heap property. So this means the complexity should be O(nLog n) I would think.</p>\n\n<p>Put another way, for each item we \"heapify\", it has the potential to have to filter down once for each level for the heap so far (which is log n levels).</p>\n\n<p>What am I missing?</p>\n    ","a":"\n<p>Your analysis is corrrect. However, it is not tight. It is not really easy to explain. You should better read it.</p>\n\n<p>Great analysis of algorithm can be seen <a href=\"http://www.cs.umd.edu/~meesh/351/mount/lectures/lect14-heapsort-analysis-part.pdf\">here</a>.</p>\n\n<hr>\n\n<p>However, the main idea is, in <code>Buildheap</code> algorithm the actual <code>Heapify</code> cost is not <code>O(logn)</code> for all elements.</p>\n\n<p>When <code>Heapify</code> is called, the running time depends on how far an element might move down in tree before the process terminates. In other words it depends on the height of node. In the worst case the element might go down all the way to the leaf\nlevel. </p>\n\n<p>Let us count the work done level by level.\nAt the bottommost level there are <code>2^(h)</code>\nnodes, but we do not call Heapify on any of these so the work is\n0. At the next to level there are \n<code>2^(h − 1)</code>\nnodes, and each might move down by 1 level. At the 3rd\nlevel from the bottom there are <code>2^(h − 2)</code>\nnodes, and each might move down by 2 levels.</p>\n\n<p>As you can see not all heapify operations are <code>O(logn)</code>, this is why you are getting <code>O(n)</code>.</p>\n    "},{"t":"What is the fastest substring search algorithm?","l":"http://stackoverflow.com/questions/3183582/what-is-the-fastest-substring-search-algorithm","q":"\n\n<p>OK, so I don't sound like an idiot I'm going to state the problem/requirements more explicitly:</p>\n\n<ul>\n<li>Needle (pattern) and haystack (text to search) are both C-style null-terminated strings. No length information is provided; if needed, it must be computed.</li>\n<li>Function should return a pointer to the first match, or <code>NULL</code> if no match is found.</li>\n<li><strong>Failure cases are not allowed. This means any algorithm with non-constant (or large constant) storage requirements will need to have a fallback case for allocation failure (and performance in the fallback care thereby contributes to worst-case performance).</strong></li>\n<li>Implementation is to be in C, although a good description of the algorithm (or link to such) without code is fine too.</li>\n</ul>\n\n<p>...as well as what I mean by \"fastest\":</p>\n\n<ul>\n<li>Deterministic <code>O(n)</code> where <code>n</code> = haystack length. (But it may be possible to use ideas from algorithms which are normally <code>O(nm)</code> (for example rolling hash) if they're combined with a more robust algorithm to give deterministic <code>O(n)</code> results).</li>\n<li>Never performs (measurably; a couple clocks for <code>if (!needle[1])</code> etc. are okay) worse than the naive brute force algorithm, especially on very short needles which are likely the most common case. (Unconditional heavy preprocessing overhead is bad, as is trying to improve the linear coefficient for pathological needles at the expense of likely needles.)</li>\n<li>Given an arbitrary needle and haystack, comparable or better performance (no worse than 50% longer search time) versus any other widely-implemented algorithm.</li>\n<li>Aside from these conditions, I'm leaving the definition of \"fastest\" open-ended. A good answer should explain why you consider the approach you're suggesting \"fastest\".</li>\n</ul>\n\n<p>My current implementation runs in roughly between 10% slower and 8 times faster (depending on the input) than glibc's implementation of Two-Way.</p>\n\n<p><strong>Update: My current optimal algorithm is as follows:</strong></p>\n\n<ul>\n<li>For needles of length 1, use <code>strchr</code>.</li>\n<li>For needles of length 2-4, use machine words to compare 2-4 bytes at once as follows: Preload needle in a 16- or 32-bit integer with bitshifts and cycle old byte out/new bytes in from the haystack at each iteration. Every byte of the haystack is read exactly once and incurs a check against 0 (end of string) and one 16- or 32-bit comparison.</li>\n<li>For needles of length &gt;4, use Two-Way algorithm with a bad shift table (like Boyer-Moore) which is applied only to the last byte of the window. To avoid the overhead of initializing a 1kb table, which would be a net loss for many moderate-length needles, I keep a bit array (32 bytes) marking which entries in the shift table are initialized. Bits that are unset correspond to byte values which never appear in the needle, for which a full-needle-length shift is possible.</li>\n</ul>\n\n<p>The big questions left in my mind are:</p>\n\n<ul>\n<li>Is there a way to make better use of the bad shift table? Boyer-Moore makes best use of it by scanning backwards (right-to-left) but Two-Way requires a left-to-right scan.</li>\n<li>The only two viable candidate algorithms I've found for the general case (no out-of-memory or quadratic performance conditions) are <a href=\"http://www-igm.univ-mlv.fr/~lecroq/string/node26.html\">Two-Way</a> and <a href=\"http://www-igm.univ-mlv.fr/~lecroq/string/node27.html\">String Matching on Ordered Alphabets</a>. But are there easily-detectable cases where different algorithms would be optimal? Certainly many of the <code>O(m)</code> (where <code>m</code> is needle length) in space algorithms could be used for <code>m&lt;100</code> or so. It would also be possible to use algorithms which are worst-case quadratic if there's an easy test for needles which provably require only linear time.</li>\n</ul>\n\n<p>Bonus points for:</p>\n\n<ul>\n<li>Can you improve performance by assuming the needle and haystack are both well-formed UTF-8? (With characters of varying byte lengths, well-formed-ness imposes some string alignment requirements between the needle and haystack and allows automatic 2-4 byte shifts when a mismatching head byte is encountered. But do these constraints buy you much/anything beyond what maximal suffix computations, good suffix shifts, etc. already give you with various algorithms?)</li>\n</ul>\n\n<p><strong>Note:</strong> I'm well aware of most of the algorithms out there, just not how well they perform in practice. Here's a good reference so people don't keep giving me references on algorithms as comments/answers: <a href=\"http://www-igm.univ-mlv.fr/~lecroq/string/index.html\">http://www-igm.univ-mlv.fr/~lecroq/string/index.html</a></p>\n    ","a":"\n<p>Build up a test library of likely needles and haystacks.  Profile the tests on several search algorithms, including brute force.  Pick the one that performs best with your data.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Boyer-Moore_string_search_algorithm\">Boyer-Moore</a> uses a bad character table with a good suffix table.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Boyer-Moore-Horspool_algorithm\">Boyer-Moore-Horspool</a> uses a bad character table.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Knuth-Morris-Pratt_algorithm\">Knuth-Morris-Pratt</a> uses a partial match table.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Rabin-Karp_string_search_algorithm\">Rabin-Karp</a> uses running hashes.</p>\n\n<p>They all trade overhead for reduced comparisons to a different degree, so the real world performance will depend on the average lengths of both the needle and haystack.  The more initial overhead, the better with longer inputs.  With very short needles, brute force may win.</p>\n\n<p>Edit:</p>\n\n<p>A different algorithm might be best for finding base pairs, english phrases, or single words.  If there were one best algorithm for all inputs, it would have been publicized.</p>\n\n<p>Think about the following little table.  Each question mark might have a different best search algorithm.</p>\n\n<pre><code>                 short needle     long needle\nshort haystack         ?               ?\nlong haystack          ?               ?\n</code></pre>\n\n<p>This should really be a graph, with a range of shorter to longer inputs on each axis.  If you plotted each algorithm on such a graph, each would have a different signature.  Some algorithms suffer with a lot of repetition in the pattern, which might affect uses like searching for genes.  Some other factors that affect overall performance are searching for the same pattern more than once and searching for different patterns at the same time.</p>\n\n<p>If I needed a sample set, I think I would scrape a site like google or wikipedia, then strip the html from all the result pages.  For a search site, type in a word then use one of the suggested search phrases.  Choose a few different languages, if applicable.  Using web pages, all the texts would be short to medium, so merge enough pages to get longer texts.  You can also find public domain books, legal records, and other large bodies of text.  Or just generate random content by picking words from a dictionary.  But the point of profiling is to test against the type of content you will be searching, so use real world samples if possible.</p>\n\n<p>I left short and long vague.  For the needle, I think of short as under 8 characters, medium as under 64 characters, and long as under 1k.  For the haystack, I think of short as under 2^10, medium as under a 2^20, and long as up to a 2^30 characters.</p>\n    "},{"t":"Mapping two integers to one, in a unique and deterministic way","l":"http://stackoverflow.com/questions/919612/mapping-two-integers-to-one-in-a-unique-and-deterministic-way","q":"\n\n<p>Imagine two positive integers A and B. I want to combine these two into a single integer C. </p>\n\n<p>There can be no other integers D and E which combine to C.\nSo combining them with the addition operator doesn't work. Eg 30 + 10 = 40 = 40 + 0 = 39 + 1\nNeither does concatination work. Eg \"31\" + \"2\" = 312 = \"3\" + \"12\"</p>\n\n<p>This combination operation should also be deterministic (always yield the same result with the same inputs) <em>and</em> should always yield an integer on either the positive or the negative side of integers.</p>\n    ","a":"\n<p>You're looking for a bijective <code>NxN -&gt; N</code> mapping. These are used for e.g. <a href=\"http://en.wikipedia.org/wiki/Dovetailing%5F%28computer%5Fscience%29\"><em>dovetailing</em></a>. Have a look at <a href=\"http://www.lsi.upc.es/~alvarez/calculabilitat/enumerabilitat.pdf\">this PDF</a> for an introduction to so-called <em>pairing functions</em>. Wikipedia introduces a specific pairing function, namely the <a href=\"http://en.wikipedia.org/wiki/Cantor%5Fpairing%5Ffunction\">Cantor pairing function</a>:</p>\n\n<ul>\n<li><em>pi(k1, k2) = 1/2(k1 + k2)(k1 + k2 + 1) + k2</em></li>\n</ul>\n\n<p>Three remarks:</p>\n\n<ul>\n<li>As others have made clear, if you plan to implement a pairing function, you may soon find you need arbitrarily large integers (bignums).</li>\n<li>If you don't want to make a distinction between the pairs (a, b) and (b, a), then sort a and b before applying the pairing function.</li>\n<li>Actually I lied. You are looking for a bijective <code>ZxZ -&gt; N</code> mapping. Cantor's function only works on non-negative numbers. This is not a problem however, because it's easy to define a bijection <code>f : Z -&gt; N</code>, like so:\n<ul>\n<li><em>f(n) = n * 2</em> if n &gt;= 0</li>\n<li><em>f(n) = -n * 2 - 1</em> if n &lt; 0</li>\n</ul></li>\n</ul>\n    "},{"t":"Implement Stack using Two Queues","l":"http://stackoverflow.com/questions/688276/implement-stack-using-two-queues","q":"\n\n<p>A similiar question was asked earlier <a href=\"http://stackoverflow.com/questions/69192/using-stack-as-queue\">there</a>, but the question here is the reverse of it, using two queues as a stack.  The question...</p>\n\n<p>Given two queues with their standard operations (<em>enqueue</em>, <em>dequeue</em>, <em>isempty</em>, <em>size</em>), implement a stack with its standard operations (<em>pop</em>, <em>push</em>, <em>isempty</em>, <em>size</em>).</p>\n\n<p>There should be TWO versions of the solution.</p>\n\n<ul>\n<li>Version A: The stack should be efficient when pushing an item.</li>\n<li>Version B: The stack should be efficient when popping an item.</li>\n</ul>\n\n<p>I am interested in the algorithm more than any specific language implementations.  However, I welcome solutions expressed in languages which I am familiar (Java, C#, Python, VB, Javascript, Php).  Thanks in advance.</p>\n    ","a":"\n<p>Version A:</p>\n\n<ul>\n<li>push: \n<ul>\n<li>enqueue in queue1</li>\n</ul></li>\n<li>pop:\n<ul>\n<li>while size of queue1 is bigger than 1, pipe dequeued items from queue1 into queue2</li>\n<li>dequeue and return the last item of queue1, then switch the names of queue1 and queue2</li>\n</ul></li>\n</ul>\n\n<p>Version B:</p>\n\n<ul>\n<li>push:\n<ul>\n<li>enqueue in queue2</li>\n<li>enqueue all items of queue1 in queue2, then switch the names of queue1 and queue2</li>\n</ul></li>\n<li>pop:\n<ul>\n<li>deqeue from queue1</li>\n</ul></li>\n</ul>\n    "},{"t":"Javascript Array.sort implementation?","l":"http://stackoverflow.com/questions/234683/javascript-array-sort-implementation","q":"\n\n<p>Which algorithm does the JavaScript <code>Array#sort()</code> function use?  I understand that it can take all manner of arguments and functions to perform different kinds of sorts, I'm simply interested in which algorithm the vanilla sort uses.</p>\n    ","a":"\n<p>If you look at this bug <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=224128\">224128</a>, it appears that MergeSort is being used by Mozilla.</p>\n    "},{"t":"How to reverse a singly linked list using only two pointers?","l":"http://stackoverflow.com/questions/1801549/how-to-reverse-a-singly-linked-list-using-only-two-pointers","q":"\n\n<p>I would be wondered if there exists some logic to reverse the linked list using only two pointers.</p>\n\n<p>The following is used to reverse the single linked list using three pointers namely p, q, r:</p>\n\n<pre><code>struct node\n{\n    int data;\n    struct node *link;\n};\n\nvoid reverse()\n{\n    struct node *p = first,\n                *q = NULL,\n                *r;\n    while (p != NULL)\n    {\n        r = q;\n        q = p;\n        p = p-&gt;link;\n        q-&gt;link = r;\n    }\n    q = first;\n}\n</code></pre>\n\n<p>Is there any other alternate to reverse the linked list?  what would be the best logic to reverse a singly linked list, in terms of time complexity?</p>\n    ","a":"\n<p>Any alternative?  No, this is as simple as it gets, and there's no fundamentally-different way of doing it.  This algorithm is already O(n) time, and you can't get any faster than that, as you must modify every node.</p>\n\n<p>It looks like your code is on the right track, but it's not quite working in the form above.  Here's a working version:</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\ntypedef struct Node {\n  char data;\n  struct Node* next;\n} Node;\n\nvoid print_list(Node* root) {\n  while (root) {\n    printf(\"%c \", root-&gt;data);\n    root = root-&gt;next;\n  }\n  printf(\"\\n\");\n}\n\nNode* reverse(Node* root) {\n  Node* new_root = 0;\n  while (root) {\n    Node* next = root-&gt;next;\n    root-&gt;next = new_root;\n    new_root = root;\n    root = next;\n  }\n  return new_root;\n}\n\nint main() {\n  Node d = { 'd', 0 };\n  Node c = { 'c', &amp;d };\n  Node b = { 'b', &amp;c };\n  Node a = { 'a', &amp;b };\n\n  Node* root = &amp;a;\n  print_list(root);\n  root = reverse(root);\n  print_list(root);\n\n  return 0;\n}\n</code></pre>\n    "},{"t":"How to compare two colors","l":"http://stackoverflow.com/questions/9018016/how-to-compare-two-colors","q":"\n\n<p>I want to design a program that can help me assess between 5 pre-defined colors which one is more similar to a variable color, and with what percentage.  The thing is that I don't know how to do that manually step by step. So it is even more difficult to think of a program.</p>\n\n<p>More details: The colors are from photographs of tubes with gel that as different colors. I have 5 tubes with different colors were each is representative of 1 of 5 levels. I want to take photographs of other samples and on the computer assess to which level that sample belongs by comparing colors, and I want to know that with a percentage of approximation too. I would like a program that does something like this: <a href=\"http://www.colortools.net/color_matcher.html\">http://www.colortools.net/color_matcher.html</a></p>\n\n<p>If you can tell me what steps to take, even if they are things for me to think and do manually. It would be very helpful. </p>\n    ","a":"\n<p>See Wikipedia's article on <a href=\"http://en.wikipedia.org/wiki/Color_difference\">Color Difference</a> for the right leads.\nBasically, you want to compute a distance metric in some multidimensional colorspace.\nBut RGB is not \"perceptually uniform\", so your Euclidean RGB distance metric suggested by Vadim will not match the human-perceived distance between colors.  For a start, L*a*b* is intended to be a perceptually uniform colorspace, and the deltaE metric is commonly used.  But there are more refined colorspaces and more refined deltaE formulas that get closer to matching human perception.</p>\n\n<p>You'll have to learn more about colorspaces and illuminants to do the conversions.  But for a quick formula that is better than the Euclidean RGB metric, just do this:  assume that your RGB values are in the sRGB colorspace, find the sRGB to L*a*b* conversion formulas, convert your sRGB colors to L*a*b*, and compute deltaE between your two L*a*b* values.  It's not computationally expensive, it's just some nonlinear formulas and some multiplies and adds.</p>\n    "},{"t":"How can I find the shortest path between 100 moving targets? (Live demo included.)","l":"http://stackoverflow.com/questions/15485473/how-can-i-find-the-shortest-path-between-100-moving-targets-live-demo-included","q":"\n\n<h2>Background</h2>\n\n<p>This picture illustrates the problem:\n<img src=\"http://i.stack.imgur.com/dNVaO.png\" alt=\"square_grid_with_arrows_giving_directions\"></p>\n\n<p>I can control the red circle.  The targets are the blue triangles.  The black arrows indicate the direction that the targets will move.</p>\n\n<p>I want to collect all targets in the minimum number of steps.</p>\n\n<p>Each turn I must move 1 step either left/right/up or down.</p>\n\n<p>Each turn the targets will also move 1 step according to the directions shown on the board.</p>\n\n<h2>Demo</h2>\n\n<p>I've put up a playable demo of the problem <a href=\"http://penguinspuzzle.appspot.com/a_fishy_problem.html\">here on Google appengine</a>.</p>\n\n<p>I would be very interested if anyone can beat the target score as this would show that my current algorithm is suboptimal.  (A congratulations message should be printed if you manage this!)</p>\n\n<h2>Problem</h2>\n\n<p>My current algorithm scales really badly with the number of targets.  The time goes up exponentially and for 16 fish it is already several seconds. </p>\n\n<p>I would like to compute the answer for board sizes of 32*32 and with 100 moving targets.</p>\n\n<h2>Question</h2>\n\n<p>What is an efficient algorithm (ideally in Javascript) for computing the minimum number of steps to collect all targets?</p>\n\n<h2>What I've tried</h2>\n\n<p>My current approach is based on memoisation but it is very slow and I don't know whether it will always generate the best solution.</p>\n\n<p>I solve the subproblem of \"what is the minimum number of steps to collect a given set of targets and end up at a particular target?\".</p>\n\n<p>The subproblem is solved recursively by examining each choice for the previous target to have visited.\nI assume that it is always optimal to collect the previous subset of targets as quickly as possible and then move from the position you ended up to the current target as quickly as possible (although I don't know whether this is a valid assumption).</p>\n\n<p>This results in n*2^n states to be computed which grows very rapidly.</p>\n\n<p>The current code is shown below:</p>\n\n<pre><code>var DX=[1,0,-1,0];\nvar DY=[0,1,0,-1]; \n\n// Return the location of the given fish at time t\nfunction getPt(fish,t) {\n  var i;\n  var x=pts[fish][0];\n  var y=pts[fish][1];\n  for(i=0;i&lt;t;i++) {\n    var b=board[x][y];\n    x+=DX[b];\n    y+=DY[b];\n  }\n  return [x,y];\n}\n\n// Return the number of steps to track down the given fish\n// Work by iterating and selecting first time when Manhattan distance matches time\nfunction fastest_route(peng,dest) {\n  var myx=peng[0];\n  var myy=peng[1];\n  var x=dest[0];\n  var y=dest[1];\n  var t=0;\n  while ((Math.abs(x-myx)+Math.abs(y-myy))!=t) {\n    var b=board[x][y];\n    x+=DX[b];\n    y+=DY[b];\n    t+=1;\n  }\n  return t;\n}\n\n// Try to compute the shortest path to reach each fish and a certain subset of the others\n// key is current fish followed by N bits of bitmask\n// value is shortest time\nfunction computeTarget(start_x,start_y) {\n  cache={};\n  // Compute the shortest steps to have visited all fish in bitmask\n  // and with the last visit being to the fish with index equal to last\n  function go(bitmask,last) {\n    var i;\n    var best=100000000;\n    var key=(last&lt;&lt;num_fish)+bitmask;\n    if (key in cache) {\n      return cache[key];\n    }\n    // Consider all previous positions\n    bitmask -= 1&lt;&lt;last;\n    if (bitmask==0) {\n      best = fastest_route([start_x,start_y],pts[last]);\n    } else {\n      for(i=0;i&lt;pts.length;i++) {\n        var bit = 1&lt;&lt;i;\n        if (bitmask&amp;bit) {\n          var s = go(bitmask,i);   // least cost if our previous fish was i\n          s+=fastest_route(getPt(i,s),getPt(last,s));\n          if (s&lt;best) best=s;\n        }\n      }\n    }\n    cache[key]=best;\n    return best;\n  }\n  var t = 100000000;\n  for(var i=0;i&lt;pts.length;i++) {\n    t = Math.min(t,go((1&lt;&lt;pts.length)-1,i));\n  }\n  return t;\n}\n</code></pre>\n\n<h2>What I've considered</h2>\n\n<p>Some options that I've wondered about are:</p>\n\n<ol>\n<li><p>Caching of intermediate results.  The distance calculation repeats a lot of simulation and intermediate results could be cached.<br>\nHowever, I don't think this would stop it having exponential complexity.</p></li>\n<li><p>An A* search algorithm although it is not clear to me what an appropriate admissible heuristic would be and how effective this would be in practice.</p></li>\n<li><p>Investigating good algorithms for the travelling salesman problem and see if they apply to this problem.</p></li>\n<li><p>Trying to prove that the problem is NP-hard and hence unreasonable to be seeking an optimal answer for it.</p></li>\n</ol>\n    ","a":"\n<p>Have you searched the literature? I found these papers which seems to analyse your problem:</p>\n\n<ul>\n<li><p>\"Tracking moving targets and the non- stationary traveling salesman\nproblem\": <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.9940\">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.9940</a></p></li>\n<li><p>\"The moving-target traveling salesman problem\": <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.6403\">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.6403</a></p></li>\n</ul>\n\n<p><strong>UPDATE 1:</strong></p>\n\n<p>The above two papers seems to concentrate on linear movement for the euclidian metric.</p>\n    "},{"t":"Most efficient way to store thousand telephone numbers","l":"http://stackoverflow.com/questions/7685649/most-efficient-way-to-store-thousand-telephone-numbers","q":"\n\n<p>This is a google interview question:</p>\n\n<p>There are around thousand phone numbers to be stored each having 10 digits. You can assume first 5 digits of each to be same across thousand numbers. You have to perform the following operations:\na. Search if a given number exists.\nb. Print all the number</p>\n\n<p>What is the most efficient space saving way to do this ?</p>\n\n<p>I answered hash table and later huffman coding but my interviewer said I was not going in right direction. Please help me here.</p>\n\n<p>Could using a suffix trie help?</p>\n\n<p>Ideally 1000 numbers storing takes 4 bytes per number so in all it would take 4000 bytes to store 1000 number. Quantitatively, I wish to reduce the storage to &lt; 4000 bytes, this is what my interviewer explained to me.</p>\n    ","a":"\n<p>Here's an improvement to <a href=\"http://stackoverflow.com/questions/7685649/most-efficient-way-to-store-thousand-telephone-numbers/7685746#7685746\">aix's answer</a>. Consider using three \"layers\" for the data structure: the first is a constant for the first five digits (17 bits); so from here on, each phone number has only the remaining five digits left. We view these remaining five digits as 17-bit binary integers and store <em>k</em> of those bits using one method and 17 - <em>k</em> = <em>m</em> with a different method, determining <em>k</em> at the end to minimize the required space.</p>\n\n<p>We first sort the phone numbers (all reduced to 5 decimal digits). Then we count how many phone numbers there are for which the binary number consisting of the first <em>m</em> bits is all 0, for how many phone numbers the first <em>m</em> bits are at most 0...01, for how many phone numbers the first <em>m</em> bits are at most 0...10, etcetera, up to the count of phone numbers for which the first <em>m</em> bits are 1...11 - this last count is 1000(decimal). There are 2^<em>m</em> such counts and each count is at most 1000. If we omit the last one (because we know it is 1000 anyway), we can store all of these numbers in a contiguous block of (2^<em>m</em> - 1) * 10 bits. (10 bits is enough for storing a number less than 1024.)</p>\n\n<p>The last <em>k</em> bits of all (reduced) phone numbers are stored contiguously in memory; so if <em>k</em> is, say, 7, then the first 7 bits of this block of memory (bits 0 thru 6) correspond to the last 7 bits of the first (reduced) phone number, bits 7 thru 13 correspond to the last 7 bits of the second (reduced) phone number, etcetera. This requires 1000 * <em>k</em> bits for a total of 17 + (2^(17 - <em>k</em>) - 1) * 10 + 1000 * <em>k</em>, which attains its minimum 11287 for <em>k</em> = 10. So we can store all phone numbers in ceil(11287/8)=1411 bytes.</p>\n\n<p>Additional space can be saved by observing that none of our numbers can start with e.g. 1111111(binary), because the lowest number that starts with that is 130048 and we have only five decimal digits. This allows us to shave a few entries off the first block of memory: instead of 2^<em>m</em> - 1 counts, we need only ceil(99999/2^<em>k</em>). That means the formula becomes</p>\n\n<p>17 + ceil(99999/2^<em>k</em>) * 10 + 1000 * <em>k</em></p>\n\n<p>which amazingly enough attains its minimum 10997 for both <em>k</em> = 9 and <em>k</em> = 10, or ceil(10997/8) = 1375 bytes.</p>\n\n<p>If we want to know whether a certain phone number is in our set, we first check if the first five binary digits match the five digits we have stored. Then we split the remaining five digits into its top <em>m</em>=7 bits (which is, say, the <em>m</em>-bit number <em>M</em>) and its lower <em>k</em>=10 bits (the number <em>K</em>). We now find the number <em>a</em>[M-1] of reduced phone numbers for which the first <em>m</em> digits are at most <em>M</em> - 1, and the number <em>a</em>[M] of reduced phone numbers for which the first <em>m</em> digits are at most <em>M</em>, both from the first block of bits. We now check between the <em>a</em>[M-1]th and <em>a</em>[M]th sequence of <em>k</em> bits in the second block of memory to see if we find <em>K</em>; in the worst case there are 1000 such sequences, so if we use binary search we can finish in O(log 1000) operations.</p>\n\n<p>Pseudocode for printing all 1000 numbers follows, where I access the <em>K</em>'th <em>k</em>-bit entry of the first block of memory as <em>a</em>[K] and the <em>M</em>'th <em>m</em>-bit entry of the second block of memory as <em>b</em>[M] (both of these would require a few bit operations that are tedious to write out). The first five digits are in the number <em>c</em>.</p>\n\n<pre><code>i := 0;\nfor K from 0 to ceil(99999 / 2^k) do\n  while i &lt; a[K] do\n    print(c * 10^5 + K * 2^k + b[i]);\n    i := i + 1;\n  end do;\nend do;\n</code></pre>\n\n<p>Maybe something goes wrong with the boundary case for <em>K</em> = ceil(99999/2^<em>k</em>), but that's easy enough to fix.</p>\n\n<p>Finally, from an entropy point of view, it is not possible to store a subset of 10^3 positive integers all less than 10^5 in fewer than ceil(log[2](binomial(10^5, 10^3))) = 8073. Including the 17 we need for the first 5 digits, there is still a gap of 10997 - 8090 = 2907 bits. It's an interesting challenge to see if there are better solutions where you can still access the numbers relatively efficiently!</p>\n    "},{"t":"How does one make a Zip bomb?","l":"http://stackoverflow.com/questions/1459673/how-does-one-make-a-zip-bomb","q":"\n\n<p><a href=\"http://stackoverflow.com/questions/1459080/how-can-i-protect-myself-from-a-zip-bomb\">This question</a> about zip bombs naturally led me to the <a href=\"http://en.wikipedia.org/wiki/Zip%5Fbomb\">Wikipedia page</a> on the topic. The article mentions an example of a 45.1 kb zip file that decompresses to 1.3 exabytes.</p>\n\n<p>What are the principles/techniques that would be used to create such a file in the first place? I don't want to actually do this, more interested in a simplified \"how-stuff-works\" explanation of the concepts involved.</p>\n\n<p>p.s.</p>\n\n<p>The article mentions 9 layers of zip files, so it's not a simple case of zipping a bunch of zeros. Why 9, why 10 files in each?</p>\n    ","a":"\n<p>Citing from the Wikipedia page:</p>\n\n<blockquote>\n  <p>One example of a Zip bomb is the file\n  45.1.zip which was 45.1 kilobytes of compressed data, <strong>containing nine\n  layers of nested zip files in sets of\n  10, each bottom layer archive\n  containing a 1.30 gigabyte file</strong> for a\n  total of 1.30 exabytes of uncompressed\n  data.</p>\n</blockquote>\n\n<p>So all you need is one single 1.3GB file full of zeroes, compress that into a ZIP file, make 10 copies, pack those into a ZIP file, and repeat this process 9 times. </p>\n\n<p>This way, you get a file which, when uncompressed completely, produces an absurd amount of data without requiring you to start out with that amount. </p>\n\n<p>Additionally, the nested archives make it much harder for programs like virus scanners (the main target of these \"bombs\") to be smart and refuse to unpack archives that are \"too large\", because until the last level the total amount of data is not that much, you don't \"see\" how large the files at the lowest level are until you have reached that level, and each individual file is not \"too large\" - only the huge number is problematic.</p>\n    "},{"t":"Fast ceiling of an integer division in C / C++","l":"http://stackoverflow.com/questions/2745074/fast-ceiling-of-an-integer-division-in-c-c","q":"\n\n<p>Given integer values <code>x</code> and <code>y</code>, C and C++ both return as the quotient <code>q = x/y</code> the floor of the floating point equivalent.  I'm interested in a method of returning the ceiling instead.  For example, <code>ceil(10/5)=2</code> and <code>ceil(11/5)=3</code>.</p>\n\n<p>The obvious approach involves something like:</p>\n\n<pre><code>q = x / y;\nif (q * y &lt; x) ++q;\n</code></pre>\n\n<p>This requires an extra comparison and multiplication; and other methods I've seen (used in fact) involve casting as a <code>float</code> or <code>double</code>.  Is there a more direct method that avoids the additional multiplication (or a second division) and branch, and that also avoids casting as a floating point number?</p>\n    ","a":"\n<p>To round up ...</p>\n\n<pre><code>q = (x + y - 1) / y;\n</code></pre>\n\n<p>or (avoiding overflow in x+y)</p>\n\n<pre><code>q = 1 + ((x - 1) / y); // if x != 0\n</code></pre>\n    "},{"t":"How does the Amazon Recommendation feature work?","l":"http://stackoverflow.com/questions/2323768/how-does-the-amazon-recommendation-feature-work","q":"\n\n<p>What technology goes in behind the screens of Amazon recommendation technology? I believe that Amazon recommendation is currently the best in the market, but how do they provide us with such relevant recommendations?</p>\n\n<p>Recently, we have been involved with similar recommendation kind of project, but would surely like to know about the in and outs of the Amazon recommendation technology from a technical standpoint. </p>\n\n<p>Any inputs would be highly appreciated. </p>\n\n<p><strong>Update:</strong></p>\n\n<p>This <a href=\"http://www.google.com/patents?id=dtp6AAAAEBAJ&amp;printsec=abstract&amp;zoom=4&amp;source=gbs_overview_r&amp;cad=0#v=onepage&amp;q=&amp;f=false\">patent</a> explains how personalized recommendations are done but it is not very technical, and so it would be really nice if some insights could be provided. </p>\n\n<p>From the comments of Dave, <a href=\"http://en.wikipedia.org/wiki/Affinity_analysis\">Affinity Analysis</a> forms the basis for such kind of Recommendation Engines. Also here are some good reads on the Topic</p>\n\n<ol>\n<li><a href=\"http://www.information-management.com/specialreports/20061031/1067598-1.html\">Demystifying Market Basket Analysis</a></li>\n<li><a href=\"http://people.revoledu.com/kardi/tutorial/MarketBasket/index.htm\">Market Basket Analysis</a></li>\n<li><a href=\"http://it.toolbox.com/blogs/enterprise-solutions/affinity-analysis-14726\">Affinity Analysis</a></li>\n</ol>\n\n<p><strong>Suggested Reading:</strong></p>\n\n<ol>\n<li><a href=\"http://rads.stackoverflow.com/amzn/click/1558604898\">Data Mining: Concepts and Technique</a></li>\n</ol>\n    ","a":"\n<p>It is both an art and a science.  Typical fields of study revolve around market basket analysis (also called affinity analysis) which is a subset of the field of data mining.  Typical components in such a system include identification of primary driver items and the identification of affinity items (accessory upsell, cross sell).    </p>\n\n<p>Keep in mind the data sources they have to mine...  </p>\n\n<ol>\n<li>Purchased shopping carts = real money from real people spent on real items = powerful data and a lot of it.  </li>\n<li>Items added to carts but abandoned.  </li>\n<li>Pricing experiments online (A/B testing, etc.) where they offer the same products at different prices and see the results  </li>\n<li>Packaging experiments (A/B testing, etc.) where they offer different products in different \"bundles\" or discount various pairings of items  </li>\n<li>Wishlists - what's on them specifically for you - and in aggregate it can be treated similarly to another stream of basket analysis data  </li>\n<li>Referral sites (identification of where you came in from can hint other items of interest)  </li>\n<li>Dwell times (how long before you click back and pick a different item)  </li>\n<li>Ratings by you or those in your social network/buying circles - if you rate things you like you get more of what you like and if you confirm with the \"i already own it\" button they create a very complete profile of you  </li>\n<li>Demographic information (your shipping address, etc.) - they know what is popular in your general area for your kids, yourself, your spouse, etc.  </li>\n<li>user segmentation = did you buy 3 books in separate months for a toddler?  likely have a kid or more.. etc.  </li>\n<li>Direct marketing click through data - did you get an email from them and click through?  They know which email it was and what you clicked through on and whether you bought it as a result.  </li>\n<li>Click paths in session - what did you view regardless of whether it went in your cart</li>\n<li>Number of times viewed an item before final purchase  </li>\n<li>If you're dealing with a brick and mortar store they might have your physical purchase history to go off of as well (i.e. toys r us or something that is online and also a physical store)</li>\n<li>etc. etc. etc.  </li>\n</ol>\n\n<p>Luckily people behave similarly in aggregate so the more they know about the buying population at large the better they know what will and won't sell and with every transaction and every rating/wishlist add/browse they know how to more personally tailor recommendations.  Keep in mind this is likely only a small sample of the full set of influences of what ends up in recommendations, etc.  </p>\n\n<p>Now I have no inside knowledge of how Amazon does business (never worked there) and all I'm doing is talking about classical approaches to the problem of online commerce - I used to be the PM who worked on data mining and analytics for the Microsoft product called Commerce Server.  We shipped in Commerce Server the tools that allowed people to build sites with similar capabilities.... but the bigger the sales volume the better the data the better the model - and Amazon is BIG.  I can only imagine how fun it is to play with models with that much data in a commerce driven site.  Now many of those algorithms (like the predictor that started out in commerce server) have moved on to live directly within <a href=\"http://msdn.microsoft.com/en-us/library/ms175595.aspx\">Microsoft SQL</a>.</p>\n\n<p>The four big take-a-ways you should have are:  </p>\n\n<ol>\n<li>Amazon (or any retailer) is looking at aggregate data for tons of transactions and tons of people... this allows them to even recommend pretty well for anonymous users on their site.  </li>\n<li>Amazon (or any sophisticated retailer) is keeping track of behavior and purchases of anyone that is logged in and using that to further refine on top of the mass aggregate data.  </li>\n<li>Often there is a means of over riding the accumulated data and taking \"editorial\" control of suggestions for product managers of specific lines (like some person who owns the 'digital cameras' vertical or the 'romance novels' vertical or similar) where they truly are experts  </li>\n<li>There are often promotional deals (i.e. sony or panasonic or nikon or canon or sprint  or verizon pays additional money to the retailer, or gives a better discount at larger quantities or other things in those lines) that will cause certain \"suggestions\" to rise to the top more often than others - there is always some reasonable business logic and business reason behind this targeted at making more on each transaction or reducing wholesale costs, etc.  </li>\n</ol>\n\n<p>In terms of actual implementation?  Just about all large online systems boil down to some set of pipelines (or a filter pattern implementation or a workflow, etc.  you call it what you will) that allow for a context to be evaluated by a series of modules that apply some form of business logic.  </p>\n\n<p>Typically a different pipeline would be associated with each separate task on the page - you might have one that does recommended \"packages/upsells\" (i.e. buy this with the item you're looking at) and one that does \"alternatives\" (i.e. buy this instead of the thing you're looking at) and another that pulls items most closely related from your wish list (by product category or similar).  </p>\n\n<p>The results of these pipelines are able to be placed on various parts of the page (above the scroll bar, below the scroll, on the left, on the right, different fonts, different size images, etc.) and tested to see which perform best.  Since you're using nice easy to plug and play modules that define the business logic for these pipelines you end up with the moral equivalent of lego blocks that make it easy to pick and choose from the business logic you want applied when you build another pipeline which allows faster innovation, more experimentation, and in the end higher profits.  </p>\n\n<p>Did that help at all?  Hope that give you a little bit of insight how this works in general for just about any ecommerce site - not just Amazon.  Amazon (from talking to friends that have worked there) is very data driven and continually measures the effectiveness of it's user experience and the pricing, promotion, packaging, etc. - they are a very sophisticated retailer online and are likely at the leading edge of a lot of the algorithms they use to optimize profit - and those are likely proprietary secrets (you know like the formula to KFC's secret spices) and guaarded as such.</p>\n    "},{"t":"What is dynamic programming?","l":"http://stackoverflow.com/questions/1065433/what-is-dynamic-programming","q":"\n\n<p>What is <strong>dynamic programming</strong>? </p>\n\n<p>How's it different from recursion, memoization, etc? </p>\n\n<p>I've read the <a href=\"http://en.wikipedia.org/wiki/Dynamic_programming\">wikipedia article</a> on it, but I still don't really understand it.</p>\n    ","a":"\n<p>Dynamic programming is when you use past knowledge to make solving a future problem easier.</p>\n\n<p>A good example is solving the fibonacci sequence for n=1,000,002.</p>\n\n<p>This will be a very long process, but what if I give you the results for n=1,000,000 and n=1,000,0001? Suddenly the problem just became more manageable.</p>\n\n<p>Dynamic programming is used a lot in string problems, such as the string edit problem. You solve a subset(s) of the problem and then use that information to solve the more difficult original problem.</p>\n\n<p>With dynamic programming, you store your results in some sort of table generally. When you need the answer to a problem, you reference the table and see if you already know what it is. If not, you use the data in your table to give yourself a stepping stone towards the answer.</p>\n\n<p>The Cormen Algorithms book has a great chapter about dynamic programming. AND it's free on Google Books! Check it out <a href=\"http://books.google.com/books?id=NLngYyWFl_YC&amp;dq=cormen+algorithms&amp;printsec=frontcover&amp;source=bn&amp;hl=en&amp;ei=K2xKSrCbGIbyMYqO_LMC&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=4\">here.</a></p>\n    "},{"t":"Select a random N elements from List<T> in C#","l":"http://stackoverflow.com/questions/48087/select-a-random-n-elements-from-listt-in-c-sharp","q":"\n\n<p>I need a quick algorithm to select a random 5 elements from a generic list.  For example, I'd like to get a random 5 elements from a List.</p>\n    ","a":"\n<p>Iterate through and for each element make the probability of selection = (number needed)/(number left)</p>\n\n<p>So if you had 40 items, the first would have a 5/40 chance of being selected. If it is, the next has a 4/39 chance, otherwise it has a 5/39 chance. By the time you get to the end you will have your 5 items, and often you'll have all of them before that.</p>\n    "},{"t":"is it possible to get all arguments of a function as single object inside that function?","l":"http://stackoverflow.com/questions/4633125/is-it-possible-to-get-all-arguments-of-a-function-as-single-object-inside-that-f","q":"\n\n<p>Like in PHP there are <code>func_num_args</code> and <code>func_get_args</code> ?</p>\n    ","a":"\n<p>Use <code>arguments</code>. You can access it like an array. Use <code>arguments.length</code> for the number of arguments.</p>\n    "},{"t":"Determine font color based on background color","l":"http://stackoverflow.com/questions/1855884/determine-font-color-based-on-background-color","q":"\n\n<p>Given a system (a website for instance) that lets a user customize the background color for some section but not the font color (to keep number of options to a minimum), is there a way to programmatically determine if a \"light\" or \"dark\" font color is necessary?</p>\n\n<p>I'm sure there is some algorithm, but I don't know enough about colors, luminosity, etc to figure it out on my own.</p>\n    ","a":"\n<p>I encountered similar problem. I had to find a good method of selecting contrastive font color to display text labels on colorscales/heatmaps. It had to be universal method and generated color had to be \"good looking\", which means that simple generating complementary color was not good solution - sometimes it generated strange, very intensive colors that were hard to watch and read.</p>\n\n<p>After long hours of testing and trying to solve this problem, I found out that the best solution is to select white font for \"dark\" colors, and black font for \"bright\" colors. </p>\n\n<p>Here's an example of function I am using in C#:</p>\n\n<pre><code>Color ContrastColor(Color color)\n{\n    int d = 0;\n\n    // Counting the perceptive luminance - human eye favors green color... \n    double a = 1 - ( 0.299 * color.R + 0.587 * color.G + 0.114 * color.B)/255;\n\n    if (a &lt; 0.5)\n       d = 0; // bright colors - black font\n    else\n       d = 255; // dark colors - white font\n\n    return  Color.FromArgb(d, d, d);\n}\n</code></pre>\n\n<p>This was tested for many various colorscales (rainbow, grayscale, heat, ice, and many others) and is the only \"universal\" method I found out. </p>\n\n<p><strong>Edit</strong><br>\nChanged the formula of counting <code>a</code> to \"perceptive luminance\" - it really looks better! Already implemented it in my software, looks great.</p>\n\n<p><strong>Edit 2</strong>\n@WebSeed provided a great working example of this algorithm: <a href=\"http://codepen.io/WebSeed/full/pvgqEq/\">http://codepen.io/WebSeed/full/pvgqEq/</a></p>\n    "},{"t":"What algorithm gives suggestions in a spell checker?","l":"http://stackoverflow.com/questions/2294915/what-algorithm-gives-suggestions-in-a-spell-checker","q":"\n\n<p>What algorithm is typically used when implementing a spell checker that is accompanied with word suggestions?</p>\n\n<p>At first I thought it might make sense to check each new word typed (if not found in the dictionary) against it's <a href=\"http://en.wikipedia.org/wiki/Levenshtein_distance\">Levenshtein distance</a> from every other word in the dictionary and returning the top results.  However, this seems like it would be highly inefficient, having to evaluate the entire dictionary repeatedly.</p>\n\n<p>How is this typically done?</p>\n    ","a":"\n<p>There is <a href=\"http://norvig.com/spell-correct.html\">good essay by Peter Norvig</a> how to implement a spelling corrector. It's basicly a brute force approach trying candidate strings with a given edit distance. (<a href=\"http://theyougen.blogspot.com/2010/02/faster-spelling-corrector.html\">Here</a> are some tips how you can improve the spelling corrector performance using a <a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">Bloom Filter</a> and <a href=\"http://theyougen.blogspot.com/2010/02/producing-candidates-for-spelling.html\">faster candidate hashing</a>.)</p>\n\n<p>The requirements for a spell checker are weaker. You have only to find out that a word is not in the dictionary. You can use a <a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">Bloom Filter</a> to build a spell checker which consumes less memory. An ancient versions is decribed in <a href=\"http://www.cs.bell-labs.com/cm/cs/pearls/\">Programming Pearls</a> by Jon Bentley using 64kb for an english dictionary.</p>\n\n<p>A <a href=\"http://en.wikipedia.org/wiki/BK-tree\">BK-Tree</a> is an alternative approach. A nice article is <a href=\"http://blog.notdot.net/2007/4/Damn-Cool-Algorithms-Part-1-BK-Trees\">here</a>.</p>\n\n<p>Levenshstein distance is not exactly the right edit distance for a spell checker. It knows only insertion, deletion and substitution. Transposition is missing and produces 2 for a transposition of 1 character (it's 1 delete and 1 insertion). <a href=\"http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\">Damerau–Levenshtein distance</a> is the right edit distance.</p>\n    "},{"t":"Rolling median algorithm in C","l":"http://stackoverflow.com/questions/1309263/rolling-median-algorithm-in-c","q":"\n\n<p>I am currently working on an algorithm to implement a rolling median filter (analogous to a rolling mean filter) in C. From my search of the literature, there appear to be two reasonably efficient ways to do it. The first is to sort the initial window of values, then perform a binary search to insert the new value and remove the exiting one at each iteration.</p>\n\n<p>The second (from Hardle and Steiger, 1995, JRSS-C, Algorithm 296) builds a double-ended heap structure, with a maxheap on one end, a minheap on the other, and the median in the middle. This yields a linear-time algorithm instead of one that is O(n log n).</p>\n\n<p>Here is my problem: implementing the former is doable, but I need to run this on millions of time series, so efficiency matters a lot. The latter is proving very difficult to implement. I found code in the Trunmed.c file of the code for the stats package of R, but it is rather indecipherable.</p>\n\n<p>Does anyone know of a well-written C implementation for the linear time rolling median algorithm?</p>\n\n<p>Edit: Link to Trunmed.c code <a href=\"http://google.com/codesearch/p?hl=en&amp;sa=N&amp;cd=1&amp;ct=rc#mYw3h%5FLb%5Fe0/R-2.2.0/src/library/stats/src/Trunmed.c\">http://google.com/codesearch/p?hl=en&amp;sa=N&amp;cd=1&amp;ct=rc#mYw3h%5FLb%5Fe0/R-2.2.0/src/library/stats/src/Trunmed.c</a></p>\n    ","a":"\n<p>I have looked at R's <code>src/library/stats/src/Trunmed.c</code> a few times as I wanted something similar too in a standalone C++ class / C subroutine. Note that this are actually two implementations in one, see <code>src/library/stats/man/runmed.Rd</code> (the source of the help file) which says</p>\n\n<pre><code>\\details{\n  Apart from the end values, the result \\code{y = runmed(x, k)} simply has\n  \\code{y[j] = median(x[(j-k2):(j+k2)])} (k = 2*k2+1), computed very\n  efficiently.\n\n  The two algorithms are internally entirely different:\n  \\describe{\n    \\item{\"Turlach\"}{is the Härdle-Steiger\n      algorithm (see Ref.) as implemented by Berwin Turlach.\n      A tree algorithm is used, ensuring performance \\eqn{O(n \\log\n        k)}{O(n * log(k))} where \\code{n &lt;- length(x)} which is\n      asymptotically optimal.}\n    \\item{\"Stuetzle\"}{is the (older) Stuetzle-Friedman implementation\n      which makes use of median \\emph{updating} when one observation\n      enters and one leaves the smoothing window.  While this performs as\n      \\eqn{O(n \\times k)}{O(n * k)} which is slower asymptotically, it is\n      considerably faster for small \\eqn{k} or \\eqn{n}.}\n  }\n}\n</code></pre>\n\n<p>It would be nice to see this re-used in a more standalone fashion. Are you volunteering?  I can help with some of the R bits.</p>\n\n<p><em>Edit 1</em>: Besides the link to the older version of Trunmed.c above, here are current SVN copies of</p>\n\n<ul>\n<li><a href=\"http://svn.r-project.org/R/trunk/src/library/stats/src/Srunmed.c\" rel=\"nofollow\"><code>Srunmed.c</code></a> (for the Stuetzle version)</li>\n<li><a href=\"http://svn.r-project.org/R/trunk/src/library/stats/src/Trunmed.c\" rel=\"nofollow\"><code>Trunmed.c</code></a> (for the Turlach version)</li>\n<li><a href=\"http://svn.r-project.org/R/trunk/src/library/stats/R/runmed.R\" rel=\"nofollow\"><code>runmed.R</code></a> for the R function calling these</li>\n</ul>\n\n<p><em>Edit 2</em>: Ryan Tibshirani has some C and Fortran code on <a href=\"http://www.stat.cmu.edu/~ryantibs/median/\" rel=\"nofollow\">fast median binning</a> which may be a suitable starting point for a windowed approach.</p>\n    "},{"t":"Generate all permutations of a list without adjacent equal elements","l":"http://stackoverflow.com/questions/25285792/generate-all-permutations-of-a-list-without-adjacent-equal-elements","q":"\n\n<p>When we sort a list, like</p>\n\n<pre><code>a = [1,2,3,3,2,2,1]\nsorted(a) =&gt; [1, 1, 2, 2, 2, 3, 3]\n</code></pre>\n\n<p>equal elements are always adjacent in the resulting list. </p>\n\n<p>How can I achieve the opposite task - shuffle the list so that equal elements are never (or as seldom as possible) adjacent?</p>\n\n<p>For example, for the above list one of the possible solutions is</p>\n\n<pre><code>p = [1,3,2,3,2,1,2]\n</code></pre>\n\n<p>More formally, given a list <code>a</code>, generate a permutation <code>p</code> of it that minimizes the number of pairs <code>p[i]==p[i+1]</code>. </p>\n\n<p>Since the lists are large, generating and filtering all permutations is not an option.</p>\n\n<p>Bonus question: how to generate all such permutations efficiently?</p>\n\n<p>This is the code I'm using to test the solutions: <a href=\"https://gist.github.com/gebrkn/9f550094b3d24a35aebd\">https://gist.github.com/gebrkn/9f550094b3d24a35aebd</a></p>\n\n<p>UPD: Choosing a winner here was a tough choice, because many people posted excellent answers. <a href=\"http://stackoverflow.com/a/25285879/989121\">@Heuster</a>, <a href=\"http://stackoverflow.com/a/25290780/989121\">@David Eisenstat</a>, <a href=\"http://stackoverflow.com/a/25291640/989121\">@Coady</a>, <a href=\"http://stackoverflow.com/a/25286137/989121\">@enrico.bacis</a> and <a href=\"http://stackoverflow.com/a/25401193/989121\">@srgerg</a> provided functions that generate the best possible permutation flawlessly. <a href=\"http://stackoverflow.com/a/25286251/989121\">@tobias_k</a> and David also answered the bonus question (generate all permutations). Additional points to David for the correctness proof.</p>\n\n<p>The code from @Heuster appears to be the fastest.</p>\n    ","a":"\n<p>This is along the lines of Thijser's currently incomplete pseudocode. The idea is to take the most frequent of the remaining item types unless it was just taken. (See also <a href=\"http://stackoverflow.com/a/25291640/2144669\">Coady's implementation</a> of this algorithm.)</p>\n\n<pre><code>import collections\nimport heapq\n\n\nclass Sentinel:\n    pass\n\n\ndef david_eisenstat(lst):\n    counts = collections.Counter(lst)\n    heap = [(-count, key) for key, count in counts.items()]\n    heapq.heapify(heap)\n    output = []\n    last = Sentinel()\n    while heap:\n        minuscount1, key1 = heapq.heappop(heap)\n        if key1 != last or not heap:\n            last = key1\n            minuscount1 += 1\n        else:\n            minuscount2, key2 = heapq.heappop(heap)\n            last = key2\n            minuscount2 += 1\n            if minuscount2 != 0:\n                heapq.heappush(heap, (minuscount2, key2))\n        output.append(last)\n        if minuscount1 != 0:\n            heapq.heappush(heap, (minuscount1, key1))\n    return output\n</code></pre>\n\n<h2>Proof of correctness</h2>\n\n<p>For two item types, with counts k1 and k2, the optimal solution has k2 - k1 - 1 defects if k1 &lt; k2, 0 defects if k1 = k2, and k1 - k2 - 1 defects if k1 &gt; k2. The = case is obvious. The others are symmetric; each instance of the minority element prevents at most two defects out of a total of k1 + k2 - 1 possible.</p>\n\n<p>This greedy algorithm returns optimal solutions, by the following logic. We call a prefix (partial solution) <em>safe</em> if it extends to an optimal solution. Clearly the empty prefix is safe, and if a safe prefix is a whole solution then that solution is optimal. It suffices to show inductively that each greedy step maintains safety.</p>\n\n<p>The only way that a greedy step introduces a defect is if only one item type remains, in which case there is only one way to continue, and that way is safe. Otherwise, let P be the (safe) prefix just before the step under consideration, let P' be the prefix just after, and let S be an optimal solution extending P. If S extends P' also, then we're done. Otherwise, let P' = Px and S = PQ and Q = yQ', where x and y are items and Q and Q' are sequences.</p>\n\n<p>Suppose first that P does not end with y. By the algorithm's choice, x is at least as frequent in Q as y. Consider the maximal substrings of Q containing only x and y. If the first substring has at least as many x's as y's, then it can be rewritten without introducing additional defects to begin with x. If the first substring has more y's than x's, then some other substring has more x's than y's, and we can rewrite these substrings without additional defects so that x goes first. In both cases, we find an optimal solution T that extends P', as needed.</p>\n\n<p>Suppose now that P does end with y. Modify Q by moving the first occurrence of x to the front. In doing so, we introduce at most one defect (where x used to be) and eliminate one defect (the yy).</p>\n\n<h2>Generating all solutions</h2>\n\n<p>This is <a href=\"http://stackoverflow.com/a/25286251/2144669\">tobias_k's answer</a> plus efficient tests to detect when the choice currently under consideration is globally constrained in some way. The asymptotic running time is optimal, since the overhead of generation is on the order of the length of the output. The worst-case delay unfortunately is quadratic; it could be reduced to linear (optimal) with better data structures.</p>\n\n<pre><code>from collections import Counter\nfrom itertools import permutations\nfrom operator import itemgetter\nfrom random import randrange\n\n\ndef get_mode(count):\n    return max(count.items(), key=itemgetter(1))[0]\n\n\ndef enum2(prefix, x, count, total, mode):\n    prefix.append(x)\n    count_x = count[x]\n    if count_x == 1:\n        del count[x]\n    else:\n        count[x] = count_x - 1\n    yield from enum1(prefix, count, total - 1, mode)\n    count[x] = count_x\n    del prefix[-1]\n\n\ndef enum1(prefix, count, total, mode):\n    if total == 0:\n        yield tuple(prefix)\n        return\n    if count[mode] * 2 - 1 &gt;= total and [mode] != prefix[-1:]:\n        yield from enum2(prefix, mode, count, total, mode)\n    else:\n        defect_okay = not prefix or count[prefix[-1]] * 2 &gt; total\n        mode = get_mode(count)\n        for x in list(count.keys()):\n            if defect_okay or [x] != prefix[-1:]:\n                yield from enum2(prefix, x, count, total, mode)\n\n\ndef enum(seq):\n    count = Counter(seq)\n    if count:\n        yield from enum1([], count, sum(count.values()), get_mode(count))\n    else:\n        yield ()\n\n\ndef defects(lst):\n    return sum(lst[i - 1] == lst[i] for i in range(1, len(lst)))\n\n\ndef test(lst):\n    perms = set(permutations(lst))\n    opt = min(map(defects, perms))\n    slow = {perm for perm in perms if defects(perm) == opt}\n    fast = set(enum(lst))\n    print(lst, fast, slow)\n    assert slow == fast\n\n\nfor r in range(10000):\n    test([randrange(3) for i in range(randrange(6))])\n</code></pre>\n    "},{"t":"What is the fastest way to compute sin and cos together?","l":"http://stackoverflow.com/questions/2683588/what-is-the-fastest-way-to-compute-sin-and-cos-together","q":"\n\n<p>I would like to compute both the sine and co-sine of a value together (for example to create a rotation matrix). Of course I could compute them separately one after another like <code>a = cos(x); b = sin(x);</code>, but I wonder if there is a faster way when needing both values.</p>\n\n<p><strong>Edit:</strong>\nTo summarize the answers so far:</p>\n\n<ul>\n<li><p><a href=\"http://stackoverflow.com/a/2683691\"><em>Vlad</em></a> said, that there is the asm command <code>FSINCOS</code> computing both of them (in almost the same time as a call to <code>FSIN</code> alone)</p></li>\n<li><p>Like <a href=\"http://stackoverflow.com/a/2683963\"><em>Chi</em></a> noticed, this optimization is sometimes already done by the compiler (when using optimization flags).</p></li>\n<li><p><a href=\"http://stackoverflow.com/a/2687338\"><em>caf</em></a> pointed out, that functions <code>sincos</code> and <code>sincosf</code> are probably available and can be called directly by just including <code>math.h</code></p></li>\n<li><p><a href=\"http://stackoverflow.com/a/2683613\"><em>tanascius</em></a> approach of using a look-up table is discussed controversial. (However on my computer and in a benchmark scenario it runs 3x faster than <code>sincos</code> with almost the same accuracy for 32-bit floating points.)</p></li>\n<li><p><a href=\"http://stackoverflow.com/a/2683853\"><em>Joel Goodwin</em></a> linked to an interesting approach of an extremly fast approximation technique with quite good accuray (for me, this is even faster then the table look-up)</p></li>\n</ul>\n    ","a":"\n<p>Modern Intel/AMD processors have instruction <code>FSINCOS</code> for calculating sine and cosine functions simultaneously. If you need strong optimization, perhaps you should use it.</p>\n\n<p>Here is a small example: <a href=\"http://home.broadpark.no/~alein/fsincos.html\">http://home.broadpark.no/~alein/fsincos.html</a></p>\n\n<p>Here is another example (for MSVC): <a href=\"http://www.codeguru.com/forum/showthread.php?t=328669\">http://www.codeguru.com/forum/showthread.php?t=328669</a></p>\n\n<p>Here is yet another example (with gcc): <a href=\"http://www.allegro.cc/forums/thread/588470\">http://www.allegro.cc/forums/thread/588470</a></p>\n\n<p>Hope one of them helps.\n(I didn't use this instruction myself, sorry.)</p>\n\n<p>As they are supported on processor level, I expect them to be way much faster than table lookups.</p>\n\n<p>Edit:<br>\n<a href=\"http://en.wikipedia.org/wiki/X86_instruction_listings\">Wikipedia</a> suggests that <code>FSINCOS</code> was added at 387 processors, so you can hardly find a processor which doesn't support it.</p>\n\n<p>Edit:<br>\n<a href=\"http://developer.intel.com/Assets/PDF/manual/248966.pdf\">Intel's documentation</a> states that <code>FSINCOS</code> is just about 5 times slower than <code>FDIV</code> (i.e., floating point division).</p>\n\n<p>Edit:<br>\nPlease note that not all modern compilers optimize calculation of sine and cosine into a call to <code>FSINCOS</code>. In particular, my VS 2008 didn't do it that way.</p>\n\n<p>Edit:<br>\nThe first example link is dead, but there is <a href=\"http://web.archive.org/web/20100729204636/http://home.broadpark.no/~alein/fsincos.html\">still a version at the Wayback Machine</a>.</p>\n    "},{"t":"Algorithms based on number base systems? [closed]","l":"http://stackoverflow.com/questions/5356502/algorithms-based-on-number-base-systems","q":"\n\n<p>I've noticed recently that there are a great many algorithms out there based in part or in whole on clever uses of numbers in creative bases.  For example:</p>\n\n<ul>\n<li>Binomial heaps are based on binary numbers, and the more complex skew binomial heaps are based on skew binary numbers.</li>\n<li>Some algorithms for generating lexicographically ordered permutations are based on the factoradic number system.</li>\n<li>Tries can be thought of as trees that look at one digit of the string at a time, for an appropriate base.</li>\n<li>Huffman encoding trees are designed to have each edge in the tree encode a zero or one in some binary representation.</li>\n<li>Fibonacci coding is used in Fibonacci search and to invert certain types of logarithms.</li>\n</ul>\n\n<p>My question is: <strong>what other algorithms are out there that use a clever number system as a key step of their intuition or proof?</strong>. I'm thinking about putting together a talk on the subject, so the more examples I have to draw from, the better.</p>\n\n<p>Thanks so much!</p>\n    ","a":"\n<p>Chris Okasaki has a very good chapter in his book <em>Purely Functional Data Structures</em> that discusses \"Numerical Representations\": essentially, take some representation of a number and convert it into a data structure. To give a flavor, here are the sections of that chapter:</p>\n\n<ol>\n<li>Positional Number Systems</li>\n<li>Binary Numbers (Binary Random-Access Lists, Zeroless Representations, Lazy Representations, Segmented Representations)</li>\n<li>Skew Binary Numbers (Skew Binary Random Access Lists, Skew Binomial Heaps)</li>\n<li>Trinary and Quaternary Numbers</li>\n</ol>\n\n<p>Some of the best tricks, distilled:</p>\n\n<ul>\n<li>Distinguish between <em>dense</em> and <em>sparse</em> representations of numbers (usually you see this in matrices or graphs, but it's applicable to numbers too!)</li>\n<li>Redundant number systems (systems that have more than one representation of a number) are useful.</li>\n<li>If you arrange the first digit to be non-zero or use a zeroless representation, retrieving the head of the data structure can be efficient.</li>\n<li>Avoid cascading borrows (from taking the tail of the list) and carries (from consing onto the list) by <em>segmenting</em> the data structure</li>\n</ul>\n\n<p>Here is also the reference list for that chapter:</p>\n\n<ul>\n<li>Guibas, McCreight, Plass and Roberts: A new representation for linear lists.</li>\n<li>Myers: An applicative random-access stack</li>\n<li>Carlsson, Munro, Poblete: An implicit binomial queue with constant insertion time.</li>\n<li>Kaplan, Tarjan: Purely functional lists with catenation via recursive slow-down.</li>\n</ul>\n    "},{"t":"Graph Algorithm To Find All Connections Between Two Arbitrary Vertices","l":"http://stackoverflow.com/questions/58306/graph-algorithm-to-find-all-connections-between-two-arbitrary-vertices","q":"\n\n<p>I am trying to determine the best time efficient algorithm to accomplish the task described below.</p>\n\n<p>I have a set of records. For this set of records I have connection data which indicates how pairs of records from this set connect to one another. This basically represents an undirected graph, with the records being the vertices and the connection data the edges.</p>\n\n<p>All of the records in the set have connection information (i.e. no orphan records are present; each record in the set connects to one or more other records in the set).</p>\n\n<p>I want to choose any two records from the set and be able to show all simple paths between the chosen records. By \"simple paths\" I mean the paths which do not have repeated records in the path (i.e. finite paths only).</p>\n\n<p>Note: The two chosen records will always be different (i.e. start and end vertex will never be the same; no cycles).</p>\n\n<p>For example:</p>\n\n<pre>    If I have the following records:\n        A, B, C, D, E\n\n    and the following represents the connections: \n        (A,B),(A,C),(B,A),(B,D),(B,E),(B,F),(C,A),(C,E),\n        (C,F),(D,B),(E,C),(E,F),(F,B),(F,C),(F,E)\n\n        [where (A,B) means record A connects to record B]\n</pre>\n\n<p>If I chose B as my starting record and E as my ending record, I would want to find all simple paths through the record connections that would connect record B to record E.</p>\n\n<pre>   All paths connecting B to E:\n      B-&gt;E\n      B-&gt;F-&gt;E\n      B-&gt;F-&gt;C-&gt;E\n      B-&gt;A-&gt;C-&gt;E\n      B-&gt;A-&gt;C-&gt;F-&gt;E\n</pre>\n\n<p>This is an example, in practice I may have sets containing hundreds of thousands of records.</p>\n    ","a":"\n<p>Robert,</p>\n\n<p>It appears that this can be accomplished with a breadth-first search of the graph. <strong>The breadth-first search will find all non-cyclical paths between two nodes.</strong> This algorithm should be very fast and scale to large graphs (The graph data structure is sparse so it only uses as much memory as it needs to).</p>\n\n<p>I noticed that the graph you specified above has only one edge that is directional (B,E). Was this a typo or is it really a directed graph? This solution works regardless. Sorry I was unable to do it in C, I'm a bit weak in that area. I expect that you will be able to translate this Java code without too much trouble though.</p>\n\n<p>Graph.java</p>\n\n\n\n<pre class=\"lang-java prettyprint-override\"><code>import java.util.HashMap;\nimport java.util.LinkedHashSet;\nimport java.util.LinkedList;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Graph {\n    private Map&lt;String, LinkedHashSet&lt;String&gt;&gt; map = new HashMap();\n\n    public void addEdge(String node1, String node2) {\n        LinkedHashSet&lt;String&gt; adjacent = map.get(node1);\n        if(adjacent==null) {\n            adjacent = new LinkedHashSet();\n            map.put(node1, adjacent);\n        }\n        adjacent.add(node2);\n    }\n\n    public void addTwoWayVertex(String node1, String node2) {\n        addEdge(node1, node2);\n        addEdge(node2, node1);\n    }\n\n    public boolean isConnected(String node1, String node2) {\n        Set adjacent = map.get(node1);\n        if(adjacent==null) {\n            return false;\n        }\n        return adjacent.contains(node2);\n    }\n\n    public LinkedList&lt;String&gt; adjacentNodes(String last) {\n        LinkedHashSet&lt;String&gt; adjacent = map.get(last);\n        if(adjacent==null) {\n            return new LinkedList();\n        }\n        return new LinkedList&lt;String&gt;(adjacent);\n    }\n}\n</code></pre>\n\n<p>Search.java</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>import java.util.LinkedList;\n\npublic class Search {\n\n    private static final String START = \"B\";\n    private static final String END = \"E\";\n\n    public static void main(String[] args) {\n        // this graph is directional\n        Graph graph = new Graph();\n        graph.addEdge(\"A\", \"B\");\n        graph.addEdge(\"A\", \"C\");\n        graph.addEdge(\"B\", \"A\");\n        graph.addEdge(\"B\", \"D\");\n        graph.addEdge(\"B\", \"E\"); // this is the only one-way connection\n        graph.addEdge(\"B\", \"F\");\n        graph.addEdge(\"C\", \"A\");\n        graph.addEdge(\"C\", \"E\");\n        graph.addEdge(\"C\", \"F\");\n        graph.addEdge(\"D\", \"B\");\n        graph.addEdge(\"E\", \"C\");\n        graph.addEdge(\"E\", \"F\");\n        graph.addEdge(\"F\", \"B\");\n        graph.addEdge(\"F\", \"C\");\n        graph.addEdge(\"F\", \"E\");\n        LinkedList&lt;String&gt; visited = new LinkedList();\n        visited.add(START);\n        new Search().breadthFirst(graph, visited);\n    }\n\n    private void breadthFirst(Graph graph, LinkedList&lt;String&gt; visited) {\n        LinkedList&lt;String&gt; nodes = graph.adjacentNodes(visited.getLast());\n        // examine adjacent nodes\n        for (String node : nodes) {\n            if (visited.contains(node)) {\n                continue;\n            }\n            if (node.equals(END)) {\n                visited.add(node);\n                printPath(visited);\n                visited.removeLast();\n                break;\n            }\n        }\n        // in breadth-first, recursion needs to come after visiting adjacent nodes\n        for (String node : nodes) {\n            if (visited.contains(node) || node.equals(END)) {\n                continue;\n            }\n            visited.addLast(node);\n            breadthFirst(graph, visited);\n            visited.removeLast();\n        }\n    }\n\n    private void printPath(LinkedList&lt;String&gt; visited) {\n        for (String node : visited) {\n            System.out.print(node);\n            System.out.print(\" \");\n        }\n        System.out.println();\n    }\n}\n</code></pre>\n\n<p>Program Output</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>B E \nB A C E \nB A C F E \nB F E \nB F C E \n</code></pre>\n    "},{"t":"Diff Algorithm [closed]","l":"http://stackoverflow.com/questions/805626/diff-algorithm","q":"\n\n<p>I've been looking like crazy for an explanation of a diff algorithm that works and is efficient.</p>\n\n<p>The closest I got is <a href=\"http://www.faqs.org/rfcs/rfc3284.html\">this link to RFC 3284</a> (from several Eric Sink blog posts), which describes in perfectly understandable terms the <em>data format</em> in which the diff results are stored. However, it has no mention whatsoever as to how a program would reach these results while doing a diff.</p>\n\n<p>I'm trying to research this out of personal curiosity, because I'm sure there must be tradeoffs when implementing a diff algorithm, which are pretty clear sometimes when you look at diffs and wonder \"why did the diff program chose this as a change instead of that?\"...</p>\n\n<p>Does anyone know where I can find a description of an efficient algorithm that'd end up outputting VCDIFF?<br>\nBy the way, if you happen to find a description of the actual algorithm used by SourceGear's DiffMerge, that'd be even better.</p>\n\n<p>NOTE: longest common subsequence doesn't seem to be the algorithm used by VCDIFF, it looks like they're doing something smarter, given the data format they use.</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p><a href=\"http://www.xmailserver.org/diff2.pdf\">An O(ND) Difference Algorithm and its Variations</a> is a fantastic paper and you may want to start there. It includes pseudo-code and a nice visualization of the graph traversals involved in doing the diff.</p>\n\n<p><strong>Section 4</strong> of the paper introduces some refinements to the algorithm that make it very effective.</p>\n\n<p>Successfully implementing this will leave you with a very useful tool in your toolbox (and probably some excellent experience as well).</p>\n\n<p>Generating the output format you need can sometimes be tricky, but if you have understanding of the algorithm internals, then you should be able to output anything you need. You can also introduce heuristics to affect the output and make certain tradeoffs.</p>\n\n<p><a href=\"http://www.mathertel.de/Diff/\">Here is a page</a> that includes a bit of documentation, <a href=\"http://www.mathertel.de/Diff/ViewSrc.aspx\">full source code</a>, and examples of a diff algorithm using the techniques in the aforementioned algorithm.</p>\n\n<p>The <a href=\"http://www.mathertel.de/Diff/ViewSrc.aspx\">source code</a> appears to follow the basic algorithm closely and is easy to read. </p>\n\n<p>There's also a bit on preparing the input, which you may find useful. There's a huge difference in output when you are diffing by character or token (word).</p>\n\n<p>Good luck!</p>\n    "},{"t":"Programmer Puzzle: Encoding a chess board state throughout a game","l":"http://stackoverflow.com/questions/1831386/programmer-puzzle-encoding-a-chess-board-state-throughout-a-game","q":"\n\n<p>Not strictly a question, more of a puzzle...</p>\n\n<p>Over the years, I've been involved in a few technical interviews of new employees. Other than asking the standard \"do you know X technology\" questions, I've also tried to get a feel for how they approach problems. Typically, I'd send them the question by email the day before the interview, and expect them to come up with a solution by the following day.</p>\n\n<p>Often the results would be quite interesting - wrong, but interesting - and the person would still get my recommendation if they could explain why they took a particular approach.</p>\n\n<p>So I thought I'd throw one of my questions out there for the Stack Overflow audience. </p>\n\n<p>Question: <strong>What is the most space-efficient way you can think of to encode the state of a chess game (or subset thereof)? That is, given a chess board with the pieces arranged legally, encode both this initial state and all subsequent legal moves taken by the players in the game.</strong></p>\n\n<p>No code required for the answer, just a description of the algorithm you would use.</p>\n\n<p>EDIT: As one of the posters has pointed out, I didn't consider the time interval between moves. Feel free to account for that too as an optional extra :)</p>\n\n<p>EDIT2: Just for additional clarification... Remember, the encoder/decoder is rule-aware. The only things that really need to be stored are the player's choices - anything else can be assumed to be known by the encoder/decoder.</p>\n\n<p>EDIT3: It's going to be difficult to pick a winner here :) Lots of great answers!</p>\n    ","a":"\n<p><strong>Update:</strong> I liked this topic so much I wrote <a href=\"http://www.cforcoding.com/2009/12/programming-puzzles-chess-positions-and.html\" rel=\"nofollow\">Programming Puzzles, Chess Positions and Huffman Coding</a>. If you read through this I've determined that the <em>only</em> way to store a complete game state is by storing a complete list of moves. Read on for why. So I use a slightly simplified version of the problem for piece layout.</p>\n\n\n\n<h1>The Problem</h1>\n\n<p>This image illustrates the starting Chess position. Chess occurs on an 8x8 board with each player starting with an identical set of 16 pieces consisting of 8 pawns, 2 rooks, 2 knights, 2 bishops, 1 queen and 1 king as illustrated here:</p>\n\n<p><img src=\"http://img222.imageshack.us/img222/5970/chess.png\" alt=\"starting chess position\"></p>\n\n<p>Positions are generally recorded as a letter for the column followed by the number for the row so White’s queen is at d1. Moves are most often stored in <a href=\"http://en.wikipedia.org/wiki/Algebraic_chess_notation\" rel=\"nofollow\">algebraic notation</a>, which is unambiguous and generally only specifies the minimal information necessary. Consider this opening:</p>\n\n<ol>\n<li>e4 e5</li>\n<li>Nf3 Nc6</li>\n<li>…</li>\n</ol>\n\n<p>which translates to:</p>\n\n<ol>\n<li>White moves king’s pawn from e2 to e4 (it is the only piece that can get to e4 hence “e4”);</li>\n<li>Black moves the king’s pawn from e7 to e5;</li>\n<li>White moves the knight (N) to f3;</li>\n<li>Black moves the knight to c6.</li>\n<li>…</li>\n</ol>\n\n<p>The board looks like this:</p>\n\n<p><img src=\"http://img222.imageshack.us/img222/371/chessx.png\" alt=\"early opening\"></p>\n\n<p>An important ability for any programmer is to be able to <em>correctly and unambiguously specify the problem</em>.</p>\n\n<p>So what’s missing or ambiguous? A lot as it turns out.</p>\n\n<h1>Board State vs Game State</h1>\n\n<p>The first thing you need to determine is whether you’re storing the state of a game or the position of pieces on the board. Encoding simply the positions of the pieces is one thing but the problem says “all subsequent legal moves”. The problem also says nothing about knowing the moves up to this point. That’s actually a problem as I’ll explain.</p>\n\n<h1>Castling</h1>\n\n<p>The game has proceeded as follows:</p>\n\n<ol>\n<li>e4 e5 </li>\n<li>Nf3 Nc6 </li>\n<li>Bb5 a6 </li>\n<li>Ba4 Bc5</li>\n</ol>\n\n<p>The board looks as follows:</p>\n\n<p><img src=\"http://i.stack.imgur.com/XxBaT.png\" alt=\"later opening\"></p>\n\n<p>White has the option of <a href=\"http://en.wikipedia.org/wiki/Castling\" rel=\"nofollow\">castling</a>. Part of the requirements for this are that the king and the relevant rook can never have moved, so whether the king or either rook of each side has moved will need to be stored. Obviously if they aren’t on their starting positions, they have moved otherwise it needs to be specified.</p>\n\n<p>There are several strategies that can be used for dealing with this problem.</p>\n\n<p>Firstly, we could store an extra 6 bits of information (1 for each rook and king) to indicate whether that piece had moved. We could streamline this by only storing a bit for one of these six squares if the right piece happens to be in it. Alternatively we could treat each unmoved piece as another piece type so instead of 6 piece types on each side (pawn, rook, knight, bishop, queen and king) there are 8 (adding unmoved rook and unmoved king).</p>\n\n<h1>En Passant</h1>\n\n<p>Another peculiar and often-neglected rule in Chess is <a href=\"http://en.wikipedia.org/wiki/En_passant\" rel=\"nofollow\">En Passant</a>.</p>\n\n<p><img src=\"http://img37.imageshack.us/img37/6535/chessa.png\" alt=\"en passant\"></p>\n\n<p>The game has progressed.</p>\n\n<ol>\n<li>e4 e5 </li>\n<li>Nf3 Nc6 </li>\n<li>Bb5 a6 </li>\n<li>Ba4 Bc5 </li>\n<li>O-O b5 </li>\n<li>Bb3 b4 </li>\n<li>c4</li>\n</ol>\n\n<p>Black’s pawn on b4 now has the option of moving his pawn on b4 to c3 taking the White pawn on c4. This only happens on the first opportunity meaning if Black passes on the option now he can’t take it next move. So we need to store this.</p>\n\n<p>If we know the previous move we can definitely answer if En Passant is possible. Alternatively we can store whether each pawn on its 4th rank has just moved there with a double move forward. Or we can look at each possible En Passant position on the board and have a flag to indicate whether its possible or not.</p>\n\n<h1>Promotion</h1>\n\n<p><img src=\"http://img689.imageshack.us/img689/5970/chess.png\" alt=\"pawn promotion\"></p>\n\n<p>It is White’s move. If White moves his pawn on h7 to h8 it can be promoted to any other piece (but not the king). 99% of the time it is promoted to a Queen but sometimes it isn’t, typically because that may force a stalemate when otherwise you’d win. This is written as:</p>\n\n<ol start=\"56\">\n<li>h8=Q</li>\n</ol>\n\n<p>This is important in our problem because it means we can’t count on there being a fixed number of pieces on each side. It is entirely possible (but incredibly unlikely) for one side to end up with 9 queens, 10 rooks, 10 bishops or 10 knights if all 8 pawns get promoted.</p>\n\n<h1>Stalemate</h1>\n\n<p>When in a position from which you cannot win your best tactic is to try for a <a href=\"http://en.wikipedia.org/wiki/Stalemate\" rel=\"nofollow\">stalemate</a>. The most likely variant is where you cannot make a legal move (usually because any move when put your king in check). In this case you can claim a draw. This one is easy to cater for.</p>\n\n<p>The second variant is by <a href=\"http://en.wikipedia.org/wiki/Threefold_repetition\" rel=\"nofollow\">threefold repetition</a>. If the same board position occurs three times in a game (or will occur a third time on the next move), a draw can be claimed. The positions need not occur in any particular order (meaning it doesn’t have to the same sequence of moves repeated three times). This one greatly complicates the problem because you have to remember every previous board position. <strong><em>If this is a requirement of the problem the only possible solution to the problem is to store every previous move.</em></strong></p>\n\n<p>Lastly, there is the <a href=\"http://en.wikipedia.org/wiki/Fifty-move_rule\" rel=\"nofollow\">fifty move rule</a>. A player can claim a draw if no pawn has moved and no piece has been taken in the previous fifty consecutive moves so we would need to store how many moves since a pawn was moved or a piece taken (the latest of the two. This requires 6 bits (0-63).</p>\n\n<h1>Whose Turn Is It?</h1>\n\n<p>Of course we also need to know whose turn it is and this is a single bit of information.</p>\n\n<h1>Two Problems</h1>\n\n<p>Because of the stalemate case, the only feasible or sensible way to store the game state is to store all the moves that led to this position. I’ll tackle that one problem. The board state problem will be simplified to this: <em>store the current position of all pieces on the board ignoring castling, en passant, stalemate conditions and whose turn it is</em>.</p>\n\n<p>Piece layout can be broadly handled in one of two ways: by storing the contents of each square or by storing the position of each piece.</p>\n\n<h1>Simple Contents</h1>\n\n<p>There are six piece types (pawn, rook, knight, bishop, queen and king). Each piece can be White or Black so a square may contain one of 12 possible pieces or it may be empty so there are 13 possibilities. 13 can be stored in 4 bits (0-15) So the simplest solution is to store 4 bits for each square times 64 squares or 256 bits of information.</p>\n\n<p>The advantage of this method is that manipulation is <em>incredibly</em> easy and fast. This could even be extended by adding 3 more possibilities without increasing the storage requirements: a pawn that has moved 2 spaces on the last turn, a king that hasn’t moved and a rook that hasn’t moved, which will cater for a lot of previously mentioned issues.</p>\n\n<p>But we can do better.</p>\n\n<p>Base 13 Encoding</p>\n\n<p>It is often helpful to think of the board position as a very large number. This is often done in computer science. For example, the <a href=\"http://en.wikipedia.org/wiki/Halting_problem\" rel=\"nofollow\">halting problem</a> treats a computer program (rightly) as a large number.</p>\n\n<p>The first solution treats the position as a 64 digit base 16 number but as demonstrated there is redundancy in this information (being the 3 unused possibilities per “digit”) so we can reduce the number space to 64 base 13 digits. Of course this can’t be done as efficiently as base 16 can but it will save on storage requirements (and minimizing storage space is our goal).</p>\n\n<p>In base 10 the number 234 is equivalent to 2 x 10<sup>2</sup> + 3 x 10<sup>1</sup> + 4 x 10<sup>0</sup>.</p>\n\n<p>In base 16 the number 0xA50 is equivalent to 10 x 16<sup>2</sup> + 5 x 16<sup>1</sup> + 0 x 16<sup>0</sup> = 2640 (decimal).</p>\n\n<p>So we can encode our position as p<sub>0</sub> x 13<sup>63</sup> + p<sub>1</sub> x 13<sup>62</sup> + ... + p<sub>63</sub> x 13<sup>0</sup> where p<sub>i</sub> represents the contents of square <em>i</em>.</p>\n\n<p>2<sup>256</sup> equals approximately 1.16e77. 13<sup>64</sup> equals approximately 1.96e71, which requires 237 bits of storage space. That saving of a mere 7.5% comes at a cost of <strong><em>significantly</em></strong> increased manipulation costs.</p>\n\n<h1>Variable Base Encoding</h1>\n\n<p>In legal boards certain pieces can’t appear in certain squares. For example, pawns cannot occur at in the first or eighth ranks, reducing the possibilities for those squares to 11. That reduces the possible boards to 11<sup>16</sup> x 13<sup>48</sup> = 1.35e70 (approximately), requiring 233 bits of storage space.</p>\n\n<p>Actually encoding and decoding such values to and from decimal (or binary) is a little more convoluted but it can be done reliably and is left as an exercise to the reader.</p>\n\n<h1>Variable Width Alphabets</h1>\n\n<p>The previous two methods can both be described as <em>fixed-width alphabetic encoding</em>. Each of the 11, 13 or 16 members of the alphabet is substituted for another value. Each “character” is the same width but the efficiency can be improved when you consider that each character is not equally likely.</p>\n\n<p><img src=\"http://i.stack.imgur.com/xjZLN.gif\" alt=\"morse code\"></p>\n\n<p>Consider <a href=\"http://en.wikipedia.org/wiki/Morse_code\" rel=\"nofollow\">Morse code</a> (pictured above). Characters in a message are encoded as a sequence of dashes and dots. Those dashes and dots are transferred over radio (typically) with a pause between them to delimit them.</p>\n\n<p>Notice how the letter E (<a href=\"http://en.wikipedia.org/wiki/Letter_frequencies\" rel=\"nofollow\">the most common letter in English</a>) is a single dot, the shortest possible sequence, whereas Z (the least frequent) is two dashes and two beeps.</p>\n\n<p>Such a scheme can significantly reduce the size of an <em>expected</em> message but comes at the cost of increasing the size of a random character sequence.</p>\n\n<p>It should be noted that Morse code has another inbuilt feature: dashes are as long as three dots so the above code is created with this in mind to minimize the use of dashes. Since 1s and 0s (our building blocks) don’t have this problem, it’s not a feature we need to replicate.</p>\n\n<p>Lastly, there are two kinds of rests in Morse code. A short rest (the length of a dot) is used to distinguish between dots and dashes. A longer gap (the length of a dash) is used to delimit characters.</p>\n\n<p>So how does this apply to our problem?</p>\n\n<h1>Huffman Coding</h1>\n\n<p>There is an algorithm for dealing with variable length codes called <a href=\"http://en.wikipedia.org/wiki/Huffman_coding\" rel=\"nofollow\">Huffman coding</a>. Huffman coding creates a variable length code substitution, typically uses expected frequency of the symbols to assign shorter values to the more common symbols.</p>\n\n<p><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Huffman_tree_2.svg/500px-Huffman_tree_2.svg.png\" alt=\"Huffman code tree\"></p>\n\n<p>In the above tree, the letter E is encoded as 000 (or left-left-left) and S is 1011. It should be clear that this encoding scheme is <em>unambiguous</em>.</p>\n\n<p>This is an important distinction from Morse code. Morse code has the character separator so it can do otherwise ambiguous substitution (eg 4 dots can be H or 2 Is) but we only have 1s and 0s so we choose an unambiguous substitution instead.</p>\n\n<p>Below is a simple implementation:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>private static class Node {\n  private final Node left;\n  private final Node right;\n  private final String label;\n  private final int weight;\n\n  private Node(String label, int weight) {\n    this.left = null;\n    this.right = null;\n    this.label = label;\n    this.weight = weight;\n  }\n\n  public Node(Node left, Node right) {\n    this.left = left;\n    this.right = right;\n    label = \"\";\n    weight = left.weight + right.weight;\n  }\n\n  public boolean isLeaf() { return left == null &amp;&amp; right == null; }\n\n  public Node getLeft() { return left; }\n\n  public Node getRight() { return right; }\n\n  public String getLabel() { return label; }\n\n  public int getWeight() { return weight; }\n}\n</code></pre>\n\n<p>with static data:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>private final static List&lt;string&gt; COLOURS;\nprivate final static Map&lt;string, integer&gt; WEIGHTS;\n\nstatic {\n  List&lt;string&gt; list = new ArrayList&lt;string&gt;();\n  list.add(\"White\");\n  list.add(\"Black\");\n  COLOURS = Collections.unmodifiableList(list);\n  Map&lt;string, integer&gt; map = new HashMap&lt;string, integer&gt;();\n  for (String colour : COLOURS) {\n    map.put(colour + \" \" + \"King\", 1);\n    map.put(colour + \" \" + \"Queen\";, 1);\n    map.put(colour + \" \" + \"Rook\", 2);\n    map.put(colour + \" \" + \"Knight\", 2);\n    map.put(colour + \" \" + \"Bishop\";, 2);\n    map.put(colour + \" \" + \"Pawn\", 8);\n  }\n  map.put(\"Empty\", 32);\n  WEIGHTS = Collections.unmodifiableMap(map);\n}\n</code></pre>\n\n<p>and:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>private static class WeightComparator implements Comparator&lt;node&gt; {\n  @Override\n  public int compare(Node o1, Node o2) {\n    if (o1.getWeight() == o2.getWeight()) {\n      return 0;\n    } else {\n      return o1.getWeight() &lt; o2.getWeight() ? -1 : 1;\n    }\n  }\n}\n\nprivate static class PathComparator implements Comparator&lt;string&gt; {\n  @Override\n  public int compare(String o1, String o2) {\n    if (o1 == null) {\n      return o2 == null ? 0 : -1;\n    } else if (o2 == null) {\n      return 1;\n    } else {\n      int length1 = o1.length();\n      int length2 = o2.length();\n      if (length1 == length2) {\n        return o1.compareTo(o2);\n      } else {\n        return length1 &lt; length2 ? -1 : 1;\n      }\n    }\n  }\n}\n\npublic static void main(String args[]) {\n  PriorityQueue&lt;node&gt; queue = new PriorityQueue&lt;node&gt;(WEIGHTS.size(),\n      new WeightComparator());\n  for (Map.Entry&lt;string, integer&gt; entry : WEIGHTS.entrySet()) {\n    queue.add(new Node(entry.getKey(), entry.getValue()));\n  }\n  while (queue.size() &gt; 1) {\n    Node first = queue.poll();\n    Node second = queue.poll();\n    queue.add(new Node(first, second));\n  }\n  Map&lt;string, node&gt; nodes = new TreeMap&lt;string, node&gt;(new PathComparator());\n  addLeaves(nodes, queue.peek(), &amp;quot;&amp;quot;);\n  for (Map.Entry&lt;string, node&gt; entry : nodes.entrySet()) {\n    System.out.printf(\"%s %s%n\", entry.getKey(), entry.getValue().getLabel());\n  }\n}\n\npublic static void addLeaves(Map&lt;string, node&gt; nodes, Node node, String prefix) {\n  if (node != null) {\n    addLeaves(nodes, node.getLeft(), prefix + \"0\");\n    addLeaves(nodes, node.getRight(), prefix + \"1\");\n    if (node.isLeaf()) {\n      nodes.put(prefix, node);\n    }\n  }\n}\n</code></pre>\n\n<p>One possible output is:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>         White    Black\nEmpty          0 \nPawn       110      100\nRook     11111    11110\nKnight   10110    10101\nBishop   10100    11100\nQueen   111010   111011\nKing    101110   101111\n</code></pre>\n\n<p>For a starting position this equates to 32 x 1 + 16 x 3 + 12 x 5 + 4 x 6 = 164 bits.</p>\n\n<h1>State Difference</h1>\n\n<p>Another possible approach is to combine the very first approach with Huffman coding. This is based on the assumption that most expected Chess boards (rather than randomly generated ones) are more likely than not to, at least in part, resemble a starting position.</p>\n\n<p>So what you do is XOR the 256 bit current board position with a 256 bit starting position and then encode that (using Huffman coding or, say, some method of <a href=\"http://en.wikipedia.org/wiki/Run-length_encoding\" rel=\"nofollow\">run length encoding</a>). Obviously this will be very efficient to start with (64 0s probably corresponding to 64 bits) but increase in storage required as the game progresses.</p>\n\n<h1>Piece Position</h1>\n\n<p>As mentioned, another way of attacking this problem is to instead store the position of each piece a player has. This works particularly well with endgame positions where most squares will be empty (but in the Huffman coding approach empty squares only use 1 bit anyway).</p>\n\n<p>Each side will have a king and 0-15 other pieces. Because of promotion the exact make up of those pieces can vary enough that you can’t assume the numbers based on the starting positions are maxima.</p>\n\n<p>The logical way to divide this up is store a Position consisting of two Sides (White and Black). Each Side has:</p>\n\n<ul>\n<li>A king: 6 bits for the location;</li>\n<li>Has pawns: 1 (yes), 0 (no);</li>\n<li>If yes, number of pawns: 3 bits (0-7+1 = 1-8);</li>\n<li>If yes, the location of each pawn is encoded: 45 bits (see below);</li>\n<li>Number of non-pawns: 4 bits (0-15);</li>\n<li>For each piece: type (2 bits for queen, rook, knight, bishop) and location (6 bits)</li>\n</ul>\n\n<p>As for the pawn location, the pawns can only be on 48 possible squares (not 64 like the others). As such, it is better not to waste the extra 16 values that using 6 bits per pawn would use. So if you have 8 pawns there are 48<sup>8</sup> possibilities, equalling 28,179,280,429,056. You need 45 bits to encode that many values.</p>\n\n<p>That’s 105 bits per side or 210 bits total. The starting position is the worst case for this method however and it will get substantially better as you remove pieces.</p>\n\n<p>It should be pointed out that there are less than 48<sup>8</sup> possibilities because the pawns can’t all be in the same square&nbsp; The first has 48 possibilities, the second 47 and so on. 48 x 47 x … x 41 = 1.52e13 = 44 bits storage.</p>\n\n<p>You can further improve this by eliminating the squares that are occupied by other pieces (including the other side) so you could first place the white non-pawns then the black non-pawns, then the white pawns and lastly the black pawns. On a starting position this reduces the storage requirements to 44 bits for White and 42 bits for Black.</p>\n\n<h1>Combined Approaches</h1>\n\n<p>Another possible optimization is that each of these approaches has its strength and weaknesses. You could, say, pick the best 4 and then encode a scheme selector in the first two bits and then the scheme-specific storage after that.</p>\n\n<p>With the overhead that small, this will by far be the best approach.</p>\n\n<h1>Game State</h1>\n\n<p>I return to the problem of storing a <em>game</em> rather than a <em>position</em>. Because of the threefold repetition we have to store the list of moves that have occurred to this point.</p>\n\n<h1>Annotations</h1>\n\n<p>One thing you have to determine is are you simply storing a list of moves or are you annotating the game? Chess games are often annotated, for example:</p>\n\n<ol start=\"17\">\n<li>Bb5!! Nc4?</li>\n</ol>\n\n<p>White’s move is marked by two exclamation points as brilliant whereas Black’s is viewed as a mistake. See <a href=\"http://en.wikipedia.org/wiki/Punctuation_(chess)\" rel=\"nofollow\">Chess punctuation</a>.</p>\n\n<p>Additionally you could also need to store free text as the moves are described.</p>\n\n<p>I am assuming that the moves are sufficient so there will be no annotations.</p>\n\n<h1>Algebraic Notation</h1>\n\n<p>We could simply store the the text of the move here (“e4”, “Bxb5”, etc). Including a terminating byte you’re looking at about 6 bytes (48 bits) per move (worst case). That’s not particularly efficient.</p>\n\n<p>The second thing to try is to store the starting location (6 bits) and end location (6 bits) so 12 bits per move. That is significantly better.</p>\n\n<p>Alternatively we can determine all the legal moves from the current position in a predictable and deterministic way and state which we’ve chosen. This then goes back to the variable base encoding mentioned above. White and Black have 20 possible moves each on their first move, more on the second and so on.</p>\n\n<h1>Conclusion</h1>\n\n<p>There is no absolutely right answer to this question. There are many possible approaches of which the above are just a few.</p>\n\n<p>What I like about this and similar problems is that it demands abilities important to any programmer like considering the usage pattern, accurately determining requirements and thinking about corner cases.</p>\n\n<p><em>Chess positions taken as screenshots from <a href=\"http://www.chesspositiontrainer.com\" rel=\"nofollow\">Chess Position Trainer</a>.</em></p>\n    "},{"t":"Cache Invalidation—Is there a General Solution?","l":"http://stackoverflow.com/questions/1188587/cache-invalidation-is-there-a-general-solution","q":"\n\n<blockquote>\n  <p>\"There are only two hard problems in Computer Science: cache invalidation and naming things.\"</p>\n</blockquote>\n\n<p><em>Phil Karlton</em></p>\n\n<p>Is there a general solution or method to invalidating a cache; to know when an entry is stale, so you are guaranteed to always get fresh data?</p>\n\n<p>For example, consider a function <code>getData()</code> that gets data from a file.\nIt caches it based on the last modified time of the file, which it checks every time it's called.<br>\nThen you add a second function <code>transformData()</code> which transforms the data, and caches its result for next time the function is called.  It has no knowledge of the file - how do you add the dependency that if the file is changed, this cache becomes invalid?</p>\n\n<p>You could call <code>getData()</code> every time <code>transformData()</code> is called and compare it with the value that was used to build the cache, but that could end up being very costly.</p>\n    ","a":"\n<p>What you are talking about is lifetime dependency chaining, that one thing is dependent on another which can be modified outside of it's control.</p>\n\n<p>If you have an idempotent function from <code>a</code>, <code>b</code> to <code>c</code> where, if <code>a</code> and <code>b</code> are the same then <code>c</code> is the same but the cost of checking <code>b</code> is high then you either:</p>\n\n<ol>\n<li>accept that you sometime operate with out of date information and do not always check <code>b</code></li>\n<li>do your level best to make checking <code>b</code> as fast as possible</li>\n</ol>\n\n<p>You cannot have your cake and eat it...</p>\n\n<p>If you can layer an additional cache based on <code>a</code> over the top then this affects the initial problem not one bit. If you chose 1 then you have whatever freedom you gave yourself and can thus cache more but must remember to consider the validity of the cached value of <code>b</code>. If you chose 2 you must still check <code>b</code> every time but can fall back on the cache for <code>a</code> if <code>b</code> checks out.</p>\n\n<p>If you layer caches you must consider whether you have violated the 'rules' of the system as a result of the combined behaviour. </p>\n\n<p>If you know that <code>a</code> always has validity if <code>b</code> does then you can arrange your cache like so (pseudocode):</p>\n\n<pre><code>private map&lt;b,map&lt;a,c&gt;&gt; cache // \nprivate func realFunction    // (a,b) -&gt; c\n\nget(a, b) \n{\n    c result;\n    map&lt;a,c&gt; endCache;\n    if (cache[b] expired or not present)\n    {\n        remove all b -&gt; * entries in cache;   \n        endCache = new map&lt;a,c&gt;();      \n        add to cache b -&gt; endCache;\n    }\n    else\n    {\n        endCache = cache[b];     \n    }\n    if (endCache[a] not present)     // important line\n        result = realFunction(a,b); \n        endCache[a] = result;\n    else   \n        result = endCache[a];\n   return result;\n}\n</code></pre>\n\n<p>Obviously successive layering (say <code>x</code>) is trivial so long as, at each stage the validity of the newly added input matches the <code>a</code>:<code>b</code> relationship for <code>x</code>:<code>b</code> and <code>x</code>:<code>a</code>.</p>\n\n<p>However it is quite possible that you could get three inputs whose validity was entirely independent (or was cyclic), so no layering would be possible. This would mean the line marked // important would have to change to   </p>\n\n<blockquote>\n  <p>if (endCache[a] <strong>expired or</strong> not present)</p>\n</blockquote>\n    "},{"t":"How do 20 questions AI algorithms work?","l":"http://stackoverflow.com/questions/887533/how-do-20-questions-ai-algorithms-work","q":"\n\n<p>Simple online games of 20 questions powered by an eerily accurate AI.</p>\n\n<p>How do they guess so well?</p>\n    ","a":"\n<p>You can think of it as the Binary Search Algorithm.\nIn each iteration, we ask a question, which should eliminate roughly half of the possible word choices. If there are total of N words, then we can expect to get an answer after log2(N) questions.</p>\n\n<p>With 20 question, we should optimally be able to find a word among 2^20 = 1 million words.</p>\n\n<p>One easy way to eliminate outliers (wrong answers) would be to probably use something like <a href=\"http://en.wikipedia.org/wiki/RANSAC\">RANSAC</a>.  This would mean, instead of taking into account all questions which have been answered, you randomly pick a smaller subset, which is enough to give you a single answer. Now you repeat that a few times with different random subset of questions, till you see that most of the time, you are getting the same result. you then know you have the right answer.</p>\n\n<p>Of course this is just one way of many ways of solving this problem.</p>\n    "},{"t":"“Last 100 bytes” Interview Scenario","l":"http://stackoverflow.com/questions/7986186/last-100-bytes-interview-scenario","q":"\n\n<p>I got this question in an interview the other day and would like to know some best possible answers(I did not answer very well haha):</p>\n\n<p><em>Scenario: There is a webpage that is monitoring the bytes sent over a some network. Every time a byte is sent the recordByte() function is called passing that byte, this could happen hundred of thousands of times per day. There is a button on this page that when pressed displays the last 100 bytes passed to recordByte() on screen (it does this by calling the print method below).</em></p>\n\n<p>The following code is what I was given and asked to fill out:</p>\n\n<pre><code>public class networkTraffic {\n    public void recordByte(Byte b){\n    }\n    public String print() {\n    }\n}\n</code></pre>\n\n<p>What is the best way to store the 100 bytes? A list? Curious how best to do this.</p>\n    ","a":"\n<p>Something like this (<strong>circular buffer</strong>) :</p>\n\n<pre><code>byte[] buffer = new byte[100];\nint index = 0;\n\npublic void recordByte(Byte b) {\n   index = (index + 1) % 100;\n   buffer[index] = b; \n}\n\npublic void print() {\n   for(int i = index; i &lt; index + 100; i++) {\n       System.out.print(buffer[i % 100]);\n   }\n}\n</code></pre>\n\n<p>The benefits of using a circular buffer:</p>\n\n<ol>\n<li>You can reserve the space statically. In a real-time network application (VoIP, streaming,..)this is often done because you don't need to store all data of a transmission, but only a window containing the new bytes to be processed. </li>\n<li>It's fast: can be implemented with an array with read and write cost of O(1).</li>\n</ol>\n    "},{"t":"How to merge two sorted arrays into a sorted array?","l":"http://stackoverflow.com/questions/5958169/how-to-merge-two-sorted-arrays-into-a-sorted-array","q":"\n\n<p>This was asked of me in an interview and this is the solution I provided:</p>\n\n<pre><code>public static int[] merge(int[] a, int[] b) {\n\n    int[] answer = new int[a.length + b.length];\n    int i = 0, j = 0, k = 0;\n    while (i &lt; a.length &amp;&amp; j &lt; b.length)\n    {\n        if (a[i] &lt; b[j])\n        {\n            answer[k] = a[i];\n            i++;\n        }\n        else\n        {\n            answer[k] = b[j];\n            j++;\n        }\n        k++;\n    }\n\n    while (i &lt; a.length)\n    {\n        answer[k] = a[i];\n        i++;\n        k++;\n    }\n\n    while (j &lt; b.length)\n    {\n        answer[k] = b[j];\n        j++;\n        k++;\n    }\n\n    return answer;\n}\n</code></pre>\n\n<p>Is there a more efficient way to do this? </p>\n\n<p>Edit: Corrected length methods.</p>\n    ","a":"\n<p>A minor improvement, but after the main loop, you could use <code>System.arraycopy</code> to copy the tail of either input array when you get to the end of the other. That won't change the <code>O(n)</code> performance characteristics of your solution, though.</p>\n    "},{"t":"Looking for a good world map generation algorithm","l":"http://stackoverflow.com/questions/2520131/looking-for-a-good-world-map-generation-algorithm","q":"\n\n<p>I'm working on a Civilization-like game and I'm looking for a good algorithm for generating Earth-like world maps. I've experimented with a few alternatives, but haven't hit on a real winner yet.</p>\n\n<p>One option is to generate a heightmap using <a href=\"http://en.wikipedia.org/wiki/Perlin_noise\">Perlin noise</a> and add water at a level so that about 30% of the world is land. While Perlin noise (or similar fractal-based techniques) is frequently used for terrain and is reasonably realistic, it doesn't offer much in the way of control over the number, size and position of the resulting continents, which I'd like to have from a gameplay perspective.</p>\n\n<p><img src=\"http://farm3.static.flickr.com/2792/4462870263_ff26c40365_o.jpg\" alt=\"Perlin noise\"></p>\n\n<p>A second option is to start with a randomly positioned one-tile seed (I'm working on a grid of tiles), determine the desired size for the continent and each turn add a tile that is horizontally or vertically adjacent to the existing continent until you've reached the desired size. Repeat for the other continents. This technique is part of the algorithm used in Civilization 4. The problem is that after placing the first few continents, it's possible to pick a starting location that's surrounded by other continents, and thus won't fit the new one. Also, it has a tendency to spawn continents too close together, resulting in something that looks more like a river than continents.</p>\n\n<p><img src=\"http://farm5.static.flickr.com/4069/4462870383_46e86b155c_o.jpg\" alt=\"Random expansion\"></p>\n\n<p>Does anyone happen to know a good algorithm for generating realistic continents on a grid-based map while keeping control over their number and relative sizes?</p>\n    ","a":"\n<p>You could take a cue from <a href=\"http://en.wikipedia.org/wiki/Plate_tectonics\">nature</a> and modify your second idea. Once you generate your continents (which are all about the same size), get them to randomly move and rotate and collide and deform each other and drift apart from each other. <em>(Note: this may not be the easiest thing ever to implement.)</em></p>\n\n<p><em>Edit:</em> Here's another way of doing it, complete with an implementation — <a href=\"http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/\">Polygonal Map Generation for Games</a>.</p>\n    "},{"t":"How do I check if a number is a palindrome?","l":"http://stackoverflow.com/questions/199184/how-do-i-check-if-a-number-is-a-palindrome","q":"\n\n<p>How do I check if a number is a palindrome?</p>\n\n<p>Any language. Any algorithm. (except the algorithm of making the number a string and then reversing the string).</p>\n    ","a":"\n<p>This is <a href=\"http://projecteuler.net/index.php?section=problems&amp;id=4\">one of the Project Euler problems</a>.  When I solved it in Haskell I did exactly what you suggest, convert the number to a String.  It's then trivial to check that the string is a pallindrome.  If it performs well enough, then why bother making it more complex?  Being a pallindrome is a lexical property rather than a mathematical one.</p>\n    "},{"t":"What's a good rate limiting algorithm?","l":"http://stackoverflow.com/questions/667508/whats-a-good-rate-limiting-algorithm","q":"\n\n<p>I could use some pseudo-code, or better, Python.  I am trying to implement a rate-limiting queue for a Python IRC bot, and it partially works, but if someone triggers less messages than the limit (e.g., rate limit is 5 messages per 8 seconds, and the person triggers only 4), and the next trigger is over the 8 seconds (e.g., 16 seconds later), the bot sends the message, but the queue becomes full and the bot waits 8 seconds, even though it's not needed since the 8 second period has lapsed.</p>\n    ","a":"\n<p>Here the simplest algorithm, if you want just to drop messages when they arrive too quickly (instead of queuing them, which makes sense because the queue might get arbitrarily large):</p>\n\n<pre><code>rate = 5.0; // unit: messages\nper  = 8.0; // unit: seconds\nallowance = rate; // unit: messages\nlast_check = now(); // floating-point, e.g. usec accuracy. Unit: seconds\n\nwhen (message_received):\n  current = now();\n  time_passed = current - last_check;\n  last_check = current;\n  allowance += time_passed * (rate / per);\n  if (allowance &gt; rate):\n    allowance = rate; // throttle\n  if (allowance &lt; 1.0):\n    discard_message();\n  else:\n    forward_message();\n    allowance -= 1.0;\n</code></pre>\n\n<p>There are no datastructures, timers etc. in this solution and it works cleanly :) To see this, 'allowance' grows at speed 5/8 units per seconds at most, i.e. at most five units per eight seconds. Every message that is forwarded deducts one unit, so you can't send more than five messages per every eight seconds.</p>\n\n<p>Note that <code>rate</code> should be an integer, i.e. without non-zero decimal part, or the algorithm won't work correctly (actual rate will not be <code>rate/per</code>). E.g. <code>rate=0.5; per=1.0;</code> does not work because <code>allowance</code> will never grow to 1.0. But <code>rate=1.0; per=2.0;</code> works fine.</p>\n    "},{"t":"Which parallel sorting algorithm has the best average case performance?","l":"http://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance","q":"\n\n<p>Sorting takes O(n log n) in the serial case. If we have O(n) processors we would hope for a linear speedup. O(log n) parallel algorithms exist but they have a very high constant. They also aren't applicable on commodity hardware which doesn't have anywhere near O(n) processors. With p processors, reasonable algorithms should take O(n/p log n) time.</p>\n\n<p>In the serial case, quick sort has the best runtime complexity on average. A parallel quick sort algorithm is easy to implement (see <a href=\"http://stackoverflow.com/questions/1897458/parallel-sort-algorithm\">here</a> and <a href=\"http://stackoverflow.com/questions/1784028/which-sorting-method-is-most-suitable-for-parallel-processing\">here</a>). However it doesn't perform well since the very first step is to partition the whole collection on a single core. I have found information on many parallel sort algorithms but so far I have not seen anything pointing to a clear winner.</p>\n\n<p>I'm looking to sort lists of 1 million to 100 million elements in a JVM language running on 8 to 32 cores.</p>\n    ","a":"\n<p>The following article (PDF download) is a comparative study of parallel sorting algorithms on various architectures:</p>\n\n<p><a href=\"http://parasol.tamu.edu/publications/download.php?file_id=191\">Parallel sorting algorithms on various architectures</a></p>\n\n<p>According to the article, <strong>sample sort</strong> seems to be best on many parallel architecture types.</p>\n\n<p>Update to address Mark's concern of age:</p>\n\n<p>Here are more recent articles introducing something more novel (from 2007, which, btw, still get compared with sample sort):</p>\n\n<p><a href=\"http://gauss.cs.ucsb.edu/publication/psort.pdf\">Improvements on sample sort</a> <br>\n<a href=\"http://www.dia.eui.upm.es/asignatu/pro_par/articulos/AASort.pdf\">AA-Sort</a></p>\n\n<p>The bleeding edge (circa 2010, some only a couple months old):</p>\n\n<p><a href=\"http://parlab.eecs.berkeley.edu/wiki/_media/patterns/sortingpattern.pdf\">Parallel sorting pattern</a><br>\n<a href=\"http://lap.epfl.ch/webdav/site/lap/shared/publications/YeApr10_HighPerformanceComparisonBasedSortingAlgorithmOnManyCoreGpus_IPDPS10.pdf\">Many-core GPU based parallel sorting</a><br>\n<a href=\"http://cass-mt.pnl.gov/docs/pubs/gpusort-2010-08-25.pdf\">Hybrid CPU/GPU parallel sort</a><br>\n<a href=\"http://www.cc.gatech.edu/~bader/COURSES/UNM/ece638-Fall2004/papers/HBJ98.pdf\">Randomized Parallel Sorting Algorithm with an Experimental Study</a><br>\n<a href=\"http://charm.cs.uiuc.edu/charmWorkshop/slides/CharmWorkshop2010_Sorting.pdf\">Highly scalable parallel sorting</a><br>\n<a href=\"http://www.scipub.org/fulltext/jcs/jcs62163-167.pdf\">Sorting N-Elements Using Natural Order: A New Adaptive Sorting Approach</a><br></p>\n\n<p><strong>Update for 2013:</strong>\nHere is the bleeding edge circa January, 2013. (Note: A few of the links are to papers at Citeseer and require registration which is free):<br></p>\n\n<p>University lectures:<br>\n<a href=\"http://www.cs.sunysb.edu/~rezaul/Spring-2012/CSE613/CSE613-lecture-9.pdf\">Parallel Partitioning for Selection and Sorting</a><br>\n<a href=\"http://homepages.math.uic.edu/~jan/mcs572/parallelsorting.pdf\">Parallel Sorting Algorithms Lecture</a><br>\n<a href=\"http://www.clear.rice.edu/comp422/lecture-notes/comp422-2012-Lecture21-Sorting.pdf\">Parallel Sorting Algorithms Lecture 2</a><br>\n<a href=\"http://www.redgenes.com/Lecture-Sorting.pdf\">Parallel Sorting Algorithms Lecture 3</a><br>\n<br>\nOther sources and papers:\n<br>\n<a href=\"http://hgpu.org/?p=7227\">A novel sorting algorithm for many-core architectures based on adaptive bitonic sort</a><br>\n<a href=\"http://charm.cs.illinois.edu/talks/SortingIPDPS10.pdf\">Highly Scalable Parallel Sorting 2</a><br>\n<a href=\"http://www.economyinformatics.ase.ro/content/EN4/alecu.pdf\">Parallel Merging</a><br>\n<a href=\"http://www.drdobbs.com/parallel/parallel-merge/229204454?queryText=parallel%2Bsort\">Parallel Merging 2</a><br>\n<a href=\"http://arxiv.org/ftp/arxiv/papers/1209/1209.3050.pdf\">Parallel Self-Sorting System for Objects</a><br>\n<a href=\"http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.252.9314\">Performance Comparison of Sequential Quick Sort and Parallel Quick Sort Algorithms</a><br>\n<a href=\"http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.217.7866\">Shared Memory, Message Passing, and Hybrid Merge Sorts for Standalone and Clustered SMPs</a><br>\n<a href=\"http://pages.videotron.com/aminer/\">Various parallel algorithms (sorting et al) including implementations</a><br>\n<br>\nGPU and CPU/GPU hybrid sources and papers:\n<br>\n<a href=\"http://hgpu.org/?p=8085\">An OpenCL Method of Parallel Sorting Algorithms for GPU Architecture</a><br>\n<a href=\"http://hgpu.org/?p=8198\">Data Sorting Using Graphics Processing Units</a><br>\n<a href=\"http://hgpu.org/?p=8031\">Efficient Algorithms for Sorting on GPUs</a><br>\n<a href=\"http://hgpu.org/?p=1203\">Designing efficient sorting algorithms for manycore GPUs</a><br>\n<a href=\"http://hgpu.org/?p=1422\">Deterministic Sample Sort For GPUs</a><br>\n<a href=\"http://hgpu.org/?p=3331\">Fast in-place sorting with CUDA based on bitonic sort</a><br>\n<a href=\"http://hgpu.org/?p=923\">Fast parallel GPU-sorting using a hybrid algorithm</a><br>\n<a href=\"http://hgpu.org/?p=8602\">Fast Parallel Sorting Algorithms on GPUs</a><br>\n<a href=\"http://hgpu.org/?p=1533\">Fast sort on CPUs and GPUs: a case for bandwidth oblivious SIMD sort</a><br>\n<a href=\"http://hgpu.org/?p=892\">GPU sample sort</a><br>\n<a href=\"http://hgpu.org/?p=2033\">GPU-ABiSort: Optimal Parallel Sorting on Stream Architectures</a><br>\n<a href=\"http://hgpu.org/?p=1177\">GPUTeraSort: high performance graphics co-processor sorting for large database management</a><br>\n<a href=\"http://hgpu.org/?p=4182\">High performance comparison-based sorting algorithm on many-core GPUs</a><br>\n<a href=\"http://hgpu.org/?p=1084\">Parallel external sorting for CUDA-enabled GPUs with load balancing and low transfer overhead</a><br>\n<a href=\"http://hgpu.org/?p=6687\">Sorting on GPUs for large scale datasets: A thorough comparison</a><br></p>\n    "},{"t":"What are the differences between segment trees, interval trees, binary indexed trees and range trees?","l":"http://stackoverflow.com/questions/17466218/what-are-the-differences-between-segment-trees-interval-trees-binary-indexed-t","q":"\n\n<p>What are differences between segment trees, interval trees, binary indexed trees and range trees in terms of:</p>\n\n<ul>\n<li>Key idea/definition  </li>\n<li>Applications  </li>\n<li>Performance/order in higher dimensions/space consumption</li>\n</ul>\n\n<p>Please do not just give definitions.</p>\n    ","a":"\n<p>All these data structures are used for solving different problems:</p>\n\n<ul>\n<li><strong>Segment tree</strong> stores intervals, and optimized for \"<em>which of these intervals contains a given point</em>\" queries.</li>\n<li><strong>Interval tree</strong> stores intervals as well, but optimized for \"<em>which of these intervals overlap with a given interval</em>\" queries. It can also be used for point queries - similar to segment tree. </li>\n<li><strong>Range tree</strong> stores points, and optimized for \"<em>which points fall within a given interval</em>\" queries.</li>\n<li><strong>Binary indexed tree</strong> stores items-count per index, and optimized for \"<em>how many items are there between index m and n</em>\" queries.</li>\n</ul>\n\n<p>Performance / Space consumption for one dimension:</p>\n\n<ul>\n<li><strong>Segment tree</strong> - O(n logn) preprocessing time, O(k+logn) query time, O(n logn) space</li>\n<li><strong>Interval tree</strong> - O(n logn) preprocessing time, O(k+logn) query time, O(n) space</li>\n<li><strong>Range tree</strong> - O(n logn) preprocessing time, O(k+logn) query time, O(n) space</li>\n<li><strong>Binary Indexed tree</strong> - O(n logn) preprocessing time, O(logn) query time, O(n) space</li>\n</ul>\n\n<p>(k is the number of reported results).</p>\n\n<p>All data structures can be dynamic, in the sense that the usage scenario includes both data  changes and queries:</p>\n\n<ul>\n<li><strong>Segment tree</strong> - interval can be added/deleted in O(logn) time (see <a href=\"http://3glab.cs.nthu.edu.tw/~spoon/courses/CS631100/Lecture05_handout.pdf\">here</a>)</li>\n<li><strong>Interval tree</strong> - interval can be added/deleted in O(logn) time</li>\n<li><strong>Range tree</strong> - new points can be added/deleted in O(logn) time (see <a href=\"http://www.cs.unb.ca/tech-reports/documents/TR95_100.pdf\">here</a>)</li>\n<li><strong>Binary Indexed tree</strong> - the items-count per index can be increased in O(logn) time</li>\n</ul>\n\n<p>Higher dimensions (d&gt;1):</p>\n\n<ul>\n<li><strong>Segment tree</strong> - O(n(logn)^d) preprocessing time, O(k+(logn)^d) query time, O(n(logn)^(d-1)) space</li>\n<li><strong>Interval tree</strong> - O(n logn) preprocessing time, O(k+(logn)^d) query time, O(n logn) space</li>\n<li><strong>Range tree</strong> - O(n(logn)^d) preprocessing time, O(k+(logn)^d) query time, O(n(logn)^(d-1))) space</li>\n<li><strong>Binary Indexed tree</strong> - O(n(logn)^d) preprocessing time, O((logn)^d) query time, O(n(logn)^d) space</li>\n</ul>\n    "},{"t":"Algorithm to detect overlapping periods","l":"http://stackoverflow.com/questions/13513932/algorithm-to-detect-overlapping-periods","q":"\n\n<p>I've to detect if two time periods are overlapping.</p>\n\n<p>Every period has a start date and an end date.</p>\n\n<p>I need to detect if my first time period (A) is overlapping with another one(B/C).</p>\n\n<p>In my case, if the start of B is equal to the end of A, they are not overlapping(the inverse too)</p>\n\n<p>I found the following cases:</p>\n\n<p><img src=\"http://i.stack.imgur.com/AIBUV.png\" alt=\"enter image description here\"></p>\n\n<p>So actually I'm doing this like this:</p>\n\n<pre><code>tStartA &lt; tStartB &amp;&amp; tStartB &lt; tEndA //For case 1\nOR\ntStartA &lt; tEndB &amp;&amp; tEndB &lt;= tEndA //For case 2\nOR\ntStartB &lt; tStartA  &amp;&amp; tEndB &gt; tEndA //For case 3\n</code></pre>\n\n<p>(The case 4 is taken in account either in case 1 or in case 2)</p>\n\n<p>It <strong>works</strong>, but it seems not very efficient.</p>\n\n<p>So, first is there an existing class in c# that can modelize this(a time period), something like a timepsan, but with a fixed start date.</p>\n\n<p>Secondly: Is there already a c# code(like in the Datetime class) which can handle this?</p>\n\n<p>Third: if no, what would be your approach to make this comparison the most fast?</p>\n    ","a":"\n<p>Simple check to see if two time periods overlap:</p>\n\n<pre><code>bool overlap = a.start &lt; b.end &amp;&amp; b.start &lt; a.end;\n</code></pre>\n\n<p>or in your code:</p>\n\n<pre><code>bool overlap = tStartA &lt; tEndB &amp;&amp; tStartB &lt; tEndA;\n</code></pre>\n\n<p>(Use <code>&lt;=</code> instead of <code>&lt;</code> if you change your mind about wanting to say that two periods that just touch each other overlap.)</p>\n    "},{"t":"How would you write a program to generate Haiku? [closed]","l":"http://stackoverflow.com/questions/504428/how-would-you-write-a-program-to-generate-haiku","q":"\n\n<p>Computer Haiku<br>\nHow would you write a program<br>\nTo make them for you</p>\n    ","a":"\n<p>Do not attempt it<br>\nPoetry does not mix well<br>\nWith metal and bits</p>\n\n<p>More seriously, good haiku (and even bad haiku) is a lot more about condensing meaning and imagery than counting syllables. It is generally also based on themes gathered from nature. Random word generation and syllable counting will get you measured gibberish, but not poetry...</p>\n    "},{"t":"Number.sign() in javascript","l":"http://stackoverflow.com/questions/7624920/number-sign-in-javascript","q":"\n\n<p>Wonder if there are any nontrivial ways of finding number's sign (<a href=\"http://en.wikipedia.org/wiki/Signum_function\">signum function</a>)?<br>\nMay be shorter / faster / more elegant solutions than the obvious one</p>\n\n<pre><code>var sign = number &gt; 0 ? 1 : number &lt; 0 ? -1 : 0;\n</code></pre>\n\n<hr>\n\n<h2>Short excerpt</h2>\n\n<p>Use this and you'll be safe and fast</p>\n\n<pre><code>function sign(x) {\n    return typeof x === 'number' ? x ? x &lt; 0 ? -1 : 1 : x === x ? 0 : NaN : NaN;\n}\n</code></pre>\n\n<hr>\n\n<h2>Results</h2>\n\n<p>For now we have these solutions:</p>\n\n<hr>\n\n<p><strong>1.</strong> Obvious and fast</p>\n\n<pre><code>function sign(x) { return x &gt; 0 ? 1 : x &lt; 0 ? -1 : 0; }\n</code></pre>\n\n<hr>\n\n<p><strong>1.1.</strong> Modification from <a href=\"/users/854291/kbec\">kbec</a> - one type cast less, more performant, shorter <strong>[fastest]</strong></p>\n\n<pre><code>function sign(x) { return x ? x &lt; 0 ? -1 : 1 : 0; }\n</code></pre>\n\n<p><em>caution:</em> <code>sign(\"0\") -&gt; 1</code></p>\n\n<hr>\n\n<p><strong>2.</strong> Elegant, short, not so fast <strong>[slowest]</strong></p>\n\n<pre><code>function sign(x) { return x &amp;&amp; x / Math.abs(x); }\n</code></pre>\n\n<p><em>caution:</em> <code>sign(+-Infinity) -&gt; NaN</code>, <code>sign(\"0\") -&gt; NaN</code></p>\n\n<p>As of <code>Infinity</code> is a legal number in JS this solution doesn't seem fully correct.</p>\n\n<hr>\n\n<p><strong>3.</strong> The art... but very slow <strong>[slowest]</strong></p>\n\n<pre><code>function sign(x) { return (x &gt; 0) - (x &lt; 0); }\n</code></pre>\n\n<hr>\n\n<p><strong>4.</strong> Using bit-shift<br>\n    <em>fast, but <code>sign(-Infinity) -&gt; 0</code></em></p>\n\n<pre><code>function sign(x) { return (x &gt;&gt; 31) + (x &gt; 0 ? 1 : 0); }\n</code></pre>\n\n<hr>\n\n<p><strong>5.</strong> Type-safe <strong>[megafast]</strong></p>\n\n<p><strong>!</strong> Seems like browsers (especially chrome's v8) make some magic optimizations and this solution turns out to be much more performant than others, even than (1.1) despite it contains 2 extra operations and logically never can't be faster.</p>\n\n<pre><code>function sign(x) {\n    return typeof x === 'number' ? x ? x &lt; 0 ? -1 : 1 : x === x ? 0 : NaN : NaN;\n}\n</code></pre>\n\n<hr>\n\n<h2>Tools</h2>\n\n<ul>\n<li><a href=\"http://jsperf.com/signs/3\">jsperf</a> preformance tests;</li>\n<li><a href=\"http://jsfiddle.net/disfated/NJFSa/\">fiddle</a> - type-cast tests;</li>\n</ul>\n\n<p>Improvements are welcome!</p>\n\n<hr>\n\n<h3>[Offtopic] Accepted answer</h3>\n\n<ul>\n<li><p><a href=\"/a/14418335/489553\">Andrey Tarantsov</a> - +100 for the art, but sadly it is about 5 times slower than the obvious approach</p></li>\n<li><p><a href=\"/a/7624945/489553\">Frédéric Hamidi</a> - somehow the most upvoted answer (for the time writing) and it's kinda cool, but it's definitely not how things should be done, imho. Also it doesn't correctly handle Infinity numbers, which are also numbers, you know.</p></li>\n<li><p><a href=\"/a/9079549/489553\">kbec</a> - is an improvement of the obvious solution. Not that revolutionary, but taking all together I consider this approach the best. Vote for him :)</p></li>\n</ul>\n    ","a":"\n<p>More elegant version of fast solution:</p>\n\n<pre><code>var sign = number?number&lt;0?-1:1:0\n</code></pre>\n\n<p>Regards</p>\n    "},{"t":"Does the range-based 'for' loop deprecate many simple algorithms?","l":"http://stackoverflow.com/questions/14258768/does-the-range-based-for-loop-deprecate-many-simple-algorithms","q":"\n\n<p>Algorithm solution:</p>\n\n<pre><code>std::generate(numbers.begin(), numbers.end(), rand);\n</code></pre>\n\n<p>Range-based for-loop solution:</p>\n\n<pre><code>for (int&amp; x : numbers) x = rand();\n</code></pre>\n\n<p>Why would I want to use the more verbose <code>std::generate</code> over range-based for-loops in C++11?</p>\n    ","a":"\n<p>The first version</p>\n\n<pre><code>std::generate(numbers.begin(), numbers.end(), rand);\n</code></pre>\n\n<p>tells us that you want to generate a sequence of values.</p>\n\n<p>In the second version the reader will have to figure that out himself.</p>\n\n<p>Saving on typing is usually suboptimal, as it is most often lost in reading time. Most code is read a lot more than it is typed. </p>\n    "},{"t":".NET - How can you split a “caps” delimited string into an array?","l":"http://stackoverflow.com/questions/155303/net-how-can-you-split-a-caps-delimited-string-into-an-array","q":"\n\n<p>How do I go from this string: \"ThisIsMyCapsDelimitedString\"</p>\n\n<p>...to this string: \"This Is My Caps Delimited String\"</p>\n\n<p>Fewest lines of code in VB.net is preferred but C# is also welcome.</p>\n\n<p>Cheers!</p>\n    ","a":"\n<p>I made this a while ago. It matches each component of a CamelCase name.</p>\n\n<pre><code>/([A-Z]+(?=$|[A-Z][a-z])|[A-Z]?[a-z]+)/g\n</code></pre>\n\n<p>For example:</p>\n\n<pre><code>\"SimpleHTTPServer\" =&gt; [\"Simple\", \"HTTP\", \"Server\"]\n\"camelCase\" =&gt; [\"camel\", \"Case\"]\n</code></pre>\n\n<p>To convert that to just insert spaces between the words:</p>\n\n<pre><code>Regex.Replace(s, \"([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))\", \"$1 \")\n</code></pre>\n\n<p><strong>Edit:</strong> Allowing initial lowercase letters, (i.e. <code>\"lowerCamelCase\"</code>), as Drew Noakes pointed out. The only change is a <code>\"?\"</code> after after the last <code>\"[A-Z]\"</code>.</p>\n    "},{"t":"How do I use node.js Crypto to create a HMAC-SHA1 hash?","l":"http://stackoverflow.com/questions/7480158/how-do-i-use-node-js-crypto-to-create-a-hmac-sha1-hash","q":"\n\n<p>I want to create a hash of \"<code>I love cupcakes</code>\" (signed with the key \"abcdeg\")</p>\n\n<p>How can I create that hash , using Node.js Crypto?</p>\n    ","a":"\n<p>Documentation for crypto: <a href=\"http://nodejs.org/api/crypto.html\">http://nodejs.org/api/crypto.html</a></p>\n\n<pre><code>var crypto = require('crypto')\n  , text = 'I love cupcakes'\n  , key = 'abcdeg'\n  , hash\n\nhash = crypto.createHmac('sha1', key).update(text).digest('hex')\n</code></pre>\n    "},{"t":"Algorithm to find top 10 search terms","l":"http://stackoverflow.com/questions/3260653/algorithm-to-find-top-10-search-terms","q":"\n\n<p>I'm currently preparing for an interview, and it reminded me of a question I was once asked in a previous interview that went something like this:</p>\n\n<p>\"You have been asked to design some software to continuously display the top 10 search terms on Google. You are given access to a feed that provides an endless real-time stream of search terms currently being searched on Google. Describe what algorithm and data structures you would use to implement this. You are to design two variations: </p>\n\n<p>(i) Display the top 10 search terms of all time (i.e. since you started reading the feed). </p>\n\n<p>(ii) Display only the top 10 search terms for the past month, updated hourly. </p>\n\n<p>You can use an approximation to obtain the top 10 list, but you must justify your choices.\"\n<br>I bombed in this interview and still have really no idea how to implement this. </p>\n\n<p>The first part asks for the 10 most frequent items in a continuously growing sub-sequence of an infinite list. I looked into selection algorithms, but couldn't find any online versions to solve this problem. </p>\n\n<p>The second part uses a finite list, but due to the large amount of data being processed, you can't really store the whole month of search terms in memory and calculate a histogram every hour. </p>\n\n<p>The problem is made more difficult by the fact that the top 10 list is being continuously updated, so somehow you need to be calculating your top 10 over a sliding window.</p>\n\n<p>Any ideas?</p>\n    ","a":"\n<p>Well, looks like an awful lot of data, with a perhaps prohibitive cost to store all frequencies. <strong>When the amount of data is so large</strong> that we cannot hope to store it all, we enter the domain of <strong>data stream algorithms</strong>.</p>\n\n<p>Useful book in this area:\n<a href=\"http://books.google.com/books?id=415loiMd_c0C&amp;lpg=PP1&amp;dq=Data%20Streams%3a%20Algorithms%20and%20Application&amp;hl=el&amp;pg=PP1#v=onepage&amp;q=Data%20Streams%3A%20Algorithms%20and%20Application&amp;f=false\">Muthukrishnan - \"Data Streams: Algorithms and Applications\"</a></p>\n\n<p>Closely related reference to the problem at hand which I picked from the above:\n<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.8594&amp;rep=rep1&amp;type=pdf\">Manku, Motwani - \"Approximate Frequency Counts over Data Streams\" [pdf]</a></p>\n\n<p>By the way, Motwani, of Stanford, (edit) was an author of the very important <a href=\"http://rads.stackoverflow.com/amzn/click/0521474655\">\"Randomized Algorithms\"</a> book. <strike><strong>The 11th chapter of this book deals with this problem</strong></strike>. <strong>Edit:</strong> Sorry, bad reference, that particular chapter is on a different problem. After checking, I instead recommend <a href=\"http://books.google.com/books?id=415loiMd_c0C&amp;lpg=PP1&amp;dq=Data%20Streams%3A%20Algorithms%20and%20Application&amp;hl=el&amp;pg=PA33#v=onepage&amp;q&amp;f=false\">section 5.1.2 of <em>Muthukrishnan's</em> book</a>, available online.</p>\n\n<p>Heh, nice interview question. </p>\n    "},{"t":"Longest equally-spaced subsequence","l":"http://stackoverflow.com/questions/18159911/longest-equally-spaced-subsequence","q":"\n\n<p>I have a million integers in sorted order and I would like to find the longest subsequence where the difference between consecutive pairs is equal. For example </p>\n\n<pre><code>1, 4, 5, 7, 8, 12\n</code></pre>\n\n<p>has a subsequence </p>\n\n<pre><code>   4,       8, 12\n</code></pre>\n\n<p>My naive method is greedy and just checks how far you can extend a subsequence from each point. This takes <code>O(n²)</code> time per point it seems.</p>\n\n<p>Is there a faster way to solve this problem?</p>\n\n<p><strong>Update.</strong> I will test the code given in the answers as soon as possible (thank you). However it is clear already that using n^2 memory will not work.   So far there is no code that terminates with the input as <code>[random.randint(0,100000) for r in xrange(200000)]</code> .</p>\n\n<p><strong>Timings.</strong>  I tested with the following input data on my 32 bit system.</p>\n\n<pre><code>a= [random.randint(0,10000) for r in xrange(20000)] \na.sort()\n</code></pre>\n\n<ul>\n<li>The dynamic programming method of ZelluX uses 1.6G of RAM and takes 2 minutes and 14 seconds.  With pypy it takes only 9 seconds! However it crashes with a memory error on large inputs.</li>\n<li>The O(nd) time method of Armin took 9 seconds with pypy but only 20MB of RAM. Of course this would be much worse if the range were much larger.  The low memory usage meant I could also test it with a= [random.randint(0,100000) for r in xrange(200000)] but it didn't finish in the few minutes I gave it with pypy.</li>\n</ul>\n\n<p>In order to be able to test the method of Kluev's I reran with </p>\n\n<pre><code>a= [random.randint(0,40000) for r in xrange(28000)] \na = list(set(a))\na.sort()\n</code></pre>\n\n<p>to make a list of length roughly <code>20000</code>.  All timings with pypy</p>\n\n<ul>\n<li>ZelluX, 9 seconds</li>\n<li>Kluev, 20 seconds</li>\n<li>Armin, 52 seconds</li>\n</ul>\n\n<p>It seems that if the ZelluX method could be made linear space it would be the clear winner.</p>\n    ","a":"\n<p><strong>Update:</strong> First algorithm described here is obsoleted by <a href=\"http://stackoverflow.com/a/18247391/1009831\">Armin Rigo's second answer</a>, which is much simpler and more efficient. But both these methods have one disadvantage. They need many hours to find the result for one million integers. So I tried two more variants (see second half of this answer) where the range of input integers is assumed to be limited. Such limitation allows much faster algorithms. Also I tried to optimize Armin Rigo's code. See my benchmarking results at the end.</p>\n\n<hr>\n\n<p>Here is an idea of algorithm using O(N) memory. Time complexity is O(N<sup>2</sup> log N), but may be decreased to O(N<sup>2</sup>).</p>\n\n<p>Algorithm uses the following data structures:</p>\n\n<ol>\n<li><code>prev</code>: array of indexes pointing to previous element of (possibly incomplete) subsequence.</li>\n<li><code>hash</code>: hashmap with key = difference between consecutive pairs in subsequence and value = two other hashmaps. For these other hashmaps: key = starting/ending index of the subsequence, value = pair of (subsequence length, ending/starting index of the subsequence).</li>\n<li><code>pq</code>: priority queue for all possible \"difference\" values for subsequences stored in <code>prev</code> and <code>hash</code>.</li>\n</ol>\n\n<p>Algorithm:</p>\n\n<ol>\n<li>Initialize <code>prev</code> with indexes <code>i-1</code>. Update <code>hash</code> and <code>pq</code> to register all (incomplete) subsequences found on this step and their \"differences\".</li>\n<li>Get (and remove) smallest \"difference\" from <code>pq</code>. Get corresponding record from <code>hash</code> and scan one of second-level hash maps. At this time all subsequences with given \"difference\" are complete. If second-level hash map contains subsequence length better than found so far, update the best result.</li>\n<li>In the array <code>prev</code>: for each element of any sequence found on step #2, decrement index and update <code>hash</code> and possibly <code>pq</code>. While updating <code>hash</code>, we could perform one of the following operations: add a new subsequence of length 1, or grow some existing subsequence by 1, or merge two existing subsequences.</li>\n<li>Remove hash map record found on step #2.</li>\n<li>Continue from step #2 while <code>pq</code> is not empty.</li>\n</ol>\n\n<p>This algorithm updates O(N) elements of <code>prev</code> O(N) times each. And each of these updates may require to add a new \"difference\" to <code>pq</code>. All this means time complexity of O(N<sup>2</sup> log N) if we use simple heap implementation for <code>pq</code>. To decrease it to O(N<sup>2</sup>) we might use more advanced priority queue implementations. Some of the possibilities are listed on this page: <a href=\"http://www.theturingmachine.com/algorithms/heaps.html\">Priority Queues</a>.</p>\n\n<p>See corresponding Python code on <a href=\"http://ideone.com/h8oTYv\">Ideone</a>. This code does not allow duplicate elements in the list. It is possible to fix this, but it would be a good optimization anyway to remove duplicates (and to find the longest subsequence beyond duplicates separately).</p>\n\n<p>And <a href=\"http://ideone.com/bW8meY\">the same code after a little optimization</a>. Here search is terminated as soon as subsequence length multiplied by possible subsequence \"difference\" exceeds source list range.</p>\n\n<hr>\n\n<p>Armin Rigo's code is simple and pretty efficient. But in some cases it does some extra computations that may be avoided. Search may be terminated as soon as subsequence length multiplied by possible subsequence \"difference\" exceeds source list range:</p>\n\n<pre><code>def findLESS(A):\n  Aset = set(A)\n  lmax = 2\n  d = 1\n  minStep = 0\n\n  while (lmax - 1) * minStep &lt;= A[-1] - A[0]:\n    minStep = A[-1] - A[0] + 1\n    for j, b in enumerate(A):\n      if j+d &lt; len(A):\n        a = A[j+d]\n        step = a - b\n        minStep = min(minStep, step)\n        if a + step in Aset and b - step not in Aset:\n          c = a + step\n          count = 3\n          while c + step in Aset:\n            c += step\n            count += 1\n          if count &gt; lmax:\n            lmax = count\n    d += 1\n\n  return lmax\n\nprint(findLESS([1, 4, 5, 7, 8, 12]))\n</code></pre>\n\n<hr>\n\n<p>If range of integers in source data (M) is small, a simple algorithm is possible with O(M<sup>2</sup>) time and O(M) space:</p>\n\n<pre><code>def findLESS(src):\n  r = [False for i in range(src[-1]+1)]\n  for x in src:\n    r[x] = True\n\n  d = 1\n  best = 1\n\n  while best * d &lt; len(r):\n    for s in range(d):\n      l = 0\n\n      for i in range(s, len(r), d):\n        if r[i]:\n          l += 1\n          best = max(best, l)\n        else:\n          l = 0\n\n    d += 1\n\n  return best\n\n\nprint(findLESS([1, 4, 5, 7, 8, 12]))\n</code></pre>\n\n<p>It is similar to the first method by Armin Rigo, but it doesn't use any dynamic data structures. I suppose source data has no duplicates. And (to keep the code simple) I also suppose that minimum input value is non-negative and close to zero.</p>\n\n<hr>\n\n<p>Previous algorithm may be improved if instead of the array of booleans we use a bitset data structure and bitwise operations to process data in parallel. The code shown below implements bitset as a built-in Python integer. It has the same assumptions: no duplicates, minimum input value is non-negative and close to zero. Time complexity is O(M<sup>2</sup> * log L) where L is the length of optimal subsequence, space complexity is O(M):</p>\n\n<pre><code>def findLESS(src):\n  r = 0\n  for x in src:\n    r |= 1 &lt;&lt; x\n\n  d = 1\n  best = 1\n\n  while best * d &lt; src[-1] + 1:\n    c = best\n    rr = r\n\n    while c &amp; (c-1):\n      cc = c &amp; -c\n      rr &amp;= rr &gt;&gt; (cc * d)\n      c &amp;= c-1\n\n    while c != 1:\n      c = c &gt;&gt; 1\n      rr &amp;= rr &gt;&gt; (c * d)\n\n    rr &amp;= rr &gt;&gt; d\n\n    while rr:\n      rr &amp;= rr &gt;&gt; d\n      best += 1\n\n    d += 1\n\n  return best\n</code></pre>\n\n<hr>\n\n<p><strong>Benchmarks:</strong></p>\n\n<p>Input data (about 100000 integers) is generated this way:</p>\n\n<pre><code>random.seed(42)\ns = sorted(list(set([random.randint(0,200000) for r in xrange(140000)])))\n</code></pre>\n\n<p>And for fastest algorithms I also used the following data (about 1000000 integers):</p>\n\n<pre><code>s = sorted(list(set([random.randint(0,2000000) for r in xrange(1400000)])))\n</code></pre>\n\n<p>All results show time in seconds:</p>\n\n<pre><code>Size:                         100000   1000000\nSecond answer by Armin Rigo:     634         ?\nBy Armin Rigo, optimized:         64     &gt;5000\nO(M^2) algorithm:                 53      2940\nO(M^2*L) algorithm:                7       711\n</code></pre>\n    "},{"t":"Equation (expression) parser with precedence?","l":"http://stackoverflow.com/questions/28256/equation-expression-parser-with-precedence","q":"\n\n<p>I've developed an equation parser using a simple stack algorithm that will handle binary (+, -, |, &amp;, *, /, etc) operators, unary (!) operators, and parenthesis.</p>\n\n<p>Using this method, however, leaves me with everything having the same precedence - it's evaluated left to right regardless of operator, although precedence can be enforced using parenthesis.</p>\n\n<p>So right now \"1+11*5\" returns 60, not 56 as one might expect.</p>\n\n<p>While this is suitable for the current project, I want to have a general purpose routine I can use for later projects.</p>\n\n<p><strong>Edited for clarity:</strong></p>\n\n<p>What is a good algorithm for parsing equations with precedence?</p>\n\n<p>I'm interested in something simple to implement and understand that I can code myself to avoid licensing issues with available code.</p>\n\n<p><strong>Grammar:</strong></p>\n\n<p>I don't understand the grammar question - I've written this by hand.  It's simple enough that I don't see the need for YACC or Bison.  I merely need to calculate strings with equations such as \"2+3 * (42/13)\".</p>\n\n<p><strong>Language:</strong></p>\n\n<p>I'm doing this in C, but I'm interested in an algorithm, not a language specific solution.  C is low level enough that it'll be easy to convert to another language should the need arise.</p>\n\n<p><strong>Code Example</strong></p>\n\n<p>I posted the <a href=\"http://www.ubasics.com/simple%5Fc%5Fequation%5Fparser\">test code for the simple expression parser</a> I was talking about above.  The project requirements altered and so I never needed to optimize the code for performance or space as it wasn't incorporated into the project.  It's in the original verbose form, and should be readily understandable.  If I do anything further with it in terms of operator precedence, I'll probably choose <a href=\"http://stackoverflow.com/questions/28256/equation-expression-parser-with-precedence/783132#783132\">the macro hack</a> because it matches the rest of the program in simplicity.  If I ever use this in a real project, though, I'll be going for a more compact/speedy parser.</p>\n\n<p><strong>Related question</strong>  </p>\n\n<blockquote>\n  <p><a href=\"http://stackoverflow.com/questions/114586/smart-design-of-a-math-parser\">Smart design of a math parser?</a></p>\n</blockquote>\n\n\n    ","a":"\n<h3>The hard way</h3>\n\n<p>You want a <a href=\"http://en.wikipedia.org/wiki/Recursive_descent_parser\">recursive descent parser</a>.</p>\n\n<p>To get precedence you need to think recursively, for example, using your sample string, </p>\n\n<pre><code>1+11*5\n</code></pre>\n\n<p>to do this manually, you would have to read the <code>1</code>, then see the plus and start a whole new recursive parse \"session\" starting with <code>11</code>... and make sure to parse the <code>11 * 5</code> into its own factor, yielding a parse tree with <code>1 + (11 * 5)</code>.</p>\n\n<p>This all feels so painful even to attempt to explain, especially with the added powerlessness of C. See, after parsing the 11, if the * was actually a + instead, you would have to abandon the attempt at making a term and instead parse the <code>11</code> itself as a factor. My head is already exploding. It's possible with the recursive decent strategy, but there is a better way...</p>\n\n<h3>The easy (right) way</h3>\n\n<p>If you use a GPL tool like Bison, you probably don't need to worry about licensing issues since the C code generated by bison is not covered by the GPL (IANAL but I'm pretty sure GPL tools don't force the GPL on generated code/binaries; for example Apple compiles code like say, Aperture with GCC and they sell it without having to GPL said code).</p>\n\n<p><a href=\"http://www.gnu.org/software/bison\">Download Bison</a> (or something equivalent, ANTLR, etc.).</p>\n\n<p>There is usually some sample code that you can just run bison on and get your desired C code that demonstrates this four function calculator:</p>\n\n<p><a href=\"http://www.gnu.org/software/bison/manual/html_mono/bison.html#Infix-Calc\">http://www.gnu.org/software/bison/manual/html_mono/bison.html#Infix-Calc</a></p>\n\n<p>Look at the generated code, and see that this is not as easy as it sounds. Also, the advantages of using a tool like Bison are 1) you learn something (especially if you read the Dragon book and learn about grammars), 2) you avoid <a href=\"http://en.wikipedia.org/wiki/Not_Invented_Here\">NIH</a> trying to reinvent the wheel. With a real parser-generator tool, you actually have a hope at scaling up later, showing other people you know that parsers are the domain of parsing tools.</p>\n\n<hr>\n\n<p><strong>Update:</strong></p>\n\n<p>People here have offered much sound advice. My only warning against skipping the parsing tools or just using the Shunting Yard algorithm or a hand rolled recursive decent parser is that little toy languages<sup><a href=\"http://docs.garagegames.com/tgea/official/content/documentation/Scripting%20Reference/Introduction/TorqueScript.html\">1</a></sup> may someday turn into big actual languages with functions (sin, cos, log) and variables, conditions and for loops.</p>\n\n<p>Flex/Bison may very well be overkill for a small, simple interpreter, but a one off parser+evaluator may cause trouble down the line when changes need to be made or features need to be added. Your situation will vary and you will need to use your judgement; just don't <a href=\"http://docs.garagegames.com/tgea/official/content/documentation/Scripting%20Reference/Introduction/TorqueScript.html\">punish other people for your sins</a> <sup>[2]</sup> and build a less than adequate tool.</p>\n\n<p><strong>My favorite tool for parsing</strong></p>\n\n<p>The best tool in the world for the job is the <a href=\"http://book.realworldhaskell.org/read/using-parsec.html\">Parsec</a> library (for recursive decent parsers) which comes with the programming language Haskell. It looks a lot like BNF, or like some specialized tool or domain specific language for parsing (sample code [3]), but it is in fact just a regular library in Haskell, meaning that it compiles in the same build step as the rest of your Haskell code, and you can write arbitrary Haskell code and call that within your parser, and you can mix and match other libraries <em>all in the same code</em>. (Embedding a parsing language like this in a language other than Haskell results in loads of syntactic cruft, by the way. I did this in C# and it works quite well but it is not so pretty and succinct.)</p>\n\n<p><strong>Notes:</strong></p>\n\n<p><a href=\"http://docs.garagegames.com/tgea/official/content/documentation/Scripting%20Reference/Introduction/TorqueScript.html\">1</a> Richard Stallman says, in <a href=\"http://web.cecs.pdx.edu/~trent/gnu/tcl-not\">Why you should not use Tcl</a></p>\n\n<blockquote>\n  <p>The principal lesson of Emacs is that\n  a language for extensions should not\n  be a mere \"extension language\".  It\n  should be a real programming language,\n  designed for writing and maintaining\n  substantial programs. Because people\n  will want to do that!</p>\n</blockquote>\n\n<p>[2] Yes, I am forever scarred from using that \"language\".</p>\n\n<p>Also note that when I submitted this entry, the preview was correct, but <strong>SO's less than adequate parser ate my close anchor tag on the first paragraph</strong>, proving that parsers are not something to be trifled with because if you use regexes and one off hacks <strong>you will probably get something subtle and small wrong</strong>.</p>\n\n<p>[3] Snippet of a Haskell parser using Parsec: a four function calculator extended with exponents, parentheses, whitespace for multiplication, and constants (like pi and e).</p>\n\n<pre class=\"lang-hs prettyprint-override\"><code>aexpr   =   expr `chainl1` toOp\nexpr    =   optChainl1 term addop (toScalar 0)\nterm    =   factor `chainl1` mulop\nfactor  =   sexpr  `chainr1` powop\nsexpr   =   parens aexpr\n        &lt;|&gt; scalar\n        &lt;|&gt; ident\n\npowop   =   sym \"^\" &gt;&gt;= return . (B Pow)\n        &lt;|&gt; sym \"^-\" &gt;&gt;= return . (\\x y -&gt; B Pow x (B Sub (toScalar 0) y))\n\ntoOp    =   sym \"-&gt;\" &gt;&gt;= return . (B To)\n\nmulop   =   sym \"*\" &gt;&gt;= return . (B Mul)\n        &lt;|&gt; sym \"/\" &gt;&gt;= return . (B Div)\n        &lt;|&gt; sym \"%\" &gt;&gt;= return . (B Mod)\n        &lt;|&gt;             return . (B Mul)\n\naddop   =   sym \"+\" &gt;&gt;= return . (B Add) \n        &lt;|&gt; sym \"-\" &gt;&gt;= return . (B Sub)\n\nscalar = number &gt;&gt;= return . toScalar\n\nident  = literal &gt;&gt;= return . Lit\n\nparens p = do\n             lparen\n             result &lt;- p\n             rparen\n             return result\n</code></pre>\n    "},{"t":"Image Segmentation using Mean Shift explained","l":"http://stackoverflow.com/questions/4831813/image-segmentation-using-mean-shift-explained","q":"\n\n<p>Could anyone please help me understand how Mean Shift segmentation actually works?  </p>\n\n<p>Here is a 8x8 matrix that I just made up</p>\n\n<pre><code>  103  103  103  103  103  103  106  104   \n  103  147  147  153  147  156  153  104   \n  107  153  153  153  153  153  153  107   \n  103  153  147  96   98   153  153  104   \n  107  156  153  97   96   147  153  107   \n  103  153  153  147  156  153  153  101   \n  103  156  153  147  147  153  153  104   \n  103  103  107  104  103  106  103  107\n</code></pre>\n\n<p>Using the matrix above is it possible to explain how Mean Shift segmentation would separate the 3 different levels of numbers?</p>\n    ","a":"\n<p><b>The basics first:</b></p>\n\n<p>The Mean Shift segmentation is a local homogenization technique that is very useful for damping shading or tonality differences in localized objects.\nAn example is better than many words:  </p>\n\n<p><img src=\"http://i.stack.imgur.com/jQyzF.png\" alt=\"enter image description here\">  </p>\n\n<p>Action:<strong>replaces each pixel with the mean of the pixels in a range-r neighborhood and whose value is within a distance d.</strong></p>\n\n<p>The Mean Shift takes usually 3 inputs:  </p>\n\n<ol>\n<li>A distance function for measuring distances between pixels. Usually the Euclidean distance, but any other well defined distance function could be used. The <a href=\"http://en.wikipedia.org/wiki/Taxicab_geometry\">Manhattan\nDistance</a> is another useful choice sometimes.</li>\n<li>A radius. All pixels within this radius (measured according the above distance) will be accounted for the calculation.</li>\n<li>A value difference. From all pixels inside radius r, we will take only those whose values are within this difference for calculating the mean </li>\n</ol>\n\n<p>Please note that the algorithm is not well defined at the borders, so different implementations will give you different results there.  </p>\n\n<p>I'll NOT discuss the gory mathematical details here, as they are impossible to show without proper mathematical notation, not available in StackOverflow, and also because they can be found <a href=\"http://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\">from good sources elsewhere</a>.</p>\n\n<p>Let's look at the center of your matrix:  </p>\n\n<pre><code>153  153  153  153 \n147  96   98   153 \n153  97   96   147   \n153  153  147  156  \n</code></pre>\n\n<p>With reasonable choices for radius and distance, the four center pixels will get the value of 97 (their mean) and will be different form the adjacent pixels.</p>\n\n<p>Let's calculate it in <a href=\"http://www.wolfram.com/mathematica/\">Mathematica</a>. Instead of showing the actual numbers, we will display a color coding, so it's easier to understand what is happening:  </p>\n\n<p>The color coding for your matrix is:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/rCDFA.png\"><img src=\"http://i.stack.imgur.com/rCDFA.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then we take a reasonable Mean Shift:  </p>\n\n<pre><code>MeanShiftFilter[a, 3, 3]\n</code></pre>\n\n<p>And we get:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/tS9Rg.png\"><img src=\"http://i.stack.imgur.com/tS9Rg.png\" alt=\"enter image description here\"></a></p>\n\n<p>Where all center elements are equal (to 97, BTW).  </p>\n\n<p>You may iterate several times with Mean Shift, trying to get a more homogeneous coloring. After a few iterations, you arrive at a stable non-isotropic configuration:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/laGV7.png\"><img src=\"http://i.stack.imgur.com/laGV7.png\" alt=\"enter image description here\"></a></p>\n\n<p>At this time, it should be clear that you can't select how many \"colors\" you get after applying Mean Shift. So, let's show how to do it, because that is the second part of your question.  </p>\n\n<p>What you need to be able to set the number of output clusters in advance is something like <a href=\"http://en.wikipedia.org/wiki/K-means_clustering\">Kmeans clustering</a>.  </p>\n\n<p>It runs this way for your matrix:  </p>\n\n<pre><code>b = ClusteringComponents[a, 3]\n\n{{1, 1, 1, 1, 1, 1, 1, 1}, \n {1, 2, 2, 3, 2, 3, 3, 1}, \n {1, 3, 3, 3, 3, 3, 3, 1}, \n {1, 3, 2, 1, 1, 3, 3, 1}, \n {1, 3, 3, 1, 1, 2, 3, 1}, \n {1, 3, 3, 2, 3, 3, 3, 1}, \n {1, 3, 3, 2, 2, 3, 3, 1}, \n {1, 1, 1, 1, 1, 1, 1, 1}}  \n</code></pre>\n\n<p>Or:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/IS5D1.png\"><img src=\"http://i.stack.imgur.com/IS5D1.png\" alt=\"enter image description here\"></a></p>\n\n<p>Which is very similar to our previous result, but as you can see, now we have only three output levels.</p>\n\n<p>HTH!</p>\n    "},{"t":"How do Trigonometric functions work?","l":"http://stackoverflow.com/questions/345085/how-do-trigonometric-functions-work","q":"\n\n<p>So in high school math, and probably college, we are taught how to use trig functions, what they do, and what kinds of problems they solve. But they have always been presented to me as a black box. If you need the Sine or Cosine of something, you hit the sin or cos button on your calculator and you're set. Which is fine.</p>\n\n<p>What I'm wondering is how trigonometric functions are typically implemented.</p>\n    ","a":"\n<p>First, you have to do some sort of range reduction.  Trig functions are periodic, so you need to reduce arguments down to a standard interval.  For starters, you could reduce angles to be between 0 and 360 degrees. But by using a few identities, you realize you could get by with less.  If you calculate sines and cosines for angles between 0 and 45 degrees, you can bootstrap your way to calculating all trig functions for all angles.</p>\n\n<p>Once you've reduced your argument, most chips use a <a href=\"http://en.wikipedia.org/wiki/Cordic\">CORDIC</a> algorithm to compute the sines and cosines. You may hear people say that computers use Taylor series.  That sounds reasonable, but it's not true.  The CORDIC algorithms are much better suited to efficient <em>hardware</em> implementation.  (<em>Software</em> libraries may use Taylor series, say on hardware that doesn't support trig functions.)  There may be some additional processing, using the CORDIC algorithm to get fairly good answers but then doing something else to improve accuracy.  </p>\n\n<p>There are some refinements to the above.  For example, for very small angles theta (in radians), sin(theta) = theta to all the precision you have, so it's more efficient to simply return theta than to use some other algorithm.  So in practice there is a lot of special case logic to squeeze out all the performance and accuracy possible.  Chips with smaller markets may not go to as much optimization effort.</p>\n    "},{"t":"How to convert floats to human-readable fractions?","l":"http://stackoverflow.com/questions/95727/how-to-convert-floats-to-human-readable-fractions","q":"\n\n<p>Let's say we have 0.33, we need to output \"1/3\". <br>\nIf we have \"0.4\", we need to output \"2/5\".</p>\n\n<p>The idea is to make it human-readable to make the user understand \"x parts out of y\" as a better way of understanding data.</p>\n\n<p>I know that percentages is a good substitute but I was wondering if there was a simple way to do this?</p>\n    ","a":"\n<p>I have found David Eppstein's <a href=\"http://www.ics.uci.edu/~eppstein/numth/frap.c\">find rational approximation to given real number</a> C code to be exactly what you are asking for.  Its based on the theory of continued fractions and very fast and fairly compact.</p>\n\n<p>I have used versions of this customized for specific numerator and denominator limits.</p>\n\n<pre><code>/*\n** find rational approximation to given real number\n** David Eppstein / UC Irvine / 8 Aug 1993\n**\n** With corrections from Arno Formella, May 2008\n**\n** usage: a.out r d\n**   r is real number to approx\n**   d is the maximum denominator allowed\n**\n** based on the theory of continued fractions\n** if x = a1 + 1/(a2 + 1/(a3 + 1/(a4 + ...)))\n** then best approximation is found by truncating this series\n** (with some adjustments in the last term).\n**\n** Note the fraction can be recovered as the first column of the matrix\n**  ( a1 1 ) ( a2 1 ) ( a3 1 ) ...\n**  ( 1  0 ) ( 1  0 ) ( 1  0 )\n** Instead of keeping the sequence of continued fraction terms,\n** we just keep the last partial product of these matrices.\n*/\n\n#include &lt;stdio.h&gt;\n\nmain(ac, av)\nint ac;\nchar ** av;\n{\n    double atof();\n    int atoi();\n    void exit();\n\n    long m[2][2];\n    double x, startx;\n    long maxden;\n    long ai;\n\n    /* read command line arguments */\n    if (ac != 3) {\n        fprintf(stderr, \"usage: %s r d\\n\",av[0]);  // AF: argument missing\n        exit(1);\n    }\n    startx = x = atof(av[1]);\n    maxden = atoi(av[2]);\n\n    /* initialize matrix */\n    m[0][0] = m[1][1] = 1;\n    m[0][1] = m[1][0] = 0;\n\n    /* loop finding terms until denom gets too big */\n    while (m[1][0] *  ( ai = (long)x ) + m[1][1] &lt;= maxden) {\n        long t;\n        t = m[0][0] * ai + m[0][1];\n        m[0][1] = m[0][0];\n        m[0][0] = t;\n        t = m[1][0] * ai + m[1][1];\n        m[1][1] = m[1][0];\n        m[1][0] = t;\n        if(x==(double)ai) break;     // AF: division by zero\n        x = 1/(x - (double) ai);\n        if(x&gt;(double)0x7FFFFFFF) break;  // AF: representation failure\n    } \n\n    /* now remaining x is between 0 and 1/ai */\n    /* approx as either 0 or 1/m where m is max that will fit in maxden */\n    /* first try zero */\n    printf(\"%ld/%ld, error = %e\\n\", m[0][0], m[1][0],\n           startx - ((double) m[0][0] / (double) m[1][0]));\n\n    /* now try other possibility */\n    ai = (maxden - m[1][1]) / m[1][0];\n    m[0][0] = m[0][0] * ai + m[0][1];\n    m[1][0] = m[1][0] * ai + m[1][1];\n    printf(\"%ld/%ld, error = %e\\n\", m[0][0], m[1][0],\n           startx - ((double) m[0][0] / (double) m[1][0]));\n}\n</code></pre>\n    "},{"t":"Rounding DateTime objects","l":"http://stackoverflow.com/questions/1393696/rounding-datetime-objects","q":"\n\n<p>I want to round dates/times to the nearest interval for a charting application. I'd like an extension method signature like follows so that the rounding can be acheived for any level of accuracy:</p>\n\n<pre><code>static DateTime Round(this DateTime date, TimeSpan span);\n</code></pre>\n\n<p>The idea is that if I pass in a timespan of ten minutes, it will round to the nearest ten minute interval. I can't get my head around the implementation and am hoping one of you will have written or used something similar before.</p>\n\n<p>I think either a floor, ceiling or nearest implementation is fine.</p>\n\n<p>Any ideas?</p>\n\n<p>Edit:  Thanks to @tvanfosson &amp; @ShuggyCoUk, the implementation looks like this:</p>\n\n<pre><code>public static class DateExtensions {\n    public static DateTime Round(this DateTime date, TimeSpan span) {\n        long ticks = (date.Ticks + (span.Ticks / 2) + 1)/ span.Ticks;\n        return new DateTime(ticks * span.Ticks);\n    }\n    public static DateTime Floor(this DateTime date, TimeSpan span) {\n        long ticks = (date.Ticks / span.Ticks);\n        return new DateTime(ticks * span.Ticks);\n    }\n    public static DateTime Ceil(this DateTime date, TimeSpan span) {\n        long ticks = (date.Ticks + span.Ticks - 1) / span.Ticks;\n        return new DateTime(ticks * span.Ticks);\n    }\n}\n</code></pre>\n\n<p>And is called like so:</p>\n\n<pre><code>DateTime nearestHour = DateTime.Now.Round(new TimeSpan(1,0,0));\nDateTime minuteCeiling = DateTime.Now.Ceil(new TimeSpan(0,1,0));\nDateTime weekFloor = DateTime.Now.Floor(new TimeSpan(7,0,0,0));\n...\n</code></pre>\n\n<p>Cheers!</p>\n    ","a":"\n<p>Floor</p>\n\n<pre><code>long ticks = date.Ticks / span.Ticks;\n\nreturn new DateTime( ticks * span.Ticks );\n</code></pre>\n\n<p>Round (up on midpoint)</p>\n\n<pre><code>long ticks = (date.Ticks + (span.Ticks / 2) + 1)/ span.Ticks;\n\nreturn new DateTime( ticks * span.Ticks );\n</code></pre>\n\n<p>Ceiling</p>\n\n<pre><code>long ticks = (date.Ticks + span.Ticks - 1)/ span.Ticks;\n\nreturn new DateTime( ticks * span.Ticks );\n</code></pre>\n    "},{"t":"What is the probability that the array will remain the same?","l":"http://stackoverflow.com/questions/11872190/what-is-the-probability-that-the-array-will-remain-the-same","q":"\n\n<p>This question has been asked in Microsoft interview. Very much curious to know why these people ask so strange questions on probability?</p>\n\n<p>Given a rand(N), a random generator which generates random number from 0 to N-1.</p>\n\n<pre><code>int A[N]; // An array of size N\nfor(i = 0; i &lt; N; i++)\n{\n    int m = rand(N);\n    int n = rand(N);\n    swap(A[m],A[n]);\n}\n</code></pre>\n\n<p><strong>EDIT:</strong> Note that the seed is not fixed.</p>\n\n<p>what is the probability that array A remains the same?<br>\nAssume that the array contains unique elements.</p>\n    ","a":"\n<p><sub>\n<em>Well I had a little fun with this one. The first thing I thought of when I first read the problem was group theory (the symmetric group S<sub>n</sub>, in particular). The for loop simply builds a permutation σ in S<sub>n</sub> by composing transpositions (i.e. swaps) on each iteration. My math is not all that spectacular and I'm a little rusty, so if my notation is off bear with me.</em> \n</sub></p>\n\n<hr>\n\n<h2>Overview</h2>\n\n<p>Let <code>A</code> be the event that our array is unchanged after permutation. We are ultimately asked to find the probability of event <code>A</code>, <code>Pr(A)</code>.</p>\n\n<p>My solution attempts to follow the following procedure:</p>\n\n<ol>\n<li>Consider all possible permutations (i.e. reorderings of our array)</li>\n<li>Partition these permutations into <em>disjoint</em> sets based on the number of so-called <em>identity transpositions</em> they contain. This helps reduce the problem to <em>even</em> permutations only. </li>\n<li>Determine the probability of obtaining the identity permutation given that the permutation is even (and of a particular length).</li>\n<li>Sum these probabilities to obtain the overall probability the array is unchanged.</li>\n</ol>\n\n<h2>1) Possible Outcomes</h2>\n\n<p>Notice that each iteration of the for loop creates a swap (or <em>transposition</em>) that results one of two things (but never both):</p>\n\n<ol>\n<li>Two elements are swapped.</li>\n<li>An element is swapped with itself. For our intents and purposes, the array is unchanged. </li>\n</ol>\n\n<p>We label the second case. Let's define an <em>identity transposition</em> as follows:</p>\n\n<blockquote>\n  <p>An <em>identity transposition</em> occurs when a number is swapped with itself.\n  That is, when n == m in the above for loop.</p>\n</blockquote>\n\n<p>For any given run of the listed code, we compose <code>N</code> transpositions. There can be <code>0, 1, 2, ... , N</code> of the identity transpositions appearing in this \"chain\".</p>\n\n<hr>\n\n<p>For example, consider an <code>N = 3</code> case:</p>\n\n<pre><code>Given our input [0, 1, 2].\nSwap (0 1) and get [1, 0, 2].\nSwap (1 1) and get [1, 0, 2]. ** Here is an identity **\nSwap (2 2) and get [1, 0, 2]. ** And another **\n</code></pre>\n\n<p>Note that there is an odd number of non-identity transpositions (1) and the array is changed.</p>\n\n<hr>\n\n<h2>2) Partitioning Based On the Number of Identity Transpositions</h2>\n\n<p>Let <code>K_i</code> be the event that <code>i</code> identity transpositions appear in a given permutation. Note this forms an exhaustive partition of all possible outcomes: </p>\n\n<ul>\n<li>No permutation can have two different quantities of identity transpositions simultaneously, and</li>\n<li>All possible permutations must have between <code>0</code> and <code>N</code> identity transpositions. </li>\n</ul>\n\n<p>Thus we can apply the <a href=\"http://en.wikipedia.org/wiki/Law_of_total_probability\">Law of Total Probability</a>:</p>\n\n<pre>                      <img src=\"http://i.stack.imgur.com/js91V.png\">\n</pre>\n\n<p>Now we can finally take advantage of the the partition. Note that when the number of <em>non-identity</em> transpositions is odd, there is no way the array can go unchanged*. Thus:</p>\n\n<pre>                        <img src=\"http://i.stack.imgur.com/p5PKc.png\">\n</pre>\n\n<p>*<sub>From group theory, a permutation is even or odd but never both. Therefore an odd permutation cannot be the identity permutation (since the identity permutation is even).</sub></p>\n\n<h2>3) Determining Probabilities</h2>\n\n<p>So we now must determine two probabilities for <code>N-i</code> even: </p>\n\n<ol>\n<li><img src=\"http://i.stack.imgur.com/z2aNn.png\" alt=\"Pr(K_i)\"></li>\n<li><img src=\"http://i.stack.imgur.com/YAoOl.png\" alt=\"Pr(A|K_i)\"></li>\n</ol>\n\n<h3>The First Term</h3>\n\n<p>The first term, <img src=\"http://i.stack.imgur.com/z2aNn.png\" alt=\"Pr(K_i)\">, represents the probability of obtaining a permutation with <code>i</code> identity transpositions. This turns out to be binomial since for each iteration of the for loop:</p>\n\n<ul>\n<li>The outcome is independent of the results before it, and</li>\n<li>The probability of creating an identity transposition is the same, namely <code>1/N</code>.</li>\n</ul>\n\n<p>Thus for <code>N</code> trials, the probability of obtaining <code>i</code> identity transpositions is:</p>\n\n<pre>                      <img src=\"http://i.stack.imgur.com/9t2yi.png\">\n</pre>\n\n<h3>The Second Term</h3>\n\n<p>So if you've made it this far, we have reduced the problem to finding <img src=\"http://i.stack.imgur.com/YAoOl.png\" alt=\"Pr(A|K_i)\"> for <code>N - i</code> even. This represents the probability of obtaining an identity permutation given <code>i</code> of the transpositions are identities. I use a naive counting approach to determine the number of ways of achieving the identity permutation over the number of possible permutations. </p>\n\n<p>First consider the permutations <code>(n, m)</code> and <code>(m, n)</code> equivalent. Then, let <code>M</code> be the number of non-identity permutations possible. We will use this quantity frequently.</p>\n\n<pre>                              <img src=\"http://i.stack.imgur.com/AvSD7.png\">\n</pre>\n\n<p>The goal here is to determine the number of ways a collections of transpositions can be combined to form the identity permutation. I will try to construct the general solution along side an example of <code>N = 4</code>.</p>\n\n<hr>\n\n<p>Let's consider the <code>N = 4</code> case with all identity transpositions (<em>i.e.</em> <code>i = N = 4</code>). Let <code>X</code> represent an identity transposition. For each <code>X</code>, there are <code>N</code> possibilities (they are: <code>n = m = 0, 1, 2, ... , N - 1</code>). Thus there are <code>N^i = 4^4</code> possibilities for achieving the identity permutation. For completeness, we add the binomial coefficient, <code>C(N, i)</code>, to consider ordering of the identity transpositions (here it just equals 1). I've tried to depict this below with the physical layout of elements above and the number of possibilities below:</p>\n\n<pre><code>I  =  _X_   _X_   _X_   _X_\n       N  *  N  *  N  *  N  * C(4, 4) =&gt; N^N * C(N, N) possibilities\n</code></pre>\n\n<p>Now without explicitly substituting <code>N = 4</code> and <code>i = 4</code>, we can look at the general case. Combining the above with the denominator found previously, we find:</p>\n\n<pre>                          <img src=\"http://i.stack.imgur.com/aaUPq.png\">\n</pre>\n\n<p>This is intuitive. In fact, any other value other than <code>1</code> should probably alarm you. Think about it: we are given the situation in which all <code>N</code> transpositions are said to be identities. What's the probably that the array is unchanged in this situation? Clearly, <code>1</code>.</p>\n\n<hr>\n\n<p>Now, again for <code>N = 4</code>, let's consider 2 identity transpositions (<em>i.e.</em> <code>i = N - 2 = 2</code>). As a convention, we will place the two identities at the end (and account for ordering later). We know now that we need to pick two transpositions which, when composed, will become the identity permutation. Let's place any element in the first location, call it <code>t1</code>. As stated above, there are <code>M</code> possibilities supposing <code>t1</code> is not an identity (it can't be as we have already placed two). </p>\n\n<pre><code>I  =  _t1_   ___   _X_   _X_\n       M   *  ?  *  N  *  N\n</code></pre>\n\n<p>The only element left that could possibly go in the second spot is the inverse of <code>t1</code>, which is in fact <code>t1</code> (and this is the only one by uniqueness of inverse). We again include the binomial coefficient: in this case we have 4 open locations and we are looking to place 2 identity permutations. How many ways can we do that? 4 choose 2.</p>\n\n<pre><code>I  =  _t1_   _t1_   _X_   _X_ \n       M   *  1   *  N  *  N  * C(4, 2) =&gt; C(N, N-2) * M * N^(N-2) possibilities\n</code></pre>\n\n<p>Again looking at the general case, this all corresponds to:</p>\n\n<pre>                      <img src=\"http://i.stack.imgur.com/ijV3f.png\">\n</pre>\n\n<hr>\n\n<p>Finally we do the <code>N = 4</code> case with no identity transpositions (<em>i.e.</em> <code>i = N - 4 = 0</code>). Since there are a lot of possibilities, it starts to get tricky and we must be careful not to double count. We start similarly by placing a single element in the first spot and working out possible combinations. Take the easiest first: the same transposition 4 times.</p>\n\n<pre><code>I  =  _t1_   _t1_   _t1_   _t1_ \n       M   *  1   *  1   *  1   =&gt; M possibilities\n</code></pre>\n\n<p>Let's now consider two unique elements <code>t1</code> and <code>t2</code>. There are <code>M</code> possibilities for <code>t1</code> and only <code>M-1</code> possibilities for <code>t2</code> (since <code>t2</code> cannot be equal to <code>t1</code>). If we exhaust all arrangements, we are left with the following patterns:</p>\n\n<pre><code>I  =  _t1_   _t1_   _t2_   _t2_ \n       M   *  1   *  M-1 *  1   =&gt; M * (M - 1) possibilities   (1)st\n\n   =  _t1_   _t2_   _t1_   _t2_\n       M   *  M-1 *  1   *  1   =&gt; M * (M - 1) possibilities   (2)nd\n\n   =  _t1_   _t2_   _t2_   _t1_\n       M   *  M-1 *  1   *  1   =&gt; M * (M - 1) possibilities   (3)rd\n</code></pre>\n\n<p>Now let's consider three unique elements, <code>t1</code>, <code>t2</code>, <code>t3</code>. Let's place <code>t1</code> first and then <code>t2</code>. As usual, we have:</p>\n\n<pre><code>I  =  _t1_   _t2_   ___   ___ \n       M   *  ?   *  ?  *  ?  \n</code></pre>\n\n<p>We can't yet say how many possible <code>t2</code>s there can be yet, and we will see why in a minute.</p>\n\n<p>We now place <code>t1</code> in the third spot. Notice, <code>t1</code> must go there since if were to go in the last spot, we would just be recreating the <code>(3)rd</code> arrangement above. Double counting is bad! This leaves the third unique element <code>t3</code> to the final position. </p>\n\n<pre><code>I  =  _t1_   _t2_   _t1_   _t3_ \n       M   *  ?   *  1  *   ?  \n</code></pre>\n\n<p>So why did we have to take a minute to consider the number of <code>t2</code>s more closely? The transpositions <code>t1</code> and <code>t2</code> <strong><em>cannot</em></strong> be disjoint permutations (<em>i.e.</em> they must share one (and only one since they also cannot be equal) of their <code>n</code> or <code>m</code>). The reason for this is because if they were disjoint, we could swap the order of permutations. This means we would be double counting the <code>(1)st</code> arrangement. </p>\n\n<p>Say <code>t1 = (n, m)</code>. <code>t2</code> must be of the form <code>(n, x)</code> or <code>(y, m)</code> for some <code>x</code> and <code>y</code> in order to be non-disjoint. Note that <code>x</code> may not be <code>n</code> or <code>m</code> and <code>y</code> many not be <code>n</code> or <code>m</code>. Thus, the number of possible permutations that <code>t2</code> could be is actually <code>2 * (N - 2)</code>. </p>\n\n<p>So, coming back to our layout:</p>\n\n<pre><code>I  =  _t1_    _t2_    _t1_   _t3_ \n       M   * 2(N-2) *  1   *  ?  \n</code></pre>\n\n<p>Now <code>t3</code> must be the inverse of the composition of <code>t1 t2 t1</code>. Let's do it out manually: </p>\n\n<pre><code>(n, m)(n, x)(n, m) = (m, x) \n</code></pre>\n\n<p>Thus <code>t3</code> must be <code>(m, x)</code>. Note this is <strong><em>not</em></strong> disjoint to <code>t1</code> and not equal to either <code>t1</code> or <code>t2</code> so there is no double counting for this case.</p>\n\n<pre><code>I  =  _t1_    _t2_    _t1_   _t3_ \n       M   * 2(N-2) *  1  *   1    =&gt; M * 2(N - 2) possibilities   \n</code></pre>\n\n<p>Finally, putting all of these together:</p>\n\n<pre>        <img src=\"http://i.stack.imgur.com/8Ab3p.png\">\n</pre>\n\n<hr>\n\n<h2>4) Putting it all together</h2>\n\n<p>So that's it. Work backwards, substituting what we found into the original summation given in step 2. I computed the answer to the <code>N = 4</code> case below. It matches the empirical number found in another answer very closely!</p>\n\n<pre>         N  =  4\n         M  =  6   _________ _____________ _________\n                  | Pr(K_i) | Pr(A | K_i) | Product | \n         _________|_________|_____________|_________|\n        |         |         |             |         |\n        |  i = 0  |  0.316  |  120 / 1296 |  0.029  |\n        |_________|_________|_____________|_________|\n        |         |         |             |         |\n        |  i = 2  |  0.211  |    6 / 36   |  0.035  |\n        |_________|_________|_____________|_________|\n        |         |         |             |         |\n        |  i = 4  |  0.004  |    1 / 1    |  0.004  |\n        |_________|_________|_____________|_________|\n                            |             |         |\n                            |     Sum:    |  0.068  |\n                            |_____________|_________|\n</pre>\n\n<h2>Correctness</h2>\n\n<p>It would be cool if there was a result in group theory to apply here-- and maybe there is! It would certainly help make all this tedious counting go away completely (and shorten the problem to something much more elegant). I stopped working at <code>N = 4</code>. For <code>N &gt; 5</code>, what is given only gives an approximation (how good, I'm not sure). It is pretty clear why that is if you think about it: for example, given <code>N = 8</code> transpositions, there are clearly ways of creating the identity with <em>four</em> unique elements which are not accounted for above. The number of ways becomes seemingly more difficult to count as the permutation gets longer (as far as I can tell...).</p>\n\n<p>Anyway, I definitely couldn't do something like this within the scope of an interview. I would get as far as the denominator step if I was lucky. Beyond that, it seems pretty nasty.</p>\n    "},{"t":"Convert light frequency to RGB?","l":"http://stackoverflow.com/questions/1472514/convert-light-frequency-to-rgb","q":"\n\n<p>Does anyone know of any formula for converting a light frequency to an RGB value?</p>\n    ","a":"\n<p>Here's a detailed explanation of the entire conversion process: <a href=\"http://www.fourmilab.ch/documents/specrend/\">http://www.fourmilab.ch/documents/specrend/</a>. Source code included!</p>\n    "},{"t":"How does Dijkstra's Algorithm and A-Star compare?","l":"http://stackoverflow.com/questions/1332466/how-does-dijkstras-algorithm-and-a-star-compare","q":"\n\n<p>I was looking at what the guys in the <a href=\"http://julian.togelius.com/mariocompetition2009/\">Mario AI Competition</a> have been doing and some of them have built some pretty neat Mario bots utilizing the A* (A-Star) Pathing Algorithm.  </p>\n\n<p><img src=\"http://julian.togelius.com/mariocompetition2009/screen1.png\" alt=\"alt text\"><br>\n(<a href=\"http://www.youtube.com/watch?v=DlkMs4ZHHr8\">Video of Mario A* Bot In Action</a>)</p>\n\n<p>My question is, how does A-Star compare with Dijkstra?  Looking over them, they seem similar.</p>\n\n<p>Why would someone use one over the other?  Especially in the context of pathing in games?</p>\n    ","a":"\n<p>Dijkstra is a special case for A* (when the heuristics is zero).</p>\n    "},{"t":"What is a good Hash Function?","l":"http://stackoverflow.com/questions/34595/what-is-a-good-hash-function","q":"\n\n<p>What is a good Hash function? I saw a lot of hash function and applications in my data structures courses in college, but I mostly got that it's pretty hard to make a good hash function. As a rule of thumb to avoid collisions my professor said that:</p>\n\n<pre><code>function Hash(key)\n  return key mod PrimeNumber\nend\n</code></pre>\n\n<p>(mod is the % operator in C and similar languages)</p>\n\n<p>with the prime number to be the size of the hash table. I get that is a somewhat good function to avoid collisions and a fast one, but how can I make a better one? Is there better hash functions for string keys against numeric keys?</p>\n    ","a":"\n<p>For doing \"normal\" hash table lookups on basically any kind of data - this one by Paul Hsieh is the best I've ever used.</p>\n\n<p><a href=\"http://www.azillionmonkeys.com/qed/hash.html\">http://www.azillionmonkeys.com/qed/hash.html</a></p>\n\n<p>If you care about cryptographically secure or anything else more advanced, then YMMV.  If you just want a kick ass general purpose hash function for a hash table lookup, then this is what you're looking for.</p>\n    "},{"t":"Where can I find the solutions to “The Algorithm Design Manual”? [closed]","l":"http://stackoverflow.com/questions/5112057/where-can-i-find-the-solutions-to-the-algorithm-design-manual","q":"\n\n<p>The book is full of interesting questions but since I am learning it myself, it would be a great help if I can find solutions to at least some of the questions. Anyone knows anything about this? Many thanks.</p>\n    ","a":"\n<p>It's website, <a href=\"http://www.algorist.com/\">http://www.algorist.com/</a> has a <a href=\"http://www.algorithm.cs.sunysb.edu/algowiki/index.php/Main_Page\">wiki</a> with solutions.</p>\n    "},{"t":"How to determine if binary tree is balanced?","l":"http://stackoverflow.com/questions/742844/how-to-determine-if-binary-tree-is-balanced","q":"\n\n<p>It's been a while from those school years. Got a job as IT specialist at a hospital.  Trying to move to do some actual programming now.  I'm working on binary trees now, and I was wondering what would be the best way to determine if the tree is height-balanced.  </p>\n\n<p>I was thinking of something along this:</p>\n\n<pre><code>public boolean isBalanced(Node root){\n    if(root==null){\n        return true;  //tree is empty\n    }\n    else{\n        int lh = root.left.height();\n        int rh = root.right.height();\n        if(lh - rh &gt; 1 || rh - lh &gt; 1){\n            return false;\n        }\n    }\n    return true;\n}\n</code></pre>\n\n<p>Is this a good implementation? or am I missing something?</p>\n    ","a":"\n<p>Stumbled across this old question while searching for something else. I notice that you never did get a complete answer.</p>\n\n<p>The way to solve this problem is to start by writing a specification for the function you are trying to write.</p>\n\n<p>Specification: A well-formed binary tree is said to be \"height-balanced\" if (1) it is empty, or (2) its left and right children are height-balanced and the height of the left tree is within 1 of the height of the right tree.</p>\n\n<p>Now that you have the specification, the code is trivial to write. Just follow the specification:</p>\n\n<pre><code>IsHeightBalanced(tree)\n    return (tree is empty) or \n           (IsHeightBalanced(tree.left) and\n            IsHeightBalanced(tree.right) and\n            abs(Height(tree.left) - Height(tree.right)) &lt;= 1)\n</code></pre>\n\n<p>Translating that into the programming language of your choice should be trivial.</p>\n\n<p>Bonus exercise: this naive code sketch traverses the tree far too many times when computing the heights. Can you make it more efficient?</p>\n\n<p>Super bonus exercise: suppose the tree is <em>massively</em> unbalanced. Like, a million nodes deep on one side and three deep on the other. Is there a scenario in which this algorithm blows the stack? Can you fix the implementation so that it never blows the stack, even when given a massively unbalanced tree?</p>\n\n<p>UPDATE: Donal Fellows points out in his answer that there are different definitions of 'balanced' that one could choose. For example, one could take a stricter definition of \"height balanced\", and require that the path length to the <em>nearest</em> empty child is within one of the path to the <em>farthest</em> empty child. My definition is less strict than that, and therefore admits more trees. </p>\n\n<p>One can also be less strict than my definition; one could say that a balanced tree is one in which the maximum path length to an empty tree on each branch differs by no more than two, or three, or some other constant. Or that the maximum path length is some fraction of the minimum path length, like a half or a quarter.</p>\n\n<p>It really doesn't matter usually. The point of any tree-balancing algorithm is to ensure that you do not wind up in the situation where you have a million nodes on one side and three on the other. Donal's definition is fine in theory, but in practice it is a pain coming up with a tree-balancing algorithm that meets that level of strictness. The performance savings usually does not justify the implementation cost. You spend a lot of time doing unnecessary tree rearrangements in order to attain a level of balance that in practice makes little difference. Who cares if sometimes it takes forty branches to get to the farthest leaf in a million-node imperfectly-balanced tree when it could in theory take only twenty in a perfectly balanced tree? The point is that it doesn't ever take a million. Getting from a worst case of a million down to a worst case of forty is usually good enough; you don't have to go all the way to the optimal case. </p>\n    "},{"t":"Where can I learn more about the Google search “did you mean” algorithm? [duplicate]","l":"http://stackoverflow.com/questions/3763640/where-can-i-learn-more-about-the-google-search-did-you-mean-algorithm","q":"\n\n<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean\">How do you implement a “Did you mean”?</a>  </p>\n</blockquote>\n\n\n\n<p>I am writing an application where I require functionality similar to Google's \"did you mean?\" feature used by their search engine:</p>\n\n<p><img src=\"http://i.stack.imgur.com/cZCpI.jpg\" alt=\"alt text\"></p>\n\n<p>Is there source code available for such a thing or where can I find articles that would help me to build my own?</p>\n    ","a":"\n<p>You should check out Peter Norvigs article about implementing the spell checker in a few lines of python:\n<a href=\"http://norvig.com/spell-correct.html\">How to Write a Spelling Corrector</a> It also has links for implementations in other languages (i.e. C#)</p>\n    "},{"t":"Generating permutations lazily","l":"http://stackoverflow.com/questions/352203/generating-permutations-lazily","q":"\n\n<p>I'm looking for an algorithm to generate permutations of a set in such a way that I could make a lazy list of them in Clojure.  i.e. I'd like to iterate over a list of permutations where each permutation is not calculated until I request it, and all of the permutations don't have to be stored in memory at once.</p>\n\n<p>Alternatively I'm looking for an algorithm where given a certain set, it will return the \"next\" permutation of that set, in such a way that repeatedly calling the function on its own output will cycle through all permutations of the original set, in some order (what the order is doesn't matter).</p>\n\n<p>Is there such an algorithm?  Most of the permutation-generating algorithms I've seen tend to generate them all at once (usually recursively), which doesn't scale to very large sets.  An implementation in Clojure (or another functional language) would be helpful but I can figure it out from pseudocode.</p>\n    ","a":"\n<p>Yes, there <em>is</em> a \"next permutation\" algorithm, and it's quite simple too. The C++ standard template library (STL) even has a function called <code>next_permutation</code>.</p>\n\n<p>The algorithm actually finds the <em>next</em> permutation -- the lexicographically next one. The idea is this: suppose you are given a sequence, say \"32541\". What is the next permutation?</p>\n\n<p>If you think about it, you'll see that it is \"34125\". And your thoughts were probably something this: In \"32541\",</p>\n\n<ul>\n<li>there is no way to keep the \"32\" fixed and find a later permutation in the \"541\" part, because that permutation is already the last one for 5,4, and 1 -- it is sorted in decreasing order.</li>\n<li>So you'll have to change the \"2\" to something bigger -- in fact, to the smallest number bigger than it in the \"541\" part, namely 4.</li>\n<li>Now, once you've decided that the permutation will start as \"34\", the rest of the numbers should be in increasing order, so the answer is \"34125\".</li>\n</ul>\n\n<p>The algorithm is to implement precisely that line of reasoning:</p>\n\n<ol>\n<li>Find the longest \"tail\" that is ordered in decreasing order. (The \"541\" part.)</li>\n<li>Change the number just before the tail (the \"2\") to the smallest number bigger than it in the tail (the 4).</li>\n<li>Sort the tail in increasing order.</li>\n</ol>\n\n<p>You can do (1.) efficiently by starting at the end and going backwards as long as the previous element is not smaller than the current element. You can do (2.) by just swapping the \"4\" with the '2\", so you'll have \"34521\". Once you do this, you can avoid using a sorting algorithm for (3.), because the tail was, and is still (think about this), sorted in decreasing order, so it only needs to be reversed.</p>\n\n<p>The C++ code does precisely this (look at the source in <code>/usr/include/c++/4.0.0/bits/stl_algo.h</code> on your system, or see <a href=\"http://marknelson.us/2002/03/01/next-permutation\">this article</a>); it should be simple to translate it to your language: [Read \"BidirectionalIterator\" as \"pointer\", if you're unfamiliar with C++ iterators. The code returns <code>false</code> if there is no next permutation, i.e. we are already in decreasing order.]</p>\n\n<pre><code>template &lt;class BidirectionalIterator&gt;\nbool next_permutation(BidirectionalIterator first,\n                      BidirectionalIterator last) {\n    if (first == last) return false;\n    BidirectionalIterator i = first;\n    ++i;\n    if (i == last) return false;\n    i = last;\n    --i;\n    for(;;) {\n        BidirectionalIterator ii = i--;\n        if (*i &lt;*ii) {\n            BidirectionalIterator j = last;\n            while (!(*i &lt;*--j));\n            iter_swap(i, j);\n            reverse(ii, last);\n            return true;\n        }\n        if (i == first) {\n            reverse(first, last);\n            return false;\n        }\n    }\n}\n</code></pre>\n\n<p>It might seem that it can take O(n) time per permutation, but if you think about it more carefully, you can prove that it takes O(n!) time for all permutations in total, so only O(1) -- constant time -- per permutation.</p>\n\n<p>The good thing is that the algorithm works even when you have a sequence with repeated elements: with, say, \"232254421\", it would find the tail as \"54421\", swap the \"2\" and \"4\" (so \"232454221\"), reverse the rest, giving \"232412245\", which is the next permutation.</p>\n    "},{"t":"Fast permutation -> number -> permutation mapping algorithms","l":"http://stackoverflow.com/questions/1506078/fast-permutation-number-permutation-mapping-algorithms","q":"\n\n<p>I have n elements.  For the sake of an example, let's say, 7 elements, 1234567.  I know there are 7! = 5040 permutations possible of these 7 elements.</p>\n\n<p>I want a fast algorithm comprising two functions:</p>\n\n<p>f(number) maps a number between 0 and 5039 to a unique permutation, and</p>\n\n<p>f'(permutation) maps the permutation back to the number that it was generated from.</p>\n\n<p>I don't care about the correspondence between number and permutation, providing each permutation has its own unique number.</p>\n\n<p>So, for instance, I might have functions where</p>\n\n<pre><code>f(0) = '1234567'\nf'('1234567') = 0\n</code></pre>\n\n<p>The fastest algorithm that comes to mind is to enumerate all permutations and create a lookup table in both directions, so that, once the tables are created, f(0) would be O(1) and f('1234567') would be a lookup on a string.  However, this is memory hungry, particularly when n becomes large.</p>\n\n<p>Can anyone propose another algorithm that would work quickly and without the memory disadvantage?</p>\n    ","a":"\n<p>To describe a permutation of n elements, you see that for the position that the first element ends up at, you have n possibilities, so you can describe this with a number between 0 and n-1. For the position that the next element ends up at, you have n-1 remaining possibilities, so you can describe this with a number between 0 and n-2.<br>\nEt cetera until you have n numbers. </p>\n\n<p>As an example for n = 5, consider the permutation that brings <code>abcde</code> to <code>caebd</code>.</p>\n\n<ul>\n<li><code>a</code>, the first element, ends up at the second position, so we assign it index <strong>1</strong>.</li>\n<li><code>b</code> ends up at the fourth position, which would be index 3, but it's the third remaining one, so we assign it <strong>2</strong>.</li>\n<li><code>c</code> ends up at the first remaining position, which is always <strong>0</strong>. </li>\n<li><code>d</code> ends up at the last remaining position, which (out of only two remaining positions) is <strong>1</strong>.</li>\n<li><code>e</code> ends up at the only remaining position, indexed at <strong>0</strong>.</li>\n</ul>\n\n<p>So we have the index sequence <strong>{1, 2, 0, 1, 0}</strong>.</p>\n\n<p>Now you know that for instance in a binary number, 'xyz' means z + 2y + 4x. For a decimal number,<br>\nit's z + 10y + 100x. Each digit is multiplied by some weight, and the results are summed. The obvious pattern in the weight is of course that the weight is w = b^k, with b the base of the number and k the index of the digit. (I will always count digits from the right and starting at index 0 for the rightmost digit. Likewise when I talk about the 'first' digit I mean the rightmost.)</p>\n\n<p>The <em>reason</em> why the weights for digits follow this pattern is that the highest number that can be represented by the digits from 0 to k must be exactly 1 lower than the lowest number that can be represented by only using digit k+1. In binary, 0111 must be one lower than 1000. In decimal, 099999 must be one lower than 100000.</p>\n\n<p><strong>Encoding to variable-base</strong><br>\nThe spacing between subsequent numbers being exactly 1 is the important rule. Realising this, we can represent our index sequence by a <em>variable-base number</em>. The base for each digit is the amount of different possibilities for that digit. For decimal each digit has 10 possibilities, for our system the rightmost digit would have 1 possibility and the leftmost will have n possibilities. But since the rightmost digit (the last number in our sequence) is always 0, we leave it out. That means we're left with bases 2 to n. In general, the k'th digit will have base b[k] = k + 2. The highest value allowed for digit k is h[k] = b[k] - 1 = k + 1.</p>\n\n<p>Our rule about the weights w[k] of digits requires that the sum of h[i] * w[i], where i goes from i = 0 to i = k, is equal to 1 * w[k+1]. Stated recurrently, w[k+1] = w[k] + h[k] * w[k] = w[k]*(h[k] + 1). The first weight w[0] should always be 1. Starting from there, we have the following values:</p>\n\n<pre><code>k    h[k] w[k]    \n\n0    1    1  \n1    2    2    \n2    3    6    \n3    4    24   \n...  ...  ...\nn-1  n    n!  \n</code></pre>\n\n<p>(The general relation w[k-1] = k! is easily proved by induction.)</p>\n\n<p>The number we get from converting our sequence will then be the sum of s[k] * w[k], with k running from 0 to n-1. Here s[k] is the k'th (rightmost, starting at 0) element of the sequence. As an example, take our {1, 2, 0, 1, 0}, with the rightmost element stripped off as mentioned before: <strong>{1, 2, 0, 1}</strong>. Our sum is 1 * 1 + 0 * 2 + 2 * 6 + 1 * 24 = <strong>37</strong>.</p>\n\n<p>Note that if we take the maximum position for every index, we'd have {4, 3, 2, 1, 0}, and that converts to 119. Since the weights in our number encoding were chosen so that we don't skip any numbers, all numbers 0 to 119 are valid. There are precisely 120 of these, which is n! for n = 5 in our example, precisely the number of different permutations. So you can see our encoded numbers completely specify all possible permutations. </p>\n\n<p><strong>Decoding from variable-base</strong><br>\nDecoding is similar to converting to binary or decimal. The common algorithm is this:</p>\n\n<pre><code>int number = 42;\nint base = 2;\nint[] bits = new int[n];\n\nfor (int k = 0; k &lt; bits.Length; k++)\n{\n    bits[k] = number % base;\n    number = number / base;\n}\n</code></pre>\n\n<p>For our variable-base number:</p>\n\n<pre><code>int n = 5;\nint number = 37;\n\nint[] sequence = new int[n - 1];\nint base = 2;\n\nfor (int k = 0; k &lt; sequence.Length; k++)\n{\n    sequence[k] = number % base;\n    number = number / base;\n\n    base++; // b[k+1] = b[k] + 1\n}\n</code></pre>\n\n<p>This correctly decodes our 37 back to {1, 2, 0, 1} (<code>sequence</code> would be <code>{1, 0, 2, 1}</code> in this code example, but whatever ... as long as you index appropriately). We just need to add 0 at the right end (remember the last element always has only one possibility for its new position) to get back our original sequence {1, 2, 0, 1, 0}.</p>\n\n<p><strong>Permuting a list using an index sequence</strong><br>\nYou can use the below algorithm to permute a list according to a specific index sequence. It's an O(n²) algorithm, unfortunately.</p>\n\n<pre><code>int n = 5;\nint[] sequence = new int[] { 1, 2, 0, 1, 0 };\nchar[] list = new char[] { 'a', 'b', 'c', 'd', 'e' };\nchar[] permuted = new char[n];\nbool[] set = new bool[n];\n\nfor (int i = 0; i &lt; n; i++)\n{\n    int s = sequence[i];\n    int remainingPosition = 0;\n    int index;\n\n    // Find the s'th position in the permuted list that has not been set yet.\n    for (index = 0; index &lt; n; index++)\n    {\n        if (!set[index])\n        {\n            if (remainingPosition == s)\n                break;\n\n            remainingPosition++;\n        }\n    }\n\n    permuted[index] = list[i];\n    set[index] = true;\n}\n</code></pre>\n\n<p><strong>Common representation of permutations</strong><br>\nNormally you would not represent a permutation as unintuitively as we've done, but simply by the absolute position of each element after the permutation is applied. Our example {1, 2, 0, 1, 0} for <code>abcde</code> to <code>caebd</code> is normally represented by {1, 3, 0, 4, 2}. Each index from 0 to 4 (or in general, 0 to n-1) occurs exactly once in this representation.</p>\n\n<p>Applying a permutation in this form is easy:</p>\n\n<pre><code>int[] permutation = new int[] { 1, 3, 0, 4, 2 };\n\nchar[] list = new char[] { 'a', 'b', 'c', 'd', 'e' };\nchar[] permuted = new char[n];\n\nfor (int i = 0; i &lt; n; i++)\n{\n    permuted[permutation[i]] = list[i];\n}\n</code></pre>\n\n<p>Inverting it is very similar:</p>\n\n<pre><code>for (int i = 0; i &lt; n; i++)\n{\n    list[i] = permuted[permutation[i]];\n}\n</code></pre>\n\n<p><strong>Converting from our representation to the common representation</strong><br>\nNote that if we take our algorithm to permute a list using our index sequence, and apply it to the identity permutation {0, 1, 2, ..., n-1}, we get the <em>inverse</em> permutation, represented in the common form. (<strong>{2, 0, 4, 1, 3}</strong> in our example).</p>\n\n<p>To get the non-inverted premutation, we apply the permutation algorithm I just showed:</p>\n\n<pre><code>int[] identity = new int[] { 0, 1, 2, 3, 4 };\nint[] inverted = { 2, 0, 4, 1, 3 };\nint[] normal = new int[n];\n\nfor (int i = 0; i &lt; n; i++)\n{\n    normal[identity[i]] = list[i];\n}\n</code></pre>\n\n<p>Or you can just apply the permutation directly, by using the inverse permutation algorithm:</p>\n\n<pre><code>char[] list = new char[] { 'a', 'b', 'c', 'd', 'e' };\nchar[] permuted = new char[n];\n\nint[] inverted = { 2, 0, 4, 1, 3 };\n\nfor (int i = 0; i &lt; n; i++)\n{\n    permuted[i] = list[inverted[i]];\n}\n</code></pre>\n\n<p>Note that all the algorithms for dealing with permutations in the common form are O(n), while applying a permutation in our form is O(n²). If you need to apply a permutation several times, first convert it to the common representation.</p>\n    "},{"t":"Sort points in clockwise order?","l":"http://stackoverflow.com/questions/6989100/sort-points-in-clockwise-order","q":"\n\n<p>Given an array of x,y points, how do I sort the points of this array in clockwise order (around their overall average center point)? My goal is to pass the points to a line-creation function to end up with something looking rather \"solid\", as convex as possible with no lines intersecting.</p>\n\n<p>For what it's worth, I'm using Lua, but any pseudocode would be appreciated. Thanks so much for any help!</p>\n\n<p><strong>Update:</strong> For reference, this is the Lua code based on Ciamej's excellent answer (ignore my \"app\" prefix):</p>\n\n<pre><code>function appSortPointsClockwise(points)\n    local centerPoint = appGetCenterPointOfPoints(points)\n    app.pointsCenterPoint = centerPoint\n    table.sort(points, appGetIsLess)\n    return points\nend\n\nfunction appGetIsLess(a, b)\n    local center = app.pointsCenterPoint\n\n    if a.x &gt;= 0 and b.x &lt; 0 then return true\n    elseif a.x == 0 and b.x == 0 then return a.y &gt; b.y\n    end\n\n    local det = (a.x - center.x) * (b.y - center.y) - (b.x - center.x) * (a.y - center.y)\n    if det &lt; 0 then return true\n    elseif det &gt; 0 then return false\n    end\n\n    local d1 = (a.x - center.x) * (a.x - center.x) + (a.y - center.y) * (a.y - center.y)\n    local d2 = (b.x - center.x) * (b.x - center.x) + (b.y - center.y) * (b.y - center.y)\n    return d1 &gt; d2\nend\n\nfunction appGetCenterPointOfPoints(points)\n    local pointsSum = {x = 0, y = 0}\n    for i = 1, #points do pointsSum.x = pointsSum.x + points[i].x; pointsSum.y = pointsSum.y + points[i].y end\n    return {x = pointsSum.x / #points, y = pointsSum.y / #points}\nend\n</code></pre>\n\n<p></p>\n    ","a":"\n<p>First compute the center point.\nThen sort the points using whatever sorting algorithm you like, but use special comparison routine to determine whether one point is less than the other.</p>\n\n<p>You can check whether one point (a) is to the left or to the right of the other (b) in relation to the center by this simple calculation:</p>\n\n<pre><code>det = (a.x - center.x) * (b.y - center.y) - (b.x - center.x) * (a.y - center.y)\n</code></pre>\n\n<p>if the result is zero, then they are on the same line from the center, if it's positive or negative, then it is on one side or the other, so one point will precede the other.\nUsing it you can construct a less-than relation to compare points and determine the order in which they should appear in the sorted array. But you have to define where is the beginning of that order, I mean what angle will be the starting one (e.g. the positive half of x-axis).</p>\n\n<p>The code for the comparison function can look like this:</p>\n\n<pre><code>bool less(point a, point b)\n{\n    if (a.x - center.x &gt;= 0 &amp;&amp; b.x - center.x &lt; 0)\n        return true;\n    if (a.x - center.x &lt; 0 &amp;&amp; b.x - center.x &gt;= 0)\n        return false;\n    if (a.x - center.x == 0 &amp;&amp; b.x - center.x == 0) {\n        if (a.y - center.y &gt;= 0 || b.y - center.y &gt;= 0)\n            return a.y &gt; b.y;\n        return b.y &gt; a.y;\n    }\n\n    // compute the cross product of vectors (center -&gt; a) x (center -&gt; b)\n    int det = (a.x - center.x) * (b.y - center.y) - (b.x - center.x) * (a.y - center.y);\n    if (det &lt; 0)\n        return true;\n    if (det &gt; 0)\n        return false;\n\n    // points a and b are on the same line from the center\n    // check which point is closer to the center\n    int d1 = (a.x - center.x) * (a.x - center.x) + (a.y - center.y) * (a.y - center.y);\n    int d2 = (b.x - center.x) * (b.x - center.x) + (b.y - center.y) * (b.y - center.y);\n    return d1 &gt; d2;\n}\n</code></pre>\n\n<p>This will order the points clockwise starting from the 12 o'clock. Points on the same \"hour\" will be ordered starting from the ones that are further from the center.</p>\n\n<p>If using integer types (which are not really present in Lua) you'd have to assure that det, d1 and d2 variables are of a type that will be able to hold the result of performed calculations.</p>\n\n<p>If you want to achieve something looking solid, as convex as possible, then I guess you're looking for a <a href=\"http://en.wikipedia.org/wiki/Convex_hull\">Convex Hull</a>. You can compute it using the <a href=\"http://en.wikipedia.org/wiki/Graham_scan\">Graham Scan</a>.\nIn this algorithm you also have to sort the points clockwise (or counter-clockwise) starting from a special pivot point. Then you repeat simple loop steps each time checking if you turn left or right adding new points to the convex hull, this check is based on cross product just like in the above comparison function.</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>Added one more if statement <code>if (a.y - center.y &gt;= 0 || b.y - center.y &gt;=0)</code> to make sure that points that have x=0 and negative y are sorted starting from the ones that are further from the center. If you don't care about order of points on the same 'hour' you can omit this if statement and always return <code>a.y &gt; b.y</code>.</p>\n\n<p>Corrected the first if statements with adding <code>-center.x</code> and <code>-center.y</code>.</p>\n\n<p>Added the second if statement <code>(a.x - center.x &lt; 0 &amp;&amp; b.x - center.x &gt;= 0)</code>. It was an obvious oversight that it was missing. The if statements could be reorganized now, because some checks are redundant. For example if the first condition in the first if statement is false, then the first condition of the second if must be true. I decided, however, to leave the code as it is for the sake of simplicity. It's quite possible that the compiler will optimize the code and produce the same result anyway.</p>\n    "},{"t":"How to find the kth smallest element in the union of two sorted arrays?","l":"http://stackoverflow.com/questions/4607945/how-to-find-the-kth-smallest-element-in-the-union-of-two-sorted-arrays","q":"\n\n<p>This is a homework question. They say it takes <code>O(logN + logM)</code> where <code>N</code> and <code>M</code> are the arrays lengths.</p>\n\n<p>Let's name the arrays <code>a</code> and <code>b</code>. Obviously we can ignore all <code>a[i]</code> and <code>b[i]</code> where i &gt; k.<br>\nFirst let's compare <code>a[k/2]</code> and <code>b[k/2]</code>. Let <code>b[k/2]</code> &gt; <code>a[k/2]</code>. Therefore we can discard also all <code>b[i]</code>, where i &gt; k/2.</p>\n\n<p>Now we have all <code>a[i]</code>, where i &lt; k and all <code>b[i]</code>, where i &lt; k/2 to find the answer.</p>\n\n<p>What is the next step? </p>\n    ","a":"\n<p>You've got it, just keep going! And be careful with the indexes...</p>\n\n<p>To simplify a bit I'll assume that N and M are &gt; k, so the complexity here is O(log k), which is O(log N + log M).</p>\n\n<p>Pseudo-code:</p>\n\n<pre><code>i = k/2\nj = k - i\nstep = k/4\nwhile step &gt; 0\n    if a[i-1] &gt; b[j-1]\n        i -= step\n        j += step\n    else\n        i += step\n        j -= step\n    step /= 2\n\nif a[i-1] &gt; b[j-1]\n    return a[i-1]\nelse\n    return b[j-1]\n</code></pre>\n\n<p>For the demonstration you can use the loop invariant i + j = k, but I won't do all your homework :)</p>\n    "},{"t":"Looping in a spiral","l":"http://stackoverflow.com/questions/398299/looping-in-a-spiral","q":"\n\n<p>A friend was in need of an algorithm that would let him loop through the elements of an NxM matrix (N and M are odd). I came up with a solution, but I wanted to see if my fellow SO'ers could come up with a better solution.</p>\n\n<p>I'm posting my solution as an answer to this question.</p>\n\n<p><strong>Example Output:</strong></p>\n\n<p>For a 3x3 matrix, the output should be:</p>\n\n<p>(0, 0)\n(1, 0)\n(1, 1)\n(0, 1)\n(-1, 1)\n(-1, 0)\n(-1, -1)\n(0, -1)\n(1, -1)</p>\n\n<p><img src=\"http://i41.tinypic.com/24dmseu.png\" alt=\"3x3 matrix\"></p>\n\n<p>Furthermore, the algorithm should support non-square matrices, so for example for a 5x3 matrix, the output should be:</p>\n\n<p>(0, 0)\n(1, 0)\n(1, 1)\n(0, 1)\n(-1, 1)\n(-1, 0)\n(-1, -1)\n(0, -1)\n(1, -1)\n(2, -1)\n(2, 0)\n(2, 1)\n(-2, 1)\n(-2, 0)\n(-2, -1)</p>\n\n<p><img src=\"http://i40.tinypic.com/9thdtc.png\" alt=\"5x3 matrix\"></p>\n    ","a":"\n<p>Here's my solution (in Python):</p>\n\n<pre><code>def spiral(X, Y):\n    x = y = 0\n    dx = 0\n    dy = -1\n    for i in range(max(X, Y)**2):\n        if (-X/2 &lt; x &lt;= X/2) and (-Y/2 &lt; y &lt;= Y/2):\n            print (x, y)\n            # DO STUFF...\n        if x == y or (x &lt; 0 and x == -y) or (x &gt; 0 and x == 1-y):\n            dx, dy = -dy, dx\n        x, y = x+dx, y+dy\n</code></pre>\n    "},{"t":"Cost of len() function","l":"http://stackoverflow.com/questions/1115313/cost-of-len-function","q":"\n\n<p>What is the cost of <a href=\"https://docs.python.org/2/library/functions.html#len\"><code>len()</code></a> function for Python built-ins? (list/tuple/string/dictionary)</p>\n    ","a":"\n<p>It's O(1) (very fast) on every type you've mentioned, plus <code>set</code> and others such as <code>array.array</code>.</p>\n    "},{"t":"C++: Rounding up to the nearest multiple of a number","l":"http://stackoverflow.com/questions/3407012/c-rounding-up-to-the-nearest-multiple-of-a-number","q":"\n\n<p>OK - I'm almost embarrassed posting this here (and I will delete if anyone votes to close) as it seems like a basic question.</p>\n\n<p>Is this the correct way to round up to a multiple of a number in C++? </p>\n\n<p>I know there are other questions related to this but I am specficially interested to know what is the best way to do this in C++:</p>\n\n<pre><code>int roundUp(int numToRound, int multiple)\n{\n if(multiple == 0)\n {\n  return numToRound;\n }\n\n int roundDown = ( (int) (numToRound) / multiple) * multiple;\n int roundUp = roundDown + multiple; \n int roundCalc = roundUp;\n return (roundCalc);\n}\n</code></pre>\n\n<p>Update:\nSorry I probably didn't make intention clear.  Here are some examples:</p>\n\n<pre><code>roundUp(7, 100)\n//return 100\n\nroundUp(117, 100)\n//return 200\n\nroundUp(477, 100)\n//return 500\n\nroundUp(1077, 100)\n//return 1100\n\nroundUp(52, 20)\n//return 60\n\nroundUp(74, 30)\n//return 90\n</code></pre>\n\n<p>EDIT:  Thanks for all the replies.  Here is what I went for:</p>\n\n<pre><code>int roundUp(int numToRound, int multiple)  \n{  \n if(multiple == 0)  \n {  \n  return numToRound;  \n }  \n\n int remainder = numToRound % multiple; \n if (remainder == 0)\n  {\n    return numToRound; \n  }\n\n return numToRound + multiple - remainder; \n}  \n</code></pre>\n    ","a":"\n<p>This works for positive numbers, not sure about negative. It only uses integer math.</p>\n\n<pre><code>int roundUp(int numToRound, int multiple) \n{ \n if(multiple == 0) \n { \n  return numToRound; \n } \n\n int remainder = numToRound % multiple;\n if (remainder == 0)\n  return numToRound;\n return numToRound + multiple - remainder;\n}\n</code></pre>\n\n<p>Edit: Here's a version that works with negative numbers, if by \"up\" you mean a result that's always &gt;= the input.</p>\n\n<pre><code>int roundUp(int numToRound, int multiple) \n{ \n    if (multiple == 0) \n        return numToRound; \n\n    int remainder = abs(numToRound) % multiple;\n    if (remainder == 0)\n        return numToRound;\n    if (numToRound &lt; 0)\n        return -(abs(numToRound) - remainder);\n    return numToRound + multiple - remainder;\n}\n</code></pre>\n    "},{"t":"Find kth smallest element in a binary search tree in Optimum way","l":"http://stackoverflow.com/questions/2329171/find-kth-smallest-element-in-a-binary-search-tree-in-optimum-way","q":"\n\n<p>I need to find the kth smallest element in the binary search tree without using any static/global variable. How to achieve it efficiently?\nThe solution that I have in my mind is doing the operation in O(n), the worst case since I am planning to do an inorder traversal of the entire tree. But deep down I feel that I am not using the BST property here. Is my assumptive solution correct or is there a better one available ?</p>\n    ","a":"\n<p>Here's just an outline of the idea:</p>\n\n<p>In a BST, the left subtree of node <code>T</code> contains only elements smaller than the value stored in <code>T</code>. If <code>k</code> is smaller than the number of elements in the left subtree, the <code>k</code>th smallest element must belong to the left subtree. Otherwise, if <code>k</code> is larger, then the <code>k</code>th smallest element is in the right subtree.</p>\n\n<p>We can augment the BST to have each node in it store the number of elements in its left subtree. With this piece of information, it is simple to traverse the tree by repeatedly asking for the number of elements in the left subtree, to decide whether to do recurse into the left or right subtree.</p>\n\n<p>Now, suppose we are at node T:</p>\n\n<ol>\n<li>If <strong>k == num_elements(left subtree of T)</strong>, then the answer we're looking for is the value in node <code>T</code>.</li>\n<li>If <strong>k &gt; num_elements(left subtree of T)</strong>, then obviously we can ignore the left subtree, because those elements will also be smaller than the <code>k</code>th smallest. So, we reduce the problem to finding the <code>k - num_elements(left subtree of T)</code> smallest element of the right subtree. </li>\n<li>If <strong>k &lt; num_elements(left subtree of T)</strong>, then the <code>k</code>th smallest is somewhere in the left subtree, so we reduce the problem to finding the <code>k</code>th smallest element in the left subtree.</li>\n</ol>\n\n<p>Complexity analysis:</p>\n\n<p>This takes <code>O(depth of node)</code> time, which is <code>O(log n)</code> in the worst case on a balanced BST, or <code>O(log n)</code> on average for a random BST.</p>\n\n<p>A BST requires <code>O(n)</code> storage, and it takes another <code>O(n)</code> to store the information about the number of elements. All BST operations take <code>O(depth of node)</code> time, and it takes <code>O(depth of node)</code> extra time to maintain the \"number of elements\" information for insertion, deletion or rotation of nodes. Therefore, storing information about the number of elements in the left subtree keeps the space and time complexity of a BST.</p>\n    "},{"t":"design a stack such that getMinimum( ) should be O(1)","l":"http://stackoverflow.com/questions/685060/design-a-stack-such-that-getminimum-should-be-o1","q":"\n\n<p>This is one of an interview question. You need to design a stack which holds an integer value such that getMinimum() function should return the minimum element in the stack.</p>\n\n<p>For example: consider the below example</p>\n\n<pre>case #1\n\n5  --&gt; TOP\n1\n4\n6\n2\n\nWhen getMinimum() is called it should return 1, which is the minimum element \nin the stack. \n\ncase #2\n\nstack.pop()\nstack.pop()\n\nNote: Both 5 and 1 are poped out of the stack. So after this, the stack\nlooks like,\n\n4  --&gt; TOP\n6\n2\n\nWhen getMinimum() is called is should return 2 which is the minimum in the \nstack.\n\n</pre>\n\n<p><strong>Constriants:</strong></p>\n\n<ol>\n<li>getMinimum should return the minimum value in O(1)</li>\n<li>Space constraint also has to be considered while designing it and if you use extra space, it should be of constant space.</li>\n</ol>\n    ","a":"\n<p>EDIT: This fails the \"constant space\" constraint - it basically doubles the space required. I very much doubt that there's a solution which <em>doesn't</em> do that though, without wrecking the runtime complexity somewhere (e.g. making push/pop O(n)). Note that this doesn't change the <em>complexity</em> of the space required, e.g. if you've got a stack with O(n) space requirements, this will still be O(n) just with a different constant factor.</p>\n\n<p><strong>Non-constant-space solution</strong></p>\n\n<p>Keep a \"duplicate\" stack of \"minimum of all values lower in the stack\". When you pop the main stack, pop the min stack too. When you push the main stack, push either the new element or the current min, whichever is lower. <code>getMinimum()</code> is then implemented as just <code>minStack.peek()</code>.</p>\n\n<p>So using your example, we'd have:</p>\n\n<pre><code>Real stack        Min stack\n\n5  --&gt; TOP        1\n1                 1\n4                 2\n6                 2\n2                 2\n</code></pre>\n\n<p>After popping twice you get:</p>\n\n<pre><code>Real stack        Min stack\n\n4                 2\n6                 2\n2                 2\n</code></pre>\n\n<p>Please let me know if this isn't enough information. It's simple when you grok it, but it might take a bit of head-scratching at first :)</p>\n\n<p>(The downside of course is that it doubles the space requirement. Execution time doesn't suffer significantly though - i.e. it's still the same complexity.)</p>\n\n<p>EDIT: There's a variation which is slightly more fiddly, but has better space in general. We still have the min stack, but we only pop from it when the value we pop from the main stack is equal to the one on the min stack. We only <em>push</em> to the min stack when the value being pushed onto the main stack is less than <em>or equal</em> to the current min value. This allows duplicate min values. <code>getMinimum()</code> is still just a peek operation. For example, taking the original version and pushing 1 again, we'd get:</p>\n\n<pre><code>Real stack        Min stack\n\n1  --&gt; TOP        1\n5                 1\n1                 2\n4                 \n6                 \n2\n</code></pre>\n\n<p>Popping from the above pops from both stacks because 1 == 1, leaving:</p>\n\n<pre><code>Real stack        Min stack\n\n5  --&gt; TOP        1\n1                 2\n4                 \n6                 \n2\n</code></pre>\n\n<p>Popping again <em>only</em> pops from the main stack, because 5 &gt; 1:</p>\n\n<pre><code>Real stack        Min stack\n\n1                 1\n4                 2\n6                 \n2\n</code></pre>\n\n<p>Popping again pops both stacks because 1 == 1:</p>\n\n<pre><code>Real stack        Min stack\n\n4                 2\n6                 \n2\n</code></pre>\n\n<p>This ends up with the same worst case space complexity (double the original stack) but much better space usage if we rarely get a \"new minimum or equal\".</p>\n\n<p>EDIT: Here's an implementation of Pete's evil scheme. I haven't tested it thoroughly, but I <em>think</em> it's okay :)</p>\n\n<pre><code>using System.Collections.Generic;\n\npublic class FastMinStack&lt;T&gt;\n{\n    private readonly Stack&lt;T&gt; stack = new Stack&lt;T&gt;();\n    // Could pass this in to the constructor\n    private readonly IComparer&lt;T&gt; comparer = Comparer&lt;T&gt;.Default;\n\n    private T currentMin;\n\n    public T Minimum\n    {\n        get { return currentMin; }\n    }\n\n    public void Push(T element)\n    {\n        if (stack.Count == 0 ||\n            comparer.Compare(element, currentMin) &lt;= 0)\n        {\n            stack.Push(currentMin);\n            stack.Push(element);\n            currentMin = element;\n        }\n        else\n        {\n            stack.Push(element);\n        }\n    }\n\n    public T Pop()\n    {\n        T ret = stack.Pop();\n        if (comparer.Compare(ret, currentMin) == 0)\n        {\n            currentMin = stack.Pop();\n        }\n        return ret;\n    }\n}\n</code></pre>\n    "},{"t":"How Do I Choose Between a Hash Table and a Trie (Prefix Tree)?","l":"http://stackoverflow.com/questions/245878/how-do-i-choose-between-a-hash-table-and-a-trie-prefix-tree","q":"\n\n<p>So if I have to choose between a hash table or a prefix tree what are the discriminating factors that would lead me to choose one over the other. From my own naive point of view it seems as though using a trie has some extra overhead since it isn't stored as an array but that in terms of run time (assuming the longest key is the longest english word) it can be essentially O(1) (in relation to the upper bound). Maybe the longest english word is 50 characters?</p>\n\n<p>Hash tables are instant look up <em>once you get the index</em>. Hashing the key to get the index however seems like it could easily take near 50 steps.</p>\n\n<p>Can someone provide me a more experienced perspective on this? Thanks!</p>\n    ","a":"\n<p>Advantages of tries:</p>\n\n<p>The basics:</p>\n\n<ul>\n<li>Predictable O(k) lookup time where k is the size of the key</li>\n<li>Lookup can take less than k time if it's not there</li>\n<li>Supports ordered traversal</li>\n<li>No need for a hash function</li>\n<li>Deletion is straightforward</li>\n</ul>\n\n<p>New operations:</p>\n\n<ul>\n<li>You can quickly look up prefixes of keys, enumerate all entries with a given prefix, etc.</li>\n</ul>\n\n<p>Advantages of linked structure:</p>\n\n<ul>\n<li>If there are many common prefixes, the space they require is shared.</li>\n<li>Immutable tries can share structure. Instead of updating a trie in place, you can build a new one that's different only along one branch, elsewhere pointing into the old trie. This can be useful for concurrency, multiple simultaneous versions of a table, etc.</li>\n<li>An immutable trie is compressible. That is, it can share structure on the <em>suffixes</em> as well, by hash-consing.</li>\n</ul>\n\n<p>Advantages of hashtables:</p>\n\n<ul>\n<li>Everyone knows hashtables, right? Your system will already have a nice well-optimized implementation, faster than tries for most purposes.</li>\n<li>Your keys need not have any special structure. </li>\n<li>More space-efficient than the obvious linked trie structure (<strong>see comments below</strong>)</li>\n</ul>\n    "},{"t":"Shortest Sudoku Solver in Python - How does it work?","l":"http://stackoverflow.com/questions/201461/shortest-sudoku-solver-in-python-how-does-it-work","q":"\n\n<p>I was playing around with my own Sudoku solver and was looking for some pointers to good and fast design when I came across this:</p>\n\n<pre><code>def r(a):i=a.find('0');~i or exit(a);[m\nin[(i-j)%9*(i/9^j/9)*(i/27^j/27|i%9/3^j%9/3)or a[j]for\nj in range(81)]or r(a[:i]+m+a[i+1:])for m in'%d'%5**18]\nfrom sys import*;r(argv[1])\n</code></pre>\n\n<p>My own implementation solves Sudokus the same way I solve them in my head but how does this cryptic algorithm work?</p>\n\n<p><a href=\"http://scottkirkwood.blogspot.com/2006/07/shortest-sudoku-solver-in-python.html\">http://scottkirkwood.blogspot.com/2006/07/shortest-sudoku-solver-in-python.html</a></p>\n    ","a":"\n<p>Well, you can make things a little easier by fixing up the syntax:</p>\n\n<pre><code>def r(a):\n  i = a.find('0')\n  ~i or exit(a)\n  [m in[(i-j)%9*(i/9^j/9)*(i/27^j/27|i%9/3^j%9/3)or a[j]for j in range(81)] or r(a[:i]+m+a[i+1:])for m in'%d'%5**18]\nfrom sys import *\nr(argv[1])\n</code></pre>\n\n<p>Cleaning up a little:</p>\n\n<pre><code>from sys import exit, argv\ndef r(a):\n  i = a.find('0')\n  if i == -1:\n    exit(a)\n  for m in '%d' % 5**18:\n    m in[(i-j)%9*(i/9^j/9)*(i/27^j/27|i%9/3^j%9/3) or a[j] for j in range(81)] or r(a[:i]+m+a[i+1:])\n\nr(argv[1])\n</code></pre>\n\n<p>Okay, so this script expects a command-line argument, and calls the function r on it.  If there are no zeros in that string, r exits and prints out its argument.  </p>\n\n<blockquote>\n  <p>(If another type of object is passed,\n  None is equivalent to passing zero,\n  and any other object is printed to\n  sys.stderr  and results in an exit\n  code of 1. In particular,\n  sys.exit(\"some error message\") is a\n  quick way to exit a program when an\n  error occurs. See\n  <a href=\"http://www.python.org/doc/2.5.2/lib/module-sys.html\">http://www.python.org/doc/2.5.2/lib/module-sys.html</a>)</p>\n</blockquote>\n\n<p>I guess this means that zeros correspond to open spaces, and a puzzle with no zeros is solved.  Then there's that nasty recursive expression.</p>\n\n<p>The loop is interesting: <code>for m in'%d'%5**18</code></p>\n\n<p>Why 5**18? It turns out that <code>'%d'%5**18</code> evaluates to <code>'3814697265625'</code>.  This is a string that has each digit 1-9 at least once, so maybe it's trying to place each of them.  In fact, it looks like this is what <code>r(a[:i]+m+a[i+1:])</code> is doing: recursively calling r, with the first blank filled in by a digit from that string.  But this only happens if the earlier expression is false.  Let's look at that:</p>\n\n<p><code>m in [(i-j)%9*(i/9^j/9)*(i/27^j/27|i%9/3^j%9/3) or a[j] for j in range(81)]</code></p>\n\n<p>So the placement is done only if m is not in that monster list.  Each element is either a number (if the first expression is nonzero) or a character (if the first expression is zero).  m is ruled out as a possible substitution if it appears as a character, which can only happen if the first expression is zero.  When is the expression zero?</p>\n\n<p>It has three parts that are multiplied:</p>\n\n<ul>\n<li><code>(i-j)%9</code> which is zero if i and j are a multiple of 9 apart, i.e. the same column.</li>\n<li><code>(i/9^j/9)</code> which is zero if i/9 == j/9, i.e. the same row.</li>\n<li><code>(i/27^j/27|i%9/3^j%9/3)</code> which is zero if both of these are zero:</li>\n<li><ul>\n<li><code>i/27^j^27</code> which is zero if i/27 == j/27, i.e. the same block of three rows</li>\n</ul></li>\n<li><ul>\n<li><code>i%9/3^j%9/3</code> which is zero if i%9/3 == j%9/3, i.e. the same block of three columns</li>\n</ul></li>\n</ul>\n\n<p>If any of these three parts is zero, the entire expression is zero.  In other words, if i and j share a row, column, or 3x3 block, then the value of j can't be used as a candidate for the blank at i.  Aha!</p>\n\n<pre><code>from sys import exit, argv\ndef r(a):\n  i = a.find('0')\n  if i == -1:\n    exit(a)\n  for m in '3814697265625':\n    okay = True\n    for j in range(81):\n      if (i-j)%9 == 0 or (i/9 == j/9) or (i/27 == j/27 and i%9/3 == j%9/3):\n        if a[j] == m:\n          okay = False\n          break\n    if okay:\n      # At this point, m is not excluded by any row, column, or block, so let's place it and recurse\n      r(a[:i]+m+a[i+1:])\n\nr(argv[1])\n</code></pre>\n\n<p>Note that if none of the placements work out, r will return and back up to the point where something else can be chosen, so it's a basic depth first algorithm.</p>\n\n<p>Not using any heuristics, it's not particularly efficient.  I took this puzzle from Wikipedia (<a href=\"http://en.wikipedia.org/wiki/Sudoku\">http://en.wikipedia.org/wiki/Sudoku</a>):</p>\n\n<pre><code>$ time python sudoku.py 530070000600195000098000060800060003400803001700020006060000280000419005000080079\n534678912672195348198342567859761423426853791713924856961537284287419635345286179\n\nreal    0m47.881s\nuser    0m47.223s\nsys 0m0.137s\n</code></pre>\n\n<p>Addendum: How I would rewrite it as a maintenance programmer (this version has about a 93x speedup :)</p>\n\n<pre><code>import sys\n\ndef same_row(i,j): return (i/9 == j/9)\ndef same_col(i,j): return (i-j) % 9 == 0\ndef same_block(i,j): return (i/27 == j/27 and i%9/3 == j%9/3)\n\ndef r(a):\n  i = a.find('0')\n  if i == -1:\n    sys.exit(a)\n\n  excluded_numbers = set()\n  for j in range(81):\n    if same_row(i,j) or same_col(i,j) or same_block(i,j):\n      excluded_numbers.add(a[j])\n\n  for m in '123456789':\n    if m not in excluded_numbers:\n      # At this point, m is not excluded by any row, column, or block, so let's place it and recurse\n      r(a[:i]+m+a[i+1:])\n\nif __name__ == '__main__':\n  if len(sys.argv) == 2 and len(sys.argv[1]) == 81:\n    r(sys.argv[1])\n  else:\n    print 'Usage: python sudoku.py puzzle'\n    print '  where puzzle is an 81 character string representing the puzzle read left-to-right, top-to-bottom, and 0 is a blank'\n</code></pre>\n    "},{"t":"How do you calculate the average of a set of angles?","l":"http://stackoverflow.com/questions/491738/how-do-you-calculate-the-average-of-a-set-of-angles","q":"\n\n<p>I want to calculate the average of a set of angles. For example, I might have several samples from the reading of a compass. The problem of course is how to deal with the wraparound. The same algorithm might be useful for a clockface. </p>\n\n<p>The actual question is more complicated - what do statistics mean on a sphere or in an algebraic space which \"wraps round\", eg the additive group mod n. The answer may not be unique, eg the average of 359 degrees and 1 degree could be 0 degrees or 180, but statistically 0 looks better.</p>\n\n<p>This is a real programming problem for me and I'm trying to make it not look like just a Math problem.</p>\n\n<p>[Edit: to resolve all the confusion, when I refer to angles you can assume I mean bearings]</p>\n    ","a":"\n<p>Compute unit vectors from the angles and take the angle of their average.</p>\n    "},{"t":"Calculating frames per second in a game","l":"http://stackoverflow.com/questions/87304/calculating-frames-per-second-in-a-game","q":"\n\n<p>What's a good algorithm for calculating frames per second in a game? I want to show it as a number in the corner of the screen. If I just look at how long it took to render the last frame the number changes too fast.</p>\n\n<p>Bonus points if your answer updates each frame and doesn't converge differently when the frame rate is increasing vs decreasing.</p>\n    ","a":"\n<p>You need a smoothed average, the easiest way is to take the current answer (the time to draw the last frame) and combine it with the previous answer.</p>\n\n<pre><code>// eg.\ntime = time * 0.9 + last_frame * 0.1\n</code></pre>\n\n<p>By adjusting the 0.9 / 0.1 ratio you can change the 'time constant' - that is how quickly the number responds to changes. A larger fraction in favour of the old answer gives a slower smoother change, a large fraction in favour of the new answer gives a quicker changing value.   Obviously the two factors must add to one!</p>\n    "},{"t":"Given a 2d array sorted in increasing order from left to right and top to bottom, what is the best way to search for a target number?","l":"http://stackoverflow.com/questions/2457792/given-a-2d-array-sorted-in-increasing-order-from-left-to-right-and-top-to-bottom","q":"\n\n<p>I was recently given this interview question and I'm curious what a good solution to it would be.</p>\n\n<blockquote>\n  <p>Say I'm given a 2d array where all the\n  numbers in the array are in increasing\n  order from left to right and top to\n  bottom.</p>\n  \n  <p>What is the best way to search and\n  determine if a target number is in the\n  array?</p>\n</blockquote>\n\n<p>Now, my first inclination is to utilize a binary search since my data is sorted.  I can determine if a number is in a single row in O(log N) time.  However, it is the 2 directions that throw me off.</p>\n\n<p>Another solution I thought may work is to start somewhere in the middle.  If the middle value is less than my target, then I can be sure it is in the left square portion of the matrix from the middle.  I then move diagnally and check again, reducing the size of the square that the target could potentially be in until I have honed in on the target number.</p>\n\n<p>Does anyone have any good ideas on solving this problem?</p>\n\n<p>Example array:  </p>\n\n<p>Sorted left to right, top to bottom.</p>\n\n<pre><code>1 2 4 5 6  \n2 3 5 7 8  \n4 6 8 9 10  \n5 8 9 10 11  \n</code></pre>\n    ","a":"\n<p>Here's a simple approach: </p>\n\n<ol>\n<li>Start at the bottom-left corner.  </li>\n<li>If the target is less than that value, it must be above us, so <strong>move up one</strong>.</li>\n<li>Otherwise we know that the target can't be in that column, so <strong>move right one</strong>.</li>\n<li>Goto 2.</li>\n</ol>\n\n<p>For an <code>NxM</code> array, this runs in <code>O(N+M)</code>.  I think it would be difficult to do better.  :)</p>\n\n<hr>\n\n<p><strong>Edit:</strong> Lots of good discussion.  I was talking about the general case above; clearly, if <code>N</code> or <code>M</code>  are small, you could use a binary search approach to do this in something approaching logarithmic time.  </p>\n\n<p>Here are some details, for those who are curious:</p>\n\n<h2>History</h2>\n\n<p>This simple algorithm is called a <a href=\"http://www.cs.geneseo.edu/~baldwin/math-thinking/saddleback.html\">Saddleback Search</a>. It's been around for a while, and it is optimal when <code>N == M</code>.  Some references:</p>\n\n<ul>\n<li>David Gries, <strong><a href=\"http://rads.stackoverflow.com/amzn/click/0387964800\">The Science of Programming</a></strong>. <em>Springer-Verlag, 1989</em>.</li>\n<li>Edsgar Dijkstra, <strong><a href=\"http://www.cs.utexas.edu/users/EWD/index09xx.html\">The Saddleback Search</a></strong>. <em>Note EWD-934, 1985</em>.</li>\n</ul>\n\n<p>However, when <code>N &lt; M</code>, intuition suggests that binary search should be able to do better than <code>O(N+M)</code>: For example, when <code>N == 1</code>, a pure binary search will run in logarithmic rather than linear time.</p>\n\n<h2>Worst-case bound</h2>\n\n<p>Richard Bird examined this intuition that binary search could improve the Saddleback algorithm in a 2006 paper:</p>\n\n<ul>\n<li>Richard S. Bird, <strong><a href=\"http://www.cs.ox.ac.uk/publications/publication2664-abstract.html\">Improving Saddleback Search: A Lesson in Algorithm Design</a></strong>, <em>in Mathematics of Program Construction, pp. 82--89, volume 4014, 2006</em>.</li>\n</ul>\n\n<p>Using a rather unusual conversational technique, Bird shows us that for <code>N &lt;= M</code>, this problem has a lower bound of <code>Ω(N * log(M/N))</code>.  This bound make sense, as it gives us linear performance when <code>N == M</code> and logarithmic performance when <code>N == 1</code>.</p>\n\n<h2>Algorithms for rectangular arrays</h2>\n\n<p>One approach that uses a row-by-row binary search looks like this:</p>\n\n<ol>\n<li>Start with a rectangular array where <code>N &lt; M</code>.  Let's say <code>N</code> is rows and <code>M</code> is columns.</li>\n<li>Do a binary search on the middle row for <code>value</code>.  If we find it, we're done.</li>\n<li>Otherwise we've found an adjacent pair of numbers <code>s</code> and <code>g</code>, where <code>s &lt; value &lt; g</code>.</li>\n<li>The rectangle of numbers above and to the left of <code>s</code> is less than <code>value</code>, so we can eliminate it.</li>\n<li>The rectangle below and to the right of <code>g</code> is greater than <code>value</code>, so we can eliminate it.</li>\n<li>Go to step (2) for each of the two remaining rectangles.</li>\n</ol>\n\n<p>In terms of worst-case complexity, this algorithm does <code>log(M)</code> work to eliminate half the possible solutions, and then recursively calls itself twice on two smaller problems.  We do have to repeat a smaller version of that <code>log(M)</code> work for every row, <strong>but if the number of rows is small compared to the number of columns, then being able to eliminate all of those columns in logarithmic time starts to become worthwhile</strong>.</p>\n\n<p>This gives the algorithm a complexity of <code>T(N,M) = log(M) + 2 * T(M/2, N/2)</code>, which Bird shows to be <code>O(N * log(M/N))</code>.</p>\n\n<p><a href=\"http://twistedoakstudios.com/blog/Post5365_searching-a-sorted-matrix-faster\">Another approach posted by Craig Gidney</a> describes an algorithm similar the approach above: it examines a row at a time using a step size of <code>M/N</code>.  His analysis shows that this results in <code>O(N * log(M/N))</code> performance as well.</p>\n\n<h2>Performance Comparison</h2>\n\n<p>Big-O analysis is all well and good, but how well do these approaches work in practice?  The chart below examines four algorithms for increasingly \"square\" arrays:</p>\n\n<p><img src=\"http://i.stack.imgur.com/SZwvl.png\" alt=\"algorithm performance vs squareness\"></p>\n\n<p>(The \"naive\" algorithm simply searches every element of the array.  The \"recursive\" algorithm is described above.  The \"hybrid\" algorithm is an implementation of <a href=\"http://twistedoakstudios.com/blog/Post5365_searching-a-sorted-matrix-faster\">Gidney's algorithm</a>. For each array size, performance was measured by timing each algorithm over fixed set of 1,000,000 randomly-generated arrays.)</p>\n\n<p>Some notable points:</p>\n\n<ul>\n<li>As expected, the \"binary search\" algorithms offer the best performance on rectangular arrays and the Saddleback algorithm works the best on square arrays.</li>\n<li>The Saddleback algorithm performs worse than the \"naive\" algorithm for 1-d arrays, presumably because it does multiple comparisons on each item.</li>\n<li>The performance hit that the \"binary search\" algorithms take on square arrays is presumably due to the overhead of running repeated binary searches.</li>\n</ul>\n\n<h2>Summary</h2>\n\n<p>Clever use of binary search can provide <code>O(N * log(M/N)</code> performance for both rectangular and square arrays.  The <code>O(N + M)</code> \"saddleback\" algorithm is much simpler, but suffers from performance degradation as arrays become increasingly rectangular.</p>\n    "},{"t":"How are ssl certificates verified?","l":"http://stackoverflow.com/questions/188266/how-are-ssl-certificates-verified","q":"\n\n<p>What is the series of steps needed to securely verify a ssl certificate?  My (very limited) understanding is that when you visit an https site, the server sends a certificate to the client (the browser) and the browser gets the certificate's issuer information from that certificate, then uses that to contact the issuerer, and somehow compares certificates for validity.  </p>\n\n<ul>\n<li>How exactly is this done? </li>\n<li>What about the process makes it immune to man-in-the-middle attacks?</li>\n<li>What prevents some random person from setting up their own verification service to use in man-in-the-middle attacks, so everything \"looks\" secure?</li>\n</ul>\n    ","a":"\n<p>Here is a very simplified explanation:</p>\n\n<ol>\n<li><p>Your web browser downloads the web server's certificate, which contains the public key of the web server.  This certificate is signed with the private key of a trusted certificate authority.</p></li>\n<li><p>Your web browser comes installed with the public keys of all of the major certificate authorities.  It uses this public key to verify that the web server's certificate was indeed signed by the trusted certificate authority.</p></li>\n<li><p>The certificate contains the domain name and/or ip address of the web server.  Your web browser confirms that the address listed in the certificate is the one to which it has an open connection.</p></li>\n<li><p>Your web browser generates a shared symmetric key which will be used to encrypt the HTTP traffic on this connection; this is much more efficient than using public/private key encryption for everything.  Your browser encrypts the symmetric key with the public key of the web server then sends it back, thus ensuring that only the web server can decrypt it, since only the web server has its private key.</p></li>\n</ol>\n\n<p>Note that the certificate authority (CA) is essential to preventing man-in-the-middle attacks.  However, even an unsigned certificate will prevent someone from passively listening in on your encrypted traffic, since they have no way to gain access to your shared symmetric key.</p>\n    "},{"t":"How does the MapReduce sort algorithm work?","l":"http://stackoverflow.com/questions/1152732/how-does-the-mapreduce-sort-algorithm-work","q":"\n\n<p>One of the main examples that is used in demonstrating the power of MapReduce is the <a href=\"http://developer.yahoo.net/blogs/hadoop/2008/07/apache%5Fhadoop%5Fwins%5Fterabyte%5Fsort%5Fbenchmark.html\">Terasort benchmark</a>. I'm having trouble understanding the basics of the sorting algorithm used in the MapReduce environment. </p>\n\n<p>To me sorting simply involves determining the relative position of an element in relationship to all other elements. So sorting involves comparing \"everything\" with \"everything\". Your average sorting algorithm (quick, bubble, ...) simply does this in a smart way.</p>\n\n<p>In my mind splitting the dataset into many pieces means you can sort a single piece and then you still have to integrate these pieces into the 'complete' fully sorted dataset. Given the terabyte dataset distributed over thousands of systems I expect this to be a huge task.</p>\n\n<p>So how is this really done? How does this MapReduce sorting algorithm work?</p>\n\n<p>Thanks for helping me understand.</p>\n    ","a":"\n<p>Here are some details on <a href=\"http://sortbenchmark.org/YahooHadoop.pdf\" rel=\"nofollow\">Hadoop's implementation for Terasort</a>:</p>\n\n<blockquote>\n  <p>TeraSort is a standard map/reduce sort, except for a custom partitioner that uses a sorted list of N − 1 sampled keys that define the key range for each reduce. In particular, all keys such that sample[i − 1] &lt;= key &lt; sample[i] are sent to reduce i. This guarantees that the output of reduce i are all less than the output of reduce i+1.\"</p>\n</blockquote>\n\n<p>So their trick is in the way they determine the keys during the map phase. Essentially they ensure that every value in a single reducer is guaranteed to be 'pre-sorted' against all other reducers.</p>\n\n<p>I found the paper reference through <a href=\"http://perspectives.mvdirona.com/2008/07/08/HadoopWinsTeraSort.aspx\" rel=\"nofollow\">James Hamilton's Blog Post</a>.</p>\n    "},{"t":"What is the difference between LR, SLR, and LALR parsers?","l":"http://stackoverflow.com/questions/2676144/what-is-the-difference-between-lr-slr-and-lalr-parsers","q":"\n\n<p>What is the actual difference between LR, SLR, and LALR parsers? I know that SLR and LALR are types of LR parsers, but what is the actual difference as far as their parsing tables are concerned?</p>\n\n<p>And how to show whether a grammar is LR, SLR, or LALR? For an LL grammar we just have to show that any cell of the parsing table should not contain multiple production rules. Any similar rules for LALR, SLR, and LR?</p>\n\n<p>For example, how can we show that the grammar</p>\n\n<pre><code>S --&gt; Aa | bAc | dc | bda\nA --&gt; d\n</code></pre>\n\n<p>is LALR(1) but not SLR(1)?</p>\n\n<hr>\n\n<p><strong>EDIT (ybungalobill)</strong>: I didn't get a satisfactory answer for what's the difference between LALR and LR. So LALR's tables are smaller in size but it can recognize only a subset of LR grammars. Can someone elaborate more on the difference between LALR and LR please? LALR(1) and LR(1) will be sufficient for an answer. Both of them use 1 token look-ahead and <em>both</em> are table driven! How they are different?</p>\n    ","a":"\n<p>LALR parsers merge similar states within an LR grammar to produce parser state tables that are exactly the same size as the equivalent SLR grammar, which are usually an order of magnitude smaller than pure LR parsing tables. However, for LR grammars that are too complex to be LALR, these merged states result in parser conflicts, or produce a parser that does not fully recognize the original LR grammar.</p>\n\n<p>BTW, I mention a few things about this in my MLR(k) parsing table algorithm <a href=\"http://david.tribble.com/text/honalee.html\" rel=\"nofollow\">here</a>.</p>\n\n<p><strong>Addendum</strong></p>\n\n<p>The short answer is that the LALR parsing tables are smaller, but the parser machinery is the same. A given LALR grammar will produce much larger parsing tables if all of the LR states are generated, with a lot of redundant (near-identical) states.</p>\n\n<p>The LALR tables are smaller because the similar (redundant) states are merged together, effectively throwing away context/lookahead info that the separate states encode. The advantage is that you get much smaller parsing tables for the same grammar.</p>\n\n<p>The drawback is that not all LR grammars can be encoded as LALR tables because more complex grammars have more complicated lookaheads, resulting in two or more states instead of a single merged state.</p>\n\n<p>The main difference is that the algorithm to produce LR tables carries more info around between the transitions from state to state while the LALR algorithm does not. So the LALR algorithm cannot tell if a given merged state should really be left as two or more separate states.</p>\n    "},{"t":"How to calculate an angle from three points?","l":"http://stackoverflow.com/questions/1211212/how-to-calculate-an-angle-from-three-points","q":"\n\n<p>Lets say you have this:</p>\n\n<pre><code>P1 = (x=2, y=50)\nP2 = (x=9, y=40)\nP3 = (x=5, y=20)\n</code></pre>\n\n<p>Assume that <code>P1</code> is the center point of a circle. It is always the same.\nI want the angle that is made up by <code>P2</code> and <code>P3</code>, or in other words the angle that is next to <code>P1</code>. The inner angle to be precise. It will always be an acute angle, so less than -90 degrees.</p>\n\n<p>I thought: Man, that's simple geometry math. But I have looked for a formula for around 6 hours now, and only find people talking about complicated NASA stuff like arccos and vector scalar product stuff. My head feels like it's in a fridge.</p>\n\n<p>Some math gurus here that think this is a simple problem? I don't think the programming language matters here, but for those who think it does: java and objective-c.  I need it for both, but haven't tagged it for these.</p>\n    ","a":"\n<p>If you mean the angle that P1 is the vertex of then this should work:</p>\n\n<blockquote>\n  <p>arcos<sup></sup>((P<sub>12</sub><sup>2</sup>\n  + P<sub>13</sub><sup>2</sup> - P<sub>23</sub><sup>2</sup>) / (2 *\n  P<sub>12</sub> * P<sub>13</sub>))</p>\n</blockquote>\n\n<p>where P<sub>12</sub> is the length of the segment from P1 to P2, calculated by</p>\n\n<blockquote>\n  <p>sqrt((P1<sub>x</sub> -\n  P2<sub>x</sub>)<sup>2</sup> +\n  (P1<sub>y</sub> -\n  P2<sub>y</sub>)<sup>2</sup>)</p>\n</blockquote>\n    "},{"t":"Finding all possible combinations of numbers to reach a given sum","l":"http://stackoverflow.com/questions/4632322/finding-all-possible-combinations-of-numbers-to-reach-a-given-sum","q":"\n\n<p>How would you go about testing all possible combinations of additions from a given set of numbers so they add up to a given final number?</p>\n\n<p>Example:</p>\n\n<ul>\n<li>Set of numbers to add: {1,5,22,15,0,...}</li>\n<li>Desired result: 12345</li>\n</ul>\n\n<p>P.S: Asking this problem as maths isn't my forte and wondering how this could be adapted in code.</p>\n    ","a":"\n<p>This problem can be solved with a recursive combinations of all possible sums filtering out those that reach the target. Here is the algorithm in Python:</p>\n\n<pre><code>def subset_sum(numbers, target, partial=[]):\n    s = sum(partial)\n\n    # check if the partial sum is equals to target\n    if s == target: \n        print \"sum(%s)=%s\" % (partial, target)\n    if s &gt;= target:\n        return  # if we reach the number why bother to continue\n\n    for i in range(len(numbers)):\n        n = numbers[i]\n        remaining = numbers[i+1:]\n        subset_sum(remaining, target, partial + [n]) \n\n\nif __name__ == \"__main__\":\n    subset_sum([3,9,8,4,5,7,10],15)\n\n    #Outputs:\n    #sum([3, 8, 4])=15\n    #sum([3, 5, 7])=15\n    #sum([8, 7])=15\n    #sum([5, 10])=15\n</code></pre>\n\n<p>This type of algorithms are very well explained in the following <a href=\"http://www.youtube.com/watch?v=NdF1QDTRkck\">Standford's Abstract Programming lecture</a> - this video is very recommendable to understand how recursion works to generate permutations of solutions.</p>\n\n<p><strong>Edit</strong></p>\n\n<p>Here is the Java version of the same algorithm:</p>\n\n<pre><code>package tmp;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\n\nclass SumSet {\n    static void sum_up_recursive(ArrayList&lt;Integer&gt; numbers, int target, ArrayList&lt;Integer&gt; partial) {\n       int s = 0;\n       for (int x: partial) s += x;\n       if (s == target)\n            System.out.println(\"sum(\"+Arrays.toString(partial.toArray())+\")=\"+target);\n       if (s &gt;= target)\n            return;\n       for(int i=0;i&lt;numbers.size();i++) {\n             ArrayList&lt;Integer&gt; remaining = new ArrayList&lt;Integer&gt;();\n             int n = numbers.get(i);\n             for (int j=i+1; j&lt;numbers.size();j++) remaining.add(numbers.get(j));\n             ArrayList&lt;Integer&gt; partial_rec = new ArrayList&lt;Integer&gt;(partial);\n             partial_rec.add(n);\n             sum_up_recursive(remaining,target,partial_rec);\n       }\n    }\n    static void sum_up(ArrayList&lt;Integer&gt; numbers, int target) {\n        sum_up_recursive(numbers,target,new ArrayList&lt;Integer&gt;());\n    }\n    public static void main(String args[]) {\n        Integer[] numbers = {3,9,8,4,5,7,10};\n        int target = 15;\n        sum_up(new ArrayList&lt;Integer&gt;(Arrays.asList(numbers)),target);\n    }\n}\n</code></pre>\n\n<p>It is exactly the same heuristic. My Java is a bit rusty but I think is easy to understand.</p>\n\n<p><strong>C# conversion of Java solution:</strong> <em>(by @JeremyThompson)</em></p>\n\n<pre><code>public static void Main(string[] args)\n{\n    List&lt;int&gt; numbers = new List&lt;int&gt;() { 3, 9, 8, 4, 5, 7, 10 };\n    int target = 15;\n    sum_up(numbers, target);\n}\n\nprivate static void sum_up(List&lt;int&gt; numbers, int target)\n{\n    sum_up_recursive(numbers, target, new List&lt;int&gt;());\n}\n\nprivate static void sum_up_recursive(List&lt;int&gt; numbers, int target, List&lt;int&gt; partial)\n{\n    int s = 0;\n    foreach (int x in partial) s += x;\n\n    if (s == target)\n        Console.WriteLine(\"sum(\" + string.Join(\",\", partial.ToArray()) + \")=\" + target);\n\n    if (s &gt;= target)\n        return;\n\n    for (int i = 0; i &lt; numbers.Count; i++)\n    {\n        List&lt;int&gt; remaining = new List&lt;int&gt;();\n        int n = numbers[i];\n        for (int j = i + 1; j &lt; numbers.Count; j++) remaining.Add(numbers[j]);\n\n        List&lt;int&gt; partial_rec = new List&lt;int&gt;(partial);\n        partial_rec.Add(n);\n        sum_up_recursive(remaining, target, partial_rec);\n    }\n}\n</code></pre>\n\n<p><strong>Ruby solution:</strong> <em>(by @emaillenin)</em></p>\n\n<pre><code>def subset_sum(numbers, target, partial=[])\n  s = partial.inject 0, :+\n# check if the partial sum is equals to target\n\n  puts \"sum(#{partial})=#{target}\" if s == target\n\n  return if s &gt;= target # if we reach the number why bother to continue\n\n  (0..(numbers.length - 1)).each do |i|\n    n = numbers[i]\n    remaining = numbers.drop(i+1)\n    subset_sum(remaining, target, partial + [n])\n  end\nend\n\nsubset_sum([3,9,8,4,5,7,10],15)\n</code></pre>\n\n<p><strong>Edit: complexity discussion</strong></p>\n\n<p>As others mention this is an <a href=\"http://en.wikipedia.org/wiki/Subset_sum_problem\">NP problem</a>. It can be solved in exponential time O(2^n), for instance for n=10 there will be 1024 possible solutions. If the targets you are trying to reach are in a low range then this algorithm works. So for instance:</p>\n\n<p><code>subset_sum([1,2,3,4,5,6,7,8,9,10],100000)</code> generates 1024 branches because the target never gets to filter out possible solutions.</p>\n\n<p>On the other hand <code>subset_sum([1,2,3,4,5,6,7,8,9,10],10)</code> generates only 175 branches, because the target to reach <code>10</code> gets to filter out many combinations.</p>\n\n<p>If <code>N</code> and <code>Target</code> are big numbers one should move into an approximate version of the solution. </p>\n    "},{"t":"How to find a duplicate element in an array of shuffled consecutive integers?","l":"http://stackoverflow.com/questions/2605766/how-to-find-a-duplicate-element-in-an-array-of-shuffled-consecutive-integers","q":"\n\n<p>I recently came across a question somewhere:</p>\n\n<blockquote>\n  <p>Suppose you have an array of 1001 integers. The integers are in random order, but you know each of the integers is between 1 and 1000 (inclusive). In addition, each number appears only once in the array, except for one number, which occurs twice. Assume that you can access each element of the array only once. Describe an algorithm to find the repeated number. If you used auxiliary storage in your algorithm, can you find an algorithm that does not require it?</p>\n</blockquote>\n\n<p>What I am interested in to know is the <strong>second part</strong>, i.e., <strong>without using auxiliary storage</strong>.  Do you have any idea?</p>\n    ","a":"\n<p>Just add them all up, and subtract the total you would expect if only 1001 numbers were used from that.</p>\n\n<p>Eg:</p>\n\n<pre><code>Input: 1,2,3,2,4 =&gt; 12\nExpected: 1,2,3,4 =&gt; 10\n\nInput - Expected =&gt; 2\n</code></pre>\n    "},{"t":"How to rank a million images with a crowdsourced sort","l":"http://stackoverflow.com/questions/164831/how-to-rank-a-million-images-with-a-crowdsourced-sort","q":"\n\n<p>I'd like to rank a collection of landscape images by making a game whereby site visitors can rate them, in order to find out which images people find the  most appealing.</p>\n\n<p>What would be a good method of doing that?</p>\n\n<ul>\n<li><strong>Hot-or-Not style</strong>? I.e. show a single image, ask the user to rank it from 1-10. As I see it, this allows me to average the scores, and I would just need to ensure that I get an even distribution of votes across all the images. Fairly simple to implement.</li>\n<li><strong>Pick A-or-B</strong>? I.e. show two images, ask user to pick the better one. This is appealing as there is no numerical ranking, it's just a comparison. But how would I implement it? My first thought was to do it as a quicksort, with the comparison operations being provided by humans, and once completed, simply repeat the sort ad-infinitum.</li>\n</ul>\n\n<p>How would <em>you</em> do it?</p>\n\n<p><em>If you need numbers, I'm talking about one million images, on a site with 20,000 daily visits. I'd imagine a small proportion might play the game, for the sake of argument, lets say I can generate 2,000 human sort operations a day! It's a non-profit website, and the terminally curious will find it through my profile :)</em></p>\n    ","a":"\n<p>As others have said, ranking 1-10 does not work that well because people have different levels.</p>\n\n<p>The problem with the <strong>Pick A-or-B</strong> method is that its not guaranteed for the system to be transitive (A can beat B, but B beats C, and C beats A).  <strong>Having nontransitive comparison operators breaks sorting algorithms</strong>.  With quicksort, against this example, the letters not chosen as the pivot will be incorrectly ranked against each other.</p>\n\n<p>At any given time, you want an absolute ranking of all the pictures (even if some/all of them are tied).  You also want your ranking not to change <strong>unless someone votes</strong>.</p>\n\n<p>I would use the <strong>Pick A-or-B (or tie)</strong> method, but determine ranking similar to the <a href=\"https://en.wikipedia.org/wiki/Elo_rating_system\" rel=\"nofollow\">Elo ratings system</a> which is used for rankings in 2 player games (originally chess):</p>\n\n<blockquote>\n  <p>The Elo player-rating\n  system compares players’ match records\n  against their opponents’ match records\n  and determines the probability of the\n  player winning the matchup. This\n  probability factor determines how many\n  points a players’ rating goes up or\n  down based on the results of each\n  match. When a player defeats an\n  opponent with a higher rating, the\n  player’s rating goes up more than if\n  he or she defeated a player with a\n  lower rating (since players should\n  defeat opponents who have lower\n  ratings). </p>\n</blockquote>\n\n<p><strong>The Elo System:</strong></p>\n\n<ol>\n<li>All new players start out with a base rating of <strong>1600</strong></li>\n<li>WinProbability = 1/(10^(( Opponent’s Current Rating–Player’s Current Rating)/400) + 1)</li>\n<li>ScoringPt = 1 point if they win the match, 0 if they lose, and 0.5 for a draw.</li>\n<li>Player’s New Rating = Player’s Old Rating + (K-Value * (ScoringPt–Player’s Win Probability))</li>\n</ol>\n\n<p>Replace \"players\" with pictures and you have a simple way of adjusting both pictures' rating based on a formula.  You can then perform a ranking using those numeric scores. (K-Value here is the \"Level\" of the tournament.  It's 8-16 for small local tournaments and 24-32 for larger invitationals/regionals.  You can just use a constant like 20).</p>\n\n<p>With this method, you only need to keep one number for each picture which is a lot less memory intensive than keeping the individual ranks of each picture to each other picture.</p>\n\n<p>EDIT: Added a little more meat based on comments.</p>\n    "},{"t":"Lazy Evaluation and Time Complexity","l":"http://stackoverflow.com/questions/12057658/lazy-evaluation-and-time-complexity","q":"\n\n<p>I was looking around stackoverflow <a href=\"http://stackoverflow.com/questions/7868507/non-trivial-lazy-evaluation\">Non-Trivial Lazy Evaluation</a>, which led me to Keegan McAllister's presentation: <a href=\"http://ugcs.net/~keegan/talks/why-learn-haskell/talk.pdf\">Why learn Haskell</a>. In slide 8, he shows the minimum function, defined as:</p>\n\n<pre><code>minimum = head . sort\n</code></pre>\n\n<p>and states that its complexity is O(n). I don't understand why the complexity is said to be linear if sorting by replacement is O(nlog n). The sorting referred in the post can't be linear, as it does not assume anything about the data, as it would be required by linear sorting methods, such as counting sort.</p>\n\n<p>Is lazy evaluation playing a mysterious role in here? If so, what is the explanation behind it?</p>\n    ","a":"\n<p>In <code>minimum = head . sort</code>, the <code>sort</code> won't be done fully, because it won't be done <em>upfront</em>. The <code>sort</code> will only be done as much as needed to produce the very first element, demanded by <code>head</code>. </p>\n\n<p>In e.g. mergesort, at first <code>n</code> numbers of the list will be compared pairwise, then the winners will be paired up and compared (<code>n/2</code> numbers), then the new winners (<code>n/4</code>), etc. In all, <code>O(n)</code> comparisons to produce the minimal element.</p>\n\n<pre class=\"lang-hs prettyprint-override\"><code>mergesortBy less [] = []\nmergesortBy less xs = head $ until (null.tail) pairs [[x] | x &lt;- xs]\n  where\n    pairs (x:y:t) = merge x y : pairs t\n    pairs xs      = xs\n    merge (x:xs) (y:ys) | less y x  = y : merge (x:xs) ys\n                        | otherwise = x : merge  xs (y:ys)\n    merge  xs     []                = xs\n    merge  []     ys                = ys\n</code></pre>\n\n<hr>\n\n<p>The above code can be augmented to tag each number it produces with a number of comparisons that went into its production:</p>\n\n<pre><code>mgsort xs = go $ map ((,) 0) xs  where\n  go [] = []\n  go xs = head $ until (null.tail) pairs [[x] | x &lt;- xs]   where\n    ....\n    merge ((a,b):xs) ((c,d):ys) \n            | (d &lt; b)   = (a+c+1,d) : merge ((a+1,b):xs) ys    -- cumulative\n            | otherwise = (a+c+1,b) : merge  xs ((c+1,d):ys)   --   cost\n    ....\n\ng n = concat [[a,b] | (a,b) &lt;- zip [1,3..n] [n,n-2..1]]   -- a little scrambler\n</code></pre>\n\n<p>Running it for several list lengths we see that <strong><em>it is indeed <code>~ n</code></em></strong>:</p>\n\n<pre><code>*Main&gt; map (fst . head . mgsort . g) [10, 20, 40, 80, 160, 1600]\n[9,19,39,79,159,1599]\n</code></pre>\n\n<hr>\n\n<p>To see whether the sorting code itself is <code>~ n log n</code>, we change it so that each produced number carries along just its own cost, and the total cost is then found by summation over the whole sorted list:</p>\n\n<pre><code>    merge ((a,b):xs) ((c,d):ys) \n            | (d &lt; b)   = (c+1,d) : merge ((a+1,b):xs) ys      -- individual\n            | otherwise = (a+1,b) : merge  xs ((c+1,d):ys)     --   cost\n</code></pre>\n\n<p>Here are the results for lists of various lengths,</p>\n\n<pre><code>*Main&gt; let xs = map (sum . map fst . mgsort . g) [20, 40, 80, 160, 320, 640]\n[138,342,810,1866,4218,9402]\n\n*Main&gt; map (logBase 2) $ zipWith (/) (tail xs) xs\n[1.309328,1.2439256,1.2039552,1.1766101,1.1564085]\n</code></pre>\n\n<p>The above shows <a href=\"http://en.wikipedia.org/wiki/Analysis_of_algorithms#Empirical_orders_of_growth\"><strong>empirical orders of growth</strong></a> for increasing lengths of list, <code>n</code>, which are rapidly diminishing as is typically exhibited by <strong><code>~ n log n</code></strong> computations. See also <a href=\"http://rjlipton.wordpress.com/2009/07/24/how-to-avoid-o-abuse-and-bribes/\">this blog post</a>. Here's a quick correlation check:</p>\n\n<pre><code>*Main&gt; let xs = [n*log n | n&lt;- [20, 40, 80, 160, 320, 640]] in \n                                    map (logBase 2) $ zipWith (/) (tail xs) xs\n[1.3002739,1.2484156,1.211859,1.1846942,1.1637106]\n</code></pre>\n\n<hr>\n\n<p><em>edit:</em> Lazy evaluation can metaphorically be seen as kind of producer/consumer idiom<sup>1</sup>, with independent memoizing storage as an intermediary. Any productive definition we write, defines a producer which will produce its output, bit by bit, as and when demanded by its consumer(s) - but not sooner. Whatever is produced is memoized, so that if another consumer consumes same output at different pace, it accesses same storage, filled previously. </p>\n\n<p>When no more consumers remain that refer to a piece of storage, it gets garbage collected. Sometimes with optimizations compiler is able to do away with the intermediate storage completely, cutting the middle man out.</p>\n\n<p><sup>1</sup> see also: <a href=\"http://lambda-the-ultimate.org/node/4690\">Simple Generators v. Lazy Evaluation</a> by Oleg Kiselyov, Simon Peyton-Jones and Amr Sabry.</p>\n    "},{"t":"Best compression algorithm for short text strings [closed]","l":"http://stackoverflow.com/questions/1138345/best-compression-algorithm-for-short-text-strings","q":"\n\n<p>I'm searching for an algorithm to compress small text strings: 50-1000 bytes (i.e. URLs). Which algorithm works best for this?</p>\n    ","a":"\n<p>Check this: <a href=\"http://github.com/antirez/smaz/tree/master\">http://github.com/antirez/smaz/tree/master</a></p>\n\n<p>\"Smaz is a simple compression library suitable for compressing very short\nstrings.\"</p>\n    "},{"t":"Why are λ-calculus optimal evaluators able to compute big modular exponentiations without formulas?","l":"http://stackoverflow.com/questions/31707614/why-are-%ce%bb-calculus-optimal-evaluators-able-to-compute-big-modular-exponentiation","q":"\n\n<p>Church numbers are an encoding of natural numbers as functions.</p>\n\n<pre><code>(\\ f x → (f x))             -- church number 1\n(\\ f x → (f (f (f x))))     -- church number 3\n(\\ f x → (f (f (f (f x))))) -- church number 4\n</code></pre>\n\n<p>Neatly, you can exponentiate 2 church numbers by just applying them. That is, if you apply 4 to 2, you get the church number <code>16</code>, or <code>2^4</code>. Obviously,  that is utterly unpractical. Church numbers need a linear amount of memory and are really, really slow. Computing something like <code>10^10</code> - which GHCI quickly answers correctly - would take ages and couldn't fit the memory on your computer anyway. </p>\n\n<p>I've been experimenting with optimal λ evaluators lately. On my tests, I accidentally typed the following on my optimal λ-calculator:</p>\n\n<pre><code>10 ^ 10 % 13\n</code></pre>\n\n<p>It was supposed to be multiplication, not exponentiation. Before I could move my fingers to abort the forever-running program in despair, it answered my request:</p>\n\n<pre><code>3\n{ iterations: 11523, applications: 5748, used_memory: 27729 }\n\nreal    0m0.104s\nuser    0m0.086s\nsys     0m0.019s\n</code></pre>\n\n<p>With my \"bug alert\" flashing, I went to Google and verified, <code>10^10%13 == 3</code> indeed. <strong>But the λ-calculator wasn't supposed to find that result, it can barely store 10^10.</strong> I started stressing it, for science. It instantly answered me <code>20^20%13 == 3</code>, <code>50^50%13 == 4</code>, <code>60^60%3 == 0</code>. I had to use <a href=\"https://www.mtholyoke.edu/courses/quenell/s2003/ma139/js/powermod.html\">external tools</a> to verify those results, since <s>Haskell itself wasn't able to compute it (due to integer overflow)</s> (it is if you use Integers not Ints, of course!). Pushing it to its limits, this was the answer to <code>200^200%31</code>:</p>\n\n<pre><code>5\n{ iterations: 10351327, applications: 5175644, used_memory: 23754870 }\n\nreal    0m4.025s\nuser    0m3.686s\nsys 0m0.341s\n</code></pre>\n\n<p>If we had one copy of the universe for each atom on the universe, and we had a computer for each atom we had in total, we couldn't store the church number <code>200^200</code>. This prompted me to question if my mac was really that powerful. Maybe the optimal evaluator was able to skip the unnecessary branches and arrive right at the answer in the same fashion Haskell does with lazy evaluation. To test this, I compiled the λ program to Haskell:</p>\n\n<pre><code>data Term = F !(Term -&gt; Term) | N !Double\ninstance Show Term where {\n    show (N x) = \"(N \"++(if fromIntegral (floor x) == x then show (floor x) else show x)++\")\";\n    show (F _) = \"(λ...)\"}\ninfixl 0 #\n(F f) # x = f x\nchurchNum = F(\\(N n)-&gt;F(\\f-&gt;F(\\x-&gt;if n&lt;=0 then x else (f#(churchNum#(N(n-1))#f#x)))))\nexpMod    = (F(\\v0-&gt;(F(\\v1-&gt;(F(\\v2-&gt;((((((churchNum # v2) # (F(\\v3-&gt;(F(\\v4-&gt;(v3 # (F(\\v5-&gt;((v4 # (F(\\v6-&gt;(F(\\v7-&gt;(v6 # ((v5 # v6) # v7))))))) # v5))))))))) # (F(\\v3-&gt;(v3 # (F(\\v4-&gt;(F(\\v5-&gt;v5)))))))) # (F(\\v3-&gt;((((churchNum # v1) # (churchNum # v0)) # ((((churchNum # v2) # (F(\\v4-&gt;(F(\\v5-&gt;(F(\\v6-&gt;(v4 # (F(\\v7-&gt;((v5 # v7) # v6))))))))))) # (F(\\v4-&gt;v4))) # (F(\\v4-&gt;(F(\\v5-&gt;(v5 # v4))))))) # ((((churchNum # v2) # (F(\\v4-&gt;(F(\\v5-&gt;v4))))) # (F(\\v4-&gt;v4))) # (F(\\v4-&gt;v4))))))) # (F(\\v3-&gt;(((F(\\(N x)-&gt;F(\\(N y)-&gt;N(x+y)))) # v3) # (N 1))))) # (N 0))))))))\nmain = print $ (expMod # N 5 # N 5 # N 4)\n</code></pre>\n\n<p>This correctly outputs <code>1</code> (<code>5 ^ 5 % 4</code>) - but throw anything above <code>10^10</code> and it will be stuck, eliminating the hypothesis. </p>\n\n<p>The <a href=\"https://github.com/SrVictorMaia/optlam\">optimal evaluator I used</a> is a 160-lines long, unoptimized JavaScript program that didn't include any sort of exponential modulus math - and the lambda-calculus modulus function I used was equally simple:</p>\n\n<pre><code>(λab.(b(λcd.(c(λe.(d(λfg.(f(efg)))e))))(λc.(c(λde.e)))(λc.(a(b(λdef.(d(λg.(egf))))(λd.d)(λde.(ed)))(b(λde.d)(λd.d)(λd.d))))))\n</code></pre>\n\n<p>I used no specific modular arithmetic algorithm or formula. <strong>So, how is the optimal evaluator able to arrive at the right answers?</strong></p>\n    ","a":"\n<p>The phenomenon comes from the amount of shared beta-reduction steps, which can be dramatically different in Haskell-style lazy evaluation (or usual call-by-value, which is not that far in this respect) and in Vuillemin-Lévy-Lamping-Kathail-Asperti-Guerrini-(et al…) \"optimal\" evaluation. This is a general feature, that is completely independent from the arithmetic formulas you could use in this particular example.</p>\n\n<p>Sharing means having a representation of your lambda-term in which one \"node\" can describe several similar parts of the actual lambda-term you represent. For instance, you can represent the term </p>\n\n<pre><code>\\x. x ((\\y.y)a) ((\\y.y)a)\n</code></pre>\n\n<p>using a (directed acyclic) graph in which there is only one occurrence of the subgraph representing <code>(\\y.y)a</code>, and two edges targeting that subgraph. In Haskell terms, you have one thunk, that you evaluate only once, and two pointers to this thunk.</p>\n\n<p>Haskell-style memoization implements sharing of complete subterms. This level of sharing can be represented by directed acyclic graphs. Optimal sharing does not have this restriction: it can also share \"partial\" subterms, which may imply cycles in the graph representation.</p>\n\n<p>To see the difference between these two levels of sharing, consider the term</p>\n\n<pre><code>\\x. (\\z.z) ((\\z.z) x)\n</code></pre>\n\n<p>If your sharing is restricted to complete subterms as it is the case in Haskell, you may have only one occurrence of <code>\\z.z</code>, but the two beta-redexes here will be distinct: one is <code>(\\z.z) x</code> and the other one is <code>(\\z.z) ((\\z.z) x)</code>, and since they are not equal terms they cannot be shared.\nIf the sharing of partial subterms is allowed, then it becomes possible to share the partial term <code>(\\z.z) []</code> (that is not just the function <code>\\z.z</code>, but \"the function <code>\\z.z</code> applied to <em>something</em>), which evaluates in one step to just <em>something</em>, whatever this argument is. Hence you can have a graph in which only one node represents the two applications of <code>\\z.z</code> to two distinct arguments, and in which these two applications can be reduced in just one step. Remark that there is a cycle on this node, since the argument of the \"first occurrence\" is precisely the \"second occurrence\".\nFinally, with optimal sharing you can go from (a graph representing) <code>\\x. (\\z.z) ((\\z.z) x))</code> to (a graph representing) the result <code>\\x.x</code> in just one step of beta-reduction (plus some bookkeeping). This is basically what happens in your optimal evaluator (and the graph representation is also what prevents space explosion).</p>\n\n<p>For slightly extended explanations, you can look at the paper <a href=\"http://www.lri.fr/~blsk/Docs/Balabonski-WeakOptimality-ICFP13.pdf\">Weak Optimality, and the Meaning of Sharing</a> (what you are interested in is the introduction and the section 4.1, and maybe some of the bibliographic pointers at the end).</p>\n\n<p>Coming back at your example, the coding of arithmetic functions working on Church integers is one of the \"well-known\" mines of examples where optimal evaluators can perform better than mainstream languages (in this sentence, well-known actually means that a handful of specialists are aware of these examples).\nFor more such examples, take a look at the paper <a href=\"http://www.pps.univ-paris-diderot.fr/~jch/safebrackets.ps.gz\">Safe Operators: Brackets Closed Forever</a> by Asperti and Chroboczek (and by the way, you will find here interesting lambda-terms that are not EAL-typeable; so I’m encouraging you to take a look at oracles, starting with this Asperti/Chroboczek paper).</p>\n\n<p>As you said yourself, this kind of encoding is utterly unpractical, but they still represent a nice way of understanding what is going on. And let me conclude with a challenge for further investigation: will you be able to find an example on which optimal evaluation on these supposedly bad encodings is actually on par with traditional evaluation on a reasonable data representation? (as far as I know this is a real open question).</p>\n\n<p>Best regards, and let me know of what happens next.</p>\n\n<p>Thibaut Balabonski.</p>\n    "},{"t":"Find the row representing the smallest integer in row wise sorted matrix","l":"http://stackoverflow.com/questions/4303813/find-the-row-representing-the-smallest-integer-in-row-wise-sorted-matrix","q":"\n\n<p>I was asked this question on in a recent Java telephonic interview:</p>\n\n<p>You are given an NxN binary (0-1) matrix with following properties:</p>\n\n<ul>\n<li>Each row is sorted (sequence of 0's followed by sequence of 1's)</li>\n<li>Every row represents an unsigned integer (by reading the bits)</li>\n<li>Each row is unique</li>\n</ul>\n\n<p>Example:</p>\n\n<pre><code>0 1 1\n1 1 1\n0 0 1\n</code></pre>\n\n<p>The bit values in each row is sorted and the rows represent the integers 3, 7 and 1.</p>\n\n<p>Find the row representing the smallest integer. In the example above, the answer is row 3, which represents the integer 1.</p>\n\n<p>I started with brute force of quadratic complexity. The interviewer replied saying I was not exploiting the sorted property.</p>\n\n<p>After thinking a lot I used binary search on each row and it came to O(nlogn). He asked if I could improve any further. I thought a lot but failed to improve.</p>\n\n<p>I would appreciate if anyone can give any pointers on imporoving it.</p>\n\n<p>Another example:</p>\n\n<pre><code>0 1 1 1\n0 0 0 1\n0 0 0 0\n1 1 1 1\n</code></pre>\n\n<p>The answer will be row 3, which represents the integer 0.</p>\n    ","a":"\n<p>Start with row 1. Go right until you hit the first <code>1</code>. Then go down to row 2, but remain in the same column and repeat the process of going right until you hit a <code>1</code>. Do this repeatedly. The row in which you last stepped right is your answer.</p>\n\n<p>This is an O(N+M) solution (for an NxM matrix, or O(N) for a square NxN matrix as given in the question).</p>\n\n<p>Using your example of:</p>\n\n<pre><code>0 1 1 1\n0 0 0 1\n0 0 0 0\n1 1 1 1\n</code></pre>\n\n<p>The <code>.</code>'s here represent the path traversed:</p>\n\n<pre><code>. . 1 1\n0 . . .\n0 0 0 . . Last right step, therefore this is our answer\n1 1 1 1 .\n</code></pre>\n\n<p>This solution works on non-square matrixes, retaining a worst case O(N+M) efficiency for an NxM matrix.</p>\n\n<p>Why does this work? The guarantee that the numbers will be sorted means every row will be a series of 0's followed by a series of 1's. So the magnitude of a row is equivalent to how far right you can go before hitting a 1. So if a row can ever take you further by just following the 0's, then it must be longer than anything we've processed before.</p>\n\n<p>Python code:</p>\n\n<pre><code>li = [[0, 1, 1, 1],\n      [0, 0, 0, 1],\n      [0, 0, 0, 0],\n      [1, 1, 1, 1]]\n\nans, j = 0, 0\nfor i, row in enumerate(li):\n  while j &lt; len(row) and row[j] == 0:\n    j += 1\n    ans = i\n\nprint \"Row\", ans+1, \"\".join(map(str, li[ans]))\n</code></pre>\n\n<p>There is also a simpler solution, because of the constraints of always having a square NxN matrix and distinct rows together. Together they mean that the row with the lowest value will be either <code>0 0 ... 0 1</code> or <code>0 0 ... 0 0</code>. This is because there are N of N+1 possible numbers represented in the matrix, so the \"missing\" number is either 0 (in which case the smallest value represented is 1) or it's something else (smallest value is 0).</p>\n\n<p>With this knowledge, we check the column second from the right for a 0. When we find one, we look to its right and if that contains another 0 we have our answer (there can only be one row ending in a <code>0</code>). Otherwise, we continue to search the column for another 0. If we don't find another 0, the first one we found was the row we're looking for (there can only be one row ending in <code>01</code> and since there was none ending in <code>00</code>, this is the smallest).</p>\n\n<p>Python code:</p>\n\n<pre><code>li = [[0, 1, 1, 1],\n      [0, 0, 0, 1],\n      [0, 0, 0, 0],\n      [1, 1, 1, 1]]\n\nfor i, row in enumerate(li):\n  if row[-2] == 0:\n    ans = i\n    if row[-1] == 0:\n      break\n\nprint \"Row\", ans+1, \"\".join(map(str, li[ans]))\n</code></pre>\n\n<p>That solution answers the question with least difficulty in O(N), but generalising it to handle non-square NxM matrixes or non-distinct numbers will make its worst-case efficiency O(N^2). I personally prefer the first solution.</p>\n    "},{"t":"Algorithm to generate a crossword","l":"http://stackoverflow.com/questions/943113/algorithm-to-generate-a-crossword","q":"\n\n<p>Given a list of words, how would you go about arranging them into a crossword grid?</p>\n\n<p>It wouldn't have to be like a \"proper\" crossword puzzle which is symmetrical or anything like that: basically just output a starting position and direction for each word.</p>\n\n<p>Would there be any Java examples available?</p>\n    ","a":"\n<p>I came up with a solution which probably isn't the most efficient, but it works well enough. Basically:</p>\n\n<ol>\n<li>Sort all the words by length, descending.</li>\n<li>Take the first word and place it on the board.</li>\n<li>Take the next word.</li>\n<li>Search through all the words that are already on the board and see if there are any possible intersections (any common letters) with this word.</li>\n<li>If there is a possible location for this word, loop through all the words that are on the board and check to see if the new word interferes.</li>\n<li>If this word doesn't break the board, then place it there and go to step 3, otherwise, continue searching for a place (step 4).</li>\n<li>Continue this loop until all the words are either placed or unable to be placed.</li>\n</ol>\n\n<p>This makes a working, yet often quite poor crossword. There were a number of alterations I made to the basic recipe above to come up with a better result.</p>\n\n<ul>\n<li>At the end of generating a crossword, give it a score based on how many of the words were placed (the more the better), how large the board is (the smaller the better), and the ratio between height and width (the closer to 1 the better). Generate a number of crosswords and then compare their scores and choose the best one.\n<ul>\n<li>Instead of running an arbitrary number of iterations, I've decided to create as many crosswords as possible in an arbitrary amount of time. If you only have a small word list, then you'll get dozens of possible crosswords in 5 seconds.  A larger crossword might only be chosen from 5-6 possibilities.</li>\n</ul></li>\n<li>When placing a new word, instead of placing it immediately upon finding an acceptable location, give that word location a score based on how much it increases the size of the grid and how many intersections there are (ideally you'd want each word to be crossed by 2-3 other words). Keep track of all the positions and their scores and then choose the best one.</li>\n</ul>\n    "},{"t":"Write a function that returns the longest palindrome in a given string","l":"http://stackoverflow.com/questions/1115001/write-a-function-that-returns-the-longest-palindrome-in-a-given-string","q":"\n\n<blockquote>\n  <p>e.g \"ccddcc\" in the string \"abaccddccefe\"  </p>\n</blockquote>\n\n<p>I thought of a solution but it runs in O(n^2) time</p>\n\n<p>Algo 1:</p>\n\n<p>Steps:\nIts a brute force method</p>\n\n<ol>\n<li>Have 2 for loops<br>\nfor i = 1 to i less than array.length -1<br>\nfor j=i+1 to j less than array.length   </li>\n<li>This way you can get substring of every possible combination from the array</li>\n<li>Have a palindrome function which checks if a string is palindrome</li>\n<li>so for every substring (i,j) call this function, if it is a palindrome store it in a string variable</li>\n<li>If you find next palindrome substring and if it is greater than the current one, replace it with current one.</li>\n<li>Finally your string variable will have the answer</li>\n</ol>\n\n<p>Issues:\n1. This algo runs in O(n^2) time.</p>\n\n<p>Algo 2:</p>\n\n<ol>\n<li>Reverse the string and store it in diferent array</li>\n<li>Now find the largest matching substring between both the array</li>\n<li>But this too runs in O(n^2) time</li>\n</ol>\n\n<p>Can you guys think of an algo which runs in a better time. If possible O(n) time</p>\n    ","a":"\n<p>You can find the the longest palindrome using <code><a href=\"http://en.wikipedia.org/wiki/Longest_palindromic_substring\">Manacher's Algorithm</a></code> in <code>O(n)</code> time! Its implementation can be found <a href=\"http://algs4.cs.princeton.edu/53substring/Manacher.java.html\">here</a> and <a href=\"http://leetcode.com/2011/11/longest-palindromic-substring-part-ii.html\">here</a>.<br>\nFor input <code>String s = \"HYTBCABADEFGHABCDEDCBAGHTFYW1234567887654321ZWETYGDE\"</code>. It finds the correct output which is <code>1234567887654321</code>.</p>\n    "},{"t":"Pluralize - Singularize","l":"http://stackoverflow.com/questions/475705/pluralize-singularize","q":"\n\n<p>Is there any algorithm in c# to singularize - pluralize a word (in english) or does exist a .net library to do this (may be also in different languages)?</p>\n    ","a":"\n<p>You also have the <a href=\"http://msdn.microsoft.com/en-us/library/system.data.entity.design.pluralizationservices.pluralizationservice.aspx\">System.Data.Entity.Design.PluralizationServices.PluralizationService</a>.</p>\n\n<p><strong>UPDATE</strong>: Old answer deserves update. There's now also Humanizer: <a href=\"https://github.com/MehdiK/Humanizer\">https://github.com/MehdiK/Humanizer</a></p>\n    "},{"t":"Least common multiple for 3 or more numbers","l":"http://stackoverflow.com/questions/147515/least-common-multiple-for-3-or-more-numbers","q":"\n\n<p>How do you calculate the least common multiple of multiple numbers?</p>\n\n<p>So far I've only been able to calculate it between two numbers. But have no idea how to expand it to calculate 3 or more numbers.</p>\n\n<p>So far this is how I did it  </p>\n\n<pre><code>LCM = num1 * num2 /  gcd ( num1 , num2 )\n</code></pre>\n\n<p>With gcd is the function to calculate the greatest common divisor for the numbers. Using euclidean algorithm</p>\n\n<p>But I can't figure out how to calculate it for 3 or more numbers.</p>\n    ","a":"\n<p>You can compute the LCM of more than two numbers by iteratively computing the LCM of two numbers, i.e.</p>\n\n<pre><code>lcm(a,b,c) = lcm(a,lcm(b,c))\n</code></pre>\n    "},{"t":"Listing all permutations of a string/integer","l":"http://stackoverflow.com/questions/756055/listing-all-permutations-of-a-string-integer","q":"\n\n<p>A common task in programming interviews (not from my experience of interviews though) is to take a string or an integer and list every possible permutation.</p>\n\n<p>Is there an example of how this is done and the logic behind solving such a problem?</p>\n\n<p>I've seen a few code snippets but they weren't well commented/explained and thus hard to follow.</p>\n    ","a":"\n<p>First of all: it smells like <em>recursion</em> of course!</p>\n\n<p>Since you also wanted to know the principle, I did my best to explain it human language. I think recursion is very easy most of the times. You only have to grasp two steps:</p>\n\n<ol>\n<li>The first step</li>\n<li>All the other steps (all with the same logic)</li>\n</ol>\n\n<p>In <strong>human language</strong>:</p>\n\n<blockquote>\n  <p>In short:<br>\n   1. The permutation of 1 element is one element.<br>\n   2. The permutation of a set of elements is a list each of the elements, concatenated with every permutation of the other elements.<br></p>\n  \n  <p><strong><em>Example:</em></strong></p>\n  \n  <p>If the set just has one element --&gt;<br>\n  return it.<br>\n  <strong>perm(a) -&gt; a</strong></p>\n  \n  <p>If the set has two characters: for\n  each element in it: return the\n  element, with the permutation of the\n  rest of the elements added, like so:<br></p>\n  \n  <p><em>perm(ab)    -&gt;</em> <br></p>\n  \n  <p>a + perm(b) -&gt; <strong>ab</strong> <br></p>\n  \n  <p>b + perm(a) -&gt; <strong>ba</strong> <br></p>\n  \n  <p>Further: for each character in the set: return a character, concatenated with a perumation of &gt; the rest of the set</p>\n  \n  <p>perm(abc) -&gt;<br></p>\n  \n  <p>a + perm(bc) --&gt;  <strong>abc</strong>, <strong>acb</strong><br></p>\n  \n  <p>b + perm(ac) --&gt;  <strong>bac</strong>, <strong>bca</strong><br></p>\n  \n  <p>c + perm(ab) --&gt;  <strong>cab</strong>, <strong>cba</strong><br></p>\n  \n  <p>perm(abc...z) --&gt;<br></p>\n  \n  <p>a + perm(...), b + perm(....) <br>\n  ....</p>\n</blockquote>\n\n<p>I found the <strong>pseudocode</strong> on <a href=\"http://www.programmersheaven.com/mb/Algorithms/369713/369713/permutation-algorithm-help/\">http://www.programmersheaven.com/mb/Algorithms/369713/369713/permutation-algorithm-help/</a>:</p>\n\n<pre><code>makePermutations(permutation) {\n  if (length permutation &lt; required length) {\n    for (i = min digit to max digit) {\n      if (i not in permutation) {\n        makePermutations(permutation+i)\n      }\n    }\n  }\n  else {\n    add permutation to list\n  }\n}\n</code></pre>\n\n<p><strong>C#</strong></p>\n\n<p>OK, and something more elaborate (and since it is tagged c #), from <a href=\"http://radio.weblogs.com/0111551/stories/2002/10/14/permutations.html\">http://radio.weblogs.com/0111551/stories/2002/10/14/permutations.html</a> :\nRather lengthy, but I decided to copy it anyway, so the post is not dependent on the original.</p>\n\n<blockquote>\n  <p>The function takes a string of characters, and writes down every possible permutation of that exact string, so for example, if \"ABC\" has been supplied, should spill out:</p>\n</blockquote>\n\n<p>ABC, ACB, BAC, BCA, CAB, CBA.</p>\n\n<p>Code:</p>\n\n<pre><code>class Program\n{\n    private void Swap(ref char a, ref char b)\n    {\n        if (a == b) return;\n\n        a ^= b;\n        b ^= a;\n        a ^= b;\n    }\n\n    public static void GetPer(char[] list)\n    {\n        int x = list.Length - 1;\n        GetPer(list, 0, x);\n    }\n\n    private static void GetPer(char[] list, int k, int m)\n    {\n        if (k == m)\n        {\n            Console.Write(list);\n        }\n        else\n            for (int i = k; i &lt;= m; i++)\n            {\n                   Swap(ref list[k], ref list[i]);\n                   GetPer(list, k + 1, m);\n                   Swap(ref list[k], ref list[i]);\n            }\n    }\n\n    static void Main()\n    {\n        string str = \"sagiv\";\n        char[] arr = str.ToCharArray();\n        GetPer(arr);\n    }\n}\n</code></pre>\n    "},{"t":"Fast method to copy memory with translation - ARGB to BGR","l":"http://stackoverflow.com/questions/6804101/fast-method-to-copy-memory-with-translation-argb-to-bgr","q":"\n\n<h2>Overview</h2>\n\n<p>I have an image buffer that I need to convert to another format.  The origin image buffer is four channels, 8 bits per channel, Alpha, Red, Green, and Blue.  The destination buffer is three channels, 8 bits per channel, Blue, Green, and Red.</p>\n\n<p>So the brute force method is:</p>\n\n<pre><code>// Assume a 32 x 32 pixel image\n#define IMAGESIZE (32*32)\n\ntypedef struct{ UInt8 Alpha; UInt8 Red; UInt8 Green; UInt8 Blue; } ARGB;\ntypedef struct{ UInt8 Blue; UInt8 Green; UInt8 Red; } BGR;\n\nARGB orig[IMAGESIZE];\nBGR  dest[IMAGESIZE];\n\nfor(x = 0; x &lt; IMAGESIZE; x++)\n{\n     dest[x].Red = orig[x].Red;\n     dest[x].Green = orig[x].Green;\n     dest[x].Blue = orig[x].Blue;\n}\n</code></pre>\n\n<p>However, I need more speed than is provided by a loop and three byte copies. I'm hoping there might be a few tricks I can use to reduce the number of memory reads and writes, given that I'm running on a 32 bit machine.</p>\n\n<h2>Additional info</h2>\n\n<p>Every image is a multiple of at least 4 pixels.  So we could address 16 ARGB bytes and move them into 12 RGB bytes per loop. Perhaps this fact can be used to speed things up, especially as it falls nicely into 32 bit boundaries.</p>\n\n<p>I have access to OpenCL - and while that requires moving the entire buffer into the GPU memory, then moving the result back out, the fact that OpenCL can work on many portions of the image simultaneously, and the fact that large memory block moves are actually quite efficient may make this a worthwhile exploration.</p>\n\n<p>While I've given the example of small buffers above, I really am moving HD video (1920x1080) and sometimes larger, mostly smaller, buffers around, so while a 32x32 situation may be trivial, copying 8.3MB of image data byte by byte is really, really bad.</p>\n\n<p>Running on Intel processors (Core 2 and above) and thus there are streaming and data processing commands I'm aware exist, but don't know about - perhaps pointers on where to look for specialized data handling instructions would be good.</p>\n\n<p>This is going into an OS X application, and I'm using XCode 4.  If assembly is painless and the obvious way to go, I'm fine traveling down that path, but not having done it on this setup before makes me wary of sinking too much time into it.</p>\n\n<p>Pseudo-code is fine - I'm not looking for a complete solution, just the algorithm and an explanation of any trickery that might not be immediately clear.</p>\n    ","a":"\n<p>I wrote 4 different versions which work by swapping bytes. I compiled them using gcc 4.2.1 with <code>-O3 -mssse3</code>, ran them 10 times over 32MB of random data and found the averages.</p>\n\n<p>The first version uses a C loop to convert each pixel separately, using the <code>OSSwapInt32</code> function (which compiles to a <code>bswap</code> instruction with <code>-O3</code>).</p>\n\n<pre><code>void swap1(ARGB *orig, BGR *dest, unsigned imageSize) {\n    unsigned x;\n    for(x = 0; x &lt; imageSize; x++) {\n        *((uint32_t*)(((uint8_t*)dest)+x*3)) = OSSwapInt32(((uint32_t*)orig)[x]);\n    }\n}\n</code></pre>\n\n<p>The second method performs the same operation, but uses an inline assembly loop instead of a C loop.</p>\n\n<pre><code>void swap2(ARGB *orig, BGR *dest, unsigned imageSize) {\n    asm (\n        \"0:\\n\\t\"\n        \"movl   (%1),%%eax\\n\\t\"\n        \"bswapl %%eax\\n\\t\"\n        \"movl   %%eax,(%0)\\n\\t\"\n        \"addl   $4,%1\\n\\t\"\n        \"addl   $3,%0\\n\\t\"\n        \"decl   %2\\n\\t\"\n        \"jnz    0b\"\n        :: \"D\" (dest), \"S\" (orig), \"c\" (imageSize)\n        : \"flags\", \"eax\"\n    );\n}\n</code></pre>\n\n<p>The third version is a modified version of <a href=\"http://stackoverflow.com/questions/6804101/fast-method-to-copy-memory-with-translation-argb-to-bgr-2-000-rep-bounty/6804399#6804399\">just a poseur's answer</a>. I converted the built-in functions to the GCC equivalents and used the <code>lddqu</code> built-in function so that the input argument doesn't need to be aligned.</p>\n\n<pre><code>typedef uint8_t v16qi __attribute__ ((vector_size (16)));\nvoid swap3(uint8_t *orig, uint8_t *dest, size_t imagesize) {\n    v16qi mask = __builtin_ia32_lddqu((const char[]){3,2,1,7,6,5,11,10,9,15,14,13,0xFF,0xFF,0xFF,0XFF});\n    uint8_t *end = orig + imagesize * 4;\n    for (; orig != end; orig += 16, dest += 12) {\n        __builtin_ia32_storedqu(dest,__builtin_ia32_pshufb128(__builtin_ia32_lddqu(orig),mask));\n    }\n}\n</code></pre>\n\n<p>Finally, the fourth version is the inline assembly equivalent of the third.</p>\n\n<pre><code>void swap2_2(uint8_t *orig, uint8_t *dest, size_t imagesize) {\n    int8_t mask[16] = {3,2,1,7,6,5,11,10,9,15,14,13,0xFF,0xFF,0xFF,0XFF};//{0xFF, 0xFF, 0xFF, 0xFF, 13, 14, 15, 9, 10, 11, 5, 6, 7, 1, 2, 3};\n    asm (\n        \"lddqu  (%3),%%xmm1\\n\\t\"\n        \"0:\\n\\t\"\n        \"lddqu  (%1),%%xmm0\\n\\t\"\n        \"pshufb %%xmm1,%%xmm0\\n\\t\"\n        \"movdqu %%xmm0,(%0)\\n\\t\"\n        \"add    $16,%1\\n\\t\"\n        \"add    $12,%0\\n\\t\"\n        \"sub    $4,%2\\n\\t\"\n        \"jnz    0b\"\n        :: \"r\" (dest), \"r\" (orig), \"r\" (imagesize), \"r\" (mask)\n        : \"flags\", \"xmm0\", \"xmm1\"\n    );\n}\n</code></pre>\n\n<p>On my 2010 MacBook Pro, 2.4 Ghz i5, 4GB RAM, these were the average times for each:</p>\n\n<pre>Version 1: 10.8630 milliseconds\nVersion 2: 11.3254 milliseconds\nVersion 3:  9.3163 milliseconds\nVersion 4:  9.3584 milliseconds\n</pre>\n\n<p>As you can see, the compiler is good enough at optimization that you don't need to write assembly. Also, the vector functions were only 1.5 milliseconds faster on 32MB of data, so it won't cause much harm if you want to support the earliest Intel macs, which didn't support SSSE3.</p>\n\n<p>Edit: liori asked for standard deviation information. Unfortunately, I hadn't saved the data points, so I ran another test with 25 iterations.</p>\n\n<pre>              Average    | Standard Deviation\nBrute force: 18.01956 ms | 1.22980 ms (6.8%)\nVersion 1:   11.13120 ms | 0.81076 ms (7.3%)\nVersion 2:   11.27092 ms | 0.66209 ms (5.9%)\nVersion 3:    9.29184 ms | 0.27851 ms (3.0%)\nVersion 4:    9.40948 ms | 0.32702 ms (3.5%)\n</pre>\n\n<p>Also, here is the raw data from the new tests, in case anyone wants it. For each iteration, a 32MB data set was randomly generated and run through the four functions. The runtime of each function in microseconds is listed below.</p>\n\n<pre>Brute force: 22173 18344 17458 17277 17508 19844 17093 17116 19758 17395 18393 17075 17499 19023 19875 17203 16996 17442 17458 17073 17043 18567 17285 17746 17845\nVersion 1:   10508 11042 13432 11892 12577 10587 11281 11912 12500 10601 10551 10444 11655 10421 11285 10554 10334 10452 10490 10554 10419 11458 11682 11048 10601\nVersion 2:   10623 12797 13173 11130 11218 11433 11621 10793 11026 10635 11042 11328 12782 10943 10693 10755 11547 11028 10972 10811 11152 11143 11240 10952 10936\nVersion 3:    9036  9619  9341  8970  9453  9758  9043 10114  9243  9027  9163  9176  9168  9122  9514  9049  9161  9086  9064  9604  9178  9233  9301  9717  9156\nVersion 4:    9339 10119  9846  9217  9526  9182  9145 10286  9051  9614  9249  9653  9799  9270  9173  9103  9132  9550  9147  9157  9199  9113  9699  9354  9314\n</pre>\n    "},{"t":"When is it practical to use DFS vs BFS?","l":"http://stackoverflow.com/questions/3332947/when-is-it-practical-to-use-dfs-vs-bfs","q":"\n\n<p>I understand the differences between DFS (Depth First Search) and BFS (Breadth First Search), but I'm interested to know when it's more practical to use one over the other? </p>\n\n<p>Could anyone give any examples of how DFS would trump BFS and vice versa?</p>\n    ","a":"\n<p>That heavily depends on the structure of the search tree and the number and location of solutions (aka searched-for items). If you know a solution is not far from the root of the tree, a breadth first search (BFS) might be better. If the tree is very deep and solutions are rare, depth first search (DFS) might take an extremely long time, but BFS could be faster. If the tree is very wide, a BFS might need too much memory, so it might be completely impractical. If solutions are frequent but located deep in the tree, BFS could be impractical. If the search tree is very deep you will need to restrict the search depth for depth first search (DFS), anyway (for example with iterative deepening). </p>\n\n<p>But these are just rules of thumb; you'll probably need to experiment. </p>\n    "},{"t":"Why are Fibonacci numbers significant in computer science?","l":"http://stackoverflow.com/questions/4571670/why-are-fibonacci-numbers-significant-in-computer-science","q":"\n\n<p><a href=\"http://en.wikipedia.org/wiki/Fibonacci_number\">Fibonacci numbers</a> have become a popular introduction to recursion for Computer Science students and there's a strong argument that they persist within nature. For these reasons, many of us are familiar with them.</p>\n\n<p>They also exist within Computer Science elsewhere too; in surprisingly efficient data structures and algorithms based upon the sequence. </p>\n\n<p>There are two main examples that come to mind:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Fibonacci_heap\">Fibonacci heaps</a> which have better\namortized running time than binomial\nheaps.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Fibonacci_search_technique\">Fibonacci search</a> which shares\nO(log N) running time with binary\nsearch on an ordered array.</li>\n</ul>\n\n<p>Is there some special property of these numbers that gives them an advantage over other numerical sequences? Is it a spatial quality? What other possible applications could they have?</p>\n\n<p>It seems strange to me as there are many natural number sequences that occur in other recursive problems, but I've never seen a <a href=\"http://en.wikipedia.org/wiki/Catalan_number\">Catalan</a> heap. </p>\n    ","a":"\n<p>The Fibonacci numbers have all sorts of really nice mathematical properties that make them excellent in computer science.  Here's a few:</p>\n\n<ol>\n<li><b>They grow exponentially fast.</b>  One interesting data structure in which the Fibonacci series comes up is the AVL tree, a form of self-balancing binary tree.  The intuition behind this tree is that each node maintains a balance factor so that the heights of the left and right subtree differ by at most one.  Because of this, you can think of the minimum number of nodes necessary to get an AVL tree of height h is defined by a recurrence that looks like N(h + 2) ~= N(h) + N(h + 1), which looks a lot like the Fibonacci series.  If you work out the math, you can show that the number of nodes necessary to get an AVL tree of height h is F(h + 2) - 1.  Because the Fibonacci series grows exponentially fast, this means that the height of an AVL tree is at most logarithmic in the number of nodes, giving you the O(lg n) lookup time we know and love about balanced binary trees.  In fact, if you can bound the size of some structure with a Fibonacci number, you're likely to get an O(lg n) runtime on some operation.  This is the real reason that Fibonacci heaps are called Fibonacci heaps - the proof that the number of heaps after a dequeue min involves bounding the number of nodes you can have in a certain depth with a Fibonacci number.</li>\n<li><b>Any number can be written as the sum of unique Fibonacci numbers.</b>  This property of the Fibonacci numbers is critical to getting Fibonacci search working at all; if you couldn't add together unique Fibonacci numbers into any possible number, this search wouldn't work.  Contrast this with a lot of other series, like 3<sup>n</sup> or the Catalan numbers.  This is also partially why a lot of algorithms like powers of two, I think.</li>\n<li><b>The Fibonacci numbers are efficiently computable.</b>  The fact that the series can be generated extremely efficiently (you can get the first n terms in O(n) or any arbitrary term in O(lg n)), then a lot of the algorithms that use them wouldn't be practical.  Generating Catalan numbers is pretty computationally tricky, IIRC.  On top of this, the Fibonacci numbers have a nice property where, given any two consecutive Fibonacci numbers, let's say F(k) and F(k + 1), we can easily compute the next or previous Fibonacci number by adding the two values (F(k) + F(k + 1) = F(k + 2)) or subtracting them (F(k + 1) - F(k) = F(k - 1)).  This property is exploited in several algorithms, in conjunction with property (2), to break apart numbers into the sum of Fibonacci numbers.  For example, Fibonacci search uses this to locate values in memory, while a similar algorithm can be used to quickly and efficiently compute logarithms.</li>\n<li><b>They're pedagogically useful.</b>  Teaching recursion is tricky, and the Fibonacci series is a great way to introduce it.  You can talk about straight recursion, about memoization, or about dynamic programming when introducing the series.  Additionally, the amazing <a href=\"http://en.wikipedia.org/wiki/Fibonacci_number#Closed-form_expression\">closed-form for the Fibonacci numbers</a> is often taught as an exercise in induction or in the analysis of infinite series, and the related <a href=\"http://en.wikipedia.org/wiki/Fibonacci_number#Matrix_form\">matrix equation for Fibonacci numbers</a> is commonly introduced in linear algebra as a motivation behind eigenvectors and eigenvalues.  I think that this is one of the reasons that they're so high-profile in introductory classes.</li>\n</ol>\n\n<p>I'm sure there are more reasons than just this, but I'm sure that some of these reasons are the main factors.  Hope this helps!</p>\n    "},{"t":"The “guess the number” game for arbitrary rational numbers?","l":"http://stackoverflow.com/questions/5440688/the-guess-the-number-game-for-arbitrary-rational-numbers","q":"\n\n<p>I once got the following as an interview question:</p>\n\n<blockquote>\n  <p>I'm thinking of a positive integer n.  Come up with an algorithm that can guess it in O(lg n) queries.  Each query is a number of your choosing, and I will answer either \"lower,\" \"higher,\" or \"correct.\"</p>\n</blockquote>\n\n<p>This problem can be solved by a modified binary search, in which you listing powers of two until you find one that exceeds n, then run a standard binary search over that range.  What I think is so cool about this is that you can search an infinite space for a particular number faster than just brute-force.</p>\n\n<p>The question I have, though, is a slight modification of this problem.  Instead of picking a positive integer, suppose that I pick an <strong>arbitrary rational number</strong> between zero and one.  My question is: what algorithm can you use to most efficiently determine which rational number I've picked?</p>\n\n<p>Right now, the best solution I have can find p/q in at most O(q) time by implicitly walking the <a href=\"http://en.wikipedia.org/wiki/Stern%E2%80%93Brocot_tree\">Stern-Brocot tree</a>, a binary search tree over all the rationals.  However, I was hoping to get a runtime closer to the runtime that we got for the integer case, maybe something like O(lg (p + q)) or O(lg pq).  Does anyone know of a way to get this sort of runtime?</p>\n\n<p>I initially considered using a standard binary search of the interval [0, 1], but this will only find rational numbers with a non-repeating binary representation, which misses almost all of the rationals.  I also thought about using some other way of enumerating the rationals, but I can't seem to find a way to search this space given just greater/equal/less comparisons.</p>\n\n<p>Thanks for any advice/suggestions you might have! </p>\n    ","a":"\n<p>Okay, here's my answer using <a href=\"http://en.wikipedia.org/wiki/Continued_fraction\">continued fractions</a> alone. </p>\n\n<p>First let's get some terminology here. </p>\n\n<p>Let X = p/q be the unknown fraction.</p>\n\n<p>Let Q(X,p/q) = sign(X - p/q) be the query function: if it is 0, we've guessed the number, and if it's +/- 1 that tells us the sign of our error.</p>\n\n<p>The <a href=\"http://en.wikipedia.org/wiki/Continued_fraction#Notations_for_continued_fractions\">conventional notation for continued fractions</a> is A = [a<sub>0</sub>; a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>, ... a<sub>k</sub>] </p>\n\n<p>= a<sub>0</sub> + 1/(a<sub>1</sub> + 1/(a<sub>2</sub> + 1/(a<sub>3</sub> + 1/( ... + 1/a<sub>k</sub>) ... )))</p>\n\n<hr>\n\n<p>We'll follow the following algorithm for 0 &lt; p/q &lt; 1.</p>\n\n<ol>\n<li><p>Initialize Y = 0 = [ 0 ], Z = 1 = [ 1 ], k = 0.</p></li>\n<li><p><strong>Outer loop</strong>: The <em>preconditions</em> are that:</p>\n\n<ul>\n<li><p>Y and Z are continued fractions of k+1 terms which are identical except in the last element, where they differ by 1, so that Y = [y<sub>0</sub>; y<sub>1</sub>, y<sub>2</sub>, y<sub>3</sub>, ... y<sub>k</sub>] and Z = [y<sub>0</sub>; y<sub>1</sub>, y<sub>2</sub>, y<sub>3</sub>, ... y<sub>k</sub> + 1] </p></li>\n<li><p>(-1)<sup>k</sup>(Y-X) &lt; 0 &lt; (-1)<sup>k</sup>(Z-X), or in simpler terms, for k even, Y &lt; X &lt; Z and for k odd, Z &lt; X &lt; Y.</p></li>\n</ul></li>\n<li><p>Extend the degree of the continued fraction by 1 step without changing the values of the numbers. In general, if the last terms are y<sub>k</sub> and y<sub>k</sub> + 1, we change that to [... y<sub>k</sub>, y<sub>k+1</sub>=∞] and [... y<sub>k</sub>, z<sub>k+1</sub>=1]. Now increase k by 1.</p></li>\n<li><p><strong>Inner loops</strong>: This is essentially the same as @templatetypedef's interview question about the integers. We do a two-phase binary search to get closer:</p></li>\n<li><p><strong>Inner loop 1</strong>: y<sub>k</sub> = ∞, z<sub>k</sub>  = a, and X is between Y and Z.</p></li>\n<li><p>Double Z's last term: Compute M = Z but with m<sub>k</sub> = 2*a = 2*z<sub>k</sub>.</p></li>\n<li><p>Query the unknown number: q = Q(X,M).</p></li>\n<li><p>If q = 0, we have our answer and go to step 17 . </p></li>\n<li><p>If q and Q(X,Y) have opposite signs, it means X is between Y and M, so set Z = M and go to step 5. </p></li>\n<li><p>Otherwise set Y = M and go to the next step:</p></li>\n<li><p><strong>Inner loop 2.</strong> y<sub>k</sub> = b, z<sub>k</sub>  = a, and X is between Y and Z.</p></li>\n<li><p>If a and b differ by 1, swap Y and Z, go to step 2.</p></li>\n<li><p>Perform a binary search: compute M where m<sub>k</sub> = floor((a+b)/2, and query q = Q(X,M).</p></li>\n<li><p>If q = 0, we're done and go to step 17.</p></li>\n<li><p>If q and Q(X,Y) have opposite signs, it means X is between Y and M, so set Z = M and go to step 11.</p></li>\n<li><p>Otherwise, q and Q(X,Z) have opposite signs, it means X is between Z and M, so set Y = M and go to step 11.</p></li>\n<li><p>Done: X = M. </p></li>\n</ol>\n\n<p>A concrete example for X = 16/113 = 0.14159292</p>\n\n<pre><code>Y = 0 = [0], Z = 1 = [1], k = 0\n\nk = 1:\nY = 0 = [0; &amp;#8734;] &lt; X, Z = 1 = [0; 1] &gt; X, M = [0; 2] = 1/2 &gt; X.\nY = 0 = [0; &amp;#8734;], Z = 1/2 = [0; 2], M = [0; 4] = 1/4 &gt; X.\nY = 0 = [0; &amp;#8734;], Z = 1/4 = [0; 4], M = [0; 8] = 1/8 &lt; X.\nY = 1/8 = [0; 8], Z = 1/4 = [0; 4], M = [0; 6] = 1/6 &gt; X.\nY = 1/8 = [0; 8], Z = 1/6 = [0; 6], M = [0; 7] = 1/7 &gt; X.\nY = 1/8 = [0; 8], Z = 1/7 = [0; 7] \n  --&gt; the two last terms differ by one, so swap and repeat outer loop.\n\nk = 2:\nY = 1/7 = [0; 7, &amp;#8734;] &gt; X, Z = 1/8 = [0; 7, 1] &lt; X,\n    M = [0; 7, 2] = 2/15 &lt; X\nY = 1/7 = [0; 7, &amp;#8734;], Z = 2/15 = [0; 7, 2],\n    M = [0; 7, 4] = 4/29 &lt; X\nY = 1/7 = [0; 7, &amp;#8734;], Z = 4/29 = [0; 7, 4], \n    M = [0; 7, 8] = 8/57 &lt; X\nY = 1/7 = [0; 7, &amp;#8734;], Z = 8/57 = [0; 7, 8],\n    M = [0; 7, 16] = 16/113 = X \n    --&gt; done!\n</code></pre>\n\n<p>At each step of computing M, the range of the interval reduces. It is probably fairly easy to prove (though I won't do this) that the interval reduces by a factor of at least 1/sqrt(5) at each step, which would show that this algorithm is O(log q) steps.</p>\n\n<p>Note that this can be combined with templatetypedef's original interview question and apply towards <em>any</em> rational number p/q, not just between 0 and 1, by first computing Q(X,0), then for either positive/negative integers, bounding between two consecutive integers, and then using the above algorithm for the fractional part.</p>\n\n<p>When I have a chance next, I will post a python program that implements this algorithm.</p>\n\n<p><strong>edit</strong>: also, note that you don't have to compute the continued fraction each step (which would be O(k), there are partial approximants to continued fractions that can compute the next step from the previous step in O(1).)</p>\n\n<p><strong>edit 2</strong>: Recursive definition of partial approximants:</p>\n\n<p>If A<sub>k</sub> = [a<sub>0</sub>; a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>, ... a<sub>k</sub>] = p<sub>k</sub>/q<sub>k</sub>, then p<sub>k</sub> = a<sub>k</sub>p<sub>k-1</sub> + p<sub>k-2</sub>, and q<sub>k</sub> = a<sub>k</sub>q<sub>k-1</sub> + q<sub>k-2</sub>. (Source: Niven &amp; Zuckerman, 4th ed, Theorems 7.3-7.5. See also <a href=\"http://en.wikipedia.org/wiki/Fundamental_recurrence_formulas\">Wikipedia</a>) </p>\n\n<p>Example: [0] = 0/1 = p<sub>0</sub>/q<sub>0</sub>, [0; 7] = 1/7 = p<sub>1</sub>/q<sub>1</sub>; so [0; 7, 16] = (16*1+0)/(16*7+1) = 16/113 = p<sub>2</sub>/q<sub>2</sub>.</p>\n\n<p>This means that if two continued fractions Y and Z have the same terms except the last one, and the continued fraction excluding the last term is p<sub>k-1</sub>/q<sub>k-1</sub>, then we can write Y = (y<sub>k</sub>p<sub>k-1</sub> + p<sub>k-2</sub>) / (y<sub>k</sub>q<sub>k-1</sub> + q<sub>k-2</sub>) and Z = (z<sub>k</sub>p<sub>k-1</sub> + p<sub>k-2</sub>) / (z<sub>k</sub>q<sub>k-1</sub> + q<sub>k-2</sub>). It should be possible to show from this that |Y-Z| decreases by at least a factor of 1/sqrt(5) at each smaller interval produced by this algorithm, but the algebra seems to be beyond me at the moment. :-(</p>\n\n<p>Here's my Python program:</p>\n\n<pre><code>import math\n\n# Return a function that returns Q(p0/q0,p/q) \n#   = sign(p0/q0-p/q) = sign(p0q-q0p)*sign(q0*q)\n# If p/q &lt; p0/q0, then Q() = 1; if p/q &lt; p0/q0, then Q() = -1; otherwise Q()=0.\ndef makeQ(p0,q0):\n  def Q(p,q):\n    return cmp(q0*p,p0*q)*cmp(q0*q,0)\n  return Q\n\ndef strsign(s):\n  return '&lt;' if s&lt;0 else '&gt;' if s&gt;0 else '=='\n\ndef cfnext(p1,q1,p2,q2,a):\n  return [a*p1+p2,a*q1+q2]\n\ndef ratguess(Q, doprint, kmax):\n# p2/q2 = p[k-2]/q[k-2]\n  p2 = 1\n  q2 = 0\n# p1/q1 = p[k-1]/q[k-1]\n  p1 = 0\n  q1 = 1\n  k = 0\n  cf = [0]\n  done = False\n  while not done and (not kmax or k &lt; kmax):\n    if doprint:\n      print 'p/q='+str(cf)+'='+str(p1)+'/'+str(q1)\n# extend continued fraction\n    k = k + 1\n    [py,qy] = [p1,q1]\n    [pz,qz] = cfnext(p1,q1,p2,q2,1)\n    ay = None\n    az = 1\n    sy = Q(py,qy)\n    sz = Q(pz,qz)\n    while not done:\n      if doprint:\n        out = str(py)+'/'+str(qy)+' '+strsign(sy)+' X '\n        out += strsign(-sz)+' '+str(pz)+'/'+str(qz)\n        out += ', interval='+str(abs(1.0*py/qy-1.0*pz/qz))\n      if ay:\n        if (ay - az == 1):\n          [p0,q0,a0] = [pz,qz,az]\n          break\n        am = (ay+az)/2\n      else:\n        am = az * 2\n      [pm,qm] = cfnext(p1,q1,p2,q2,am)\n      sm = Q(pm,qm)\n      if doprint:\n        out = str(ay)+':'+str(am)+':'+str(az) + '   ' + out + ';  M='+str(pm)+'/'+str(qm)+' '+strsign(sm)+' X '\n        print out\n      if (sm == 0):\n        [p0,q0,a0] = [pm,qm,am]\n        done = True\n        break\n      elif (sm == sy):\n        [py,qy,ay,sy] = [pm,qm,am,sm]\n      else:\n        [pz,qz,az,sz] = [pm,qm,am,sm]     \n\n    [p2,q2] = [p1,q1]\n    [p1,q1] = [p0,q0]    \n    cf += [a0]\n\n  print 'p/q='+str(cf)+'='+str(p1)+'/'+str(q1)\n  return [p1,q1]\n</code></pre>\n\n<p>and a sample output for <code>ratguess(makeQ(33102,113017), True, 20)</code>:</p>\n\n<pre><code>p/q=[0]=0/1\nNone:2:1   0/1 &lt; X &lt; 1/1, interval=1.0;  M=1/2 &gt; X \nNone:4:2   0/1 &lt; X &lt; 1/2, interval=0.5;  M=1/4 &lt; X \n4:3:2   1/4 &lt; X &lt; 1/2, interval=0.25;  M=1/3 &gt; X \np/q=[0, 3]=1/3\nNone:2:1   1/3 &gt; X &gt; 1/4, interval=0.0833333333333;  M=2/7 &lt; X \nNone:4:2   1/3 &gt; X &gt; 2/7, interval=0.047619047619;  M=4/13 &gt; X \n4:3:2   4/13 &gt; X &gt; 2/7, interval=0.021978021978;  M=3/10 &gt; X \np/q=[0, 3, 2]=2/7\nNone:2:1   2/7 &lt; X &lt; 3/10, interval=0.0142857142857;  M=5/17 &gt; X \nNone:4:2   2/7 &lt; X &lt; 5/17, interval=0.00840336134454;  M=9/31 &lt; X \n4:3:2   9/31 &lt; X &lt; 5/17, interval=0.00379506641366;  M=7/24 &lt; X \np/q=[0, 3, 2, 2]=5/17\nNone:2:1   5/17 &gt; X &gt; 7/24, interval=0.00245098039216;  M=12/41 &lt; X \nNone:4:2   5/17 &gt; X &gt; 12/41, interval=0.00143472022956;  M=22/75 &gt; X \n4:3:2   22/75 &gt; X &gt; 12/41, interval=0.000650406504065;  M=17/58 &gt; X \np/q=[0, 3, 2, 2, 2]=12/41\nNone:2:1   12/41 &lt; X &lt; 17/58, interval=0.000420521446594;  M=29/99 &gt; X \nNone:4:2   12/41 &lt; X &lt; 29/99, interval=0.000246366100025;  M=53/181 &lt; X \n4:3:2   53/181 &lt; X &lt; 29/99, interval=0.000111613371282;  M=41/140 &lt; X \np/q=[0, 3, 2, 2, 2, 2]=29/99\nNone:2:1   29/99 &gt; X &gt; 41/140, interval=7.21500721501e-05;  M=70/239 &lt; X \nNone:4:2   29/99 &gt; X &gt; 70/239, interval=4.226364059e-05;  M=128/437 &gt; X \n4:3:2   128/437 &gt; X &gt; 70/239, interval=1.91492009996e-05;  M=99/338 &gt; X \np/q=[0, 3, 2, 2, 2, 2, 2]=70/239\nNone:2:1   70/239 &lt; X &lt; 99/338, interval=1.23789953207e-05;  M=169/577 &gt; X \nNone:4:2   70/239 &lt; X &lt; 169/577, interval=7.2514738621e-06;  M=309/1055 &lt; X \n4:3:2   309/1055 &lt; X &lt; 169/577, interval=3.28550190148e-06;  M=239/816 &lt; X \np/q=[0, 3, 2, 2, 2, 2, 2, 2]=169/577\nNone:2:1   169/577 &gt; X &gt; 239/816, interval=2.12389981991e-06;  M=408/1393 &lt; X \nNone:4:2   169/577 &gt; X &gt; 408/1393, interval=1.24415093544e-06;  M=746/2547 &lt; X \nNone:8:4   169/577 &gt; X &gt; 746/2547, interval=6.80448470014e-07;  M=1422/4855 &lt; X \nNone:16:8   169/577 &gt; X &gt; 1422/4855, interval=3.56972657711e-07;  M=2774/9471 &gt; X \n16:12:8   2774/9471 &gt; X &gt; 1422/4855, interval=1.73982239227e-07;  M=2098/7163 &gt; X \n12:10:8   2098/7163 &gt; X &gt; 1422/4855, interval=1.15020646951e-07;  M=1760/6009 &gt; X \n10:9:8   1760/6009 &gt; X &gt; 1422/4855, interval=6.85549088053e-08;  M=1591/5432 &lt; X \np/q=[0, 3, 2, 2, 2, 2, 2, 2, 9]=1591/5432\nNone:2:1   1591/5432 &lt; X &lt; 1760/6009, interval=3.06364213998e-08;  M=3351/11441 &lt; X \np/q=[0, 3, 2, 2, 2, 2, 2, 2, 9, 1]=1760/6009\nNone:2:1   1760/6009 &gt; X &gt; 3351/11441, interval=1.45456726663e-08;  M=5111/17450 &lt; X \nNone:4:2   1760/6009 &gt; X &gt; 5111/17450, interval=9.53679318849e-09;  M=8631/29468 &lt; X \nNone:8:4   1760/6009 &gt; X &gt; 8631/29468, interval=5.6473816179e-09;  M=15671/53504 &lt; X \nNone:16:8   1760/6009 &gt; X &gt; 15671/53504, interval=3.11036635336e-09;  M=29751/101576 &gt; X \n16:12:8   29751/101576 &gt; X &gt; 15671/53504, interval=1.47201634215e-09;  M=22711/77540 &gt; X \n12:10:8   22711/77540 &gt; X &gt; 15671/53504, interval=9.64157420569e-10;  M=19191/65522 &gt; X \n10:9:8   19191/65522 &gt; X &gt; 15671/53504, interval=5.70501257346e-10;  M=17431/59513 &gt; X \np/q=[0, 3, 2, 2, 2, 2, 2, 2, 9, 1, 8]=15671/53504\nNone:2:1   15671/53504 &lt; X &lt; 17431/59513, interval=3.14052228667e-10;  M=33102/113017 == X\n</code></pre>\n\n<p>Since Python handles biginteger math from the start, and this program uses only integer math (except for the interval calculations), it should work for arbitrary rationals.</p>\n\n<hr>\n\n<p><strong>edit 3</strong>: Outline of proof that this is O(log q), not O(log^2 q):</p>\n\n<p>First note that until the rational number is found, the # of steps n<sub>k</sub> for each new continued fraction term is <em>exactly</em> 2b(a_k)-1 where b(a_k) is the # of bits needed to represent a_k = ceil(log2(a_k)): it's b(a_k) steps to widen the \"net\" of the binary search, and b(a_k)-1 steps to narrow it). See the example above, you'll note that the # of steps is always 1, 3, 7, 15, etc.</p>\n\n<p>Now we can use the recurrence relation q<sub>k</sub> = a<sub>k</sub>q<sub>k-1</sub> + q<sub>k-2</sub> and induction to prove the desired result.</p>\n\n<p>Let's state it in this way: that the value of q after the N<sub>k</sub> = sum(n<sub>k</sub>) steps required for reaching the kth term has a minimum: q &gt;= A*2<sup>cN</sup> for some fixed constants A,c. (so to invert, we'd get that the # of steps N is &lt;= (1/c) * log<sub>2</sub> (q/A) = O(log q).)</p>\n\n<p>Base cases: </p>\n\n<ul>\n<li>k=0: q = 1, N = 0, so q &gt;= 2<sup>N</sup></li>\n<li>k=1: for N = 2b-1 steps, q = a<sub>1</sub> &gt;= 2<sup>b-1</sup> = 2<sup>(N-1)/2</sup> = 2<sup>N/2</sup>/sqrt(2). </li>\n</ul>\n\n<p>This implies A = 1, c = 1/2 could provide desired bounds. In reality, q may <em>not</em> double each term (counterexample: [0; 1, 1, 1, 1, 1] has a growth factor of phi = (1+sqrt(5))/2) so let's use c = 1/4.  </p>\n\n<p>Induction:</p>\n\n<ul>\n<li><p>for term k, q<sub>k</sub> = a<sub>k</sub>q<sub>k-1</sub> + q<sub>k-2</sub>. Again, for the n<sub>k</sub> = 2b-1 steps needed for this term, a<sub>k</sub> &gt;= 2<sup>b-1</sup> = 2<sup>(n<sub>k</sub>-1)/2</sup>.</p>\n\n<p>So  a<sub>k</sub>q<sub>k-1</sub> &gt;= 2<sup>(N<sub>k</sub>-1)/2</sup> * q<sub>k-1</sub> &gt;=  2<sup>(n<sub>k</sub>-1)/2</sup> * A*2<sup>N<sub>k-1</sub>/4</sup> = A*2<sup>N<sub>k</sub>/4</sup>/sqrt(2)*2<sup>n<sub>k</sub>/4</sup>.</p></li>\n</ul>\n\n<p>Argh -- the tough part here is that if a<sub>k</sub> = 1, q may not increase much for that one term, and we need to use q<sub>k-2</sub> but that may be much smaller than q<sub>k-1</sub>.</p>\n    "},{"t":"Finding the max/min value in an array of primitives using Java","l":"http://stackoverflow.com/questions/1484347/finding-the-max-min-value-in-an-array-of-primitives-using-java","q":"\n\n<p>It's trivial to write a function to determine the min/max value in an array, such as:</p>\n\n<pre><code>/**\n * \n * @param chars\n * @return the max value in the array of chars\n */\nprivate static int maxValue(char[] chars) {\n\tint max = chars[0];\n\tfor (int ktr = 0; ktr &lt; chars.length; ktr++) {\n\t\tif (chars[ktr] &gt; max) {\n\t\t\tmax = chars[ktr];\n\t\t}\n\t}\n\treturn max;\n}\n</code></pre>\n\n<p>but isn't this already done somewhere?</p>\n    ","a":"\n<p>Using Commons Lang (to convert) + Collections (to min/max)</p>\n\n<pre><code>import java.util.Arrays;\nimport java.util.Collections;\n\nimport org.apache.commons.lang.ArrayUtils;\n\npublic class MinMaxValue {\n\n    public static void main(String[] args) {\n        char[] a = {'3', '5', '1', '4', '2'};\n\n        List b = Arrays.asList(ArrayUtils.toObject(a));\n\n        System.out.println(Collections.min(b));\n        System.out.println(Collections.max(b));\n   }\n}\n</code></pre>\n\n<p>Note that <code>Arrays.asList()</code> wraps the underlying array, so it should not be too memory intensive and it should not perform a copy on the elements of the array.</p>\n    "},{"t":"How does the HyperLogLog algorithm work?","l":"http://stackoverflow.com/questions/12327004/how-does-the-hyperloglog-algorithm-work","q":"\n\n<p>I've been learning about different algorithms in my spare time recently, and one that I came across which appears to be very interesting is called the HyperLogLog algorithm - which estimates how many unique items are in a set.</p>\n\n<p>This was particularly interesting to me because it brought me back to my MySQL days when I saw that \"Cardinality\" value (which I always assumed until recently that it was calculated not estimated).</p>\n\n<p>So I know how to write an algorithm in O(n) that will calculate how many unique items are in an array. I wrote this in Javascript</p>\n\n<pre><code>function countUniqueAlgo1(arr) {\n    var Table = {};\n    var numUnique = 0;\n    var numDataPoints = arr.length;\n    for (var j = 0; j &lt; numDataPoints; j++) {\n        var val = arr[j];\n        if (Table[val] != null) {\n            continue;\n        }\n        Table[val] = 1;\n        numUnique++;\n    }\n    return numUnique;\n}\n</code></pre>\n\n<p>But the problem is that my algorithm, while O(n), uses a lot of memory (storing values in <code>Table</code>).</p>\n\n<p>I've been reading this paper about how to count duplicates in a set in O(n) time and using minimal memory.\n<a href=\"http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf\">http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf</a></p>\n\n<p>It explains that by hashing and counting bits or something one can estimate within a certain probability (assuming the set is evenly distributed) the number of unique items in a set.</p>\n\n<p>I've read the paper, but I can't seem to understand it. Can someone give a more layperson's explanation? I know what hashes are, but I don't understand how they are used in this HyperLogLog algorithm.</p>\n    ","a":"\n<p>We use the <code>HyperLogLog</code> algorithm quite extensively in our infrastructure. As a result we end up trying to explain it to everyone from executives to developers. To make our lives a little easier, we have put up a <a href=\"https://web.archive.org/web/20150323055945/http://research.neustar.biz/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/\" rel=\"nofollow\">blog post</a> on it and have included a simulation written in <code>D3</code> to help illustrate it. I hope it helps!</p>\n    "},{"t":"How to detect BPM of the song by programming [closed]","l":"http://stackoverflow.com/questions/657073/how-to-detect-bpm-of-the-song-by-programming","q":"\n\n<p>How can the tempo/BPM of a song be determined programmatically?  What algorithms are commonly used, and what considerations must be made?</p>\n    ","a":"\n<p>This is challenging to explain in a single StackOverflow post. In general, the simplest beat-detection algorithms work by locating peaks in sound energy, which is easy to detect. More sophisticated methods use <strong><a href=\"http://en.wikipedia.org/wiki/Comb_filter\">comb filters</a></strong> and other statistical/waveform methods. For a detailed explication including code samples, check <a href=\"http://archive.gamedev.net/archive/reference/programming/features/beatdetection/index.html\"><strong>this GameDev article</strong></a> out.</p>\n    "},{"t":"Quicksort: Choosing the pivot","l":"http://stackoverflow.com/questions/164163/quicksort-choosing-the-pivot","q":"\n\n<p>When implementing Quicksort, one of the things you have to do is to choose a pivot. But when I look at pseudocode like the one below, it is not clear how I should choose the pivot. First element of list? Something else?</p>\n\n<pre><code> function quicksort(array)\n     var list less, greater\n     if length(array) ≤ 1  \n         return array  \n     select and remove a pivot value pivot from array\n     for each x in array\n         if x ≤ pivot then append x to less\n         else append x to greater\n     return concatenate(quicksort(less), pivot, quicksort(greater))\n</code></pre>\n\n<p>Can someone help me grasp the concept of choosing a pivot and whether or not different scenarios call for different strategies.</p>\n    ","a":"\n<p>Choosing a random pivot minimizes the chance that you will encounter worst-case O(n<sup>2</sup>) performance (always choosing first or last would cause worst-case performance for nearly-sorted or nearly-reverse-sorted data).  Choosing the middle element would also be acceptable in the majority of cases.</p>\n\n<p>Also, if you are implementing this yourself, there are versions of the algorithm that work in-place (i.e. without creating two new lists and then concatenating them).</p>\n    "},{"t":"What guarantees are there on the run-time complexity (Big-O) of LINQ methods?","l":"http://stackoverflow.com/questions/2799427/what-guarantees-are-there-on-the-run-time-complexity-big-o-of-linq-methods","q":"\n\n<p>I've recently started using LINQ quite a bit, and I haven't really seen any mention of run-time complexity for any of the LINQ methods. Obviously, there are many factors at play here, so let's restrict the discussion to the plain <code>IEnumerable</code> LINQ-to-Objects provider. Further, let's assume that any <code>Func</code> passed in as a selector / mutator / etc. is a cheap O(1) operation.  </p>\n\n<p>It seems obvious that all the single-pass operations (<code>Select</code>, <code>Where</code>, <code>Count</code>, <code>Take/Skip</code>, <code>Any/All</code>, etc.) will be O(n), since they only need to walk the sequence once; although even this is subject to laziness. </p>\n\n<p>Things are murkier for the more complex operations; the set-like operators (<code>Union</code>, <code>Distinct</code>, <code>Except</code>, etc.) work using <code>GetHashCode</code> by default (afaik), so it seems reasonable to assume they're using a hash-table internally, making these operations O(n) as well, in general. What about the versions that use an <code>IEqualityComparer</code>? </p>\n\n<p><code>OrderBy</code> would need a sort, so most likely we're looking at O(n log n). What if it's already sorted? How about if I say <code>OrderBy().ThenBy()</code> and provide the same key to both? </p>\n\n<p>I could see <code>GroupBy</code> (and <code>Join</code>) using either sorting, or hashing. Which is it? </p>\n\n<p><code>Contains</code> would be O(n) on a <code>List</code>, but O(1) on a <code>HashSet</code> - does LINQ check the underlying container to see if it can speed things up? </p>\n\n<p>And the real question - so far, I've been taking it on faith that the operations are performant. However, can I bank on that? STL containers, for example, clearly specify the complexity of every operation. Are there any similar guarantees on LINQ performance in the .NET library specification? </p>\n\n<p>More question (in response to comments):<br>\nHadn't really thought about overhead, but I didn't expect there to be very much for simple Linq-to-Objects. The CodingHorror post is talking about Linq-to-SQL, where I can understand parsing the query and making SQL would add cost - is there a similar cost for the Objects provider too? If so, is it different if you're using the declarative or functional syntax? </p>\n    ","a":"\n<p>There are very, very few guarantees, but there are a few optimizations:</p>\n\n<ul>\n<li><p>Extension methods that use indexed access, such as <code>ElementAt</code>, <code>Skip</code>, <code>Last</code> or <code>LastOrDefault</code>, will check to see whether or not the underlying type implements <code>IList&lt;T&gt;</code>, so that you get O(1) access instead of O(N).</p></li>\n<li><p>The <code>Count</code> method checks for an <code>ICollection</code> implementation, so that this operation is O(1) instead of O(N).</p></li>\n<li><p><code>Distinct</code>, <code>GroupBy</code> <code>Join</code>, and I believe also the set-aggregation methods (<code>Union</code>, <code>Intersect</code> and <code>Except</code>) use hashing, so they should be close to O(N) instead of O(N²).</p></li>\n<li><p><code>Contains</code> checks for an <code>ICollection</code> implementation, so it <em>may</em> be O(1) if the underlying collection is also O(1), such as a <code>HashSet&lt;T&gt;</code>, but this is depends on the actual data structure and is not guaranteed.  Hash sets override the <code>Contains</code> method, that's why they are O(1).</p></li>\n<li><p><code>OrderBy</code> methods use a stable quicksort, so they're O(N log N) average case.</p></li>\n</ul>\n\n<p>I think that covers most if not all of the built-in extension methods.  There really are very few performance guarantees; Linq itself will try to take advantage of efficient data structures but it isn't a free pass to write potentially inefficient code.</p>\n    "},{"t":"How can I compare two sets of 1000 numbers against each other?","l":"http://stackoverflow.com/questions/3942551/how-can-i-compare-two-sets-of-1000-numbers-against-each-other","q":"\n\n<p>I must check approximately 1000 numbers against 1000 other numbers.</p>\n\n<p>I loaded both and compared them server-side:</p>\n\n<pre><code>foreach( $numbers1 as $n1 ) {\n  foreach( $numbers2 as $n2 ) {\n    if( $n1 == $n2 ) {\n      doBla();\n    }\n  }\n}\n</code></pre>\n\n<p>This took a long time, so I tried to do the same comparison client side using two hidden\n<code>div</code> elements. Then compared them using JavaScript. It still takes 45 seconds to load the page (using hidden <code>div</code> elements).</p>\n\n<p>I do not need to load the numbers that are not the same.</p>\n\n<p>Is there a faster algorithm? I am thinking of comparing them database side and just load the error numbers, then do an Ajax call for the remaining non-error numbers. But is a MySQL database fast enough?</p>\n    ","a":"\n<p>Sort the lists first. Then you can walk up both lists from the start, comparing as you go.</p>\n\n<p>The loop would look something like this:</p>\n\n<pre><code>var A = getFirstArray().sort(), B = getSecondArray().sort();\n\nvar i = 0, j = 0;\nwhile (i &lt; A.length &amp;&amp; j &lt; B.length) {\n    if (A[i] === B[j]) {\n        doBla(A[i]);\n        i++; j++;\n    }\n    else if (A[i] &lt; B[j]) {\n        i++;\n    }\n    else\n        j++;\n}\n</code></pre>\n\n<p>(That's JavaScript; you could do it server-side too, but I don't know PHP.)</p>\n\n<p><em>Edit</em> — just to be fair to all the hashtable fans (whom I respect of course), it's pretty easy to do that in JavaScript:</p>\n\n<pre><code>var map = {};\nfor (var i = 0; i &lt; B.length; ++i) map[B[i]] = true; // Assume integers.\nfor (var i = 0; i &lt; A.length; ++i) if (map[A[i]]) doBla(A[i]);\n</code></pre>\n\n<p>Or if the numbers are or might be floats:</p>\n\n<pre><code>var map = {};\nfor (var i = 0; i &lt; B.length; ++i) map['' + B[i]] = true; // Assume integers.\nfor (var i = 0; i &lt; A.length; ++i) if (map['' + A[i]]) doBla(A[i]);\n</code></pre>\n\n<p>Since numbers are pretty cheap to hash (even in JavaScript, converting to string before hashing is surprisingly cheap), this would be pretty fast.</p>\n    "},{"t":"Performance issue: Java vs C++","l":"http://stackoverflow.com/questions/24914525/performance-issue-java-vs-c","q":"\n\n<p>I have always heard that C++ was way more efficient than Java (and that is why most games are developped in C++).</p>\n\n<p>I wrote a small algorithm to solve the \"Eight queens puzzle\" in both Java and C++, using the exact same algorithm, and then started to raise the number or squares.\nWhen reaching checkboards of 20*20 or even 22*22, it appears Java is much more effective (3 seconds vs 66 seconds for C++).</p>\n\n<p>I have no idea why, but I am pretty beginning with C++, so it is possible I made some huge performance mistakes, so I will gladly accept any information that would help me understand what is happening.</p>\n\n<p>Below is the code I use in Java:</p>\n\n<pre><code>import java.awt.Point;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class HuitDames {\n\n    /**\n     * La liste des coordnnées des dames.\n     */\n    private static List&lt;Point&gt; positions = new ArrayList&lt;&gt;();\n\n    /**\n     * Largeur de la grille.\n     */\n    private static final int LARGEUR_GRILLE = 22;\n\n\n    /**\n     * @param args the command line arguments\n     */\n    public static void main(String[] args) {\n        int i = 1;\n        placerDame(i);\n        for (Point point : positions) {\n            System.out.println(\"(\" + point.x + \"; \" + point.y + \")\");\n        }\n    }\n\n    /**\n     * Place une dame et return true si la position est bonne.\n     * @param i le numéro de la dame.\n     * @return si la position est bonne.\n     */\n    private static boolean placerDame(int i) {\n\n        boolean bonnePosition = false;\n        for (int j = 1; j &lt;= LARGEUR_GRILLE &amp;&amp; bonnePosition == false; j++) {\n            Point emplacement = new Point(i, j);\n            positions.add(emplacement);\n            if (verifierPrise(emplacement) &amp;&amp; (i == LARGEUR_GRILLE || placerDame(i + 1))) {\n                bonnePosition = true;\n            }\n            else {\n                positions.remove(i - 1);\n            }\n        }\n\n        return bonnePosition;\n    }\n\n    /**\n     * Vérifie que la nouvelle position n'est pas en prise avec une position déjà présente.\n     * @param position la position de la nouvelle dame.\n     * @return Si la position convient par rapport aux positions des autres dames.\n     */\n    private static boolean verifierPrise(Point position) {\n        boolean nonPrise = true;\n        for (Point point : positions) {\n            if (!point.equals(position)) {\n                // Cas où sur la même colonne.\n                if (position.y == point.y) {\n                    nonPrise = false;\n                }\n                // Cas où sur même diagonale.\n                if (Math.abs(position.y - point.y) == Math.abs(position.x - point.x)) {\n                    nonPrise = false;\n                }\n            }\n        }\n\n        return nonPrise;\n    }\n}\n</code></pre>\n\n<p>And below is the code in C++:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;list&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\n\nusing namespace std;\n\n\n// Class to represent points.\nclass Point {\n\n    private:\n        double xval, yval;\n\n    public:\n        // Constructor uses default arguments to allow calling with zero, one,\n        // or two values.\n        Point(double x = 0.0, double y = 0.0) {\n                xval = x;\n                yval = y;\n        }\n\n        // Extractors.\n        double x() { return xval; }\n        double y() { return yval; }\n};\n\n#define LARGEUR_GRILLE 22\nlist&lt;Point&gt; positions;\n\n\nbool verifierNonPrise(Point emplacement) {\n    bool nonPrise = true;\n    for (list&lt;Point&gt;::iterator it = positions.begin(); it!= positions.end(); it++) {\n        if (it-&gt;x() != emplacement.x()) {\n            if (it-&gt;y() == emplacement.y()) {\n                nonPrise = false;\n            }\n            if (abs(it-&gt;y() - emplacement.y()) == abs(it-&gt;x() - emplacement.x())) {\n                nonPrise = false;\n            }\n        }\n    }\n\n    return nonPrise;\n}\n\nbool placerDame(int i) {\n    bool bonnePosition = false;\n    for (int j = 1; j &lt;= LARGEUR_GRILLE &amp;&amp; !bonnePosition; j++) {\n        Point emplacement(i,j);\n        positions.push_back(emplacement);\n        if (verifierNonPrise(emplacement) &amp;&amp; (i == LARGEUR_GRILLE || placerDame(i + 1))) {\n            bonnePosition = true;\n        }\n        else {\n            positions.pop_back();\n        }\n    }\n\n    return bonnePosition;\n}\n\nint main()\n{\n    int i = 1;\n    placerDame(i);\n    for (list&lt;Point&gt;::iterator it = positions.begin(); it!= positions.end(); it++) {\n        cout &lt;&lt; \"(\" &lt;&lt; it-&gt;x() &lt;&lt; \"; \" &lt;&lt; it-&gt;y() &lt;&lt; \")\" &lt;&lt; endl;\n    }\n    return 0;\n}\n</code></pre>\n    ","a":"\n<p><code>std::list</code> in C++ is a linked list, whereas <code>java.util.ArrayList</code> is an array. Try replacing <code>std::list</code> by <code>std::vector</code>. Also, be sure to compile with optimization turned on.</p>\n    "},{"t":"Non recursive Depth first search algorithm","l":"http://stackoverflow.com/questions/5278580/non-recursive-depth-first-search-algorithm","q":"\n\n<p>I am looking for a Non recursive Depth first search algorithm for a non binary tree. Any help is very much appreciated.</p>\n    ","a":"\n<p>DFS:</p>\n\n<pre><code>list nodes_to_visit = {root};\nwhile( nodes_to_visit isn't empty ) {\n  currentnode = nodes_to_visit.first();\n  nodes_to_visit.prepend( currentnode.children );\n  //do something\n}\n</code></pre>\n\n<p>BFS:</p>\n\n<pre><code>list nodes_to_visit = {root};\nwhile( nodes_to_visit isn't empty ) {\n  currentnode = nodes_to_visit.first();\n  nodes_to_visit.append( currentnode.children );\n  //do something\n}\n</code></pre>\n\n<p>The symmetry of the two is quite cool.</p>\n\n<p><strong>Update:</strong> As pointed out, <code>first()</code> removes and returns the first element in the list.</p>\n    "},{"t":"Breadth First Vs Depth First","l":"http://stackoverflow.com/questions/687731/breadth-first-vs-depth-first","q":"\n\n<p>When Traversing a Tree/Graph what is the difference between Breadth First and Depth first? Any coding or pseudocode examples would be great.</p>\n    ","a":"\n<p>These two terms differentiate between two different ways of walking a tree.</p>\n\n<p>It is probably easiest just to exhibit the difference. Consider the tree:</p>\n\n<pre><code>    A\n   / \\\n  B   C\n /   / \\\nD   E   F\n</code></pre>\n\n<p>A <strong>depth</strong> first traversal would visit the nodes in this order</p>\n\n<pre><code>A, B, D, C, E, F\n</code></pre>\n\n<p>Notice that you go all the way <strong>down</strong> one leg before moving on.</p>\n\n<p>A <strong>breadth</strong> first traversal would visit the node in this order</p>\n\n<pre><code>A, B, C, D, E, F\n</code></pre>\n\n<p>Here we work all the way <strong>across</strong> each level before going down.</p>\n\n<p>(Note that there is some ambiguity in the traversal orders, and I've cheated to maintain the \"reading\" order at each level of the tree. In either case I could get to B before or after C, and likewise I could get to E before or after F. This may or may not matter, depends on you application...)</p>\n\n<hr>\n\n<p>Both kinds of traversal can be achieved with the pseudocode:</p>\n\n<pre><code>Store the root node in Container\nWhile (there are nodes in Container)\n   N = Get the \"next\" node from Container\n   Store all the children of N in Container\n   Do some work on N\n</code></pre>\n\n<p>The difference between the two traversal orders lies in the choice of <code>Container</code>. </p>\n\n<ul>\n<li>For <strong>depth first</strong> use a stack. (The recursive implementation uses the call-stack...)</li>\n<li>For <strong>breadth-first</strong> use a queue.</li>\n</ul>\n\n<hr>\n\n<p>The recursive implementation looks like</p>\n\n<pre><code>ProcessNode(Node)\n   Work on the payload Node\n   Foreach child of Node\n      ProcessNode(child)\n   /* Alternate time to work on the payload Node (see below) */\n</code></pre>\n\n<p>The recursion ends when you reach a node that has no children, so it is guaranteed to end for\nfinite, acyclic graphs.</p>\n\n<hr>\n\n<p>At this point, I've still cheated a little. With a little cleverness you can also <em>work-on</em> the nodes in this order:</p>\n\n<pre><code>D, B, E, F, C, A\n</code></pre>\n\n<p>which is a variation of depth-first, where I don't do the work at each node until I'm walking back up the tree. I have however <em>visited</em> the higher nodes on the way down to find their children.</p>\n\n<p>This traversal is fairly natural in the recursive implementation (use the \"Alternate time\" line above instead of the first \"Work\" line), and not <em>too</em> hard if you use a explicit stack, but I'll leave it as an exercise.</p>\n    "},{"t":"What, if anything, is wrong with this shuffling algorithm and how can I know?","l":"http://stackoverflow.com/questions/3944556/what-if-anything-is-wrong-with-this-shuffling-algorithm-and-how-can-i-know","q":"\n\n<p>Just as background, I'm aware of the <a href=\"http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle\">Fisher-Yates</a> perfect shuffle.  It is a great shuffle with its O(n) complexity and its guaranteed uniformity and I'd be a fool not to use it ... in an environment that permits in-place updates of arrays (so in most, if not all, <em>imperative</em> programming environments).</p>\n\n<p>Sadly the functional programming world doesn't give you access to mutable state.</p>\n\n<p>Because of Fisher-Yates, however, there's not a lot of literature I can find on how to design a shuffling algorithm.  The few places that address it at all do so briefly before saying, in effect, \"so here's Fisher-Yates which is all the shuffling you need to know\".  I had to, in the end, come up with my own solution.</p>\n\n<p>The solution I came up with works like this to shuffle any list of data:</p>\n\n<ul>\n<li>If the list is empty, return the empty set.</li>\n<li>If the list has a single item, return that single item.</li>\n<li>If the list is non-empty, partition the list with a random number generator and apply the algorithm recursively to each partition, assembling the results.</li>\n</ul>\n\n<p>In Erlang code it looks something like this:</p>\n\n<pre><code>shuffle([])  -&gt; [];\nshuffle([L]) -&gt; [L];\nshuffle(L)   -&gt;\n  {Left, Right} = lists:partition(fun(_) -&gt; \n                                    random:uniform() &lt; 0.5 \n                                  end, L),\n  shuffle(Left) ++ shuffle(Right).\n</code></pre>\n\n<p>(If this looks like a deranged quick sort to you, well, that's what it is, basically.)</p>\n\n<p>So here's my problem: the same situation that makes finding shuffling algorithms that aren't Fisher-Yates difficult makes finding tools to <strong>analyse</strong> a shuffling algorithm equally difficult.  There's lots of literature I can find on analysing PRNGs for uniformity, periodicity, etc. but not a lot of information out there on how to analyse a shuffle.  (Indeed some of the information I found on analysing shuffles was just plain wrong -- easily deceived through simple techniques.)</p>\n\n<p>So my question is this: how do I analyse my shuffling algorithm (assuming that the <code>random:uniform()</code> call up there is up to the task of generating apropriate random numbers with good characteristics)?  What mathematical tools are there at my disposal to judge whether or not, say, 100,000 runs of the shuffler over a list of integers ranging 1..100 has given me plausibly good shuffling results?  I've done a few tests of my own (comparing increments to decrements in the shuffles, for example), but I'd like to know a few more.</p>\n\n<p>And if there's any insight into that shuffle algorithm itself that would be appreciated too.</p>\n    ","a":"\n<h3>General remark</h3>\n\n<p>My personal approach about correctness of probability-using algorithms: if you know how to <em>prove</em> it's correct, then it's probably correct; if you don't, it's certainly wrong.</p>\n\n<p>Said differently, it's generally hopeless to try to analyse every algorithm you could come up with: you have to keep looking for an algorithm until you find one that you <em>can</em> prove correct.</p>\n\n<h3>Analysing a random algorithm by computing the distribution</h3>\n\n<p>I know of one way to \"automatically\" analyse a shuffle (or more generally a random-using algorithm) that is stronger than the simple \"throw lots of tests and check for uniformity\". You can mechanically compute the distribution associated to each input of your algorithm.</p>\n\n<p>The general idea is that a random-using algorithm explores a part of a world of possibilities. Each time your algorithm asks for a random element in a set ({<code>true</code>, <code>false</code>} when flipping a coin), there are two possible outcomes for your algorithm, and one of them is chosen. You can change your algorithm so that, instead of returning one of the possible outcomes, it explores <em>all</em> solutions in parallel and returns all possible outcomes with the associated distributions.</p>\n\n<p>In general, that would require rewriting your algorithm in depth. If your language supports delimited continuations, you don't have to; you can implement \"exploration of all possible outcomes\" inside the function asking for a random element (the idea is that the random generator, instead of returning a result, capture the continuation associated to your program and run it with all different results). For an example of this approach, see oleg's <a href=\"http://okmij.org/ftp/kakuritu/index.html\">HANSEI</a>.</p>\n\n<p>An intermediary, and probably less arcane, solution is to represent this \"world of possible outcomes\" as a monad, and use a language such as Haskell with facilities for monadic programming. Here is an example implementation of a variant¹ of your algorithm, in Haskell, using the probability monad of the <a href=\"http://hackage.haskell.org/package/probability\">probability</a> package :</p>\n\n<pre><code>import Numeric.Probability.Distribution\n\nshuffleM :: (Num prob, Fractional prob) =&gt; [a] -&gt; T prob [a]\nshuffleM [] = return []\nshuffleM [x] = return [x]\nshuffleM (pivot:li) = do\n        (left, right) &lt;- partition li\n        sleft &lt;- shuffleM left\n        sright &lt;- shuffleM right\n        return (sleft ++ [pivot] ++ sright)\n  where partition [] = return ([], [])\n        partition (x:xs) = do\n                  (left, right) &lt;- partition xs\n                  uniform [(x:left, right), (left, x:right)]\n</code></pre>\n\n<p>You can run it for a given input, and get the output distribution :</p>\n\n<pre><code>*Main&gt; shuffleM [1,2]\nfromFreqs [([1,2],0.5),([2,1],0.5)]\n*Main&gt; shuffleM [1,2,3]\nfromFreqs\n  [([2,1,3],0.25),([3,1,2],0.25),([1,2,3],0.125),\n   ([1,3,2],0.125),([2,3,1],0.125),([3,2,1],0.125)]\n</code></pre>\n\n<p>You can see that this algorithm is uniform with inputs of size 2, but non-uniform on inputs of size 3.</p>\n\n<p>The difference with the test-based approach is that we can gain absolute certainty in a finite number of steps : it can be quite big, as it amounts to an exhaustive exploration of the world of possibles (but generally smaller than 2^N, as there are factorisations of similar outcomes), but if it returns a non-uniform distribution we know for sure that the algorithm is wrong. Of course, if it returns an uniform distribution for <code>[1..N]</code> and <code>1 &lt;= N &lt;= 100</code>, you only know that your algorithm is uniform up to lists of size 100; it may still be wrong.</p>\n\n<p>¹: this algorithm is a variant of your Erlang's implementation, because of the specific pivot handling. If I use no pivot, like in your case, the input size doesn't decrease at each step anymore : the algorithm also considers the case where all inputs are in the left list (or right list), and get lost in an infinite loop. This is a weakness of the probability monad implementation (if an algorithm has a probability 0 of non-termination, the distribution computation may still diverge), that I don't yet know how to fix.</p>\n\n<h3>Sort-based shuffles</h3>\n\n<p>Here is a simple algorithm that I feel confident I could prove correct:</p>\n\n<ol>\n<li>Pick a random key for each element in your collection.</li>\n<li>If the keys are not all distinct, restart from step 1.</li>\n<li>Sort the collection by these random keys.</li>\n</ol>\n\n<p>You can omit step 2 if you know the probability of a collision (two random numbers picked are equal) is sufficiently low, but without it the shuffle is not perfectly uniform.</p>\n\n<p>If you pick your keys in [1..N] where N is the length of your collection, you'll have lots of collisions (<a href=\"http://en.wikipedia.org/wiki/Birthday_attack\">Birthday problem</a>). If you pick your key as a 32-bit integer, the probability of conflict is low in practice, but still subject to the birthday problem.</p>\n\n<p>If you use infinite (lazily evaluated) bitstrings as keys, rather than finite-length keys, the probability of a collision becomes 0, and checking for distinctness is no longer necessary.</p>\n\n<p>Here is a shuffle implementation in OCaml, using lazy real numbers as infinite bitstrings:</p>\n\n<pre><code>type 'a stream = Cons of 'a * 'a stream lazy_t\n\nlet rec real_number () =\n  Cons (Random.bool (), lazy (real_number ()))\n\nlet rec compare_real a b = match a, b with\n| Cons (true, _), Cons (false, _) -&gt; 1\n| Cons (false, _), Cons (true, _) -&gt; -1\n| Cons (_, lazy a'), Cons (_, lazy b') -&gt;\n    compare_real a' b'\n\nlet shuffle list =\n  List.map snd\n    (List.sort (fun (ra, _) (rb, _) -&gt; compare_real ra rb)\n       (List.map (fun x -&gt; real_number (), x) list))\n</code></pre>\n\n<p>There are other approaches to \"pure shuffling\". A nice one is apfelmus's <a href=\"http://apfelmus.nfshost.com/articles/random-permutations.html\">mergesort-based solution</a>.</p>\n\n<p>Algorithmic considerations: the complexity of the previous algorithm depends on the probability that all keys are distinct. If you pick them as 32-bit integers, you have a one in ~4 billion probability that a particular key collides with another key. Sorting by these keys is O(n log n), assuming picking a random number is O(1).</p>\n\n<p>If you infinite bitstrings, you never have to restart picking, but the complexity is then related to \"how many elements of the streams are evaluated on average\". I conjecture it is O(log n) in average (hence still O(n log n) in total), but have no proof.</p>\n\n<h3>... and I think your algorithm works</h3>\n\n<p>After more reflexion, I think (like douplep), that your implementation is correct. Here is an informal explanation.</p>\n\n<p>Each element in your list is <em>tested</em> by several <code>random:uniform() &lt; 0.5</code> tests. To an element, you can associate the list of outcomes of those tests, as a list of booleans or {<code>0</code>, <code>1</code>}. At the beginning of the algorithm, you don't know the list associated to any of those number. After the first <code>partition</code> call, you know the first element of each list, etc. When your algorithm returns, the list of tests are completely known and the elements are <em>sorted</em> according to those lists (sorted in lexicographic order, or considered as binary representations of real numbers).</p>\n\n<p>So, your algorithm is equivalent to sorting by infinite bitstring keys. The action of partitioning the list, reminiscent of quicksort's partition over a pivot element, is actually a way of separating, for a given position in the bitstring, the elements with valuation <code>0</code> from the elements with valuation <code>1</code>.</p>\n\n<p>The sort is uniform because the bitstrings are all different. Indeed, two elements with real numbers equal up to the <code>n</code>-th bit are on the same side of a partition occurring during a recursive <code>shuffle</code> call of depth <code>n</code>. The algorithm only terminates when all the lists resulting from partitions are empty or singletons : all elements have been separated by at least one test, and therefore have one distinct binary decimal.</p>\n\n<h3>Probabilistic termination</h3>\n\n<p>A subtle point about your algorithm (or my equivalent sort-based method) is that the termination condition is <em>probabilistic</em>. Fisher-Yates always terminates after a known number of steps (the number of elements in the array). With your algorithm, the termination depends on the output of the random number generator.</p>\n\n<p>There are possible outputs that would make your algorithm <em>diverge</em>, not terminate. For example, if the random number generator always output <code>0</code>, each <code>partition</code> call will return the input list unchanged, on which you recursively call the shuffle : you will loop indefinitely.</p>\n\n<p>However, this is not an issue if you're confident that your random number generator is fair : it does not cheat and always return independent uniformly distributed results. In that case, the probability that the test <code>random:uniform() &lt; 0.5</code> always returns <code>true</code> (or <code>false</code>) is exactly 0 :</p>\n\n<ul>\n<li>the probability that the first N calls return <code>true</code> is 2^{-N}</li>\n<li>the probability that all calls return <code>true</code> is the probability of the infinite intersection, for all N, of the event that the first N calls return <code>0</code>; it is the infimum limit¹ of the 2^{-N}, which is 0</li>\n</ul>\n\n<p>¹: for the mathematical details, see <a href=\"http://en.wikipedia.org/wiki/Measure_(mathematics)#Measures_of_infinite_intersections_of_measurable_sets\">http://en.wikipedia.org/wiki/Measure_(mathematics)#Measures_of_infinite_intersections_of_measurable_sets</a></p>\n\n<p>More generally, the algorithm does not terminate if and only if some of the elements get associated to the same boolean stream. This means that at least two elements have the same boolean stream. But the probability that two random boolean streams are equal is again 0 : the probability that the digits at position K are equal is 1/2, so the probability that the N first digits are equal is 2^{-N}, and the same analysis applies.</p>\n\n<p>Therefore, you know that your algorithm <em>terminates with probability 1</em>. This is a slightly weaker guarantee that the Fisher-Yates algorithm, which <em>always terminate</em>. In particular, you're vulnerable to an attack of an evil adversary that would control your random number generator.</p>\n\n<p>With more probability theory, you could also compute the distribution of running times of your algorithm for a given input length. This is beyond my technical abilities, but I assume it's good : I suppose that you only need to look at O(log N) first digits on average to check that all N lazy streams are different, and that the probability of much higher running times decrease exponentially.</p>\n    "},{"t":"Parabolic knapsack","l":"http://stackoverflow.com/questions/5084817/parabolic-knapsack","q":"\n\n<p>Lets say I have a parabola. Now I also have a bunch of sticks that are all of the same width (yes my drawing skills are amazing!).  How can I stack these sticks within the parabola such that I am minimizing the space it uses as much as possible?  I believe that this falls under the category of <a href=\"http://en.wikipedia.org/wiki/List_of_knapsack_problems\">Knapsack problems</a>,  but this Wikipedia page doesn't appear to bring me closer to a real world solution.  Is this a NP-Hard problem?   </p>\n\n<p>In this problem we are trying to minimize the amount of area consumed (eg: Integral),  which includes vertical area.</p>\n\n<p><img src=\"http://i.stack.imgur.com/XqA0t.jpg\" alt=\"enter image description here\"></p>\n    ","a":"\n<h2>Simplifying</h2>\n\n<p>First I want to simplify the problem, to do that:</p>\n\n<ul>\n<li>I switch the axes and add them to each other, this results in x2 growth</li>\n<li>I assume it is parabola on a closed interval <code>[a, b], where a = 0</code> and for this example <code>b = 3</code>  </li>\n</ul>\n\n<p>Lets say you are given <code>b</code> (second part of interval) and <code>w</code> (width of a segment), then you can find total number of segments by <code>n=Floor[b/w]</code>. In this case there exists a trivial case to maximize Riemann sum and function to get i'th segment height is: <code>f(b-(b*i)/(n+1)))</code>. Actually it is an assumption and I'm not 100% sure.</p>\n\n<p>Max'ed example for <code>17</code> segments on closed interval <code>[0, 3]</code> for function <code>Sqrt[x]</code> real values:  </p>\n\n<p><img src=\"http://i.stack.imgur.com/sAEPL.gif\" alt=\"enter image description here\"></p>\n\n<p>And the segment <strong>heights</strong> function in this case is <code>Re[Sqrt[3-3*Range[1,17]/18]]</code>, and values are:</p>\n\n<ul>\n<li>Exact form: </li>\n</ul>\n\n<blockquote>\n  <p>{Sqrt[17/6], 2 Sqrt[2/3], Sqrt[5/2],\n  Sqrt[7/3], Sqrt[13/6], Sqrt[2],\n  Sqrt[11/6], Sqrt[5/3], Sqrt[3/2],\n  2/Sqrt[3], Sqrt[7/6], 1, Sqrt[5/6],\n  Sqrt[2/3], 1/Sqrt[2], 1/Sqrt[3],\n  1/Sqrt[6]}</p>\n</blockquote>\n\n<ul>\n<li>Approximated form: </li>\n</ul>\n\n<blockquote>\n  <p>{1.6832508230603465,\n  1.632993161855452, 1.5811388300841898, 1.5275252316519468, 1.4719601443879744, 1.4142135623730951, 1.35400640077266, 1.2909944487358056, 1.224744871391589, 1.1547005383792517, 1.0801234497346435, 1, 0.9128709291752769, 0.816496580927726, 0.7071067811865475, 0.5773502691896258, 0.4082482904638631}</p>\n</blockquote>\n\n<p>What you have archived is a <strong>Bin-Packing problem</strong>, with partially filled bin.</p>\n\n<h2>Finding b</h2>\n\n<p>If <code>b</code> is unknown or our task is to find smallest possible <code>b</code> under what all sticks form the initial bunch fit. Then we can limit at least <code>b</code> values to:</p>\n\n<ol>\n<li>lower limit : if sum of segment heights = sum of stick heights</li>\n<li>upper limit : <s>number of segments = number of sticks </s>  longest stick &lt; longest  segment height</li>\n</ol>\n\n<p>One of the simplest way to find <code>b</code> is to take a pivot at <code>(higher limit-lower limit)/2</code> find if solution exists. Then it becomes new higher or lower limit and you repeat the process until required precision is met. </p>\n\n<hr>\n\n<p>When you are looking for <code>b</code> you do not need exact result, but suboptimal and it would be much faster if you use efficient algorithm to find relatively close pivot point to actual <code>b</code>.</p>\n\n<p>For example:</p>\n\n<ul>\n<li>sort the stick by length: largest to smallest</li>\n<li>start 'putting largest items' into first bin thy fit</li>\n</ul>\n    "},{"t":"Where do I find a standard Trie based map implementation in Java?","l":"http://stackoverflow.com/questions/623892/where-do-i-find-a-standard-trie-based-map-implementation-in-java","q":"\n\n<p>I have a Java program that stores a lot of mappings from Strings to various objects. </p>\n\n<p>Right now, my options are either to rely on hashing (via HashMap) or on binary searches (via TreeMap). I am wondering if there is an efficient and standard trie-based map implementation in a popular and quality collections library?</p>\n\n<p>I've written my own in the past, but I'd rather go with something standard, if available.</p>\n\n<p>Quick clarification: While my question is general, in the current project I am dealing with a lot of data that is indexed by fully-qualified class name or method signature. Thus, there are many shared prefixes. </p>\n    ","a":"\n<p>You might want to look at the <a href=\"https://code.google.com/p/guava-libraries/issues/detail?id=10\">Trie implementation that Limewire is contributing</a> to the Google Guava.</p>\n    "},{"t":"How do you like your primary keys? [closed]","l":"http://stackoverflow.com/questions/404040/how-do-you-like-your-primary-keys","q":"\n\n<p>In a fairly animated discussion in my team I was made to think what most people like as primary keys. We had the following groups-</p>\n\n<ol>\n<li>Int/ BigInt which autoincrement are good enough primary keys.</li>\n<li>There should be at least 3 columns that make up the primary key.</li>\n<li>Id, GUID and human readable row identifiers all should be treated differently.</li>\n</ol>\n\n<p>Whats the best approach for PKs? It would be awesome if you could justify your opinion. Is there a better approach that the above? </p>\n\n<p>EDIT: Anyone has a simple sample/algorithm to generate human readable identifiers for rows that scales well?</p>\n    ","a":"\n<p>If you're going to be doing any syncing between databases with occasionally connected apps, then you should be using GUIDs for your primary keys.  It is kind of a pain for debugging, so apart from that case I tend to stick to ints that autoincrement.</p>\n\n<p>Autoincrement ints should be your default, and <strong>not</strong> using them should be justified.</p>\n    "},{"t":"Quick Sort Vs Merge Sort [duplicate]","l":"http://stackoverflow.com/questions/680541/quick-sort-vs-merge-sort","q":"\n\n<div class=\"question-status question-originals-of-duplicate\">\n    <p>This question already has an answer here:</p>\n    <ul>\n        <li>\n            <a href=\"/questions/70402/why-is-quicksort-better-than-mergesort\" dir=\"ltr\">Why is quicksort better than mergesort?</a>\n                <span class=\"question-originals-answer-count\">\n                    22 answers\n                </span>\n        </li>\n    </ul>\n</div>\n<p>Why might quick sort be better than merge sort ?</p>\n    ","a":"\n<p>See <a href=\"http://en.wikipedia.org/wiki/Quicksort\">Quicksort on wikipedia</a>:</p>\n\n<blockquote>\n  <p>Typically, quicksort is significantly\n  faster in practice than other Θ(nlogn)\n  algorithms, because its inner loop can\n  be efficiently implemented on most\n  architectures, and in most real-world\n  data, it is possible to make design\n  choices which minimize the probability\n  of requiring quadratic time.</p>\n</blockquote>\n\n<p>Note that the very low memory requirement is a big plus as well.</p>\n    "},{"t":"What distribution do you get from this broken random shuffle?","l":"http://stackoverflow.com/questions/5131341/what-distribution-do-you-get-from-this-broken-random-shuffle","q":"\n\n<p>The famous Fisher-Yates shuffle algorithm can be used to randomly permute an array A of length N:</p>\n\n<pre><code>For k = 1 to N\n    Pick a random integer j from k to N\n    Swap A[k] and A[j]\n</code></pre>\n\n<p>A common mistake that I've been told over and over again not to make is this:</p>\n\n<pre><code>For k = 1 to N\n    Pick a random integer j from 1 to N\n    Swap A[k] and A[j]\n</code></pre>\n\n<p>That is, instead of picking a random integer from k to N, you pick a random integer from 1 to N.</p>\n\n<p>What happens if you make this mistake?  I know that the resulting permutation isn't uniformly distributed, but I don't know what guarantees there are on what the resulting distribution will be.  In particular, does anyone have an expression for the probability distributions over the final positions of the elements?</p>\n\n<p>Thanks so much!</p>\n    ","a":"\n<p><strong>An Empirical Approach.</strong>  </p>\n\n<p>Let's implement the erroneous algorithm in Mathematica:  </p>\n\n<pre><code>p = 10; (* Range *)\ns = {}\nFor[l = 1, l &lt;= 30000, l++, (*Iterations*)\n   a = Range[p];\n   For[k = 1, k &lt;= p, k++, \n     i = RandomInteger[{1, p}];\n     temp = a[[k]];\n     a[[k]] = a[[i]];\n     a[[i]] = temp\n   ];\n   AppendTo[s, a];\n]  \n</code></pre>\n\n<p>Now get the number of times each integer is in each position:  </p>\n\n<pre><code>r = SortBy[#, #[[1]] &amp;] &amp; /@ Tally /@ Transpose[s]  \n</code></pre>\n\n<p>Let's take three positions in the resulting arrays and plot the frequency distribution for each integer in that position:  </p>\n\n<p>For position 1 the freq distribution is:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/NUWia.png\"><img src=\"http://i.stack.imgur.com/NUWia.png\" alt=\"enter image description here\"></a></p>\n\n<p>For position 5 (middle)  </p>\n\n<p><a href=\"http://i.stack.imgur.com/lbJ2e.png\"><img src=\"http://i.stack.imgur.com/lbJ2e.png\" alt=\"enter image description here\"></a></p>\n\n<p>And for position 10 (last):  </p>\n\n<p><a href=\"http://i.stack.imgur.com/7Ndie.png\"><img src=\"http://i.stack.imgur.com/7Ndie.png\" alt=\"enter image description here\"></a></p>\n\n<p>and here you have the distribution for all positions plotted together:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/EOVuI.png\"><img src=\"http://i.stack.imgur.com/EOVuI.png\" alt=\"enter image description here\"></a></p>\n\n<p>Here you have a better statistics over 8 positions:</p>\n\n<p><a href=\"http://i.stack.imgur.com/zQSsu.png\"><img src=\"http://i.stack.imgur.com/zQSsu.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>Some observations:</strong></p>\n\n<ul>\n<li>For all positions the probability of\n\"1\" is the same (1/n).</li>\n<li>The probability matrix is symmetrical\nwith respect to the big anti-diagonal</li>\n<li>So, the probability for any number in the last\nposition is also uniform (1/n)</li>\n</ul>\n\n<p>You may visualize those properties looking at the starting of all lines from the same point (first property) and the last horizontal line (third property). </p>\n\n<p>The second property can be seen from the following matrix representation example, where the rows are the positions, the columns are the occupant number, and the color represents the experimental probability:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/ba1eL.png\"><img src=\"http://i.stack.imgur.com/ba1eL.png\" alt=\"enter image description here\"></a></p>\n\n<p>For a 100x100 matrix:  </p>\n\n<p><a href=\"http://i.stack.imgur.com/khCPM.png\"><img src=\"http://i.stack.imgur.com/khCPM.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>Edit</strong>  </p>\n\n<p>Just for fun, I calculated the exact formula for the second diagonal element (the first is 1/n). The rest can be done, but it's a lot of work. </p>\n\n<pre><code>h[n_] := (n-1)/n^2 + (n-1)^(n-2) n^(-n)\n</code></pre>\n\n<p>Values verified from n=3 to 6 ( {8/27, 57/256, 564/3125, 7105/46656} )</p>\n\n<p><strong>Edit</strong></p>\n\n<p>Working out a little the general explicit calculation in @wnoise answer, we can get a little more info.  </p>\n\n<p>Replacing 1/n by p[n], so the calculations are hold unevaluated, we get for example for the first part of the matrix with n=7 (click to see a bigger image):  </p>\n\n<p><a href=\"http://i.stack.imgur.com/YmSJR.png\"><img src=\"http://i.stack.imgur.com/YmSJR.png\" alt=\"enter image description here\"></a></p>\n\n<p>Which, after comparing with results for other values of n, let us identify some known integer sequences in the matrix:  </p>\n\n<pre><code>{{  1/n,    1/n      , ...},\n {... .., A007318, ....},\n {... .., ... ..., ..},\n ... ....,\n {A129687, ... ... ... ... ... ... ..},\n {A131084, A028326 ... ... ... ... ..},\n {A028326, A131084 , A129687 ... ....}}\n</code></pre>\n\n<p>You may find those sequences (in some cases with different signs) in the wonderful <a href=\"http://oeis.org/\">http://oeis.org/</a>  </p>\n\n<p>Solving the general problem is more difficult, but I hope this is a start</p>\n    "},{"t":"Why there is no std::copy_if algorithm?","l":"http://stackoverflow.com/questions/1448817/why-there-is-no-stdcopy-if-algorithm","q":"\n\n<p>Is there any specific reason for not having std::copy_if algorithm in C++ ? I know I can use std::remove_copy_if to achieve the required behavior. I think it is coming in C++0x, but a simple copy_if which takes a range, a output iterator and a functor would have been nice. Was it just simply missed out or is there some other reason behind it? </p>\n    ","a":"\n<p>According to Stroustrup's \"The C++ Programming Language\" it was just an over-sight. </p>\n\n<p>(as a citation, the same question answered in boost mail-lists: <a href=\"http://lists.boost.org/Archives/boost/2001/01/8022.php\">copy_if</a>)</p>\n    "},{"t":"Converting a Uniform Distribution to a Normal Distribution","l":"http://stackoverflow.com/questions/75677/converting-a-uniform-distribution-to-a-normal-distribution","q":"\n\n<p>How can I convert a uniform distribution (as most random number generators produce, e.g. between 0.0 and 1.0) into a normal distribution? What if I want a mean and standard deviation of my choosing?</p>\n    ","a":"\n<p>The <a href=\"http://en.wikipedia.org/wiki/Ziggurat_algorithm\">Ziggurat algorithm</a> is pretty efficient for this, although the <a href=\"http://en.wikipedia.org/wiki/Box-Muller_transform\">Box-Muller transform</a> is easier to implement from scratch (and not crazy slow).</p>\n    "},{"t":"Problem: Bob's Sale","l":"http://stackoverflow.com/questions/4898511/problem-bobs-sale","q":"\n\n<p><em>Note: this is an abstract rewording of a real-life problem regarding ordering records in a SWF file. A solution will help me improve an open-source application.</em></p>\n\n<p>Bob has a store, and wants to do a sale. His store carries a number of products, and he has a certain integer quantity of units of each product in stock. He also has a number of shelf-mounted price labels (as many as the number of products), with the prices already printed on them. He can place any price label on any product (unitary price for one item for his entire stock of that product), however some products have an additional restriction - any such product may not be cheaper than a certain other product.</p>\n\n<p>You must find how to arrange the price labels, such that the total cost of all of Bob's wares is as low as possible. The total cost is the sum of each product's assigned price label multiplied by the quantity of that product in stock.</p>\n\n<hr>\n\n<p>Given:</p>\n\n<ul>\n<li>N – the number of products and price labels</li>\n<li>S<sub><em>i</em></sub>, 0≤<em>i</em>&lt;N – the quantity in stock of product with index <em>i</em> (integer)</li>\n<li>P<sub><em>j</em></sub>, 0≤<em>j</em>&lt;N – the price on price label with index <em>j</em> (integer)</li>\n<li>K – the number of additional constraint pairs</li>\n<li>A<sub><em>k</em></sub>, B<sub><em>k</em></sub>, 0≤<em>k</em>&lt;K – product indices for the additional constraint\n<ul>\n<li>Any product index may appear at most once in B. Thus, the graph formed by this adjacency list is actually a set of directed trees.</li>\n</ul></li>\n</ul>\n\n<p>The program must find:</p>\n\n<ul>\n<li>M<sub><em>i</em></sub>, 0≤<em>i</em>&lt;N – mapping from product index to price label index (P<sub>M<sub><em>i</em></sub></sub> is price of product <em>i</em>)</li>\n</ul>\n\n<p>To satisfy the conditions:</p>\n\n<ol>\n<li>P<sub>M<sub>A<sub><em>k</em></sub></sub></sub> ≤ P<sub>M<sub>B<sub><em>k</em></sub></sub></sub>, for 0≤<em>k</em>&lt;K</li>\n<li>Σ(S<sub><em>i</em></sub> × P<sub>M<sub><em>i</em></sub></sub>) for 0≤<em>i</em>&lt;N is minimal</li>\n</ol>\n\n<hr>\n\n<p>Note that if not for the first condition, the solution would be simply sorting labels by price and products by quantity, and matching both directly.</p>\n\n<p>Typical values for input will be N,K&lt;10000. In the real-life problem, there are only several distinct price tags (1,2,3,4).</p>\n\n<hr>\n\n<p>Here's one example of why most simple solutions (including topological sort) won't work:</p>\n\n<p>You have 10 items with the quantities 1 through 10, and 10 price labels with the prices $1 through $10. There is one condition: the item with the quantity 10 must not be cheaper than the item with the quantity 1.</p>\n\n<p>The optimal solution is:</p>\n\n<pre><code>Price, $   1  2  3  4  5  6  7  8  9 10\nQty        9  8  7  6  1 10  5  4  3  2\n</code></pre>\n\n<p>with a total cost of $249. If you place the 1,10 pair near either extreme, the total cost will be higher.</p>\n    ","a":"\n<p>The problem is NP-complete for the general case. This can be shown via a reduction of 3-partition (which is a still strong NP-complete version of bin packing).</p>\n\n<p>Let <em>w<sub>1</sub>, ..., w<sub>n</sub></em> be the weights of objects of the 3-partition instance, let <em>b</em> be the bin size, and <em>k = n/3</em> the number of bins that are allowed to be filled. Hence, there is a 3-partition if objects can be partitioned such that there are exactly 3 objects per bin.</p>\n\n<p>For the reduction, we set N=<em>kb</em> and each bin is represented by <em>b</em> price labels of the same price (think of P<sub><em>i</em></sub> increasing every <em>b</em>th label). Let <em>t<sub>i</sub></em>, 1≤<em>i</em>≤<em>k</em>, be the price of the labels corresponding to the <em>i</em>th bin.\nFor each <em>w<sub>i</sub></em> we have one product S<sub><em>j</em></sub> of quantity <em>w<sub>i</sub> + 1</em> (lets call this the root product of <em>w<sub>i</sub></em>) and another <em>w<sub>i</sub> - 1</em> products of quantity 1 which are required to be cheaper than S<sub><em>j</em></sub> (call these the leave products).</p>\n\n<p>For <em>t<sub>i</sub></em> = (2b + 1)<sup>i</sup>, 1≤<em>i</em>≤<em>k</em>, there is a 3-partition if and only if Bob can sell for <em>2b</em>Σ<sub>1≤<em>i</em>≤<em>k</em></sub> <em>t<sub>i</sub></em>:</p><ul>\n<li>If there is a solution for 3-partition, then all the <em>b</em> products corresponding to objects <em>w<sub>i</sub></em>, <em>w<sub>j</sub></em>, <em>w<sub>l</sub></em> that are assigned to the same bin can be labeled with the same price without violating the restrictions.\nThus, the solution has cost <em>2b</em>Σ<sub>1≤<em>i</em>≤<em>k</em></sub> <em>t<sub>i</sub></em> (since the total quantity of products with price <em>t<sub>i</sub></em> is <em>2b</em>).<br>\n</li><li>Consider an optimal solution of Bob's Sale.\nFirst observe that in any solution were more than 3 root products share the same price label, for each such root product that is \"too much\" there is a cheaper price tag which sticks on less than 3 root products. This is worse than any solution were there are exactly 3 root products per price label (if existent).<br>\nNow there can still be a solution of Bob's Sale with 3 root labels per price, but their leave products do not wear the same price labels (the bins sort of flow over).\nSay the most expensive price label tags a root product of <em>w<sub>i</sub></em> which has a cheaper tagged leave product. This implies that the 3 root labels <em>w<sub>i</sub></em>, <em>w<sub>j</sub></em>, <em>w<sub>l</sub></em> tagged with the most expensive price do not add up to <em>b</em>. Hence, the total cost of products tagged with this price is at least <em>2b+1</em>.<br>\nHence, such a solution has cost <em>t<sub>k</sub>(2b+1)</em> + some other assignment cost. Since the optimal cost for an existent 3-partition is <em>2b</em>Σ<sub>1≤<em>i</em>≤<em>k</em></sub> <em>t<sub>i</sub></em> , we have to show that the just considered case is worse. This is the case if <em>t<sub>k</sub> &gt; 2b</em> Σ<sub>1≤<em>i</em>≤<em>k-1</em></sub> <em>t<sub>i</sub></em> (note that it's <em>k-1</em> in the sum now). Setting <em>t<sub>i</sub></em> = (2b + 1)<sup>i</sup>, 1≤<em>i</em>≤<em>k</em>, this is the case. This also holds if not the most expensive price tag is the \"bad\" one, but any other.\n</li></ul><p></p>\n\n<p>So, this is the destructive part ;-) However, if the number of different price tags is a constant, you can use dynamic programming to solve it in polynomial time.</p>\n    "},{"t":"Maximum single-sell profit","l":"http://stackoverflow.com/questions/7086464/maximum-single-sell-profit","q":"\n\n<p>Suppose we are given an array of n integers representing stock prices on a single day.  We want to find a pair (buyDay, sellDay), with buyDay ≤ sellDay, such that if we bought the stock on buyDay and sold it on sellDay, we would maximize our profit.</p>\n\n<p>Clearly there is an O(n<sup>2</sup>) solution to the algorithm by trying out all possible (buyDay, sellDay) pairs and taking the best out of all of them.  However, is there a better algorithm, perhaps one that runs in O(n) time?</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p>I love this problem.  It's a classic interview question and depending on how you think about it, you'll end up getting better and better solutions.  It's certainly possible to do this in better than O(n<sup>2</sup>) time, and I've listed three different ways that you can think about the problem here.  Hopefully this answers your question!</p>\n\n<p>First, the divide-and-conquer solution.  Let's see if we can solve this by splitting the input in half, solving the problem in each subarray, then combining the two together.  Turns out we actually can do this, and can do so efficiently!  The intuition is as follows.  If we have a single day, the best option is to buy on that day and then sell it back on the same day for no profit.  Otherwise, split the array into two halves.  If we think about what the optimal answer might be, it must be in one of three places:</p>\n\n<ol>\n<li>The correct buy/sell pair occurs completely within the first half.</li>\n<li>The correct buy/sell pair occurs completely within the second half.</li>\n<li>The correct buy/sell pair occurs across both halves - we buy in the first half, then sell in the second half.</li>\n</ol>\n\n<p>We can get the values for (1) and (2) by recursively invoking our algorithm on the first and second halves.  For option (3), the way to make the highest profit would be to buy at the lowest point in the first half and sell in the greatest point in the second half.  We can find the minimum and maximum values in the two halves by just doing a simple linear scan over the input and finding the two values.  This then gives us an algorithm with the following recurrence:</p>\n\n<pre><code>T(1) &lt;= O(1)\nT(n) &lt;= 2T(n / 2) + O(n)\n</code></pre>\n\n<p>Using the <a href=\"http://en.wikipedia.org/wiki/Master_theorem\">Master Theorem</a> to solve the recurrence, we find that this runs in O(n lg n) time and will use O(lg n) space for the recursive calls.  We've just beaten the naive O(n<sup>2</sup>) solution!</p>\n\n<p>But wait!  We can do much better than this.  Notice that the only reason we have an O(n) term in our recurrence is that we had to scan the entire input trying to find the minimum and maximum values in each half.  Since we're already recursively exploring each half, perhaps we can do better by having the recursion also hand back the minimum and maximum values stored in each half!  In other words, our recursion hands back three things:</p>\n\n<ol>\n<li>The buy and sell times to maximize profit.</li>\n<li>The minimum value overall in the range.</li>\n<li>The maximum value overall in the range.</li>\n</ol>\n\n<p>These last two values can be computed recursively using a straightforward recursion that we can run at the same time as the recursion to compute (1):</p>\n\n<ol>\n<li>The max and min values of a single-element range are just that element.</li>\n<li>The max and min values of a multiple element range can be found by splitting the input in half, finding the max and min values of each half, then taking their respective max and min.</li>\n</ol>\n\n<p>If we use this approach, our recurrence relation is now</p>\n\n<pre><code>T(1) &lt;= O(1)\nT(n) &lt;= 2T(n / 2) + O(1)\n</code></pre>\n\n<p>Using the Master Theorem here gives us a runtime of O(n) with O(lg n) space, which is even better than our original solution!</p>\n\n<p>But wait a minute - we can do even better than this!  Let's think about solving this problem using dynamic programming.  The idea will be to think about the problem as follows.  Suppose that we knew the answer to the problem after looking at the first k elements.  Could we use our knowledge of the (k+1)st element, combined with our initial solution, to solve the problem for the first (k+1) elements?  If so, we could get a great algorithm going by solving the problem for the first element, then the first two, then the first three, etc. until we'd computed it for the first n elements.</p>\n\n<p>Let's think about how to do this.  If we have just one element, we already know that it has to be the best buy/sell pair.  Now suppose we know the best answer for the first k elements and look at the (k+1)st element.  Then the only way that this value can create a solution better than what we had for the first k elements is if the difference between the smallest of the first k elements and that new element is bigger than the biggest difference we've computed so far.  So suppose that as we're going across the elements, we keep track of two values - the minimum value we've seen so far, and the maximum profit we could make with just the first k elements.  Initially, the minimum value we've seen so far is the first element, and the maximum profit is zero.  When we see a new element, we first update our optimal profit by computing how much we'd make by buying at the lowest price seen so far and selling at the current price.  If this is better than the optimal value we've computed so far, then we update the optimal solution to be this new profit.  Next, we update the minimum element seen so far to be the minimum of the current smallest element and the new element.</p>\n\n<p>Since at each step we do only O(1) work and we're visiting each of the n elements exactly once, this takes O(n) time to complete!  Moreover, it only uses O(1) auxiliary storage.  This is as good as we've gotten so far!</p>\n\n<p>As an example, on your inputs, here's how this algorithm might run.  The numbers in-between each of the values of the array correspond to the values held by the algorithm at that point.  You wouldn't actually store all of these (it would take O(n) memory!), but it's helpful to see the algorithm evolve:</p>\n\n<pre><code>        5        10         4         6        7\nmin         5         5        4          4         4    \nbest      (5,5)     (5,10)   (5,10)     (5,10)    (5,10)\n</code></pre>\n\n<p>Answer: (5, 10)</p>\n\n<pre><code>        5        10         4         6        12\nmin         5         5        4          4         4    \nbest      (5,5)     (5,10)   (5,10)     (5,10)    (4,12)\n</code></pre>\n\n<p>Answer: (4, 12)</p>\n\n<pre><code>        1       2       3      4      5\nmin         1       1       1      1      1\nbest      (1,1)   (1,2)   (1,3)  (1,4)  (1,5)\n</code></pre>\n\n<p>Answer: (1, 5)</p>\n\n<p>Can we do better now?  Unfortunately, not in an asymptotic sense.  If we use less than O(n) time, we can't look at all the numbers on large inputs and thus can't guarantee that we won't miss the optimal answer (we could just \"hide\" it in the elements we didn't look at).  Plus, we can't use any less than O(1) space.  There might be some optimizations to the constant factors hidden in the big-O notation, but otherwise we can't expect to find any radically better options.</p>\n\n<p>Overall, this means that we have the following algorithms:</p>\n\n<ul>\n<li>Naive: O(n<sup>2</sup>) time, O(1) space.</li>\n<li>Divide-and-Conquer: O(n lg n) time, O(lg n) space.</li>\n<li>Optimized Divide-and-Conquer: O(n) time, O(lg n) space.</li>\n<li>Dynamic programming: O(n) time, O(1) space.</li>\n</ul>\n\n<p>Hope this helps!</p>\n\n<p><strong>EDIT</strong>: If you're interested, I've coded up <strong><a href=\"http://keithschwarz.com/interesting/code/?dir=single-sell-profit\">a Python version of these four algorithms</a></strong> so that you can play around with them and judge their relative performances.  Enjoy!</p>\n    "},{"t":"Nearest neighbors in high-dimensional data?","l":"http://stackoverflow.com/questions/5751114/nearest-neighbors-in-high-dimensional-data","q":"\n\n<p>I have asked a <a href=\"http://stackoverflow.com/questions/5684370/finding-k-nearest-neighbors-for-a-given-vector\">question</a> a few days back on how to find the nearest neighbors for a given vector. My vector is now 21 dimensions and before I proceed further, because I am not from the domain of Machine Learning nor Math, I am beginning to ask myself some fundamental questions:</p>\n\n<ul>\n<li>Is Euclidean distance a good metric for finding the nearest neighbors in the first place? If not, what are my options?</li>\n<li>In addition, how does one go about deciding the right threshold for determining the k-neighbors? Is there some analysis that can be done to figure this value out?</li>\n<li>Previously, I was suggested to use kd-Trees but the Wikipedia page clearly says that for high-dimensions, kd-Tree is almost equivalent to a brute-force search. In that case, what is the best way to find nearest-neighbors in a million point dataset efficiently?</li>\n</ul>\n\n<p>Can someone please clarify the some (or all) of the above questions?</p>\n    ","a":"\n<p>I currently study such problems -- classification, nearest neighbor searching -- for music information retrieval.</p>\n\n<p>You may be interested in <strong>Approximate Nearest Neighbor</strong> (<strong>ANN</strong>) algorithms. The idea is that you allow the algorithm to return sufficiently <em>near neighbors</em> (perhaps not the nearest neighbor); in doing so, you reduce complexity. You mentioned the <strong>kd-tree</strong>; that is one example. But as you said, <strong>kd-tree</strong> works poorly in high dimensions. In fact, <em>all</em> current indexing techniques (based on space partitioning) degrade to linear search for sufficiently high dimensions [1][2][3].</p>\n\n<p>Among <strong>ANN</strong> algorithms proposed recently, perhaps the most popular is <strong>Locality-Sensitive Hashing</strong> (<strong>LSH</strong>), which maps a set of points in a high-dimensional space into a set of bins, i.e., a hash table [1][3]. But unlike traditional hashes, a <em>locality-sensitive</em> hash places <em>nearby</em> points into the same bin.</p>\n\n<p><strong>LSH</strong> has some huge advantages. First, it is simple. You just compute the hash for all points in your database, then make a hash table from them. To query, just compute the hash of the query point, then retrieve all points in the same bin from the hash table.</p>\n\n<p>Second, there is a rigorous theory that supports its performance. It can be shown that the query time is <em>sublinear</em> in the size of the database, i.e., faster than linear search. How much faster depends upon how much approximation we can tolerate.</p>\n\n<p>Finally, <strong>LSH</strong> is compatible with any Lp norm for <code>0 &lt; p &lt;= 2</code>. Therefore, to answer your first question, you can use <strong>LSH</strong> with the Euclidean distance metric, or you can use it with the Manhattan (L1) distance metric. There are also variants for Hamming distance and cosine similarity.</p>\n\n<p>A decent overview was written by Malcolm Slaney and Michael Casey for IEEE Signal Processing Magazine in 2008 [4].</p>\n\n<p><strong>LSH</strong> has been applied seemingly everywhere. You may want to give it a try.</p>\n\n<hr>\n\n<p>[1] Datar, Indyk, Immorlica, Mirrokni, \"Locality-Sensitive Hashing Scheme Based on p-Stable Distributions,\" 2004.</p>\n\n<p>[2] Weber, Schek, Blott, \"A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces,\" 1998.</p>\n\n<p>[3] Gionis, Indyk, Motwani, \"Similarity search in high dimensions via hashing,\" 1999.</p>\n\n<p>[4] Slaney, Casey, \"Locality-sensitive hashing for finding nearest neighbors\", 2008.</p>\n    "},{"t":"What is the Difference between a Heuristic and an Algorithm?","l":"http://stackoverflow.com/questions/2334225/what-is-the-difference-between-a-heuristic-and-an-algorithm","q":"\n\n<p>What is the Difference between a Heuristic and an Algorithm?</p>\n    ","a":"\n<p>An algorithm is the description of an <strong>automated solution to a problem</strong>. What the algorithm does is precisely defined. The solution could or could not be the best possible one but you know from the start what kind of result you will get. You implement the <strong>algorithm</strong> using some programming language to get (a part of) a <strong>program</strong>.</p>\n\n<p>Now, some problems are hard and you may not be able to get an acceptable solution in an acceptable time. In such cases you often can get a not too bad solution much faster, by applying some arbitrary choices (educated guesses): that's a <strong>heuristic</strong>.</p>\n\n<p>A heuristic is still a kind of an algorithm, but one that will not explore all possible states of the problem, or will begin by exploring the most likely ones.</p>\n\n<p>Typical examples are from games. When writing a chess game program you could imagine trying every possible move at some depth level and applying some evaluation function to the board. A heuristic would exclude full branches that begin with obviously bad moves.</p>\n\n<p>In some cases you're not searching for the best solution, but for any solution fitting some constraint. A good heuristic would help to find a solution in a short time, but may also fail to find any if the only solutions are in the states it chose not to try.</p>\n    "},{"t":"Detecting patterns in waves","l":"http://stackoverflow.com/questions/2196124/detecting-patterns-in-waves","q":"\n\n<p>I'm trying to read a image from a electrocardiography and detect each one of the main waves in it (P wave, QRS complex and T wave). Now I can read the image and get a vector like (4.2; 4.4; 4.9; 4.7; ...) representative of the values in the electrocardiography, what is half of the problem. I need a algorithm that can walk through this vector and detect when each of this waves start and end. </p>\n\n<p>Here is a example of one of its graphs:</p>\n\n<p><img src=\"http://i.stack.imgur.com/U4t3t.jpg\" alt=\"alt text\"></p>\n\n<p>Would be easy if they always had the same size, but it's not like it works, or if I knew how many waves the ecg would have, but it can vary too. Does anyone have some ideas?</p>\n\n<p>Thanks!</p>\n\n<p><strong>Updating</strong></p>\n\n<p>Example of what I'm trying to achieve:</p>\n\n<p>Given the wave</p>\n\n<p><img src=\"http://i.stack.imgur.com/BHOaG.jpg\" alt=\"alt text\"></p>\n\n<p>I can extract the vector</p>\n\n<p>[0; 0; 20; 20; 20; 19; 18; 17; 17; 17; 17; 17; 16; 16; 16; 16; 16; 16; 16; 17; 17; 18; 19; 20; 21; 22; 23; 23; 23; 25; 25; 23; 22; 20; 19; 17; 16; 16; 14; 13; 14; 13; 13; 12; 12; 12; 12; 12; 11; 11; 10; 12; 16; 22; 31; 38; 45; 51; 47; 41; 33; 26; 21; 17; 17; 16; 16; 15; 16; 17; 17; 18; 18; 17; 18; 18; 18; 18; 18; 18; 18; 17; 17; 18; 19; 18; 18; 19; 19; 19; 19; 20; 20; 19; 20; 22; 24; 24; 25; 26; 27; 28; 29; 30; 31; 31; 31; 32; 32; 32; 31; 29; 28; 26; 24; 22; 20; 20; 19; 18; 18; 17; 17; 16; 16; 15; 15; 16; 15; 15; 15; 15; 15; 15; 15; 15; 15; 14; 15; 16; 16; 16; 16; 16; 16; 16; 16; 16; 15; 16; 15; 15; 15; 16; 16; 16; 16; 16; 16; 16; 16; 15; 16; 16; 16; 16; 16; 15; 15; 15; 15; 15; 16; 16; 17; 18; 18; 19; 19; 19; 20; 21; 22; 22; 22; 22; 21; 20; 18; 17; 17; 15; 15; 14; 14; 13; 13; 14; 13; 13; 13; 12; 12; 12; 12; 13; 18; 23; 30; 38; 47; 51; 44; 39; 31; 24; 18; 16; 15; 15; 15; 15; 15; 15; 16; 16; 16; 17; 16; 16; 17; 17; 16; 17; 17; 17; 17; 18; 18; 18; 18; 19; 19; 20; 20; 20; 20; 21; 22; 22; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 32; 33; 33; 33; 32; 30; 28; 26; 24; 23; 23; 22; 20; 19; 19; 18; 17; 17; 18; 17; 18; 18; 17; 18; 17; 18; 18; 17; 17; 17; 17; 16; 17; 17; 17; 18; 18; 17; 17; 18; 18; 18; 19; 18; 18; 17; 18; 18; 17; 17; 17; 17; 17; 18; 17; 17; 18; 17; 17; 17; 17; 17; 17; 17; 18; 17; 17; 18; 18; 18; 20; 20; 21; 21; 22; 23; 24; 23; 23; 21; 21; 20; 18; 18; 17; 16; 14; 13; 13; 13; 13; 13; 13; 13; 13; 13; 12; 12; 12; 16; 19; 28; 36; 47; 51; 46; 40; 32; 24; 20; 18; 16; 16; 16; 16; 15; 16; 16; 16; 17; 17; 17; 18; 17; 17; 18; 18; 18; 18; 19; 18; 18; 19; 20; 20; 20; 20; 20; 21; 21; 22; 22; 23; 25; 26; 27; 29; 29; 30; 31; 32; 33; 33; 33; 34; 35; 35; 35; 0; 0; 0; 0;]</p>\n\n<p>I would like to detect, for example</p>\n\n<p>P wave in [19 - 37]</p>\n\n<p>QRS complex in [51 - 64]</p>\n\n<p>etc...</p>\n    ","a":"\n<p>The first thing that <em>I</em> would do is <strong>see what is already out there</strong>. Indeed, this specific problem has already been heavily researched. Here is a brief overview of some really simple methods: <a href=\"http://www.physik.uni-freiburg.de/~severin/A_comparison_of_the_noise_sensitivity_of_nine_QRS_detection_Algorithms.pdf\" rel=\"nofollow\">link</a>. </p>\n\n<p>I must respond to another answer, as well. I do research in signal processing and music information retrieval. On the surface, this problem does appear similar to onset detection, but the problem context is not the same. This type of biological signal processing, i.e., detection of the P, QRS, and T phases, can exploit knowledge of <em>specific time-domain characteristics</em> of each of these waveforms. Onset detection in MIR doesn't, really. (Not reliably, at least.)</p>\n\n<p>One approach that would work well for QRS detection (but not necessarily for note onset detection) is dynamic time warping. When time-domain characteristics remain invariant, DTW can work remarkably well. Here is a short IEEE paper that uses DTW for this problem: <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.6782&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">link</a>.</p>\n\n<p>This is a nice IEEE magazine article that compares many methods: <a href=\"http://www.sm.luth.se/csee/courses/sms/046/2004/QRS_tutorial.pdf\" rel=\"nofollow\">link</a>. You'll see that many common signal processing models have been tried. Skim the paper, and try one that you understand at a basic level.</p>\n\n<p>EDIT: After browsing these articles, a wavelet-based approach seems most intuitive to me. DTW will work well, too, and there exist DTW modules out there, but the wavelet approach seems best to me. Someone else answered by exploiting derivatives of the signal. My first link examines methods from before 1990 that do that, but I suspect that they are not as robust as more modern methods.</p>\n\n<p>EDIT: I'll try to give a simple solution when I get the chance, but the reason <em>why</em> I think wavelets are suited here are because they are useful at parameterizing a wide variety of shapes regardless of <em>time or amplitude scaling</em>. In other words, if you have a signal with the same repeated temporal shape but at varying time scales and amplitudes, wavelet analysis can still recognize these shapes as being similar (roughly speaking). Also note that I am sort of lumping filter banks into this category. Similar things.</p>\n    "},{"t":"An algorithm to space out overlapping rectangles?","l":"http://stackoverflow.com/questions/3265986/an-algorithm-to-space-out-overlapping-rectangles","q":"\n\n<p>This problem actually deals with roll-overs, I'll just generalized below as such:</p>\n\n<p>I have a 2D view, and I have a number of rectangles within an area on the screen. How do I spread out those boxes such that they don't overlap each other, but only adjust them with minimal moving? </p>\n\n<p>The rectangles' positions are dynamic and dependent on user's input, so their positions could be anywhere. </p>\n\n<p>Attached<img src=\"http://i137.photobucket.com/albums/q239/extrakun/rectangles-auto-position.jpg\" alt=\"alt text\"> images show the problem and desired solution</p>\n\n<p>The real life problem deals with rollovers, actually.</p>\n\n<p>Answers to the questions in the comments</p>\n\n<ol>\n<li><p>Size of rectangles is not fixed, and is dependent on the length of the text in the rollover</p></li>\n<li><p>About screen size, right now I think it's better to assume that the size of the screen is enough for the rectangles. If there is too many rectangles and the algo produces no solution, then I just have to tweak the content.</p></li>\n<li><p>The requirement to 'move minimally' is more for asethetics than an absolute engineering requirement. One could space out two rectangles by adding a vast distance between them, but it won't look good as part of the GUI. The idea is to get the rollover/rectangle as close as to its source (which I will then connect to the source with a black line). So either 'moving just one for x' or 'moving both for half x' is fine.</p></li>\n</ol>\n    ","a":"\n<p>I was working a bit in this, as I also needed something similar, but I had delayed the algorithm development. You helped me to get some impulse :D  </p>\n\n<p>I also needed the source code, so here it is.  I worked it out in Mathematica, but as I haven't used heavily the functional features, I guess it'll be easy to translate to any procedural language.</p>\n\n<h2>A historic perspective</h2>\n\n<p>  \nFirst I decided to develop the algorithm for circles, because the intersection is easier to calculate. It just depends on the centers and radii.  \n</p>\n\n<p>I was able to use the Mathematica equation solver, and it performed nicely.  </p>\n\n<p>Just look:</p>\n\n<p><a href=\"http://i.stack.imgur.com/SiElU.png\"><img src=\"http://i.stack.imgur.com/SiElU.png\" alt=\"alt text\"></a></p>\n\n<p>It was easy. I just loaded the solver with the following problem:</p>\n\n<pre><code>For each circle\n Solve[\n  Find new coördinates for the circle\n  Minimizing the distance to the geometric center of the image\n  Taking in account that\n      Distance between centers &gt; R1+R2 *for all other circles\n      Move the circle in a line between its center and the \n                                         geometric center of the drawing\n   ]\n</code></pre>\n\n<p>As straightforward as that, and Mathematica did all the work.  </p>\n\n<p>I said \"Ha! it's easy, now let's go for the rectangles!\". But I was wrong ...</p>\n\n<h2>Rectangular Blues</h2>\n\n<p>The main problem with the rectangles is that querying the intersection is a nasty function. Something like:</p>\n\n<p><img src=\"http://i.stack.imgur.com/g8R2k.png\" width=\"420\" height=\"60\"></p>\n\n<p>\nSo, when I tried to feed up Mathematica with a lot of these conditions for the equation, it performed so badly that I decided to do something procedural.\n</p>\n\n<p>\nMy algorithm ended up as follows:  \n</p>\n\n<pre><code>Expand each rectangle size by a few points to get gaps in final configuration\nWhile There are intersections\n    sort list of rectangles by number of intersections\n    push most intersected rectangle on stack, and remove it from list\n// Now all remaining rectangles doesn't intersect each other\nWhile stack not empty\n    pop  rectangle from stack and re-insert it into list\n    find the geometric center G of the chart (each time!)\n    find the movement vector M (from G to rectangle center)\n    move the rectangle incrementally in the direction of M (both sides) \n                                                 until no intersections  \nShrink the rectangles to its original size\n</code></pre>\n\n<p>You may note that the \"smallest movement\" condition is not completely satisfied (only in one direction). But I found that moving the rectangles in any direction to satisfy it, sometimes ends up with a confusing map changing for the user.  </p>\n\n<p>As I am designing a user interface, I choose to move the rectangle a little further, but in a more predictable way. You can change the algorithm to inspect all angles and all radii surrounding its current position until an empty place is found, although it'll be much more demanding.  </p>\n\n<p>Anyway, these are examples of the results (before/ after):  </p>\n\n<p><a href=\"http://i.stack.imgur.com/tFpAc.png\"><img src=\"http://i.stack.imgur.com/tFpAc.png\" alt=\"alt text\"></a></p>\n\n<p>Edit&gt; More examples <a href=\"http://i.stack.imgur.com/Lz5F4.png\">here</a></p>\n\n<p>As you may see, the \"minimum movement\" is not satisfied, but the results are good enough.   </p>\n\n<p>I'll post the code here because I'm having some trouble with my SVN repository. I'll remove it when the problems are solved.  </p>\n\n<h2>Edit: </h2>\n\n<p>You may also use <a href=\"http://en.wikipedia.org/wiki/R-tree\">R-Trees</a> for finding rectangle intersections, but it seems an overkill for dealing with a small number of rectangles. And I haven't the algorithms already implemented. Perhaps someone else can point you to an existing implementation on your platform of choice. </p>\n\n<p>Warning! Code is a first approach .. not great quality yet, and surely has some bugs.  </p>\n\n<p>It's Mathematica.  </p>\n\n<pre><code>(*Define some functions first*)\n\nClear[\"Global`*\"];\nrn[x_] := RandomReal[{0, x}];\nrnR[x_] := RandomReal[{1, x}];\nrndCol[] := RGBColor[rn[1], rn[1], rn[1]];\n\nminX[l_, i_] := l[[i]][[1]][[1]]; (*just for easy reading*)\nmaxX[l_, i_] := l[[i]][[1]][[2]];\nminY[l_, i_] := l[[i]][[2]][[1]];\nmaxY[l_, i_] := l[[i]][[2]][[2]];\ncolor[l_, i_]:= l[[i]][[3]];\n\nintersectsQ[l_, i_, j_] := (* l list, (i,j) indexes, \n                              list={{x1,x2},{y1,y2}} *) \n                           (*A rect does intesect with itself*)\n          If[Max[minX[l, i], minX[l, j]] &lt; Min[maxX[l, i], maxX[l, j]] &amp;&amp;\n             Max[minY[l, i], minY[l, j]] &lt; Min[maxY[l, i], maxY[l, j]], \n                                                           True,False];\n\n(* Number of Intersects for a Rectangle *)\n(* With i as index*)\ncountIntersects[l_, i_] := \n          Count[Table[intersectsQ[l, i, j], {j, 1, Length[l]}], True]-1;\n\n(*And With r as rectangle *)\ncountIntersectsR[l_, r_] := (\n    Return[Count[Table[intersectsQ[Append[l, r], Length[l] + 1, j], \n                       {j, 1, Length[l] + 1}], True] - 2];)\n\n(* Get the maximum intersections for all rectangles*)\nfindMaxIntesections[l_] := Max[Table[countIntersects[l, i], \n                                       {i, 1, Length[l]}]];\n\n(* Get the rectangle center *)\nrectCenter[l_, i_] := {1/2 (maxX[l, i] + minX[l, i] ), \n                       1/2 (maxY[l, i] + minY[l, i] )};\n\n(* Get the Geom center of the whole figure (list), to move aesthetically*)\ngeometryCenter[l_] :=  (* returs {x,y} *)\n                      Mean[Table[rectCenter[l, i], {i, Length[l]}]]; \n\n(* Increment or decr. size of all rects by a bit (put/remove borders)*)\nchangeSize[l_, incr_] :=\n                 Table[{{minX[l, i] - incr, maxX[l, i] + incr},\n                        {minY[l, i] - incr, maxY[l, i] + incr},\n                        color[l, i]},\n                        {i, Length[l]}];\n\nsortListByIntersections[l_] := (* Order list by most intersecting Rects*)\n        Module[{a, b}, \n               a = MapIndexed[{countIntersectsR[l, #1], #2} &amp;, l];\n               b = SortBy[a, -#[[1]] &amp;];\n               Return[Table[l[[b[[i]][[2]][[1]]]], {i, Length[b]}]];\n        ];\n\n(* Utility Functions*)\ndeb[x_] := (Print[\"--------\"]; Print[x]; Print[\"---------\"];)(* for debug *)\ntableForPlot[l_] := (*for plotting*)\n                Table[{color[l, i], Rectangle[{minX[l, i], minY[l, i]},\n                {maxX[l, i], maxY[l, i]}]}, {i, Length[l]}];\n\ngenList[nonOverlap_, Overlap_] :=    (* Generate initial lists of rects*)\n      Module[{alist, blist, a, b}, \n          (alist = (* Generate non overlapping - Tabuloid *)\n                Table[{{Mod[i, 3], Mod[i, 3] + .8}, \n                       {Mod[i, 4], Mod[i, 4] + .8},  \n                       rndCol[]}, {i, nonOverlap}];\n           blist = (* Random overlapping *)\n                Table[{{a = rnR[3], a + rnR[2]}, {b = rnR[3], b + rnR[2]}, \n                      rndCol[]}, {Overlap}];\n           Return[Join[alist, blist] (* Join both *)];)\n      ];\n</code></pre>\n\n<p><strong>Main</strong></p>\n\n<pre><code>clist = genList[6, 4]; (* Generate a mix fixed &amp; random set *)\n\nincr = 0.05; (* may be some heuristics needed to determine best increment*)\n\nclist = changeSize[clist,incr]; (* expand rects so that borders does not \n                                                         touch each other*)\n\n(* Now remove all intercepting rectangles until no more intersections *)\n\nworkList = {}; (* the stack*)\n\nWhile[findMaxIntesections[clist] &gt; 0,          \n                                      (*Iterate until no intersections *)\n    clist    = sortListByIntersections[clist]; \n                                      (*Put the most intersected first*)\n    PrependTo[workList, First[clist]];         \n                                      (* Push workList with intersected *)\n    clist    = Delete[clist, 1];      (* and Drop it from clist *)\n];\n\n(* There are no intersections now, lets pop the stack*)\n\nWhile [workList != {},\n\n    PrependTo[clist, First[workList]];       \n                                 (*Push first element in front of clist*)\n    workList = Delete[workList, 1];          \n                                 (* and Drop it from worklist *)\n\n    toMoveIndex = 1;                        \n                                 (*Will move the most intersected Rect*)\n    g = geometryCenter[clist];               \n                                 (*so the geom. perception is preserved*)\n    vectorToMove = rectCenter[clist, toMoveIndex] - g;\n    If [Norm[vectorToMove] &lt; 0.01, vectorToMove = {1,1}]; (*just in case*)  \n    vectorToMove = vectorToMove/Norm[vectorToMove];      \n                                            (*to manage step size wisely*)\n\n    (*Now iterate finding minimum move first one way, then the other*)\n\n    i = 1; (*movement quantity*)\n\n    While[countIntersects[clist, toMoveIndex] != 0, \n                                           (*If the Rect still intersects*)\n                                           (*move it alternating ways (-1)^n *)\n\n      clist[[toMoveIndex]][[1]] += (-1)^i i incr vectorToMove[[1]];(*X coords*)\n      clist[[toMoveIndex]][[2]] += (-1)^i i incr vectorToMove[[2]];(*Y coords*)\n\n            i++;\n    ];\n];\nclist = changeSize[clist, -incr](* restore original sizes*);\n</code></pre>\n\n<p>HTH!</p>\n\n<h2>Edit: Multi-angle searching</h2>\n\n<p></p><p>\nI implemented a change in the algorithm allowing to search in all directions, but giving preference to the axis imposed by the geometric symmetry.<br>\nAt the expense of more cycles, this resulted in more compact final configurations, as you can see here below:</p>\n\n<p><a href=\"http://i.stack.imgur.com/r3vpU.png\"><img src=\"http://i.stack.imgur.com/r3vpU.png\" alt=\"enter image description here\"></a></p>\n\n<p>More samples <a href=\"http://i.stack.imgur.com/8gY7c.png\">here</a>.</p>\n\n<p>The pseudocode for the main loop changed to:</p>\n\n<pre><code>Expand each rectangle size by a few points to get gaps in final configuration\nWhile There are intersections\n    sort list of rectangles by number of intersections\n    push most intersected rectangle on stack, and remove it from list\n// Now all remaining rectangles doesn't intersect each other\nWhile stack not empty\n    find the geometric center G of the chart (each time!)\n    find the PREFERRED movement vector M (from G to rectangle center)\n    pop  rectangle from stack \n    With the rectangle\n         While there are intersections (list+rectangle)\n              For increasing movement modulus\n                 For increasing angle (0, Pi/4)\n                    rotate vector M expanding the angle alongside M\n                    (* angle, -angle, Pi + angle, Pi-angle*)\n                    re-position the rectangle accorging to M\n    Re-insert modified vector into list\nShrink the rectangles to its original size\n</code></pre>\n\n<p>I'm not including the source code for brevity, but just ask for it if you think you can use it. I think that, should you go this way, it's better to switch to R-trees (a lot of interval tests needed here)</p>\n    "},{"t":"Best way to randomize a string array with .NET","l":"http://stackoverflow.com/questions/108819/best-way-to-randomize-a-string-array-with-net","q":"\n\n<p>What is the best way to randomize an array of strings with .NET? My array contains about 500 strings and I'd like to create a new <code>Array</code> with the same strings but in a random order.</p>\n\n<p>Please include a C# example in your answer.</p>\n    ","a":"\n<p>If you're on .NET 3.5, you can use the following IEnumerable coolness (VB.NET, not C#, but the idea should be clear...):</p>\n\n<pre><code>Random rnd=new Random();\nstring[] MyRandomArray = MyArray.OrderBy(x =&gt; rnd.Next()).ToArray();    \n</code></pre>\n\n<p>Edit: OK and here's the corresponding VB.NET code:</p>\n\n<pre><code>Dim rnd As New System.Random\nDim MyRandomArray = MyArray.OrderBy(Function() rnd.Next)\n</code></pre>\n\n<p>Second edit, in response to remarks that System.Random \"isn't threadsafe\" and \"only suitable for toy apps\" due to returning a time-based sequence: as used in my example, Random() is perfectly thread-safe, unless you're allowing the routine in which you randomize the array to be re-entered, in which case you'll need something like <code>lock (MyRandomArray)</code> anyway in order not to corrupt your data, which will protect <code>rnd</code> as well. </p>\n\n<p>Also, it should be well-understood that System.Random as a source of entropy isn't very strong. As noted in the <a href=\"http://msdn.microsoft.com/en-us/library/system.random.aspx\" rel=\"nofollow\">MSDN documentation</a>, you should use something derived from <code>System.Security.Cryptography.RandomNumberGenerator</code> if you're doing anything security-related. For example:</p>\n\n<pre><code>using System.Security.Cryptography;\n</code></pre>\n\n<p>...</p>\n\n<pre><code>RNGCryptoServiceProvider rnd = new RNGCryptoServiceProvider();\nstring[] MyRandomArray = MyArray.OrderBy(x =&gt; GetNextInt32(rnd)).ToArray();\n</code></pre>\n\n<p>...</p>\n\n<pre><code>static int GetNextInt32(RNGCryptoServiceProvider rnd)\n    {\n        byte[] randomInt = new byte[4];\n        rnd.GetBytes(randomInt);\n        return Convert.ToInt32(randomInt[0]);\n    }\n</code></pre>\n    "},{"t":"Algorithm for creating a school timetable","l":"http://stackoverflow.com/questions/2177836/algorithm-for-creating-a-school-timetable","q":"\n\n<p>I've been wondering if there are known solutions for algorithm of creating a school timetable. Basically, it's about  optimizing \"hour-dispersion\" (both in teachers and classes case) for given class-subject-teacher associations. We can assume that we have sets of classes, lesson subjects and teachers associated with each other at the input and that timetable should fit between 8AM and 4PM. </p>\n\n<p>I guess that there is probably no accurate algorithm for that, but maybe someone knows a good approximation or hints for developing it. </p>\n    ","a":"\n<p>This problem is <strong>NP-Complete</strong>!<br>\nIn a nutshell one needs to explore all possible combinations  to find the list of acceptable solutions.  Because of the variations in the circumstances in which the problem appears at various schools (for example: Are there constraints with regards to classrooms?, Are some of the classes split in sub-groups some of the time?, Is this a weekly schedule? etc.) there isn't a well known problem class which corresponds to all the scheduling problems.  Maybe, the <a href=\"http://en.wikipedia.org/wiki/Knapsack_problem\"><strong>Knapsack problem</strong></a> has many elements of similarity with these problems at large.</p>\n\n<p>A confirmation that this is both a hard problem and one for which people perennially seek a solution, is to check this (long) <a href=\"http://directory.google.com/Top/Computers/Software/Educational/Administration_and_School_Management/Scheduling_Utilities/\"><strong>list of (mostly commercial) software scheduling tools</strong></a></p>\n\n<p>Because of the big number of variables involved, the biggest source of which are, typically, the faculty member's desires ;-)..., it is typically <strong>impractical to consider enumerating all possible combinations</strong>.  Instead we need to choose an approach which visits a subset of the problem/solution spaces.<br>\n- <strong>Genetic Algorithms</strong>, cited in another answer is (or, IMHO, <em>seems</em>) well equipped to perform this kind of semi-guided search (The problem being to find a good evaluation function for the candidates to be kept for the next generation)<br>\n- <a href=\"http://en.wikipedia.org/wiki/Graph_rewriting\"><strong>Graph Rewriting</strong></a> approaches are also of use with this type of combinatorial optimization problems.</p>\n\n<p>Rather than focusing on particular implementations of an automatic schedule generator program, I'd like to suggest <strong>a few strategies which can be applied, <em>at the level of the definition of the problem</em></strong>.<br>\nThe general rationale is that in most real world scheduling problems, some compromises will be required, not all constraints, expressed and implied: will be satisfied fully.  Therefore we help ourselves by:</p>\n\n<ul>\n<li>Defining and ranking all known constraints</li>\n<li>Reducing the problem space, by manually, providing a set of <em>additional</em> constraints.<br>This may seem counter-intuitive but for example by providing an initial, partially filled schedule (say roughly 30% of the time-slots), in a way that fully satisfies all constraints, and by considering this partial schedule immutable, we significantly reduce the time/space needed to produce candidate solutions.<br>   Another way additional constraints help is for example \"artificially\" adding a constraint which prevent teaching some subjects on some days of the week (if this is a weekly schedule...); this type of constraints results in reducing the problem/solution spaces, without, typically, excluding a significant number of good candidates.</li>\n<li>Ensuring that some of the constraints of the problem can be quickly computed.  This is often associated with the choice of data model used to represent the problem; the idea is to be able to quickly opt-for (or prune-out) some of the options.</li>\n<li>Redefining the problem and allowing some of the constraints to be broken, a few times, (typically towards the end nodes of the graph).  The idea here is to either remove <em>some</em> of constraints for filling-in the last few slots in the schedule, or to have the automatic schedule generator program stop shy of completing the whole schedule, instead providing us with a list of a dozen or so plausible candidates.  A human is often in a better position to complete the puzzle, as indicated, possibly breaking a few of the contraints, using information which is not typically shared with the automated logic  (eg \"No mathematics in the afternoon\" rule can be broken on occasion for the \"advanced math and physics\" class;  or \"It is better to break one of Mr Jones requirements than one of Ms Smith ... ;-) )</li>\n</ul>\n\n<p>In proof-reading this answer , I realize it is quite shy of providing a definite response, but it none the less full of practical suggestions.  I hope this help, with what is, after all, a \"hard problem\".</p>\n    "},{"t":"rounding to an arbitrary number of significant digits","l":"http://stackoverflow.com/questions/202302/rounding-to-an-arbitrary-number-of-significant-digits","q":"\n\n<p>How can you round <em>any</em> number (not just integers &gt; 0) to N significant digits?</p>\n\n<p>For example, if I want to round to 3 significant digits, I'm looking for a formula that could take:</p>\n\n<p>1,239,451 and return 1,240,000</p>\n\n<p>12.1257 and return 12.1</p>\n\n<p>.0681 and return .0681</p>\n\n<p>5 and return 5</p>\n\n<p>Naturally the algorithm should not be hard-coded to only handle N of 3, although that would be a start.</p>\n    ","a":"\n<p>Here's the same code in Java without the 12.100000000000001 bug other answers have</p>\n\n<p>I also removed repeated code, changed <code>power</code> to a type integer to prevent floating issues when <code>n - d</code> is done, and made the long intermediate more clear</p>\n\n<p>The bug was caused by multiplying a large number with a small number.  Instead I divide two numbers of similar size.</p>\n\n<p><strong>EDIT</strong><br>\nFixed more bugs.  Added check for 0 as it would result in NaN.  Made the function actually work with negative numbers (The original code doesn't handle negative numbers because a log of a negative number is a complex number)</p>\n\n<pre><code>public static double roundToSignificantFigures(double num, int n) {\n    if(num == 0) {\n        return 0;\n    }\n\n    final double d = Math.ceil(Math.log10(num &lt; 0 ? -num: num));\n    final int power = n - (int) d;\n\n    final double magnitude = Math.pow(10, power);\n    final long shifted = Math.round(num*magnitude);\n    return shifted/magnitude;\n}\n</code></pre>\n    "},{"t":"Negative weights using Dijkstra's Algorithm","l":"http://stackoverflow.com/questions/6799172/negative-weights-using-dijkstras-algorithm","q":"\n\n<p>I am trying to understand why Dijkstra's algorithm will not work with negative weights. Reading an example on <a href=\"http://www.ics.uci.edu/~eppstein/161/960208.html\">Shortest Paths</a>, I am trying to figure out the following scenario:</p>\n\n<pre><code>    2\nA-------B\n \\     /\n3 \\   / -2\n   \\ /\n    C\n</code></pre>\n\n<p>From the website:</p>\n\n<blockquote>\n  <p>Assuming the edges are all directed from left to right, If we start\n  with A, Dijkstra's algorithm will choose the edge (A,x) minimizing\n  d(A,A)+length(edge), namely (A,B). It then sets d(A,B)=2 and chooses\n  another edge (y,C) minimizing d(A,y)+d(y,C); the only choice is (A,C)\n  and it sets d(A,C)=3. But it never finds the shortest path from A to\n  B, via C, with total length 1.</p>\n</blockquote>\n\n<p>I can not understand why using the following implementation of Dijkstra, d[B] will not be updated to <code>1</code> (When the algorithm reaches vertex C, it will run a relax on B, see that the d[B] equals to <code>2</code>, and therefore update its value to <code>1</code>).</p>\n\n<pre><code>Dijkstra(G, w, s)  {\n   Initialize-Single-Source(G, s)\n   S ← Ø\n   Q ← V[G]//priority queue by d[v]\n   while Q ≠ Ø do\n      u ← Extract-Min(Q)\n      S ← S U {u}\n      for each vertex v in Adj[u] do\n         Relax(u, v)\n}\n\nInitialize-Single-Source(G, s) {\n   for each vertex v  V(G)\n      d[v] ← ∞\n      π[v] ← NIL\n   d[s] ← 0\n}\n\nRelax(u, v) {\n   //update only if we found a strictly shortest path\n   if d[v] &gt; d[u] + w(u,v) \n      d[v] ← d[u] + w(u,v)\n      π[v] ← u\n      Update(Q, v)\n}\n</code></pre>\n\n<p>Thanks,</p>\n\n<p>Meir</p>\n    ","a":"\n<p>The algorithm you have suggested will indeed find the shortest path in this graph, but not all graphs in general.  For example, consider this graph:</p>\n\n<p><img src=\"http://i.stack.imgur.com/rmowk.png\" alt=\"Figure of graph\"></p>\n\n<p>Assume the edges are directed from left to right as in your example,</p>\n\n<p>Your algorithm will work as follows:</p>\n\n<ol>\n<li>First, you set <code>d(A)</code> to <code>zero</code> and the other distances to <code>infinity</code>. </li>\n<li>You then expand out node <code>A</code>, setting <code>d(B)</code> to <code>1</code>, <code>d(C)</code> to <code>zero</code>, and <code>d(D)</code> to <code>99</code>.</li>\n<li>Next, you expand out <code>C</code>, with no net changes.</li>\n<li>You then expand out <code>B</code>, which has no effect. </li>\n<li>Finally, you expand <code>D</code>, which changes <code>d(B)</code> to <code>-201</code>.</li>\n</ol>\n\n<p>Notice that at the end of this, though, that <code>d(C)</code> is still <code>0</code>, <strong>even though the shortest path to <code>C</code> has length <code>-200</code>.</strong>  Your algorithm thus fails to accurately compute distances in some cases.  Moreover, even if you were to store back pointers saying how to get from each node to the start node <code>A</code>, you'd end taking the wrong path back from <code>C</code> to <code>A</code>.</p>\n    "},{"t":"Obtaining a powerset of a set in Java","l":"http://stackoverflow.com/questions/1670862/obtaining-a-powerset-of-a-set-in-java","q":"\n\n<p>The powerset of <code>{1, 2, 3}</code> is:</p>\n\n<p><code>{{}, {2}, {3}, {2, 3}, {1, 2}, {1, 3}, {1, 2, 3}, {1}}</code></p>\n\n<p>Let's say I have a <code>Set</code> in Java:</p>\n\n<pre><code>Set&lt;Integer&gt; mySet = new HashSet&lt;Integer&gt;();\nmySet.add(1);\nmySet.add(2);\nmySet.add(3);\nSet&lt;Set&lt;Integer&gt;&gt; powerSet = getPowerset(mySet);\n</code></pre>\n\n<p>How do I write the function getPowerset, with the best possible order of complexity?\n(I think it might be O(2^n).)</p>\n    ","a":"\n<p>Yes, it is <code>O(2^n)</code> indeed, since you need to generate, well, <code>2^n</code> possible combinations. Here's a working implementation, using generics and sets:</p>\n\n<pre><code>public static &lt;T&gt; Set&lt;Set&lt;T&gt;&gt; powerSet(Set&lt;T&gt; originalSet) {\n    Set&lt;Set&lt;T&gt;&gt; sets = new HashSet&lt;Set&lt;T&gt;&gt;();\n    if (originalSet.isEmpty()) {\n    \tsets.add(new HashSet&lt;T&gt;());\n    \treturn sets;\n    }\n    List&lt;T&gt; list = new ArrayList&lt;T&gt;(originalSet);\n    T head = list.get(0);\n    Set&lt;T&gt; rest = new HashSet&lt;T&gt;(list.subList(1, list.size())); \n    for (Set&lt;T&gt; set : powerSet(rest)) {\n    \tSet&lt;T&gt; newSet = new HashSet&lt;T&gt;();\n    \tnewSet.add(head);\n    \tnewSet.addAll(set);\n    \tsets.add(newSet);\n    \tsets.add(set);\n    }\t\t\n    return sets;\n}\n</code></pre>\n\n<p>And a test, given your example input:</p>\n\n<pre><code> Set&lt;Integer&gt; mySet = new HashSet&lt;Integer&gt;();\n mySet.add(1);\n mySet.add(2);\n mySet.add(3);\n for (Set&lt;Integer&gt; s : SetUtils.powerSet(mySet)) {\n     System.out.println(s);\n }\n</code></pre>\n    "},{"t":"Performing Breadth First Search recursively","l":"http://stackoverflow.com/questions/2549541/performing-breadth-first-search-recursively","q":"\n\n<p>Let's say you wanted to implement a breadth-first search of a binary tree <em>recursively</em>. How would you go about it?</p>\n\n<p>Is it possible using only the call-stack as auxiliary storage?</p>\n    ","a":"\n<p>(I'm assuming that this is just some kind of thought exercise, or even a trick homework/interview question, but I suppose I could imagine some bizarre scenario where you're not allowed any heap space for some reason [some really bad custom memory manager? some bizarre runtime/OS issues?] while you still have access to the stack...)</p>\n\n<p>Breadth-first traversal traditionally uses a queue, not a stack.  The nature of a queue and a stack are pretty much opposite, so trying to use the call stack (which is a stack, hence the name) as the auxiliary storage (a queue) is pretty much doomed to failure, unless you're doing something stupidly ridiculous with the call stack that you shouldn't be.</p>\n\n<p>On the same token, the nature of any non-tail recursion you try to implement is essentially adding a stack to the algorithm.  This makes it no longer breadth first search on a binary tree, and thus the run-time and whatnot for traditional BFS no longer completely apply.  Of course, you can always trivially turn any loop into a recursive call, but that's not any sort of meaningful recursion.</p>\n\n<p>However, there are ways, as demonstrated by others, to implement something that follows the semantics of BFS at some cost.  If the cost of comparison is expensive but node traversal is cheap, then as <a href=\"http://stackoverflow.com/questions/2549541/performing-breadth-first-search-recursively/2549825#2549825\">@Simon Buchan</a> did, you can simply run an iterative depth-first search, only processing the leaves.  This would mean no growing queue stored in the heap, just a local depth variable, and stacks being built up over and over on the call stack as the tree is traversed over and over again.  And as <a href=\"http://stackoverflow.com/questions/2549541/performing-breadth-first-search-recursively/2549744#2549744\">@Patrick</a> noted, a binary tree backed by an array is typically stored in breadth-first traversal order anyway, so a breadth-first search on that would be trivial, also without needing an auxiliary queue.</p>\n    "},{"t":"Algorithm for N-way merge","l":"http://stackoverflow.com/questions/5055909/algorithm-for-n-way-merge","q":"\n\n<p>A 2-way merge is widely studied as a part of Mergesort algorithm.\nBut I am interested to find out the best way one can perform an N-way merge?</p>\n\n<p>Lets say, I have <code>N</code> files which have sorted 1 million integers each.\nI have to merge them into 1 single file which will have those 100 million sorted integers.</p>\n\n<p>Please keep in mind that use case for this problem is actually external sorting which is disk based. Therefore, in real scenarios there would be memory limitation as well. So a naive approach of merging 2 files at a time (99 times) won't work. Lets say we have only a small sliding window of memory available for each array.</p>\n\n<p>I am not sure if there is already a standardized solution to this N-way merge. <em>(Googling didn't tell me much)</em>.</p>\n\n<p>But if you know if a good n-way merge algorithm, please post algo/link.</p>\n\n<p><strong>Time complexity:</strong> If we greatly increase the number of files (<code>N</code>) to be merged, how would that affect the time complexity of your algorithm?</p>\n\n<p>Thanks for your answers.</p>\n\n<p><em>I haven't been asked this anywhere, but I felt this could be an interesting interview question. Therefore tagged.</em></p>\n    ","a":"\n<p>How about the following idea:</p>\n\n<ol>\n<li>Create a priority queue<br><br></li>\n<li>Iterate through each file <em>f</em>\n<ol>\n<li>enqueue the pair <em>(nextNumberIn(f), f)</em> using the first value as priority key<br><br></li>\n</ol></li>\n<li>While queue not empty\n<ol>\n<li>dequeue head <em>(m, f)</em> of queue</li>\n<li>output <em>m</em></li>\n<li>if <em>f</em> not depleted\n<ol>\n<li>enqueue <em>(nextNumberIn(f), f)</em></li>\n</ol></li>\n</ol></li>\n</ol>\n\n<p>Since adding elements to a priority queue can be done in logarithmic time, item 2 is <em>O(N × log N)</em>. Since (almost all) iterations of the while loop adds an element, the whole while-loop is be <em>O(M × log N)</em> where <em>M</em> is the total number of numbers to sort.</p>\n\n<p>Assuming all files have a non-empty sequence of numbers, we have <em>M &gt; N</em> and thus the whole algorithm should be <em>O(M × log N)</em>.</p>\n    "},{"t":"How does code completion work?","l":"http://stackoverflow.com/questions/1220099/how-does-code-completion-work","q":"\n\n<p>Lots of editors and IDEs have code completion. Some of them are very \"intelligent\" others are not really. I am interested in the more intelligent type. For example I have seen IDEs that only offer a function if it is a) available in the current scope b) its return value is valid. (For example after \"5 + foo[tab]\" it only offers functions that return something that can be added to an integer or variable names of the correct type.) I have also seen that they place the more often used or longest option ahead of the list.</p>\n\n<p>I realize you need to parse the code. But usually while editing the current code is invalid there are syntax errors in it. How do you parse something when it is incomplete and contains errors?</p>\n\n<p>There is also a time constraint. The completion is useless if it takes seconds to come up with a list. Sometimes the completion algorithm deals with thousands of classes.</p>\n\n<p>What are the good algorithms and data structures for this?</p>\n    ","a":"\n<p>The IntelliSense engine in my UnrealScript language service product is complicated, but I'll give as best an overview here as I can. The C# language service in VS2008 SP1 is my performance goal (for good reason). It's not there yet, but it's fast/accurate enough that I can safely offer suggestions after a single character is typed, without waiting for ctrl+space or the user typing a <code>.</code> (dot). The more information people [working on language services] get about this subject, the better end-user experience I get should I ever use their products. There are a number of products I've had the unfortunate experience of working with that didn't pay such close attention to details, and as a result I was fighting with the IDE more than I was coding.</p>\n\n<p>In my language service, it's laid out like the following:</p>\n\n<ol>\n<li>Get the expression at the cursor. This walks from the beginning of the <em>member access expression</em> to the end of the identifier the cursor is over. The member access expression is generally in the form <code>aa.bb.cc</code>, but can also contain method calls as in <code>aa.bb(3+2).cc</code>.</li>\n<li>Get the <em>context</em> surrounding the cursor. This is very tricky, because it doesn't always follow the same rules as the compiler (long story), but for here assume it does. Generally this means get the cached information about the method/class the cursor is within.</li>\n<li>Say the context object implements <code>IDeclarationProvider</code>, where you can call <code>GetDeclarations()</code> to get an <code>IEnumerable&lt;IDeclaration&gt;</code> of all items visible in the scope. In my case, this list contains the locals/parameters (if in a method), members (fields and methods, static only unless in an instance method, and no private members of base types), globals (types and constants for the language I'm working on), and keywords. In this list will be an item with the name <code>aa</code>. As a first step in evaluating the expression in #1, we select the item from the context enumeration with the name <code>aa</code>, giving us an <code>IDeclaration</code> for the next step.</li>\n<li>Next, I apply the operator to the <code>IDeclaration</code> representing <code>aa</code> to get another <code>IEnumerable&lt;IDeclaration&gt;</code> containing the \"members\" (in some sense) of <code>aa</code>. Since the <code>.</code> operator is different from the <code>-&gt;</code> operator, I call <code>declaration.GetMembers(\".\")</code> and expect the <code>IDeclaration</code> object to correctly apply the listed operator.</li>\n<li>This continues until I hit <code>cc</code>, where the declaration list <em>may or may not</em> contain an object with the name <code>cc</code>. As I'm sure you're aware, if multiple items begin with <code>cc</code>, they should appear as well. I solve this by taking the final enumeration and passing it through <a href=\"http://blog.280z28.org/archives/2008/10/20/\">my documented algorithm</a> to provide the user with the most helpful information possible.</li>\n</ol>\n\n<p><strong>Here are some additional notes for the IntelliSense backend:</strong></p>\n\n<ul>\n<li>I make extensive use of LINQ's lazy evaluation mechanisms in implementing <code>GetMembers</code>. Each object in my cache is able to provide a functor that evaluates to its members, so performing complicated actions with the tree is near trivial.</li>\n<li>Instead of each object keeping a <code>List&lt;IDeclaration&gt;</code> of its members, I keep a <code>List&lt;Name&gt;</code>, where <code>Name</code> is a struct containing the hash of a specially-formatted string describing the member. There's an enormous cache that maps names to objects. This way, when I re-parse a file, I can remove all items declared in the file from the cache and repopulate it with the updated members. Due to the way the functors are configured, all expressions immediately evaluate to the new items.</li>\n</ul>\n\n<p><strong>IntelliSense \"frontend\"</strong></p>\n\n<p>As the user types, the file is syntactically <em>incorrect</em> more often than it is correct. As such, I don't want to haphazardly remove sections of the cache when the user types. I have a large number of special-case rules in place to handle incremental updates as quickly as possible. The incremental cache is only kept local to an open file and helps make ensure the user doesn't realize that their typing is causing the backend cache to hold incorrect line/column information for things like each method in the file.</p>\n\n<ul>\n<li>One redeeming factor is my parser is <em>fast</em>. It can handle a full cache update of a 20000 line source file in 150ms while operating self-contained on a low priority background thread. Whenever this parser completes a pass on an open file successfully (syntactically), the current state of the file is moved into the global cache.</li>\n<li>If the file is not syntactically correct, I use an <a href=\"http://www.antlr.org/wiki/display/ANTLR3/ANTLR+3+Wiki+Home\">ANTLR filter parser (sorry about the link - most info is on the mailing list or gathered from reading the source)</a> to reparse the file looking for:\n<ul>\n<li>Variable/field declarations.</li>\n<li>The signature for class/struct definitions.</li>\n<li>The signature for method definitions.</li>\n</ul></li>\n<li>In the local cache, class/struct/method definitions begin at the signature and end when the brace nesting level goes back to even. Methods can also end if another method declaration is reached (no nesting methods).</li>\n<li>In the local cache, variables/fields are linked to the immediately preceding <em>unclosed</em> element. See the brief code snippet below for an example of why this is important.</li>\n<li>Also, as the user types, I keep a remap table marking the added/removed character ranges. This is used for:\n<ul>\n<li>Making sure I can identify the correct context of the cursor, since a method can/does move in the file between full parses.</li>\n<li>Making sure Go To Declaration/Definition/Reference locates items correctly in open files.</li>\n</ul></li>\n</ul>\n\n<p>Code snippet for the previous section:</p>\n\n<pre><code>class A\n{\n    int x; // linked to A\n\n    void foo() // linked to A\n    {\n        int local; // linked to foo()\n\n    // foo() ends here because bar() is starting\n    void bar() // linked to A\n    {\n        int local2; // linked to bar()\n    }\n\n    int y; // linked again to A\n</code></pre>\n\n<p>I figured I'd add a list of the IntelliSense features I've implemented with this layout. <a href=\"http://wiki.pixelminegames.com/index.php?title=Tools:nFringe:Features\">Pictures of each are located here.</a></p>\n\n<ul>\n<li>Auto-complete</li>\n<li>Tool tips</li>\n<li>Method Tips</li>\n<li>Class View</li>\n<li>Code Definition Window</li>\n<li>Call Browser (VS 2010 finally adds this to C#)</li>\n<li>Semantically correct Find All References</li>\n</ul>\n    "},{"t":"LogLog and HyperLogLog algorithms for counting of large cardinalities","l":"http://stackoverflow.com/questions/5990713/loglog-and-hyperloglog-algorithms-for-counting-of-large-cardinalities","q":"\n\n<p>Where can I find a valid implementation of <a href=\"http://algo.inria.fr/flajolet/Publications/DuFl03.pdf\">LogLog algorithm</a>? Have tried to implement it by myself but my draft implementation yields strange results.</p>\n\n<p><a href=\"http://jsfiddle.net/LCDLL/1/\">Here</a> it is:</p>\n\n<pre><code>function LogLog(max_error, max_count)\n{\n    function log2(x)\n    {\n         return Math.log(x) / Math.LN2;\n    }\n\n    var m = 1.30 / max_error;\n    var k = Math.ceil(log2(m * m));\n    m = Math.pow(2, k);\n\n    var k_comp = 32 - k;\n\n    var l = log2(log2(max_count / m));\n    if (isNaN(l)) l = 1; else l = Math.ceil(l);\n    var l_mask = ((1 &lt;&lt; l) - 1) &gt;&gt;&gt; 0;\n\n    var M = [];\n    for (var i = 0; i &lt; m; ++i) M[i] = 0;\n\n    function count(hash)\n    {\n          if (hash !== undefined)\n          {\n                var j = hash &gt;&gt;&gt; k_comp;\n\n                var rank = 0;\n                for (var i = 0; i &lt; k_comp; ++i)\n                {\n                     if ((hash &gt;&gt;&gt; i) &amp; 1)\n                     {\n                          rank = i + 1;\n                          break;\n                     }\n                }\n\n                M[j] = Math.max(M[j], rank &amp; l_mask);\n          }\n          else\n          {\n                var c = 0;\n                for (var i = 0; i &lt; m; ++i) c += M[i];\n                return 0.79402 * m * Math.pow(2, c / m);\n          }\n    }\n\n    return {count: count};\n}\n\nfunction fnv1a(text)\n{\n     var hash = 2166136261;\n     for (var i = 0; i &lt; text.length; ++i)\n     {\n          hash ^= text.charCodeAt(i);\n          hash += (hash &lt;&lt; 1) + (hash &lt;&lt; 4) + (hash &lt;&lt; 7) +\n            (hash &lt;&lt; 8) + (hash &lt;&lt; 24);\n     }\n    return hash &gt;&gt;&gt; 0;\n}\n\nvar words = ['aardvark', 'abyssinian', ... ,'zoology']; // about 2 300 words\n\nvar log_log = LogLog(0.01, 100000);\nfor (var i = 0; i &lt; words.length; ++i) log_log.count(fnv1a(words[i]));\nalert(log_log.count());\n</code></pre>\n\n<p>For unknown reason implementation is very sensitive to <code>max_error</code> parameter, it is the main factor that determines the magnitude of the result. I'm sure, there is some stupid mistake :)</p>\n\n<p><strong>UPDATE:</strong> This problem is solved in the <a href=\"http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf\">newer version</a> of algorithm. I will post its implementation later.</p>\n    ","a":"\n<p><a href=\"http://jsfiddle.net/mM6bY/4/\">Here</a> it is the updated version of the algorithm based on the <a href=\"http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf\">newer paper</a>:</p>\n\n<pre><code>var pow_2_32 = 0xFFFFFFFF + 1;\n\nfunction HyperLogLog(std_error)\n{\n     function log2(x)\n     {\n          return Math.log(x) / Math.LN2;\n     }\n\n     function rank(hash, max)\n     {\n          var r = 1;\n          while ((hash &amp; 1) == 0 &amp;&amp; r &lt;= max) { ++r; hash &gt;&gt;&gt;= 1; }\n          return r;\n     }\n\n     var m = 1.04 / std_error;\n     var k = Math.ceil(log2(m * m)), k_comp = 32 - k;\n     m = Math.pow(2, k);\n\n     var alpha_m = m == 16 ? 0.673\n          : m == 32 ? 0.697\n          : m == 64 ? 0.709\n          : 0.7213 / (1 + 1.079 / m);\n\n     var M = []; for (var i = 0; i &lt; m; ++i) M[i] = 0;\n\n     function count(hash)\n     {\n          if (hash !== undefined)\n          {\n                var j = hash &gt;&gt;&gt; k_comp;\n                M[j] = Math.max(M[j], rank(hash, k_comp));\n          }\n          else\n          {\n                var c = 0.0;\n                for (var i = 0; i &lt; m; ++i) c += 1 / Math.pow(2, M[i]);\n                var E = alpha_m * m * m / c;\n\n                // -- make corrections\n\n                if (E &lt;= 5/2 * m)\n                {\n                     var V = 0;\n                     for (var i = 0; i &lt; m; ++i) if (M[i] == 0) ++V;\n                     if (V &gt; 0) E = m * Math.log(m / V);\n                }\n                else if (E &gt; 1/30 * pow_2_32)\n                     E = -pow_2_32 * Math.log(1 - E / pow_2_32);\n\n                // --\n\n                return E;\n          }\n    }\n\n    return {count: count};\n}\n\nfunction fnv1a(text)\n{\n     var hash = 2166136261;\n     for (var i = 0; i &lt; text.length; ++i)\n     {\n          hash ^= text.charCodeAt(i);\n          hash += (hash &lt;&lt; 1) + (hash &lt;&lt; 4) + (hash &lt;&lt; 7) +\n            (hash &lt;&lt; 8) + (hash &lt;&lt; 24);\n     }\n     return hash &gt;&gt;&gt; 0;\n}\n\nvar words = ['aardvark', 'abyssinian', ..., 'zoology']; // 2336 words\n\nvar seed = Math.floor(Math.random() * pow_2_32); // make more fun\n\nvar log_log = HyperLogLog(0.065);\nfor (var i = 0; i &lt; words.length; ++i) log_log.count(fnv1a(words[i]) ^ seed);\nvar count = log_log.count();\nalert(count + ', error ' +\n    (count - words.length) / (words.length / 100.0) + '%');\n</code></pre>\n    "},{"t":"Sorting algorithms for data of known statistical distribution?","l":"http://stackoverflow.com/questions/6166546/sorting-algorithms-for-data-of-known-statistical-distribution","q":"\n\n<p>It just occurred to me, if you know something about the distribution (in the statistical sense) of the data to sort, the performance of a sorting algorithm might benefit if you take that information into account.</p>\n\n<p>So my question is, are there any sorting algorithms that take into account that kind of information? How good are they?</p>\n\n<p>Edit : an example to clarify: if you know the distribution of your data to be Gaussian, you could estimate mean and average on the fly as you process the data. This would give you an estimate of the final position of each number, which you could use to place them close to their final position.</p>\n\n<p>Edit #2: I'm pretty surprised the answer isn't a wiki link to a thourough page discussing this issue. Isn't this a very common case (the Gaussian case, for example)?</p>\n\n<p>Edit #3: I'm adding a bounty to this question, because I'm looking for definite answers with sources, not speculation. Something like \"in the case of gaussian distributed data, XYZ algorithm is the fastest on average, as was proved by Smith et al. [1]\". However any additional information is welcome.</p>\n\n<p><strong>Note</strong>: I will award the bounty to the highest-voted answer. Vote wisely!</p>\n    ","a":"\n<p>If the data you are sorting has a known distribution, I would use a <strong><a href=\"http://en.wikipedia.org/wiki/Bucket_sort\">Bucket Sort</a></strong> algorithm.   You could add some extra logic to it so that you calculated the size and/or positions of the various buckets based upon properties of the distribution (ex: for Gaussian, you might have a bucket every (sigma/k) away from the mean, where sigma is the standard deviation of the distribution).</p>\n\n<p>By having a known distribution and modifying the standard Bucket Sort algorithm in this way, you would probably get the <strong><a href=\"http://xlinux.nist.gov/dads//HTML/histogramSort.html\">Histogram Sort</a></strong> algorithm or something close to it.  Of course, your algorithm would be computationally faster than the the Histogram Sort algorithm because there would probably not be a need to do the first pass (described in the link) since you already know the distribution.</p>\n\n<p><strong>Edit:</strong> given your new criteria of your question, (though my previous answer concerning Histogram Sort links to the respectable NIST and contains performance information), here is a peer review journal article from the International Conference on Parallel Processing:</p>\n\n<p><a href=\"http://www.cs.rochester.edu/~cding/Documents/Publications/icpp04.pdf\">Adaptive Data Partition for Sorting Using Probability Distribution</a></p>\n\n<p>The authors claim this algorithm has better performance (up to 30% better) than the popular Quick-Sort Algorithm.</p>\n    "},{"t":"A* Algorithm for very large graphs, any thoughts on caching shortcuts?","l":"http://stackoverflow.com/questions/29656932/a-algorithm-for-very-large-graphs-any-thoughts-on-caching-shortcuts","q":"\n\n<p>I'm writing a courier/logistics simulation on OpenStreetMap maps and have realised that the basic A* algorithm as pictured below is not going to be fast enough for large maps (like Greater London).</p>\n\n<p><img src=\"http://i.stack.imgur.com/GqBBr.jpg\" alt=\"http://i.imgur.com/u2tVpML.jpg\"></p>\n\n<p>The green nodes correspond to ones that were put in the open set/priority queue and due to the huge number (the whole map is something like 1-2 million), it takes 5 seconds or so to find the route pictured. Unfortunately 100ms per route is about my absolute limit.</p>\n\n<p>Currently, the nodes are stored in both an adjacency list and also a spatial 100x100 2D array.</p>\n\n<p>I'm looking for methods where I can trade off preprocessing time, space and if needed optimality of the route, for faster queries. The straight-line Haversine formula for the heuristic cost is the most expensive function according to the profiler - I have optimised my basic A* as much as I can.</p>\n\n<p>For example, I was thinking if I chose an arbitrary node X from each quadrant of the 2D array and run A* between each, I can store the routes to disk for subsequent simulations. When querying, I can run A* search only in the quadrants, to get between the precomputed route and the X.</p>\n\n<p>Is there a more refined version of what I've described above or perhaps a different method I should pursue. Many thanks!</p>\n\n<p>For the record, here are some benchmark results for arbitrarily weighting the heuristic cost and computing the path between 10 pairs of randomly picked nodes:</p>\n\n<pre><code>Weight // AvgDist% // Time (ms)\n1       1       1461.2\n1.05    1       1327.2\n1.1     1       900.7\n1.2     1.019658848     196.4\n1.3     1.027619169     53.6\n1.4     1.044714394     33.6\n1.5     1.063963413     25.5\n1.6     1.071694171     24.1\n1.7     1.084093229     24.3\n1.8     1.092208509     22\n1.9     1.109188175     22.5\n2       1.122856792     18.2\n2.2     1.131574742     16.9\n2.4     1.139104895     15.4\n2.6     1.140021962     16\n2.8     1.14088128      15.5\n3       1.156303676     16\n4       1.20256964      13\n5       1.19610861      12.9\n</code></pre>\n\n<p>Surprisingly increasing the coefficient to 1.1 almost halved the execution time whilst keeping the same route.</p>\n    ","a":"\n<p>You should be able to make it much faster by trading off optimality. See <a href=\"http://en.wikipedia.org/wiki/A*_search_algorithm\">Admissibility and optimality</a> on wikipedia.</p>\n\n<p>The idea is to use an <code>epsilon</code> value which will lead to a solution no worse than <code>1 + epsilon</code> times the optimal path, but which will cause fewer nodes to be considered by the algorithm. Note that this does not mean that the returned solution will always be <code>1 + epsilon</code> times the optimal path. This is just the worst case. I don't know exactly how it would behave in practice for your problem, but I think it is worth exploring.</p>\n\n<p>You are given a number of algorithms that rely on this idea on wikipedia. I believe this is your best bet to improve the algorithm and that it has the potential to run in your time limit while still returning good paths.</p>\n\n<p>Since your algorithm does deal with millions of nodes in 5 seconds, I assume you also use binary heaps for the implementation, correct? If you implemented them manually, make sure they are implemented as simple arrays and that they are binary heaps.</p>\n    "},{"t":"Good algorithm and data structure for looking up words with missing letters?","l":"http://stackoverflow.com/questions/1953080/good-algorithm-and-data-structure-for-looking-up-words-with-missing-letters","q":"\n\n<p>so I need to write an efficient algorithm for looking up words with missing letters in a dictionary and I want the set of possible words.</p>\n\n<p>For example, if I have th??e, I might get back these, those, theme, there.etc.</p>\n\n<p>I was wondering if anyone can suggest some data structures or algorithm I should use.</p>\n\n<p>Thanks!</p>\n\n<p>EDIT: A Trie is too space-inefficient and would make it too slow. Any other ideas modifications?</p>\n\n<p>UPDATE: There will be up to TWO question marks and when two question marks do occur, they will occur in sequence.</p>\n\n<p>Currently I am using 3 hash tables for when it is an exact match, 1 question mark, and 2 question marks.\nGiven a dictionary I hash all the possible words. For example, if I have the word WORD. I hash WORD, ?ORD, W?RD, WO?D, WOR?, ??RD, W??D, WO??. into the dictionary. Then I use a link list to link the collisions together. So say hash(W?RD) = hash(STR?NG) = 17. hashtab(17) will point to WORD and WORD points to STRING because it is a linked list.</p>\n\n<p>The timing on average lookup of one word is about 2e-6s. I am looking to do better, preferably on the order of 1e-9.</p>\n\n<p>EDIT: I haven't looked at this problem again but it took 0.5 seconds for 3m entries insertions and it took 4 seconds for 3m entries lookup.</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p>I believe in this case it is best to just use a flat file where each word stands in one line. With this you can conveniently use the power of a regular expression search, which is highly optimized and will probably beat any data structure you can devise yourself for this problem.</p>\n\n<h2>Solution #1: Using Regex</h2>\n\n<p>This is working Ruby code for this problem:</p>\n\n<pre><code>def query(str, data)    \n  r = Regexp.new(\"^#{str.gsub(\"?\", \".\")}$\")\n  idx = 0\n  begin\n    idx = data.index(r, idx)\n    if idx\n      yield data[idx, str.size]\n      idx += str.size + 1\n    end\n  end while idx\nend\n\nstart_time = Time.now\nquery(\"?r?te\", File.read(\"wordlist.txt\")) do |w|\n  puts w\nend\nputs Time.now - start_time\n</code></pre>\n\n<p>The file <code>wordlist.txt</code> contains 45425 words (downloadable <a href=\"http://martin.ankerl.com/2008/08/09/two-word-anagram-finder-algorithm/\">here</a>). The program's output for query <code>?r?te</code> is:</p>\n\n<pre><code>brute\ncrate\nCrete\ngrate\nirate\nprate\nwrite\nwrote\n0.013689\n</code></pre>\n\n<p>So it takes just 37 milliseconds to both read the whole file and to find all matches in it. And it scales very well for all kinds of query patterns, even where a Trie is very slow:</p>\n\n<p>query <code>????????????????e</code></p>\n\n<pre><code>counterproductive\nindistinguishable\nmicroarchitecture\nmicroprogrammable\n0.018681\n</code></pre>\n\n<p>query <code>?h?a?r?c?l?</code></p>\n\n<pre><code>theatricals\n0.013608\n</code></pre>\n\n<p>This looks fast enough for me.</p>\n\n<h2>Solution #2: Regex with Prepared Data</h2>\n\n<p>If you want to go even faster, you can split the wordlist into strings that contain words of equal lengths and just search the correct one based on your query length. Replace the last 5 lines with this code:</p>\n\n<pre><code>def query_split(str, data)\n  query(str, data[str.length]) do |w|\n    yield w\n  end\nend\n\n# prepare data    \ndata = Hash.new(\"\")\nFile.read(\"wordlist.txt\").each_line do |w|\n  data[w.length-1] += w\nend\n\n# use prepared data for query\nstart_time = Time.now\nquery_split(\"?r?te\", data) do |w|\n  puts w\nend\nputs Time.now - start_time\n</code></pre>\n\n<p>Building the data structure takes now about 0.4 second, but all queries are about 10 times faster (depending on the number of words with that length):</p>\n\n<ul>\n<li><code>?r?te</code> 0.001112 sec</li>\n<li><code>?h?a?r?c?l?</code> 0.000852 sec</li>\n<li><code>????????????????e</code> 0.000169 sec</li>\n</ul>\n\n<h2>Solution #3: One Big Hashtable (Updated Requirements)</h2>\n\n<p>Since you have changed your requirements, you can easily expand on your idea to use just one big hashtable that contains all precalculated results. But instead of working around collisions yourself you could rely on the performance of a properly implemented hashtable.</p>\n\n<p>Here I create one big hashtable, where each possible query maps to a list of its results:</p>\n\n<pre><code>def create_big_hash(data)\n  h = Hash.new do |h,k|\n    h[k] = Array.new\n  end    \n  data.each_line do |l|\n    w = l.strip\n    # add all words with one ?\n    w.length.times do |i|\n      q = String.new(w)\n      q[i] = \"?\"\n      h[q].push w\n    end\n    # add all words with two ??\n    (w.length-1).times do |i|\n      q = String.new(w)      \n      q[i, 2] = \"??\"\n      h[q].push w\n    end\n  end\n  h\nend\n\n# prepare data    \nt = Time.new\nh = create_big_hash(File.read(\"wordlist.txt\"))\nputs \"#{Time.new - t} sec preparing data\\n#{h.size} entries in big hash\"\n\n# use prepared data for query\nt = Time.new\nh[\"?ood\"].each do |w|\n  puts w\nend\nputs (Time.new - t)\n</code></pre>\n\n<p>Output is</p>\n\n<pre><code>4.960255 sec preparing data\n616745 entries in big hash\nfood\ngood\nhood\nmood\nwood\n2.0e-05\n</code></pre>\n\n<p>The query performance is O(1), it is just a lookup in the hashtable. The time 2.0e-05 is probably below the timer's precision. When running it 1000 times, I get an average of 1.958e-6 seconds per query. To get it faster, I would switch to C++ and use the <a href=\"http://goog-sparsehash.sourceforge.net/\">Google Sparse Hash</a> which is extremely memory efficient, and fast.</p>\n\n<h2>Solution #4: Get Really Serious</h2>\n\n<p>All above solutions work and should be good enough for many use cases. If you really want to get serious and have lots of spare time on your hands, read some good papers:</p>\n\n<ul>\n<li><a href=\"http://scholar.google.at/scholar?cluster=9144225622336489050&amp;hl=en&amp;as_sdt=2000\">Tries for Approximate String Matching</a> - If well implemented, tries can have very compact memory requirements (50% less space than the dictionary itself), and are very fast.</li>\n<li><a href=\"http://scholar.google.at/scholar?cluster=7784984624741923177&amp;hl=en&amp;as_sdt=2000\">Agrep - A Fast Approximate Pattern-Matching Tool</a> - Agrep is based on a new efficient and flexible algorithm for approximate string matching.</li>\n<li><a href=\"http://scholar.google.at/scholar?hl=en&amp;q=approximate+string+matching+dictionary&amp;btnG=Search&amp;as_sdt=2000&amp;as_ylo=&amp;as_vis=0\">Google Scholar search for approximate string matching</a> - More than enough to read on this topic.</li>\n</ul>\n    "},{"t":"When is each sorting algorithm used?","l":"http://stackoverflow.com/questions/1933759/when-is-each-sorting-algorithm-used","q":"\n\n<p>What are the use cases when a particular sorting algorithm is preferred - <code>merge sort</code> vs <code>quick sort</code> vs <code>heap sort</code> vs <code>introsort</code>, etc?<br></p>\n\n<p>Is there a recommended guide in using them based on the size, type of data strucutre, available memory and cache, and CPU performance.</p>\n    ","a":"\n<p>First, a definition, since it's pretty important:  A <strong>stable sort</strong> is one that's guaranteed not to reorder elements with identical keys.</p>\n\n<p>Recommendations:</p>\n\n<p><strong>Quick sort:</strong>  When you don't need a stable sort and average case performance matters more than worst case performance.  A quick sort is O(N log N) on average, O(N^2) in the worst case.  A good implementation uses O(log N) auxiliary storage in the form of stack space for recursion.</p>\n\n<p><strong>Merge sort:</strong>  When you need a stable, O(N log N) sort, this is about your only option.  The only downsides to it are that it uses O(N) auxiliary space and has a slightly larger constant than a quick sort.  There are some in-place merge sorts, but AFAIK they are all either not stable or worse than O(N log N).  Even the O(N log N) in place sorts have so much larger a constant than the plain old merge sort that they're more theoretical curiosities than useful algorithms.</p>\n\n<p><strong>Heap sort:</strong>  When you don't need a stable sort and you care more about worst case performance than average case performance.  It's guaranteed to be O(N log N), and uses O(1) auxiliary space, meaning that you won't unexpectedly run out of heap or stack space on very large inputs.  </p>\n\n<p><strong>Introsort:</strong>  This is a quick sort that switches to a heap sort after a certain recursion depth to get around quick sort's O(N^2) worst case.  It's almost always better than a plain old quick sort, since you get the average case of a quick sort, with guaranteed O(N log N) performance.  Probably the only reason to use a heap sort instead of this is in severely memory constrained systems where O(log N) stack space is practically significant.</p>\n\n<p><strong>Insertion sort</strong>:  When N is guaranteed to be small, including as the base case of a quick sort or merge sort.  While this is O(N^2), it has a very small constant and is a stable sort.</p>\n\n<p><strong>Bubble sort, selection sort</strong>:  When you're doing something quick and dirty and for some reason you can't just use the standard library's sorting algorithm.  The only advantage these have over insertion sort is being slightly easier to implement.</p>\n\n<p></p><hr><p></p>\n\n<p><strong>Non-comparison sorts:</strong>  Under some fairly limited conditions it's possible to break the O(N log N) barrier and sort in O(N).  Here are some cases where that's worth a try:</p>\n\n<p><strong>Counting sort:</strong>  When you are sorting integers with a limited range.</p>\n\n<p><strong>Radix sort:</strong>  When log(N) is significantly larger than K, where K is the number of radix digits.</p>\n\n<p><strong>Bucket sort:</strong>  When you can guarantee that your input is approximately uniformly distributed.</p>\n    "},{"t":"Difference between Divide and Conquer Algo and Dynamic Programming","l":"http://stackoverflow.com/questions/13538459/difference-between-divide-and-conquer-algo-and-dynamic-programming","q":"\n\n<p>What is the difference between <code>Divide and Conquer Algorithms</code> and <code>Dynamic Programming Algorithms</code> ? How are the two terms different ? I do not understand the difference between them.</p>\n\n<p>Please take a simple example to explain any difference between the two and on what ground they seem to be similar.</p>\n    ","a":"\n<p><strong>Divide and Conquer</strong></p>\n\n<p>Divide and Conquer works by dividing the problem into sub-problems, conquer each sub-problem recursively and combine these solutions.</p>\n\n<p><strong>Dynamic Programming</strong></p>\n\n<p>Dynamic Programming is a technique for solving problems with overlapping subproblems. Each sub-problem is solved only once and the result of each sub-problem is stored in a table ( generally implemented as an array or a hash table) for future references. These sub-solutions may be used to obtain the original solution and the technique of storing the sub-problem solutions is known as memoization.</p>\n\n<p>You may think of <code>DP = recursion + re-use</code></p>\n\n<p>A classic example to understand the difference would be to see both these approaches towards obtaining the nth fibonacci number. Check this <a href=\"http://courses.csail.mit.edu/6.006/spring08/notes/lecture19.pdf\">material</a> from MIT.</p>\n\n<hr>\n\n<p><strong>EDIT</strong></p>\n\n<hr>\n\n<p><em>Divide and Conquer approach</em>\n<img src=\"http://i.stack.imgur.com/QBJIj.png\" alt=\"Divide and Conquer approach\"></p>\n\n<p><em>Dynamic Programming Approach</em>\n<img src=\"http://i.stack.imgur.com/rFqdb.png\" alt=\"enter image description here\"></p>\n    "},{"t":"Faster algorithm to find unique element between two arrays?","l":"http://stackoverflow.com/questions/19203868/faster-algorithm-to-find-unique-element-between-two-arrays","q":"\n\n<p><strong>EDIT</strong>: For anyone new to this question, I have posted an answer clarifying what was going on. The accepted answer is the one I feel best answers my question as originally posted, but for further details please see my answer.</p>\n\n<p><strong>NOTE</strong>: This problem was originally pseudocode and used lists. I have adapted it to Java and arrays. So while I'd love to see any solutions that use Java-specific tricks (or tricks in any language for that matter!), just remember that the original problem is language-independent.</p>\n\n<h2>The Problem</h2>\n\n<p>Let's say that there are two unsorted integer arrays <code>a</code> and <code>b</code>, with element repetition allowed. They are identical (with respect to contained elements) <em>except</em> one of the arrays has an extra element. As an example:</p>\n\n<pre><code>int[] a = {6, 5, 6, 3, 4, 2};\nint[] b = {5, 7, 6, 6, 2, 3, 4};\n</code></pre>\n\n<p>Design an algorithm that takes as input these two arrays and outputs the single unique integer (in the above case, 7).</p>\n\n<h2>The Solution (So Far)</h2>\n\n<p>I came up with this:</p>\n\n<pre><code>public static int getUniqueElement(int[] a, int[] b) {\n    int ret = 0;\n    for (int i = 0; i &lt; a.length; i++) {\n        ret ^= a[i];\n    }\n    for (int i = 0; i &lt; b.length; i++) {\n        ret ^= b[i];\n    }\n    return ret;\n}\n</code></pre>\n\n<p>The \"official\" solution presented in class:</p>\n\n<pre><code>public static int getUniqueElement(int[] a, int[] b) {\n    int ret = 0;\n    for (int i = 0; i &lt; a.length; i++) {\n        ret += a[i];\n    }\n    for (int i = 0; i &lt; b.length; i++) {\n        ret -= b[i];\n    }\n    return Math.abs(ret);\n}\n</code></pre>\n\n<p>So, both are conceptually doing the same thing. And given that <code>a</code> is of length m and <code>b</code> is of length n, then both solutions have running time of O(m + n).</p>\n\n<h2>The Question</h2>\n\n<p>I later got to talking with my teacher and he hinted that there was an even <em>faster</em> way of doing it. Honestly I don't see how; to find out whether an element <em>is</em> unique it seems you'd have to at least look at every element. At that's at least O(m + n)...right?</p>\n\n<p>So is there a faster way? And if so, what is it?</p>\n    ","a":"\n<p>This is probably the fastest you can do it in Java using HotLick's suggestion in the comments. It makes the assumption that <code>b.length == a.length + 1</code> so b is the larger array with the extra \"unique\" element.</p>\n\n<pre><code>public static int getUniqueElement(int[] a, int[] b) {\n    int ret = 0;\n    int i;\n    for (i = 0; i &lt; a.length; i++) {\n        ret = ret ^ a[i] ^ b[i];\n    }\n    return ret ^ b[i];\n}\n</code></pre>\n\n<p>Even if the assumption cannot be made, you can easily expand it to include the case where either a or b can be the larger array with the unique element. It's still O(m+n) though and only loop/assignment overhead is reduced.</p>\n\n<h1>Edit:</h1>\n\n<p>Due to details of language implementation, this is still (surprisingly) the fastest way to do it in CPython.</p>\n\n<pre><code>def getUniqueElement1(A, B):\n    ret = 0\n    for a in A: ret = ret ^ a\n    for b in B: ret = ret ^ b\n    return ret\n</code></pre>\n\n<p>I have tested this with the <code>timeit</code> module and found some interesting results. It turns out that the longhand <code>ret = ret ^ a</code> is indeed faster in Python than the shorthand <code>ret ^= a</code>. Also iterating over the elements of a loop is much much faster than iterating over the indexes and then making subscript operations in Python. That is why this code is much faster than my previous method where I tried to copy Java.</p>\n\n<p>I guess the moral of the story is that there is no correct answer because the question is bogus anyways. As the OP noted in another answer below, it turns out you can't really go any faster than O(m+n) on this and his teacher was just pulling his leg. Thus the problem reduces to finding the fastest way to iterate over all elements in the two arrays and accumulating the XOR of all of them. And this means it's entirely dependent on language implementation, and you have to do some testing and playing around to get the true \"fastest\" solution in whatever implementation you are using, because the overall algorithm will not change.</p>\n    "},{"t":"Algorithm: efficient way to remove duplicate integers from an array","l":"http://stackoverflow.com/questions/1532819/algorithm-efficient-way-to-remove-duplicate-integers-from-an-array","q":"\n\n<p>I got this problem from an interview with Microsoft.</p>\n\n<blockquote>\n  <p>Given an array of random integers,\n  write an algorithm in C that removes\n  duplicated numbers and return the unique numbers in the original\n  array.</p>\n</blockquote>\n\n<p>E.g Input: <code>{4, 8, 4, 1, 1, 2, 9}</code>  Output: <code>{4, 8, 1, 2, 9, ?, ?}</code></p>\n\n<p>One caveat is that the expected algorithm should not required the array to be sorted first. And when an element has been removed, the following elements must be shifted forward as well. Anyway, value of elements at the tail of the array where elements were shifted forward are negligible. </p>\n\n<p><strong>Update:</strong> The result must be returned in the original array and helper data structure (e.g. hashtable) should not be used. However, I guess order preservation is not necessary.</p>\n\n<p><strong>Update2:</strong> For those who wonder why these impractical constraints, this was an interview question and all these constraints are discussed during the thinking process to see how I can come up with different ideas.</p>\n    ","a":"\n<p>How about:</p>\n\n<pre><code>void rmdup(int *array, int length)\n{\n    int *current , *end = array + length - 1;\n\n    for ( current = array + 1; array &lt; end; array++, current = array + 1 )\n    {\n        while ( current &lt;= end )\n        {\n            if ( *current == *array )\n            {\n                *current = *end--;\n            }\n            else\n            {\n                current++;\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>Should be O(n^2) or less.</p>\n    "},{"t":"Binary Trees vs. Linked Lists vs. Hash Tables","l":"http://stackoverflow.com/questions/371136/binary-trees-vs-linked-lists-vs-hash-tables","q":"\n\n<p>I'm building a symbol table for a project I'm working on. I was wondering what peoples opinions are on the advantages and disadvantages of the various methods available for storing + creating a symbol table.</p>\n\n<p>I've done a fair bit of searching and the most commonly recommended are binary trees or linked lists or hash tables. I was wondering what are the advantages and or disadvantages of all of the above (I can't find anything on this).</p>\n\n<p>Thanks,\nBen</p>\n\n<p>Update: am working in c++</p>\n    ","a":"\n<p>Your use case is presumably going to be \"insert the data once (e.g., application startup) and then perform lots of reads but few if any extra insertions\".</p>\n\n<p>Therefore you need to use an algorithm that is fast for looking up the information that you need.</p>\n\n<p>I'd therefore think the HashTable was the most suitable algorithm to use, as it is simply generating a hash of your key object and using that to access the target data - it is O(1). The others are O(N) (Linked Lists of size N - you have to iterate through the list one at a time, an average of N/2 times) and O(log N) (Binary Tree - you halve the search space with each iteration - only if the tree is balanced, so this depends on your implementation, an unbalanced tree can have significantly worse performance).</p>\n\n<p>Just make sure that there are enough spaces (buckets) in the HashTable for your data (R.e., Soraz's comment on this post). Most framework implementations (Java, .NET, etc) will be of a quality that you won't need to worry about the implementations.</p>\n\n<p>Did you do a course on data structures and algorithms at university?</p>\n    "},{"t":"What's the fastest algorithm for sorting a linked list?","l":"http://stackoverflow.com/questions/1525117/whats-the-fastest-algorithm-for-sorting-a-linked-list","q":"\n\n<p>I'm curious if O(n log n) is the best a linked list can do.</p>\n    ","a":"\n<p>It is reasonable to expect that you cannot do any better than O(N log N) in <em>running time</em>.  </p>\n\n<p>However, the interesting part is to investigate if you can sort it <a href=\"http://en.wikipedia.org/wiki/In-place_algorithm\">in-place</a>, <a href=\"http://en.wikipedia.org/wiki/Stable_sort#Stability\">stably</a>, worst-case behavior and so on.</p>\n\n<p>Simon Tatham, of Putty fame, explains how to <a href=\"http://www.chiark.greenend.org.uk/~sgtatham/algorithms/listsort.html\">sort a linked list with merge sort</a>.  He concludes with the following comments:</p>\n\n<blockquote>\n  <p>Like any self-respecting sort algorithm, this has running time O(N log N). Because this is Mergesort, the worst-case running time is still O(N log N); there are no pathological cases.</p>\n  \n  <p>Auxiliary storage requirement is small and constant (i.e. a few variables within the sorting routine). Thanks to the inherently different behaviour of linked lists from arrays, this Mergesort implementation avoids the O(N) auxiliary storage cost normally associated with the algorithm.</p>\n</blockquote>\n\n<p>There is also an example implementation in C that work for both singly and doubly linked lists.</p>\n\n<p>As @Jørgen Fogh mentions below, big-O notation may hide some constant factors that can cause one algorithm to perform better because of memory locality, because of a low number of items, etc.</p>\n    "},{"t":"Algorithm for finding similar images","l":"http://stackoverflow.com/questions/75891/algorithm-for-finding-similar-images","q":"\n\n<p>I need an algorithm that can determine whether two images are 'similar' and recognizes similar patterns of color, brightness, shape etc.. I might need some pointers as to what parameters the human brain uses to 'categorize' images. .. </p>\n\n<p>I have looked at hausdorff based matching but that seems mainly for matching transformed objects and patterns of shape.</p>\n    ","a":"\n<p>I have done something similar, by decomposing images into signatures using <a href=\"http://en.wikipedia.org/wiki/Wavelet\" rel=\"nofollow\">wavelet transform</a>. </p>\n\n<p>My approach was to pick the most significant <em>n</em> coefficients from each transformed channel, and recording their location. This was done by sorting the list of (power,location) tuples according to abs(power). Similar images will share similarities in that they will have significant coefficients in the same places.</p>\n\n<p>I found it was best to transform in the image into YUV format, which effectively allows you weight similarity in shape (Y channel) and colour (UV channels).</p>\n\n<p>You can in find my implementation of the above in <a href=\"https://github.com/freespace/mactorii\" rel=\"nofollow\">mactorii</a>, which unfortunately I haven't been working on as much as I should have :-)</p>\n\n<p>Another method, which some friends of mine have used with surprisingly good results, is to simply resize your image down to say, a 4x4 pixel and store that are your signature. How similar 2 images are can be scored by say, computing the <a href=\"http://en.wikipedia.org/wiki/Manhattan_distance\" rel=\"nofollow\">Manhattan distance</a> between the 2 images, using corresponding pixels. I don't have the details of how they performed the resizing, so you may have to play with the various algorithms available for that task to find one which is suitable.</p>\n    "},{"t":"Select k random elements from a list whose elements have weights","l":"http://stackoverflow.com/questions/2140787/select-k-random-elements-from-a-list-whose-elements-have-weights","q":"\n\n<p>Selecting without any weights (equal probabilities) is beautifully described <a href=\"http://stackoverflow.com/questions/48087/select-a-random-n-elements-from-listt-in-c/48089#48089\">here</a>.</p>\n\n<p>I was wondering if there is a way to convert this approach to a weighted one.</p>\n\n<p>I am also interested in other approaches as well.</p>\n\n<p>Update: Sampling <strong>without</strong> replacement</p>\n    ","a":"\n<p>I know this is a very old question, but I think there's a neat trick to do this in O(n) time if you apply a little math!</p>\n\n<p>The <a href=\"http://en.wikipedia.org/wiki/Exponential_distribution\" rel=\"nofollow\">exponential distribution</a> has two very useful properties.</p>\n\n<ol>\n<li><p>Given n samples from different exponential distributions with different rate parameters, the probability that a given sample is the minimum is equal to its rate parameter divided by the sum of all rate parameters.</p></li>\n<li><p>It is \"memoryless\". So if you already know the minimum, then the probability that any of the remaining elements is the 2nd-to-min is the same as the probability that if the true min were removed (and never generated), that element would have been the new min. This seems obvious, but I think because of some conditional probability issues, it might not be true of other distributions.</p></li>\n</ol>\n\n<p>Using fact 1, we know that choosing a single element can be done by generating these exponential distribution samples with rate parameter equal to the weight, and then choosing the one with minimum value.</p>\n\n<p>Using fact 2, we know that we don't have to re-generate the exponential samples. Instead, just generate one for each element, and take the k elements with lowest samples.</p>\n\n<p>Finding the lowest k can be done in O(n). Use the <a href=\"http://en.wikipedia.org/wiki/Quickselect\" rel=\"nofollow\">Quickselect</a> algorithm to find the k-th element, then simply take another pass through all elements and output all lower than the k-th.</p>\n\n<p>A useful note: if you don't have immediate access to a library to generate exponential distribution samples, it can be easily done by: <code>-ln(rand())/weight</code></p>\n    "},{"t":"Efficient list intersection algorithm","l":"http://stackoverflow.com/questions/497338/efficient-list-intersection-algorithm","q":"\n\n<p>Given two lists (not necessarily sorted), what is the most efficient non-recursive algorithm to find the intersection of those lists?</p>\n    ","a":"\n<p>You could put all elements of the first list into a hash set. Then, iterate the second one and, for each of its elements, check the hash to see if it exists in the first list. If so, output it as an element of the intersection.</p>\n    "},{"t":"Could a truly random number be generated using pings to psuedo-randomly selected IP addresses?","l":"http://stackoverflow.com/questions/137340/could-a-truly-random-number-be-generated-using-pings-to-psuedo-randomly-selected","q":"\n\n<p>The question posed came about during a 2nd Year Comp Science lecture while discussing the impossibility of generating numbers in a deterministic computational device.</p>\n\n<p>This was the only suggestion which didn't depend on non-commodity-class hardware.</p>\n\n<p>Subsequently nobody would put their reputation on the line to argue definitively for or against it.  </p>\n\n<p>Anyone care to make a stand for or against.  If so, how about a mention as to a possible implementation?</p>\n    ","a":"\n<p>I'll put my rep on the line (at least, 2 points of it per downvote).</p>\n\n<p>No.</p>\n\n<p>A malicious machine on your network could use ARP spoofing (or a number of other techniques) to intercept your pings and reply to them after certain periods. They would then not only know what your random numbers are, they would control them.</p>\n\n<p>Of course there's still the question of how deterministic your local network is, so it might not be as easy as all that in practice. But since you get no benefit from pinging random IPs on the internet, you might just as well draw entropy from ethernet traffic. </p>\n\n<p>Drawing entropy from devices attached to your machine is a well-studied principle, and the pros and cons of various kinds of devices and methods of measuring can be e.g. stolen from the implementation of /dev/random.</p>\n\n<p>[<strong>Edit</strong>: as a general principle, when working in the fundamentals of security (and the only practical needs for significant quantities of truly random data are security-related) you MUST assume that a fantastically well-resourced, determined attacker will do everything in their power to break your system.</p>\n\n<p>For practical security, you can assume that nobody wants your PGP key that badly, and settle for a trade-off of security against cost. But when inventing algorithms and techniques, you need to give them the strongest security guarantees that they could ever possibly face. Since I can believe that someone, somewhere, might want someone else's private key badly enough do build this bit of kit to defeat your proposal, I can't accept it as an advance over current best practice. AFAIK /dev/random follows fairly close to best practice for generating truly random data on a cheap home PC]</p>\n\n<p>[<strong>Another edit</strong>: it has suggested in comments that (1) it is true of any TRNG that the physical process could be influenced, and (2) that security concerns don't apply here anyway.</p>\n\n<p>The answer to (1) is that it's possible on any realistic hardware to do so much better than ping response times, and gather more entropy faster, that this proposal is a non-solution. In CS terms, obviously you can't generate random numbers on a deterministic machine, which is what provoked the question. But then in CS terms a machine with any external input stream is non-deterministic by definition, so if we're talking about ping then we aren't talking about deterministic machines. So it makes sense to look at the real inputs that real machines have, and consider them as sources of randomness. No matter what your machine, raw ping times are not high on the list of sources available, so they can be ruled out before worrying about good the better ones are. Assuming that a network is not subverted is a much bigger (and unnecessary) assumption than assuming that your own hardware is not subverted.</p>\n\n<p>The answer to (2) is philosophical. If you don't mind your random numbers having the property that they can be chosen at whim instead of by chance, then this proposal is OK. But that's not what I understand by the term 'random'. Just because something is inconsistent doesn't mean it's necessarily random.</p>\n\n<p>Finally, to address the implementation details of the proposal as requested: assuming you accept ping times as random, you still can't use the unprocessed ping times as RNG output. You don't know their probability distribution, and they certainly aren't uniformly distributed (which is normally what people want from an RNG). </p>\n\n<p>So, you need to decide how many bits of entropy per ping you are willing to rely on. Entropy is a precisely-defined mathematical property of a random variable which can reasonably be considered a measure of how 'random' it actually is. In practice, you find a lower bound you're happy with. Then hash together a number of inputs, and convert that into a number of bits of output less than or equal to the total relied-upon entropy of the inputs. 'Total' doesn't necessarily mean sum: if the inputs are statistically independent then it is the sum, but this is unlikely to be the case for pings, so part of your entropy estimate will be to account for correlation. The sophisticated big sister of this hashing operation is called an 'entropy collector', and all good OSes have one.</p>\n\n<p>If you're using the data to seed a PRNG, though, and the PRNG can use arbitrarily large seed input, then you don't have to hash because it will do that for you. You still have to estimate entropy if you want to know how 'random' your seed value was - you can use the best PRNG in the world, but its entropy is still limited by the entropy of the seed.]</p>\n    "},{"t":"Programming theory: Solve a maze","l":"http://stackoverflow.com/questions/3097556/programming-theory-solve-a-maze","q":"\n\n<p>What are the possible ways to solve a maze?<br>\nIve got two ideas, but I think they are not very elegant.</p>\n\n<p><strong>Base situation:</strong> We have a matrix, and the elements in this matrix are ordered in a way that it represents a maze, with one way in, and one out.</p>\n\n<p>My first idea was to send a robot through the maze, following one side, until it's out of the maze. I think this is a very slow solution.</p>\n\n<p>The second one passes through every successive item marked with 1, checks where it can go (up, right, down, left) chooses one way and it continues its path there. This is even slower than the first one.</p>\n\n<p>Of course it's a bit faster if I make the two bots multi-threaded at every junction, but thats also not the best way.</p>\n\n<p>There needs to be better solutions to send a bot through a maze.</p>\n\n<p><strong>EDIT</strong><br>\nFirst: Thanks for the nice answers!</p>\n\n<p><strong>The second part of my question is:</strong> What to do in the case if we have a multi-dimensional graph? Are there special practices for that, or is the answer of Justin L. usable for that too?<br>\nI think it's not the best way for this case.</p>\n\n<p><strong>The third question:</strong><br>\nWhich of these maze solver algorithms is/are the fastest? (Purely hypothetically)</p>\n    ","a":"\n<p>You can think of your maze as a tree.</p>\n\n<pre>     A\n    / \\\n   /   \\\n  B     C\n / \\   / \\\nD   E F   G\n   / \\     \\\n  H   I     J\n / \\\nL   M\n   / \\\n  **  O\n\n(which could possibly represent)\n\n        START\n        +   +---+---+\n        | A   C   G |\n    +---+   +   +   +\n    | D   B | F | J |\n+---+---+   +---+---+\n| L   H   E   I |\n+---+   +---+---+\n    | M   O |\n    +   +---+\n    FINISH\n\n(ignoring left-right ordering on the tree)\n</pre>\n\n<p>Where each node is a junction of paths.  D, I, J, L and O are dead ends, and ** is the goal.\nOf course, in your actual tree, each node has a possibility of having as many as <em>three</em> children.</p>\n\n<p>Your goal is now simply finding what nodes to traverse to to find the finish.  Any ol' tree search algorithm will do.</p>\n\n<p>Looking at the tree, it's pretty easy to see your correct solution by simply \"tracing up\" from the ** at the deepest part of the tree:</p>\n\n<pre><code>A B E H M **\n</code></pre>\n\n<p>Note that this approach becomes <em>only slightly</em> more complicated when you have \"loops\" in your maze (i.e., when it is possible, without backtracing, you re-enter a passage you've already traversed through).  Check the comments for one nice solution.</p>\n\n<p>Now, let's look at your first solution you mentioned, applied to this tree.</p>\n\n<p>Your first solution is basically a <a href=\"http://en.wikipedia.org/wiki/Depth-first_search\"><strong>Depth-First Search</strong></a>, which really isn't that bad.  It's actually a pretty good recursive search.  Basically, it says, \"Always take the rightmost approach first.  If nothing is there, backtrack until the first place you can go straight or left, and then repeat.</p>\n\n<p>A depth-first search will search the above tree in this order:</p>\n\n<pre><code>A B D (backtrack) E H L (backtrack) M ** (backtrack) O (backtrack thrice) I\n(backtrack thrice) C F (backtrack) G J\n</code></pre>\n\n<p>Note that you can stop as soon as you find the **.</p>\n\n<p>However, when you actually code a depth-first search, using <strong>recursive programming</strong> makes makes everything much easier.  Even iterative methods work too, and you never have to explicitly program how to backtrack.  Check out the linked article for implementations.</p>\n\n<p>Another way of searching a tree is the <a href=\"http://en.wikipedia.org/wiki/Breadth-first_search\"><strong>Breadth-First</strong></a> solution, which searches through trees by depth.  It'd search through the above tree in this order:</p>\n\n<pre><code>A (next level) B C (next level) D E F G (next level)\nH I J (next level) L M (next level) ** O\n</code></pre>\n\n<p>Note that, due to the nature of a maze, breadth-first has a much higher average amount of nodes it checks.  Breadth-first is easily implementing by having a queue of paths to search, and each iteration popping a path out of a queue, \"exploding it\" by getting all of the paths that it can turn into after one step, and putting those new paths at the end of the queue.  There are no explicit \"next level\" commands to code, and those were just there to aid in understanding.</p>\n\n<p>In fact, there is a whole <a href=\"http://en.wikipedia.org/wiki/Tree_traversal\">expansive list of ways to search a tree</a>.  I've just mentioned the two simplest, most straightforward way.</p>\n\n<p>If your maze is very, very long and deep, and has loops and crazies, and is complicated, I suggest the <a href=\"http://en.wikipedia.org/wiki/A%2a_search_algorithm\"><strong>A*</strong></a> algorithm, which is the industry standard pathfinding algorithm which combines a Breadth-First search with heuristics...sort of like an \"intelligent breadth-first search\".</p>\n\n<p>It basically works like this:</p>\n\n<ol>\n<li>Put one path in a queue (the path where you only walk one step straight into the maze).  A path has a \"weight\" given by its current length + its straight-line distance from the end (which can be calculated mathematically)</li>\n<li>Pop the path with the lowest weight from the queue.</li>\n<li>\"Explode\" the path into every path that it could be after one step. (i.e., if your path is Right Left Left Right, then your exploded paths are R L L R R and R L L R L, not including illegal ones that go through walls)</li>\n<li>If one of these paths has the goal, then Victory!  Otherwise:</li>\n<li>Calculate the weights of the exploded paths, and put all of them back into the queue (not including the original path)</li>\n<li>Sort the queue by weight, lowest first.  Then repeat from Step #2</li>\n</ol>\n\n<p>And that's <strong>A*</strong>, which I present specially highlighted because it is more or less the industry standard pathfinding algorithm for <em>all</em> applications of pathfinding, including moving from one edge of the map to another while avoiding off-road paths or mountains, etc.  It works so well because it uses a <em>shortest possible distance heuristic</em>, which gives it its \"intelligence\".  A* is so versatile because, given any problem, if you have a shortest possible distance heuristic available (ours is easy -- the straight line), you can apply it.</p>\n\n<p><strong>BUT</strong> it is of great value to note that A* is <em>not</em> your only option.</p>\n\n<p>In fact, the <a href=\"http://en.wikipedia.org/wiki/Category%3aSearch_algorithms\">wikipedia category of tree traversal algorithms</a> lists 97 alone! (the best will still be on <a href=\"http://en.wikipedia.org/wiki/Tree_traversal\">this page</a> linked earlier)</p>\n\n<p>Sorry for the length =P (I tend to ramble)</p>\n    "},{"t":"The inverse Fibonacci problem?","l":"http://stackoverflow.com/questions/5162780/the-inverse-fibonacci-problem","q":"\n\n<p>There are dozens of ways of computing F(n) for an arbitrary n, many of which have great runtime and memory usage.</p>\n\n<p>However, suppose I wanted to ask the opposite question:</p>\n\n<blockquote>\n  <p>Given F(n) for n &gt; 2, what is n?</p>\n</blockquote>\n\n<p>(The n &gt; 2 restriction is in there since F(1) = F(2) = 1 and there's no unambiguous inverse).</p>\n\n<p>What would be the most efficient way of solving this problem?  It's easy to do this in linear time by enumerating the Fibonacci numbers and stopping when you hit the target number, but is there some way of doing this any faster than that?</p>\n\n<p><strong>EDIT:</strong> currently, the best solution posted here runs in O(log n) time using O(log n) memory, assuming that mathematical operations run in O(1) and that a machine word can hold any number in O(1) space.  I'm curious if it's possible to drop the memory requirements, since you can compute Fibonacci numbers using O(1) space.</p>\n\n<p>Thanks, and I hope you think this is as cool as I do. :-)</p>\n    ","a":"\n<p>Since OP has asked about matrix solution not involving any floating point computations, here it is. We can achieve <code>O(logn)</code> complexity this way, assuming numeric operations have <code>O(1)</code> complexity.</p>\n\n<p>Let's take 2x2 matrix <code>A</code> having following structure</p>\n\n<pre><code>1 1\n1 0\n</code></pre>\n\n<p>Now consider vector <code>(8, 5)</code>, storing two consecutive fibonacci numbers. If you multiply it by this matrix, you'll get <code>(8*1 + 5*1, 8*1 + 5*0) = (13, 8)</code> - the next fibonacci number.<br>\nIf we generalize, <code>A^n * (1, 0) = (f(n), f(n - 1))</code>.  </p>\n\n<p>The actual algorithm takes two steps.</p>\n\n<ol>\n<li>Calculate <code>A^2</code>, <code>A^4</code>, <code>A^8</code>, etc. until we pass desired number.</li>\n<li>Do a binary search by <code>n</code>, using calculated powers of <code>A</code>.</li>\n</ol>\n\n<p>On a side note, any sequence of the form <code>f(n) = k1*f(n-1) + k2*f(n-2) + k3*f(n-3) + .. + kt*f(n-t)</code> can be presented like this.        </p>\n    "},{"t":"Big-O complexity of a piece of code","l":"http://stackoverflow.com/questions/30233242/big-o-complexity-of-a-piece-of-code","q":"\n\n<p>I have a question in algorithm design about complexity. In this question a piece of code is given and I should calculate this code's complexity. \nThe pseudo-code is:</p>\n\n<pre><code>for(i=1;i&lt;=n;i++){\n    j=i\n    do{\n        k=j;\n        j = j / 2;\n    }while(k is even);\n}\n</code></pre>\n\n<p>I tried this algorithm for some numbers. and I have gotten different results. for example if n = 6 this algorithm output is like below</p>\n\n<pre><code>i = 1 -&gt; executes 1 time\ni = 2 -&gt; executes 2 times\ni = 3 -&gt; executes 1 time\ni = 4 -&gt; executes 3 times\ni = 5 -&gt; executes 1 time\ni = 6 -&gt; executes 2 times\n</code></pre>\n\n<p>It doesn't have a regular theme, how should I calculate this?</p>\n    ","a":"\n<p>The upper bound given by the other answers is actually too high. This algorithm has a O(n) runtime, which is a tighter upper bound than O(n*logn).</p>\n\n<p>Proof: Let's count how many total iterations the inner loop will perform.</p>\n\n<p>The outer loop runs <code>n</code> times. The inner loop runs at least once for each of those.</p>\n\n<ul>\n<li>For even <code>i</code>, the inner loop runs at least twice. This happens <code>n/2</code> times.</li>\n<li>For <code>i</code> divisible by 4, the inner loop runs at least three times. This happens <code>n/4</code> times.</li>\n<li>For <code>i</code> divisible by 8, the inner loop runs at least four times. This happens <code>n/8</code> times.</li>\n<li>...</li>\n</ul>\n\n<p>So the total amount of times the inner loop runs is:</p>\n\n<pre><code>n + n/2 + n/4 + n/8 + n/16 + ... &lt;= 2n\n</code></pre>\n\n<p>The total amount of inner loop iterations is between <code>n</code> and <code>2n</code>, i.e. it's Θ(n).</p>\n    "},{"t":"Find all combinations of coins when given some dollar value","l":"http://stackoverflow.com/questions/1106929/find-all-combinations-of-coins-when-given-some-dollar-value","q":"\n\n<p>I found a piece of code that I was writing for interview prep few months ago.</p>\n\n<p>According to the comment I had, it was trying to solve this problem:</p>\n\n<p><strong>Given some dollar value in cents (e.g. 200 = 2 dollars, 1000 = 10 dollars),\nfind all the combinations of coins that make up the dollar value.\nThere are only penny, nickel, dime, and quarter. (quarter = 25 cents, dime = 10 cents, nickel = 5 cents, penny = 1 cent)</strong></p>\n\n<p>For example, if 100 was given, the answer should be...<br>\n4 quarter(s) 0 dime(s) 0 nickel(s) 0 pennies<br>\n3 quarter(s) 1 dime(s) 0 nickel(s) 15 pennies<br>\netc.</p>\n\n<p>This can be solved in both iterative and recursive ways, I believe. My recursive solution is quite buggy, and I was wondering how other people would solve this problem. The difficult part of this problem was making it as efficient as possible.</p>\n\n<p><strong>UPDATE</strong><br>\nMaking this thread into a community wiki, because apparently, we have gathered many different implementations :)</p>\n    ","a":"\n<p>I looked into this once a long time ago, and you can read my <a href=\"http://andrew.neitsch.ca/publications/m496pres1.nb.pdf\">little write-up on it</a>. Here’s the <a href=\"http://andrew.neitsch.ca/publications/m496pres1.nb\">Mathematica source</a>.</p>\n\n<p>By using generating functions, you can get a closed-form constant-time solution to the problem. Graham, Knuth, and Patashnik’s <em>Concrete Mathematics</em> is the book for this, and contains a fairly extensive discussion of the problem. Essentially you define a polynomial where the *n*th coefficient is the number of ways of making change for <em>n</em> dollars.</p>\n\n<p>Pages 4-5 of the writeup show how you can use Mathematica (or any other convenient computer algebra system) to compute the answer for 10^10^6 dollars in a couple seconds in three lines of code.</p>\n\n<p>(And this was long enough ago that that’s a couple of seconds on a 75Mhz Pentium...)</p>\n    "},{"t":"“On-line” (iterator) algorithms for estimating statistical median, mode, skewness, kurtosis?","l":"http://stackoverflow.com/questions/1058813/on-line-iterator-algorithms-for-estimating-statistical-median-mode-skewnes","q":"\n\n<p>Is there an algorithm to estimate the median, mode, skewness, and/or kurtosis of set of values, but that does NOT require storing all the values in memory at once?</p>\n\n<p>I'd like to calculate the basic statistics:</p>\n\n<ul>\n<li>mean:  arithmetic average</li>\n<li>variance:  average of squared deviations from the mean</li>\n<li>standard deviation:  square root of the variance</li>\n<li>median:  value that separates larger half of the numbers from the smaller half</li>\n<li>mode:  most frequent value found in the set</li>\n<li>skewness:  tl; dr</li>\n<li>kurtosis:  tl; dr</li>\n</ul>\n\n<p>The basic formulas for calculating any of these is grade-school arithmetic, and I do know them.  There are many stats libraries that implement them, as well.</p>\n\n<p>My problem is the large number (billions) of values in the sets I'm handling:  Working in Python, I can't just make a list or hash with billions of elements.  Even if I wrote this in C, billion-element arrays aren't too practical.</p>\n\n<p>The data is not sorted.  It's produced randomly, on-the-fly, by other processes.  The size of each set is highly variable, and the sizes will not be known in advance.</p>\n\n<p>I've already figured out how to handle the mean and variance pretty well, iterating through each value in the set in any order.  (Actually, in my case, I take them in the order in which they're generated.)  Here's the algorithm I'm using, courtesy <a href=\"http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#On-line_algorithm\">http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#On-line_algorithm</a>:</p>\n\n<ul>\n<li>Initialize three variables:  count, sum, and sum_of_squares</li>\n<li>For each value:\n<ul>\n<li>Increment count.</li>\n<li>Add the value to sum.</li>\n<li>Add the square of the value to sum_of_squares.</li>\n</ul></li>\n<li>Divide sum by count, storing as the variable mean.</li>\n<li>Divide sum_of_squares by count, storing as the variable mean_of_squares.</li>\n<li>Square mean, storing as square_of_mean.</li>\n<li>Subtract square_of_mean from mean_of_squares, storing as variance.</li>\n<li>Output mean and variance.</li>\n</ul>\n\n<p>This \"on-line\" algorithm has weaknesses (e.g., accuracy problems as sum_of_squares quickly grows larger than integer range or float precision), but it basically gives me what I need, without having to store every value in each set.</p>\n\n<p>But I don't know whether similar techniques exist for estimating the additional statistics (median, mode, skewness, kurtosis).  I could live with a biased estimator, or even a method that compromises accuracy to a certain degree, as long as the memory required to process N values is substantially less than O(N).</p>\n\n<p>Pointing me to an existing stats library will help, too, if the library has functions to calculate one or more of these operations \"on-line\".</p>\n    ","a":"\n<p><strong>Skewness and Kurtosis</strong></p>\n\n<p>For the on-line algorithms for Skewness and Kurtosis (along the lines of the variance), see in the same wiki page <a href=\"http://en.wikipedia.org/wiki/Algorithms%5Ffor%5Fcalculating%5Fvariance#Higher-order_statistics\">here</a> the parallel algorithms for higher-moment statistics.</p>\n\n<p><strong>Median</strong></p>\n\n<p>Median is tough without sorted data. If you know, how many data points you have, in theory you only have to partially sort, e.g. by using a <a href=\"http://en.wikipedia.org/wiki/Selection%5Falgorithm\">selection algorithm</a>. However, that doesn't help too much with billions of values. I would suggest using frequency counts, see the next section.</p>\n\n<p><strong>Median and Mode with Frequency Counts</strong></p>\n\n<p>If it is integers, I would count\n<a href=\"http://en.wikipedia.org/wiki/Frequency_(statistics)\">frequencies</a>, probably cutting off the highest and lowest values beyond some value where I am sure that it is no longer relevant. For floats (or too many integers), I would probably create buckets / intervals, and then use the same approach as for integers. (Approximate) mode and median calculation than gets easy, based on the frequencies table.</p>\n\n<p><strong>Normally Distributed Random Variables</strong></p>\n\n<p>If it is normally distributed, I would use the population sample <a href=\"http://en.wikipedia.org/wiki/Mean#Population_and_sample_means\">mean</a>, <a href=\"http://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance\">variance</a>, <a href=\"http://en.wikipedia.org/wiki/Skewness#Sample_skewness\">skewness</a>, and <a href=\"http://en.wikipedia.org/wiki/Kurtosis#Sample_kurtosis\">kurtosis</a> as maximum likelihood estimators for a small subset. The (on-line) algorithms to calculate those, you already now. E.g. read in a couple of hundred thousand or million datapoints, until your estimation error gets small enough. Just make sure that you pick randomly from your set (e.g. that you don't introduce a bias by picking the first 100'000 values). The same approach can also be used for estimating mode and median for the normal case (for both the sample mean is an estimator).</p>\n\n<p><strong>Further comments</strong></p>\n\n<p>All the algorithms above can be run in parallel (including many sorting and selection algorithm, e.g. QuickSort and QuickSelect), if this helps.</p>\n\n<p>I have always assumed (with the exception of the section on the normal distribution) that we talk about sample moments, median, and mode, not estimators for theoretical moments given a known distribution.</p>\n\n<p>In general, sampling the data (i.e. only looking at a sub-set) should be pretty successful given the amount of data, as long as all observations are realizations of the same random variable (have the same distributions) and the moments, mode and median actually exist for this distribution. The last caveat is not innocuous. For example, the mean (and all higher moments) for the <a href=\"http://en.wikipedia.org/wiki/Cauchy%5Fdistribution\">Cauchy Distribution</a> do not exist. In this case, the sample mean of a \"small\" sub-set might be massively off from the sample mean of the whole sample.</p>\n    "},{"t":"Storing 1 million phone numbers [closed]","l":"http://stackoverflow.com/questions/5262465/storing-1-million-phone-numbers","q":"\n\n<p>What is the most efficient way, memory-wise, to store 1 million phone numbers?</p>\n\n<p>Apparently this is an interview question at Google, please give your ideas.</p>\n    ","a":"\n<p>If memory is our biggest consideration, then we don't need to store the number at all, just the delta between i and i+1. </p>\n\n<p>Now if numbers range from 200 0000 - 999 9999, then there are 7,999,999 possible phone numbers. Since we have 1-million numbers, and if we assume that they are uniformly distributed, we have an Expected distance of E = n_i+1 - n_i ~ 8 (3 bits) between sequential numbers n_i and n_i+1. So for a 32-bit int we could potentially store up to 10 sequential offsets (~400kb optimal total memory footprint), however its likely that we'll have some cases where we need an offset greater than 8 (Maybe we have 400, or 1500??). In which case, we can simply reserve the first 2 bits of the int as a header which tells us what frame size we use to read the bits it stores. For example, maybe we use: 00 = 3x10, 01 = 5x6, 10 = 7x4, 11 = 1*30.</p>\n    "},{"t":"How to make rounded percentages add up to 100%","l":"http://stackoverflow.com/questions/13483430/how-to-make-rounded-percentages-add-up-to-100","q":"\n\n<p>Consider the four percentages below, represented as <code>float</code> numbers:</p>\n\n<pre><code>    13.626332%\n    47.989636%\n     9.596008%\n    28.788024%\n   -----------\n   100.000000%\n</code></pre>\n\n<p>I need to represent these percentages as whole numbers. If I simply use <code>Math.round()</code>, I end up with a total of 101%. </p>\n\n<pre><code>14 + 48 + 10 + 29 = 101\n</code></pre>\n\n<p>If I use <code>parseInt()</code>, I end up with a total of 97%. </p>\n\n<pre><code>13 + 47 + 9 + 28 = 97\n</code></pre>\n\n<p>What's a good algorithm to represent any number of  percentages as whole numbers while still maintaining a total of 100%?</p>\n\n<hr>\n\n<p><strong>Edit</strong>: After reading some of the comments and answers, there are clearly many ways to go about solving this.</p>\n\n<p>In my mind, to remain true to the numbers, the \"right\" result is the one that minimizes the overall error, defined by how much error rounding would introduce relative to the actual value:</p>\n\n<pre><code>        value  rounded     error               decision\n   ----------------------------------------------------\n    13.626332       14      2.7%          round up (14)\n    47.989636       48      0.0%          round up (48)\n     9.596008       10      4.0%    don't round up  (9)\n    28.788024       29      2.7%          round up (29)\n</code></pre>\n\n<p>In case of a tie (3.33, 3.33, 3.33) an arbitrary decision can be made (e.g. 3, 4, 3).</p>\n    ","a":"\n<p>Since none of the answers here seem to solve it properly, here's my semi-obfuscated version using <a href=\"http://underscorejs.org/\">underscorejs</a>:</p>\n\n<pre><code>function foo(l, target) {\n    var off = target - _.reduce(l, function(acc, x) { return acc + Math.round(x) }, 0);\n    return _.chain(l).\n            sortBy(function(x) { return Math.round(x) - x }).\n            map(function(x, i) { return Math.round(x) + (off &gt; i) - (i &gt;= (l.length + off)) }).\n            value();\n}\n\nfoo([13.626332, 47.989636, 9.596008, 28.788024], 100) // =&gt; [48, 29, 14, 9]\nfoo([16.666, 16.666, 16.666, 16.666, 16.666, 16.666], 100) // =&gt; [17, 17, 17, 17, 16, 16]\nfoo([33.333, 33.333, 33.333], 100) // =&gt; [34, 33, 33]\nfoo([33.3, 33.3, 33.3, 0.1], 100) // =&gt; [34, 33, 33, 0]\n</code></pre>\n    "},{"t":"How to convert a byte array to its numeric value (Java)?","l":"http://stackoverflow.com/questions/1026761/how-to-convert-a-byte-array-to-its-numeric-value-java","q":"\n\n<p>I have an 8 byte array and I want to convert it to its corresponding numeric value.</p>\n\n<p>e.g.</p>\n\n<pre><code>byte[] by = new byte[8];  // the byte array is stored in 'by'\n\n// CONVERSION OPERATION\n// return the numeric value\n</code></pre>\n\n<p>I want a method that will perform the above conversion operation.</p>\n    ","a":"\n<p>Assuming the first byte is the least significant byte:</p>\n\n<pre><code>long value = 0;\nfor (int i = 0; i &lt; by.length; i++)\n{\n   value += ((long) by[i] &amp; 0xffL) &lt;&lt; (8 * i);\n}\n</code></pre>\n\n<p>Is the first byte the most significant, then it is a little bit different:</p>\n\n<pre><code>long value = 0;\nfor (int i = 0; i &lt; by.length; i++)\n{\n   value = (value &lt;&lt; 8) + (by[i] &amp; 0xff);\n}\n</code></pre>\n\n<p>Replace long with <a href=\"http://java.sun.com/javase/6/docs/api/java/math/BigInteger.html\">BigInteger</a>, if you have more than 8 bytes.</p>\n\n<p>Thanks to Aaron Digulla for the correction of my errors.</p>\n    "},{"t":"Why do we use Base64?","l":"http://stackoverflow.com/questions/3538021/why-do-we-use-base64","q":"\n\n<p><a href=\"http://en.wikipedia.org/wiki/Base64\">Wikipedia</a> says</p>\n\n<blockquote>\n  <p>Base64 encoding schemes are commonly used when there is a need to encode binary data that needs be stored and transferred over media that are designed to deal with textual data. This is to ensure that the data remains intact without modification during transport.</p>\n</blockquote>\n\n<p>But is it not that data is always stored/transmitted in binary because the memory that our machines have store binary and it just depends how you interpret it? So, whether you encode the bit pattern <code>010011010110000101101110</code> as <code>Man</code> in ASCII or as <code>TWFu</code> in Base64, you are eventually going to store the same bit pattern.</p>\n\n<p><img src=\"http://i.stack.imgur.com/hLMTo.png\" alt=\"alt text\"></p>\n\n<p><strong>If the ultimate encoding is in terms of zeros and ones and every machine and media can deal with them, how does it matter if the data is represented as ASCII or Base64?</strong></p>\n\n<p>What does it mean \"media that are designed to deal with textual data\"? They can deal with binary =&gt; they can deal with anything.</p>\n\n<hr>\n\n<p>Thanks everyone, I think I understand now.</p>\n\n<p>When we send over data, we cannot be sure that the data would be interpreted in the same format as we intended it to be. So, we send over data coded in some format (like Base64) that both parties understand. That way even if sender and receiver interpret same things differently, but because they agree on the coded format, the data will not get interpreted wrongly.</p>\n\n<p>From <a href=\"http://stackoverflow.com/questions/3538021/why-do-we-use-base64/3538079#3538079\">Mark Byers example</a></p>\n\n<p>If I want to send </p>\n\n<pre><code>Hello\nworld!\n</code></pre>\n\n<p>One way is to send it in ASCII like </p>\n\n<pre><code>72 101 108 108 111 10 119 111 114 108 100 33\n</code></pre>\n\n<p>But byte 10 might not be interpreted correctly as a newline at the other end. So, we use a subset of ASCII to encode it like this</p>\n\n<pre><code>83 71 86 115 98 71 56 115 67 110 100 118 99 109 120 107 73 61 61\n</code></pre>\n\n<p>which at the cost of more data transferred for the same amount of information ensures that the receiver can decode the data in the intended way, even if the receiver happens to have different interpretations for the rest of the character set.</p>\n    ","a":"\n<p>Your first mistake is thinking that ASCII encoding and Base64 encoding are interchangeable. They are not. They are used for different purposes.</p>\n\n<ul>\n<li>When you encode text in ASCII, you start with a text string and convert it to a sequence of bytes.</li>\n<li>When you encode data in Base64, you start with a sequence of bytes and convert it to a text string. </li>\n</ul>\n\n<p>To understand why Base64 was necessary in the first place we need a little history of computing.</p>\n\n<hr>\n\n<p>Computers communicate in binary - 0s and 1s - but people typically want to communicate with more rich forms data such as text or images. In order to transfer this data between computers it first has to be encoded into 0s and 1s, sent, then decoded again. To take text as an example - there are many different ways to perform this encoding. It would be much simpler if we could all agree on a single encoding, but sadly this is not the case.</p>\n\n<p>Originally a lot of different encodings were created (e.g. <a href=\"http://en.wikipedia.org/wiki/Baudot_code\">Baudot code</a>) which used a different number of bits per character until eventually ASCII became a standard with 7 bits per character. However most computers store binary data in bytes consisting of 8 bits each so <a href=\"http://en.wikipedia.org/wiki/ASCII\">ASCII</a> is unsuitable for tranferring this type of data. Some systems would even wipe the most significant bit. Furthermore the difference in line ending encodings across systems mean that the ASCII character 10 and 13 were also sometimes modified.</p>\n\n<p>To solve these problems <a href=\"http://en.wikipedia.org/wiki/Base64\">Base64</a> encoding was introduced. This allows you to encode aribtrary bytes to bytes which are known to be safe to send without getting corrupted (ASCII alphanumeric characters and a couple of symbols). The disadvantage is that encoding the message using Base64 increases its length - every 3 bytes of data is encoded to 4 ASCII characters.</p>\n\n<p>To send text reliably you can <strong>first</strong> encode to bytes using a text encoding of your choice (for example UTF-8) and then <strong>afterwards</strong> Base64 encode the resulting binary data into a text string that is safe to send encoded as ASCII. The receiver will have to reverse this process to recover the original message. This of course requires that the receiver knows which encodings were used, and this information often needs to be sent separately.</p>\n\n<p>Historically it has been used to encode binary data in email messages where the email server might modify line-endings. A more modern example is the use of Base64 encoding to <a href=\"http://www.sweeting.org/mark/blog/2005/07/12/base64-encoded-images-embedded-in-html\">embed image data directly in HTML source code</a>. Here it is necessary to encode the data to avoid characters like '&lt;' and '&gt;' being interpreted as tags.</p>\n\n<hr>\n\n<p>Here is a worked example:</p>\n\n<p>I wish to send a text message with two lines</p>\n\n<pre>Hello\nworld!\n</pre>\n\n<p>If I send it as ASCII (or UTF-8) it will look like this:</p>\n\n<pre><code>72 101 108 108 111 10 119 111 114 108 100 33\n</code></pre>\n\n<p>The byte 10 is corrupted in some systems so we can base 64 encode these bytes as a Base64 string:</p>\n\n<pre>SGVsbG8sCndvcmxkIQ==</pre>\n\n<p>Which when encoded using ASCII looks like this:</p>\n\n<pre><code>83 71 86 115 98 71 56 115 67 110 100 118 99 109 120 107 73 61 61\n</code></pre>\n\n<p>All the bytes here are known safe bytes, so there is very little chance that any system will corrupt this message. I can send this instead of my original message and let the receiver reverse the process to recover the original message.</p>\n    "},{"t":"Most elegant way to generate prime numbers","l":"http://stackoverflow.com/questions/1042902/most-elegant-way-to-generate-prime-numbers","q":"\n\n<p>What is the most elegant way to implement this function:</p>\n\n<pre><code>ArrayList generatePrimes(int n)\n</code></pre>\n\n<p>This function generates the first <code>n</code> primes (edit: where <code>n&gt;1</code>), so <code>generatePrimes(5)</code> will return an <code>ArrayList</code> with <code>{2, 3, 5, 7, 11}</code>. (I'm doing this in C#, but I'm happy with a Java implementation - or any other similar language for that matter (so not Haskell)).</p>\n\n<p>I do know how to write this function, but when I did it last night it didn't end up as nice as I was hoping. Here is what I came up with:</p>\n\n<pre><code>ArrayList generatePrimes(int toGenerate)\n{\n    ArrayList primes = new ArrayList();\n    primes.Add(2);\n    primes.Add(3);\n    while (primes.Count &lt; toGenerate)\n    {\n        int nextPrime = (int)(primes[primes.Count - 1]) + 2;\n        while (true)\n        {\n            bool isPrime = true;\n            foreach (int n in primes)\n            {\n                if (nextPrime % n == 0)\n                {\n                    isPrime = false;\n                    break;\n                }\n            }\n            if (isPrime)\n            {\n                break;\n            }\n            else\n            {\n                nextPrime += 2;\n            }\n        }\n        primes.Add(nextPrime);\n    }\n    return primes;\n}\n</code></pre>\n\n<p>I'm not too concerned about speed, although I don't want it to be obviously inefficient. I don't mind which method is used (naive or sieve or anything else), but I do want it to be fairly short and obvious how it works.</p>\n\n<p><strong>Edit</strong>: Thanks to all who have responded, although many didn't answer my actual question. To reiterate, I wanted a nice clean piece of code that generated a list of prime numbers. I already know how to do it a bunch of different ways, but I'm prone to writing code that isn't as clear as it could be. In this thread a few good options have been proposed:</p>\n\n<ul>\n<li>A nicer version of what I originally had (Peter Smit, jmservera and Rekreativc)</li>\n<li>A very clean implementation of the sieve of Eratosthenes (starblue)</li>\n<li>Use Java's <code>BigInteger</code>s and <code>nextProbablePrime</code> for very simple code, although I can't imagine it being particularly efficient (dfa)</li>\n<li>Use LINQ to lazily generate the list of primes (Maghis)</li>\n<li>Put lots of primes in a text file and read them in when necessary (darin)</li>\n</ul>\n\n<p><strong>Edit 2</strong>: I've <a href=\"http://stackoverflow.com/questions/1042902/most-elegant-way-to-generate-prime-numbers/1072205#1072205\">implemented in C#</a> a couple of the methods given here, and another method not mentioned here. They all find the first <em>n</em> primes effectively (and I have a <a href=\"http://stackoverflow.com/questions/1042717/is-there-a-way-to-find-the-approximate-value-of-the-nth-prime/1069023#1069023\">decent method</a> of finding the limit to provide to the sieves).</p>\n    ","a":"\n<p>Many thanks to all who gave helpful answers. Here are my implementations of a few different methods of finding the first <em>n</em> primes in C#. The first two methods are pretty much what was posted here. (The posters names are next to the title.) I plan on doing the sieve of Atkin sometime, although I suspect it won't be quite as simple as the methods here currently. If anybody can see any way of improving any of these methods I'd love to know :-)</p>\n\n<p><strong>Standard Method</strong> (<a href=\"http://stackoverflow.com/questions/1042902/most-elegant-way-to-generate-prime-numbers/1043007#1043007\">Peter Smit</a>, <a href=\"http://stackoverflow.com/questions/1042902/most-elegant-way-to-generate-prime-numbers/1043002#1043002\">jmservera</a>, <a href=\"http://stackoverflow.com/questions/1042902/most-elegant-way-to-generate-prime-numbers/1042969#1042969\">Rekreativc</a>)</p>\n\n<p>The first prime number is 2. Add this to a list of primes. The next prime is the next number that is not evenly divisible by any number on this list.</p>\n\n<pre><code>public static List&lt;int&gt; GeneratePrimesNaive(int n)\n{\n    List&lt;int&gt; primes = new List&lt;int&gt;();\n    primes.Add(2);\n    int nextPrime = 3;\n    while (primes.Count &lt; n)\n    {\n        int sqrt = (int)Math.Sqrt(nextPrime);\n        bool isPrime = true;\n        for (int i = 0; (int)primes[i] &lt;= sqrt; i++)\n        {\n            if (nextPrime % primes[i] == 0)\n            {\n                isPrime = false;\n                break;\n            }\n        }\n        if (isPrime)\n        {\n            primes.Add(nextPrime);\n        }\n        nextPrime += 2;\n    }\n    return primes;\n}\n</code></pre>\n\n<p>This has been optimised by only testing for divisibility up to the square root of the number being tested; and by only testing odd numbers. This can be further optimised by testing only numbers of the form <code>6k+[1, 5]</code>, or <code>30k+[1, 7, 11, 13, 17, 19, 23, 29]</code> or <a href=\"http://en.wikipedia.org/wiki/Prime_number#Examples_and_first_properties\">so on</a>.</p>\n\n<p><strong>Sieve of Eratosthenes</strong> (<a href=\"http://stackoverflow.com/questions/1042902/most-elegant-way-to-generate-prime-numbers/1043247#1043247\">starblue</a>)</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes\">This finds all the primes to <em>k</em></a>. To make a list of the first <em>n</em> primes, we first need to approximate value of the *n*th prime. The following method, <a href=\"http://stackoverflow.com/questions/1042717/is-there-a-way-to-find-the-approximate-value-of-the-nth-prime/1069023#1069023\">as described here</a>, does this.</p>\n\n<pre><code>public static int ApproximateNthPrime(int nn)\n{\n    double n = (double)nn;\n    double p;\n    if (nn &gt;= 7022)\n    {\n        p = n * Math.Log(n) + n * (Math.Log(Math.Log(n)) - 0.9385);\n    }\n    else if (nn &gt;= 6)\n    {\n        p = n * Math.Log(n) + n * Math.Log(Math.Log(n));\n    }\n    else if (nn &gt; 0)\n    {\n        p = new int[] { 2, 3, 5, 7, 11 }[nn - 1];\n    }\n    else\n    {\n        p = 0;\n    }\n    return (int)p;\n}\n\n// Find all primes up to and including the limit\npublic static BitArray SieveOfEratosthenes(int limit)\n{\n    BitArray bits = new BitArray(limit + 1, true);\n    bits[0] = false;\n    bits[1] = false;\n    for (int i = 0; i * i &lt;= limit; i++)\n    {\n        if (bits[i])\n        {\n            for (int j = i * i; j &lt;= limit; j += i)\n            {\n                bits[j] = false;\n            }\n        }\n    }\n    return bits;\n}\n\npublic static List&lt;int&gt; GeneratePrimesSieveOfEratosthenes(int n)\n{\n    int limit = ApproximateNthPrime(n);\n    BitArray bits = SieveOfEratosthenes(limit);\n    List&lt;int&gt; primes = new List&lt;int&gt;();\n    for (int i = 0, found = 0; i &lt; limit &amp;&amp; found &lt; n; i++)\n    {\n        if (bits[i])\n        {\n            primes.Add(i);\n            found++;\n        }\n    }\n    return primes;\n}\n</code></pre>\n\n<p><strong>Sieve of Sundaram</strong></p>\n\n<p>I only discovered <a href=\"http://en.wikipedia.org/wiki/Sieve_of_Sundaram\">this sieve</a> recently, but it can be implemented quite simply. My implementation isn't as fast as the sieve of Eratosthenes, but it is significantly faster than the naive method.</p>\n\n<pre><code>public static BitArray SieveOfSundaram(int limit)\n{\n    limit /= 2;\n    BitArray bits = new BitArray(limit + 1, true);\n    for (int i = 1; 3 * i + 1 &lt; limit; i++)\n    {\n        for (int j = 1; i + j + 2 * i * j &lt;= limit; j++)\n        {\n            bits[i + j + 2 * i * j] = false;\n        }\n    }\n    return bits;\n}\n\npublic static List&lt;int&gt; GeneratePrimesSieveOfSundaram(int n)\n{\n    int limit = ApproximateNthPrime(n);\n    BitArray bits = SieveOfSundaram(limit);\n    List&lt;int&gt; primes = new List&lt;int&gt;();\n    primes.Add(2);\n    for (int i = 1, found = 1; 2 * i + 1 &lt;= limit &amp;&amp; found &lt; n; i++)\n    {\n        if (bits[i])\n        {\n            primes.Add(2 * i + 1);\n            found++;\n        }\n    }\n    return primes;\n}\n</code></pre>\n    "},{"t":"Implement a queue in which push_rear(), pop_front() and get_min() are all constant time operations","l":"http://stackoverflow.com/questions/4802038/implement-a-queue-in-which-push-rear-pop-front-and-get-min-are-all-consta","q":"\n\n<p>I came across this question:\n<strong>Implement a queue in which push_rear(), pop_front() and get_min() are all constant time operations.</strong></p>\n\n<p>I initially thought of using a min-heap data structure which has O(1) complexity for a get_min(). But push_rear() and pop_front() would be O(log(n)).</p>\n\n<p>Does anyone know what would be the best way to implement such a queue which has O(1) push(), pop() and min()?</p>\n\n<p>I googled about this, and wanted to point out this <a href=\"http://groups.google.com/group/algogeeks/browse_thread/thread/157129d83d284e07?pli=1\">Algorithm Geeks thread</a>. But it seems that none of the solutions follow constant time rule for all 3 methods: push(), pop() and min().</p>\n\n<p>Thanks for all the suggestions.</p>\n    ","a":"\n<p>You can implement a stack with O(1) pop(), push() and get_min(): just store the current minimum together with each element. So, for example, the stack <code>[4,2,5,1]</code> (1 on top) becomes <code>[(4,4), (2,2), (5,2), (1,1)]</code>.</p>\n\n<p>Then you can <a href=\"http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15750-s03/www/amortized-analysis.txt\">use two stacks to implement the queue</a>. Push to one stack, pop from another one; if the second stack is empty during the pop, move all elements from the first stack to the second one.</p>\n\n<p>E.g for a <code>pop</code> request, moving all the elements from first stack <code>[(4,4), (2,2), (5,2), (1,1)]</code>, the second stack would be <code>[(1,1), (5,1), (2,1), (4,1)]</code>. and now return top element from second stack.</p>\n\n<p>To find the minimum element of the queue, look at the smallest two elements of the individual min-stacks, then take the minimum of those two values.  (Of course, there's some extra logic here is case one of the stacks is empty, but that's not too hard to work around).</p>\n\n<p>It will have O(1) <code>get_min()</code> and <code>push()</code> and amortized O(1) <code>pop()</code>. </p>\n    "},{"t":"How can I measure the similarity between two images?","l":"http://stackoverflow.com/questions/25977/how-can-i-measure-the-similarity-between-two-images","q":"\n\n<p>I would like to compare a screenshot of one application (could be a Web page) with a previously taken screenshot to determine whether the application is displaying itself correctly. I don't want an exact match comparison, because the aspect could be slightly different (in the case of a Web app, depending on the browser, some element could be at a slightly different location). It should give a measure of how similar are the screenshots.</p>\n\n<p>Is there a library / tool that already does that? How would you implement it?</p>\n    ","a":"\n<p>This depends entirely on how smart you want the algorithm to be.</p>\n\n<p>For instance, here are some issues:</p>\n\n<ul>\n<li>cropped images vs. an uncropped image</li>\n<li>images with a text added vs. another without</li>\n<li>mirrored images</li>\n</ul>\n\n<p>The easiest and simplest <em>algorithm</em> I've seen for this is just to do the following steps to each image:</p>\n\n<ol>\n<li>scale to something small, like 64x64 or 32x32, disregard aspect ratio, use a combining scaling algorithm instead of nearest pixel</li>\n<li>scale the color ranges so that the darkest is black and lightest is white</li>\n<li>rotate and flip the image so that the lighest color is top left, and then top-right is next darker, bottom-left is next darker (as far as possible of course)</li>\n</ol>\n\n<p><strong>Edit</strong> A <em>combining scaling algorithm</em> is one that when scaling 10 pixels down to one will do it using a function that takes the color of all those 10 pixels and combines them into one. Can be done with algorithms like averaging, mean-value, or more complex ones like bicubic splines.</p>\n\n<p>Then calculate the mean distance pixel-by-pixel between the two images.</p>\n\n<p>To look up a possible match in a database, store the pixel colors as individual columns in the database, index a bunch of them (but not all, unless you use a very small image), and do a query that uses a range for each pixel value, ie. every image where the pixel in the small image is between -5 and +5 of the image you want to look up.</p>\n\n<p>This is easy to implement, and fairly fast to run, but of course won't handle most advanced differences. For that you need much more advanced algorithms.</p>\n    "},{"t":"Circular lock-free buffer","l":"http://stackoverflow.com/questions/871234/circular-lock-free-buffer","q":"\n\n<p>I'm in the process of designing a system which connects to one or more stream of data feeds and do some analysis on the data than trigger events based on the result. In a typical multi-threaded producer/consumer setup, I will have multiple producer threads putting data into a queue, and multiple consumer threads reading the data, and the consumers are only interested in the latest data point plus n number of points. The producer threads will have to block if slow consumer can not keep up, and of course consumer threads will block when there are no unprocessed updates. Using a typical concurrent queue with reader/writer lock will work nicely but the rate of data coming in could be huge, so i wanted to reduce my locking overhead especially writer locks for the producers. I think a circular lock-free buffer is what I needed. </p>\n\n<p><strong>Now two questions:</strong></p>\n\n<ol>\n<li><p>Is circular lock-free buffer the answer?</p></li>\n<li><p>If so, before i roll my own, do you know any public implementation that will fit my need?</p></li>\n</ol>\n\n<p>Any pointers in implementing a circular lock-free buffer are always welcome.</p>\n\n<p>BTW, doing this in C++ on Linux.</p>\n\n<p><strong>Some additional info:</strong></p>\n\n<p>The response time is critical for my system. Ideally the consumer threads will want to see any updates coming in as soon as possible because an extra 1 millisecond delay could make the system worthless, or worth a lot less.</p>\n\n<p>The design idea I'm leaning toward is a semi-lock-free circular buffer where the producer thread put data in the buffer as fast as it can, let's call the head of the buffer A, without blocking unless the buffer is full, when A meets the end of buffer Z. Consumer threads will each hold two pointers to the circular buffer, P and P<sub>n</sub>, where P is the thread's local buffer head, and P<sub>n</sub> is nth item after P. Each consumer thread will advance its P and P<sub>n</sub> once it finish processing current P and the end of buffer pointer Z is advanced with the slowest P<sub>n</sub>. When P catch up to A, which means no more new update to process, the consumer spins and do busy wait for A to advance again. If consumer thread spin for too long, it can be put to sleep and wait for a condition variable, but i'm okay with consumer taking up CPU cycle waiting for update because that does not increase my latency (I'll have more CPU cores than threads). Imagine you have a circular track, and the producer is running in front of a bunch of consumers, the key is to tune the system so that the producer is usually runing just a few step ahead of the consumers, and most of these operation can be done using lock-free techniques. I understand getting the details of the implementation right is not easy...okay, very hard, that's why I want to learn from others' mistakes before making a few of my own. </p>\n    ","a":"\n<p>The term of art for what you want is a <strong>lock-free queue</strong>.  There's an <a href=\"http://www.audiomulch.com/~rossb/code/lockfree/\">excellent set of notes with links to code and papers</a> by Ross Bencina.  The guy whose work I trust the most is <a href=\"http://www.cs.brown.edu/people/mph/\">Maurice Herlihy</a> (for Americans, he pronounces his first name like \"Morris\").</p>\n    "},{"t":"Explain Merkle Trees for use in Eventual Consistency","l":"http://stackoverflow.com/questions/5486304/explain-merkle-trees-for-use-in-eventual-consistency","q":"\n\n<p><a href=\"http://en.wikipedia.org/wiki/Hash_tree\">Merkle Trees</a> are used as an anti-entropy mechanism in several distributed, replicated key/value stores:</p>\n\n<ul>\n<li><a href=\"http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Dynamo</a></li>\n<li><a href=\"http://doc.erlagner.org/riak_core/merkerl.html\">Riak</a></li>\n<li><a href=\"http://wiki.apache.org/cassandra/AntiEntropy\">Cassandra</a></li>\n</ul>\n\n<p>No doubt an anti-entropy mechanism is A Good Thing - transient failures just happen, in production.\nI'm just not sure I understand why Merkle <em>Trees</em> are the popular approach.</p>\n\n<ul>\n<li><p>Sending a complete Merkle tree to a peer involves sending the local key-space to that peer, along with\nhashes of each key value, stored in the lowest levels of the tree.</p></li>\n<li><p>Diffing a Merkle tree sent from a peer requires having a Merkle tree of your own.</p></li>\n</ul>\n\n<p>Since both peers must already have a sorted key / value-hash space on hand, why not do a linear merge to detect discrepancies?</p>\n\n<p>I'm just not convinced that the tree structure provides any kind of savings when you factor in upkeep costs, and the fact\nthat <em>linear passes over the tree leaves are already being done just to serialize the representation over the wire</em>.</p>\n\n<p>To ground this out, a straw-man alternative might be to have nodes exchange arrays of hash digests,\nwhich are incrementally updated and bucketed by modulo ring-position.</p>\n\n<p>What am I missing?</p>\n    ","a":"\n<p>Merkle trees limit the amount of data transferred when synchronizing. The general assumptions are:</p>\n\n<ol>\n<li>Network I/O is more expensive than local I/O + computing the hashes.</li>\n<li>Transferring the entire sorted key space is more expensive than progressively limiting the comparison over several steps.</li>\n<li>The key spaces have fewer discrepancies than similarities.</li>\n</ol>\n\n<p>A Merkle Tree exchange would look like this:</p>\n\n<ol>\n<li>Start with the root of the tree (a list of one hash value).</li>\n<li>The origin sends the list of hashes at the current level.</li>\n<li>The destination diffs the list of hashes against its own and then\n requests subtrees that are different. If there are no\n differences, the request can terminate.</li>\n<li>Repeat steps 2 and 3 until leaf nodes are reached.</li>\n<li>The origin sends the values of the keys in the resulting set.</li>\n</ol>\n\n<p>In the typical case, the complexity of synchronizing the key spaces will be log(N). Yes, at the extreme, where there are no keys in common, the operation will be equivalent to sending the entire sorted list of hashes, O(N). One could amortize the expense of building Merkle trees by building them dynamically as writes come in and keeping the serialized form on disk.</p>\n\n<p>I can't speak to how Dynamo or Cassandra use Merkle trees, but Riak stopped using them for intra-cluster synchronization (hinted handoff and read-repair are sufficient in most cases). We have plans to add them back later after some internal architectural bits have changed.</p>\n\n<p>For more information about Riak, we encourage you to join the mailing list: <a href=\"http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com\">http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com</a></p>\n    "},{"t":"Non-intersecting line segments while minimizing the cumulative length","l":"http://stackoverflow.com/questions/9863108/non-intersecting-line-segments-while-minimizing-the-cumulative-length","q":"\n\n<p>I would like to find a better algorithm to solve the following problem:</p>\n\n<p>There are <em>N</em> starting points (purple) and <em>N</em> target points (green) in 2D. I want an algorithm that connects starting points to target points by a line segment (brown) without any of these segments intersecting (red) and while minimizing the cumulative length of all segments.</p>\n\n<p>My first effort in C++ was permuting all possible states, find intersection-free states, and among those the state with minimum total segment length  <em>O(n!)</em> . But I think there has to be a better way.</p>\n\n<p><img src=\"http://i.stack.imgur.com/h17NF.png\" alt=\"enter image description here\"></p>\n\n<p>Any idea? Or good keywords for search?</p>\n    ","a":"\n<p>This is <a href=\"http://maven.smith.edu/~orourke/TOPP/P6.html\">Minimum Euclidean Matching in 2D</a>. The link contains a bibliography of what's known about this problem. Given that you want to minimize the total length, the non-intersection constraint is redundant, as the length of any pair of segments that cross can be reduced by uncrossing them.</p>\n    "},{"t":"how to check if a string looks randomized, or human generated and pronouncable?","l":"http://stackoverflow.com/questions/1164186/how-to-check-if-a-string-looks-randomized-or-human-generated-and-pronouncable","q":"\n\n<p>For the purpose of identifying [possible] bot-generated usernames.</p>\n\n<p>Suppose you have a username like \"bilbomoothof\" .. it may be nonsense, but it still contains pronouncable sounds and so appears human-generated.</p>\n\n<p>I accept that it could have been randomly generated from a dictionary of syllables, or word parts, but let's assume for a moment that the bot in question is a bit rubbish.</p>\n\n<ol>\n<li>Suppose you have a username like\n\"sdfgbhm342r3f\", to a human this is\nclearly a random string. But can\nthis be identified programatically?</li>\n<li>Are there any algorithms available\n(similar to Soundex, etc..) that can\nidentify pronounceable sounds within\na string like this?</li>\n</ol>\n\n<p>Solutions applicable in PHP/MySQL most appreciated.</p>\n    ","a":"\n<p>I guess you could think of something like that if you could restrict yourself to pronounceable sounds <strong>in english</strong>. For me (I am French), words like <em>szczepan</em> or <em>wawrzyniec</em> are unpronounceable and certainly have a certain randomness.</p>\n\n<p>But they are actually <a href=\"http://www.rootsweb.ancestry.com/~polwgw/namelist.html\" rel=\"nofollow\">Polish first names</a> (meaning <em>steven</em> and <em>lawrence</em>)...</p>\n    "},{"t":"Test if a number is fibonacci","l":"http://stackoverflow.com/questions/2432669/test-if-a-number-is-fibonacci","q":"\n\n<p>I know how to make the list of the Fibonacci numbers, but i don't know how can i test if a given number belongs to the fibonacci list - one way that comes in mind is generate the list of fib. numbers up to that number and see if it belongs to the array, but there's got to be another, simpler and faster method.</p>\n\n<p>Any ideas ?</p>\n    ","a":"\n<p>A very nice test is that N is a Fibonacci number if and only if <code>5 N^2 + 4</code> or <code>5N^2 – 4</code> is a square number.  For ideas on how to efficiently test that a number is square refer to the <a href=\"http://stackoverflow.com/questions/295579/fastest-way-to-determine-if-an-integers-square-root-is-an-integer\">SO discussion</a>.</p>\n\n<p>Hope this helps</p>\n    "},{"t":"Explain how finding cycle start node in cycle linked list work?","l":"http://stackoverflow.com/questions/2936213/explain-how-finding-cycle-start-node-in-cycle-linked-list-work","q":"\n\n<p>I understand that Tortoise and Hare's meeting concludes the existence of loop, but how does moving tortoise to beginning of linked list while keeping the hare at meeting place, followed by moving both one step at a time make them meet at starting point of cycle?</p>\n    ","a":"\n<p>This is <a href=\"http://en.wikipedia.org/wiki/Cycle_detection#Tortoise_and_hare\">Floyd's algorithm for cycle detection</a>.  You are asking about the second phase of the algorithm -- once you've found a node that's part of a cycle, how does one find the <em>start</em> of the cycle?</p>\n\n<p>In the first part of Floyd's algorithm, the hare moves two steps for every step of the tortoise.  If the tortoise and hare ever meet, there is a cycle, and the meeting point is part of the cycle, but not necessarily the first node in the cycle.</p>\n\n<p>When the tortoise and hare meet, we have found the smallest i (the number of steps taken by the tortoise) such that X<sub>i</sub> = X<sub>2i</sub>. Let mu represent the number of steps to get from X<sub>0</sub> to the start of the cycle, and let lambda represent the length of the cycle.  Then i = mu + a*lambda, and 2i = mu + b*lambda, where a and b are integers denoting how many times the tortoise and hare went around the cycle. Subtracting\nthe first equation from the second gives i = (b-a)*lambda, so i is an integer multiple\nof lambda.  <strong>Therefore, X<sub>i + mu</sub> = X<sub>mu</sub></strong>.  X<sub>i</sub> represents the meeting point of the tortoise and hare.  If you move the tortoise back to the starting node X<sub>0</sub>, and let the tortoise and hare continue at the same speed, after mu additional steps the tortoise will have reached X<sub>mu</sub>, and the hare will have reached X<sub>i + mu</sub> = X<sub>mu</sub>, so the second meeting point denotes the start of the cycle.</p>\n    "},{"t":"Fast String Hashing Algorithm with low collision rates with 32 bit integer","l":"http://stackoverflow.com/questions/114085/fast-string-hashing-algorithm-with-low-collision-rates-with-32-bit-integer","q":"\n\n<p>I have lots of unrelated named things that I'd like to do quick searches against. An \"aardvark\" is always an \"aardvark\" everywhere, so hashing the string and reusing the integer would work well to speed up comparisons. The entire set of names is unknown (and changes over time). What is a fast string hashing algorithm that will generate small (32 or 16) bit values and have a low collision rate?</p>\n\n<p>I'd like to see an optimized implementation specific to C/C++.</p>\n    ","a":"\n<p>One of the <a href=\"http://isthe.com/chongo/tech/comp/fnv/\">FNV variants</a> should meet your requirements. They're fast, and produce fairly evenly distributed outputs.</p>\n    "},{"t":"Fast stable sorting algorithm implementation in javascript","l":"http://stackoverflow.com/questions/1427608/fast-stable-sorting-algorithm-implementation-in-javascript","q":"\n\n<p>I'm looking to sort an array of about 200-300 objects, sorting on a specific key and a given order (asc/desc). The order of results must be consistent and stable.</p>\n\n<p>What would be the best algorithm to use, and could you provide an example of it's implementation in javascript?</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p>It is possible to get a stable sorting from a non-stable sort function.</p>\n\n<p>Before sorting you get the position of all the elements.\nIn your sort condition, if both elements are equal, then you sort by the position.</p>\n\n<p>Tada! You've got a stable sort.</p>\n\n<p>I've written an article about it on my blog if you want to know more about this technique and how to implement it: <a href=\"http://blog.vjeux.com/2010/javascript/javascript-sorting-table.html\">http://blog.vjeux.com/2010/javascript/javascript-sorting-table.html</a></p>\n    "},{"t":"javascript data structures library","l":"http://stackoverflow.com/questions/5909452/javascript-data-structures-library","q":"\n\n<p>I'd like to ask for recommendation of JavaScript library/libraries that supply an implementation of some basic data structures such as a priority queue, map with arbitrary keys, tries, graphs, etc. along with some algorithms that operate on them.</p>\n\n<p>I'm mostly interested in: </p>\n\n<ul>\n<li>The set of features covered,</li>\n<li>Flexibility of the solution - this mostly applies to graphs. For example do I have to use a supplied graph implementation,</li>\n<li>Use of functional features of the language - again it sometimes gives greater flexibility,</li>\n<li>Performance of the implementation</li>\n</ul>\n\n<p><strong>EDIT</strong></p>\n\n<p>Ok, I'd like to point out that I'm aware that it's possible to implement using js the following data structures:</p>\n\n<ul>\n<li>A map, if key values are either strings or numbers,</li>\n<li>A set, (using a map implementation),</li>\n<li>A queue, although as was pointed out below, it's inefficient on some browsers,</li>\n</ul>\n\n<p>At the moment I'm mostly interested in priority queues (not to confuse with regular queues), graph implementations that aren't very intrusive as to the format of the input graph. For example they could use callbacks to traverse the structure of the graph rather than access some concrete properties with fixed names.</p>\n    ","a":"\n<p>I recommend to use Closure Library (especially with closure compiler).</p>\n\n<p>Here you have a library with data structures:\n<a href=\"http://docs.closure-library.googlecode.com/git/namespace_goog_structs.html\" rel=\"nofollow\">http://docs.closure-library.googlecode.com/git/namespace_goog_structs.html</a>\nThe library contains:</p>\n\n<pre><code>goog.structs.AvlTree\ngoog.structs.CircularBuffer\ngoog.structs.Heap\ngoog.structs.InversionMap\ngoog.structs.LinkedMap\ngoog.structs.Map\ngoog.structs.PriorityQueue\ngoog.structs.Set\n</code></pre>\n\n<p>As example you can use unit test:\n<a href=\"http://code.google.com/p/closure-library/source/browse/trunk/closure/goog/structs/priorityqueue_test.html\" rel=\"nofollow\">http://code.google.com/p/closure-library/source/browse/trunk/closure/goog/structs/priorityqueue_test.html</a></p>\n\n<p>If you need to work on arrays, there's also an array lib:\n<a href=\"http://docs.closure-library.googlecode.com/git/local_closure_goog_array_array.js.html\" rel=\"nofollow\">http://docs.closure-library.googlecode.com/git/local_closure_goog_array_array.js.html</a></p>\n    "},{"t":"Combine Gyroscope and Accelerometer Data","l":"http://stackoverflow.com/questions/1586658/combine-gyroscope-and-accelerometer-data","q":"\n\n<p>I am building a balancing robot using the Lego Mindstorm's NXT system. I am using two sensors from HiTechnic, the first being an Accelerometer and the second being a Gyroscope. I've successfully filtered out noise from both sensors and derived angles for both in a range between -90 and 90 degrees, with 0 degrees being perfectly balanced.</p>\n\n<p>My next challenge is to combine both of the sensor values to correct for the Gyroscope's drift over time. Below is an example graph I created from actual data to demonstrate the drift from the gyroscope:</p>\n\n<p><img src=\"http://i.stack.imgur.com/AHx4r.png\" alt=\"enter image description here\"></p>\n\n<p>The most commonly used approach I've seen to make combining these sensors rock solid is by using a Kalman filter. However, I'm not an expert in calculus and I really don't understand mathematical symbols, I do understand math in source code though.</p>\n\n<p>I'm using RobotC (which is like any other C derivative) and would really appreciate if someone can give me examples of how to accomplish this in C.</p>\n\n<p>Thank you for your help!</p>\n\n<p><strong>SOLUTION RESULTS:</strong></p>\n\n<p>Alright, kersny solved my problem by introducing me to complementary filters. This is a graph illustrating my results:</p>\n\n<p><em>Result #1</em></p>\n\n<p><img src=\"http://i.stack.imgur.com/MqRid.png\" alt=\"enter image description here\"></p>\n\n<p><em>Result #2</em></p>\n\n<p><img src=\"http://i.stack.imgur.com/mePBt.png\" alt=\"enter image description here\"></p>\n\n<p>As you can see, the filter corrects for gyroscopic drift and combines both signals into a single smooth signal.</p>\n\n<p><strong>Edit:</strong> Since I was fixing the broken images anyways, I thought it would be fun to show the rig I used to generate this data:</p>\n\n<p><img src=\"http://i.stack.imgur.com/hWbBh.jpg\" alt=\"enter image description here\"></p>\n    ","a":"\n<p>Kalman Filters are great and all, but I find the Complementary Filter much easier to implement with similar results. The best articles that I have found for coding a Complementary Filter are <a href=\"http://web.archive.org/web/20091121085323/http://www.mikroquad.com/bin/view/Research/ComplementaryFilter\" rel=\"nofollow\">this wiki</a> (along with <a href=\"http://wiki.orbswarm.com/index.php?title=IMU\" rel=\"nofollow\">this article</a> about converting sensors to Engineering units) and a PDF in the zip file on <a href=\"http://web.mit.edu/first/segway/#misc\" rel=\"nofollow\">this page</a> (Under Technical Documentation, I believe the file name in the zip is filter.pdf); </p>\n\n<p>PS. If your stuck on a Kalman Filter, <a href=\"http://diydrones.com/profiles/blogs/705844:BlogPost:23188\" rel=\"nofollow\">here</a> is some C-syntax code for the Arduino that implements it.</p>\n    "},{"t":"Counting inversions in an array","l":"http://stackoverflow.com/questions/337664/counting-inversions-in-an-array","q":"\n\n<p>I'm designing an algorithm to do the following: Given array <code>A[1... n]</code>, for every <code>i &lt; j</code>, find all inversion pairs such that <code>A[i] &gt; A[j]</code>. I'm using merge sort and copying array A to array B and then comparing the two arrays, but I'm having a difficult time seeing how I can use this to find the number of inversions. Any hints or help would be greatly appreciated.</p>\n    ","a":"\n<p>The only advice I could give to this (which looks suspiciously like a homework question ;)   ) is to first do it manually with a small set of numbers (e.g. 5), and then write down the steps you took to solve the problem.</p>\n\n<p>This should allow you to figure out a generic solution you can use to write the code.</p>\n    "},{"t":"How to implement tag system","l":"http://stackoverflow.com/questions/1810356/how-to-implement-tag-system","q":"\n\n<p>I was wondering what the best way is to implement a tag system, like the one used on SO. I was thinking of this but I can't come up with a good scalable solution.</p>\n\n<p>I was thinking of having a basic 3 table solution: having a <code>tags</code> table, an <code>articles</code> tables and a <code>tag_to_articles</code> table.</p>\n\n<p>Is this the best solution to this problem, or are there alternatives? Using this method the table would get extremely large in time, and for searching this is not too efficient I assume. On the other hand it is not that important that the query executes fast.</p>\n    ","a":"\n<p>I believe you'll find interesting this blog post: <a href=\"http://tagging.pui.ch/post/37027745720/tags-database-schemas\">Tags: Database schemas</a></p>\n\n<blockquote>\n  <p>The Problem: You want to have a database schema where you can tag a\n  bookmark (or a blog post or whatever) with as many tags as you want.\n  Later then, you want to run queries to constrain the bookmarks to a\n  union or intersection of tags. You also want to exclude (say: minus)\n  some tags from the search result.</p>\n</blockquote>\n\n<h2>“MySQLicious” solution</h2>\n\n<p>In this solution, the schema has got just one table, it is denormalized. This type is called “MySQLicious solution” because MySQLicious imports del.icio.us data into a table with this structure.</p>\n\n<p><img src=\"http://i.stack.imgur.com/qeLsx.png\" alt=\"enter image description here\"><img src=\"http://i.stack.imgur.com/6RINy.png\" alt=\"enter image description here\"></p>\n\n<p><strong>Intersection (AND)\nQuery for “search+webservice+semweb”:</strong></p>\n\n<pre><code>SELECT *\nFROM `delicious`\nWHERE tags LIKE \"%search%\"\nAND tags LIKE \"%webservice%\"\nAND tags LIKE \"%semweb%\"\n</code></pre>\n\n<p><strong>Union (OR)\nQuery for “search|webservice|semweb”:</strong></p>\n\n<pre><code>SELECT *\nFROM `delicious`\nWHERE tags LIKE \"%search%\"\nOR tags LIKE \"%webservice%\"\nOR tags LIKE \"%semweb%\"\n</code></pre>\n\n<p><strong>Minus\nQuery for “search+webservice-semweb”</strong></p>\n\n<pre><code>SELECT *\nFROM `delicious`\nWHERE tags LIKE \"%search%\"\nAND tags LIKE \"%webservice%\"\nAND tags NOT LIKE \"%semweb%\"\n</code></pre>\n\n<hr>\n\n<h2>“Scuttle” solution</h2>\n\n<p><a href=\"http://sourceforge.net/projects/scuttle/\">Scuttle</a> organizes its data in two tables. That table “scCategories” is the “tag”-table and has got a foreign key to the “bookmark”-table.</p>\n\n<p><img src=\"http://i.stack.imgur.com/sdPKq.png\" alt=\"enter image description here\"></p>\n\n<p><strong>Intersection (AND)\nQuery for “bookmark+webservice+semweb”:</strong></p>\n\n<pre><code>SELECT b.*\nFROM scBookmarks b, scCategories c\nWHERE c.bId = b.bId\nAND (c.category IN ('bookmark', 'webservice', 'semweb'))\nGROUP BY b.bId\nHAVING COUNT( b.bId )=3\n</code></pre>\n\n<p>First, all bookmark-tag combinations are searched, where the tag is “bookmark”, “webservice” or “semweb” (c.category IN ('bookmark', 'webservice', 'semweb')), then just the bookmarks that have got all three tags searched for are taken into account (HAVING COUNT(b.bId)=3).</p>\n\n<p><strong>Union (OR)\nQuery for “bookmark|webservice|semweb”:</strong>\nJust leave out the HAVING clause and you have union:</p>\n\n<pre><code>SELECT b.*\nFROM scBookmarks b, scCategories c\nWHERE c.bId = b.bId\nAND (c.category IN ('bookmark', 'webservice', 'semweb'))\nGROUP BY b.bId\n</code></pre>\n\n<p><strong>Minus (Exclusion)\nQuery for “bookmark+webservice-semweb”, that is: bookmark AND webservice AND NOT semweb.</strong></p>\n\n<pre><code>SELECT b. *\nFROM scBookmarks b, scCategories c\nWHERE b.bId = c.bId\nAND (c.category IN ('bookmark', 'webservice'))\nAND b.bId NOT\nIN (SELECT b.bId FROM scBookmarks b, scCategories c WHERE b.bId = c.bId AND c.category = 'semweb')\nGROUP BY b.bId\nHAVING COUNT( b.bId ) =2\n</code></pre>\n\n<p><strong>Leaving out the HAVING COUNT leads to the Query for “bookmark|webservice-semweb”.</strong></p>\n\n<hr>\n\n<h2>“Toxi” solution</h2>\n\n<p><a href=\"http://toxi.co.uk/\">Toxi</a> came up with a three-table structure. Via the table “tagmap” the bookmarks and the tags are n-to-m related. Each tag can be used together with different bookmarks and vice versa. This DB-schema is also used by wordpress.\nThe queries are quite the same as in the “scuttle” solution.</p>\n\n<p><img src=\"http://i.stack.imgur.com/GdJC4.png\" alt=\"enter image description here\"></p>\n\n<p><strong>Intersection (AND)\nQuery for “bookmark+webservice+semweb”</strong></p>\n\n<pre><code>SELECT b.*\nFROM tagmap bt, bookmark b, tag t\nWHERE bt.tag_id = t.tag_id\nAND (t.name IN ('bookmark', 'webservice', 'semweb'))\nAND b.id = bt.bookmark_id\nGROUP BY b.id\nHAVING COUNT( b.id )=3\n</code></pre>\n\n<p><strong>Union (OR)\nQuery for “bookmark|webservice|semweb”</strong></p>\n\n<pre><code>SELECT b.*\nFROM tagmap bt, bookmark b, tag t\nWHERE bt.tag_id = t.tag_id\nAND (t.name IN ('bookmark', 'webservice', 'semweb'))\nAND b.id = bt.bookmark_id\nGROUP BY b.id\n</code></pre>\n\n<p><strong>Minus (Exclusion)\nQuery for “bookmark+webservice-semweb”, that is: bookmark AND webservice AND NOT semweb.</strong></p>\n\n<pre><code>SELECT b. *\nFROM bookmark b, tagmap bt, tag t\nWHERE b.id = bt.bookmark_id\nAND bt.tag_id = t.tag_id\nAND (t.name IN ('Programming', 'Algorithms'))\nAND b.id NOT IN (SELECT b.id FROM bookmark b, tagmap bt, tag t WHERE b.id = bt.bookmark_id AND bt.tag_id = t.tag_id AND t.name = 'Python')\nGROUP BY b.id\nHAVING COUNT( b.id ) =2\n</code></pre>\n\n<p><strong>Leaving out the HAVING COUNT leads to the Query for “bookmark|webservice-semweb”.</strong></p>\n    "},{"t":"One-way flight trip problem","l":"http://stackoverflow.com/questions/2991950/one-way-flight-trip-problem","q":"\n\n<p>You are going on a one-way indirect flight trip that includes <strike>billions</strike> <em>an unknown very large number</em> of transfers. </p>\n\n<ul>\n<li>You are not stopping twice in the same airport. </li>\n<li>You have 1 ticket for each part of your trip. </li>\n<li>Each ticket contains <strong>src</strong> and <strong>dst</strong> airport. </li>\n<li>All the tickets you have are randomly sorted. </li>\n<li>You forgot the original departure airport (very first src) and your destination (last dst).</li>\n</ul>\n\n<p>Design an algorithm to reconstruct your trip with minimum big-<strong>O</strong> complexity.</p>\n\n<hr>\n\n<p>Attempting to solve this problem I have started to use a <a href=\"http://en.wikipedia.org/wiki/Symmetric_difference\">symmetric difference</a> of two sets, Srcs and Dsts:</p>\n\n<p>1)Sort all src keys in array Srcs <br>\n2)Sort all dst keys in array Dsts <br>\n3)Create an union set of both arrays to find non-duplicates - they are your first src and last dst <br>\n4)Now, having the starting point, traverse both arrays using the binary search.</p>\n\n<p>But I suppose there must be another more effective method.</p>\n    ","a":"\n<p>Construct a hashtable and add each airport into the hash table. </p>\n\n<p><code>&lt;key,value&gt; = &lt;airport, count&gt;</code></p>\n\n<p>Count for the airport increases if the airport is either the source or the destination. So for every airport the count will be 2 ( 1 for src and 1 for dst)  except for the source and the destination of your trip which will have the count as 1.</p>\n\n<p>You need to look at each ticket at least once. So complexity is O(n).</p>\n    "},{"t":"What is the difference between tree depth and height?","l":"http://stackoverflow.com/questions/2603692/what-is-the-difference-between-tree-depth-and-height","q":"\n\n<p>This is a simple question from algorithms theory. The difference between them is that in one case you count number of nodes and in other number of edges on the shortest path between root and concrete node. Which is which?</p>\n    ","a":"\n<p>I learned it as a <em>node</em> having a depth and height:</p>\n\n<ul>\n<li><p>The <strong>depth</strong> of a node is the number of edges from the node to the tree's root node.<br>A root node will have a depth of 0.</p></li>\n<li><p>The <strong>height</strong> of a node is the number of edges on the <em>longest path</em> from the node to a leaf.<br>A leaf node will have a height of 0.</p></li>\n</ul>\n\n<p><img src=\"http://i.stack.imgur.com/RHEqu.png\" alt=\"A tree, with height and depth of each node\"></p>\n    "},{"t":"Given Prime Number N, Compute the Next Prime?","l":"http://stackoverflow.com/questions/4475996/given-prime-number-n-compute-the-next-prime","q":"\n\n<p>A coworker just told me that the C# Dictionary collection resizes by prime numbers for arcane reasons relating to hashing. And my immediate question was, \"how does it know what the next prime is? do they story a giant table or compute on the fly? that's a scary non-deterministic runtime on an insert causing a resize\"</p>\n\n<p>So my question is, given N, which is a prime number, what is the most efficient way to compute the next prime number?</p>\n    ","a":"\n<p>The <a href=\"http://en.wikipedia.org/wiki/Prime_gap\">gaps between consecutive prime numbers</a> is known to be quite small, with the first gap of over 100 occurring for prime number 370261. That means that even a simple brute force will be fast enough in most circumstances, taking O(ln(p)*sqrt(p)) on average.</p>\n\n<p>For p=10,000 that's O(921) operations. Bearing in mind we'll be performing this once every O(ln(p)) insertion (roughly speaking), this is well within the constraints of most problems taking on the order of a millisecond on most modern hardware.</p>\n    "},{"t":"How do I check if a directed graph is acyclic?","l":"http://stackoverflow.com/questions/583876/how-do-i-check-if-a-directed-graph-is-acyclic","q":"\n\n<p>How do I check if a directed graph is acyclic? And how is the algorithm called? I would appreciate a reference.</p>\n    ","a":"\n<p>I would try to <a href=\"http://stackoverflow.com/questions/4168/graph-serialization/4577#4577\">sort the graph topologically</a>, and if you can't, then it has cycles.</p>\n    "},{"t":"Why use Dijkstra's Algorithm if Breadth First Search (BFS) can do the same thing faster?","l":"http://stackoverflow.com/questions/3818079/why-use-dijkstras-algorithm-if-breadth-first-search-bfs-can-do-the-same-thing","q":"\n\n<p>Both can be used to find the shortest path from single source. BFS runs in O(E+V), while Dijkstra's runs in O((V+E)*log(V)).</p>\n\n<p>Also, I've seen Dijkstra used a lot like in routing protocols.</p>\n\n<p>Thus, why use Dijkstra's algorithm if BFS can do the same thing faster?</p>\n    ","a":"\n<p>Dijkstra allows assigning distances other than 1 for each step. For example, in routing the distances (or weights) could be assigned by speed, cost, preference, etc. The algorithm then gives you the shortest path from your source to every node in the traversed graph.</p>\n\n<p>Meanwhile BFS basically just expands the search by one “step” (link, edge, whatever you want to call it in your application) on every iteration, which happens to have the effect of finding the smallest <em>number of steps</em> it takes to get to any given node from your source (“root”).</p>\n    "},{"t":"Can XOR of two integers go out of bounds?","l":"http://stackoverflow.com/questions/28320454/can-xor-of-two-integers-go-out-of-bounds","q":"\n\n<p>I had been studying the algorithm for finding lonely integers in an array, and here is the implementation:</p>\n\n<pre><code>int arr[] = {10, 20, 30 ,5, 20, 10,30 };\nint LonelyInteger = 0;\nfor(int i=0; i&lt; 7; i++)\n{\n    LonelyInteger = LonelyInteger ^ arr[i];\n}\n</code></pre>\n\n<p>The eesult is <code>5</code>.</p>\n\n<p>My question is - supposedly the integers (getting generated by the <code>XOR</code> operation) are <strong>too large</strong> due to this operation:</p>\n\n<blockquote>\n  <p>LonelyInteger ^ arr[i]</p>\n</blockquote>\n\n<p>That is, this leads to a potentially large integer which cannot be represented by the datatype say <code>int</code> in this case. My questions are:</p>\n\n<ol>\n<li>Is it even possible that <code>XOR</code> will generate such a large integer value that cannot be stored in the <code>int</code> type?</li>\n<li>If it is not possible that this can happen then is there a proof for this?</li>\n</ol>\n    ","a":"\n<p><code>XOR</code> will never go out of bounds because it combines bits and doesn't create new bits where no bits were set before.</p>\n\n<p>The result <code>5</code> is correct. Look at the binary representation of your value and the <code>XOR</code> result</p>\n\n<pre><code>10    00001010\n20    00010100\n30    00011110\n 5    00000101\n20    00010100\n10    00001010\n30    00011110\n--------------\n      00000101 =&gt; 5\n</code></pre>\n\n<p>An easy help for calculating a result of many <code>XOR</code>ed values is: The result will have a bit set where an odd number of bits are combined, no bit set for even number of bits.</p>\n\n<blockquote>\n  <p>If it is not possible that this can happen then is there a proof for this?</p>\n</blockquote>\n\n<p><code>XOR</code> is equivalent to addition without carry on the individual bits. When you add bits without carry, no overflow can happen and so the <code>int</code> value can't go out of bounds.</p>\n    "},{"t":"B-tree faster than AVL or RedBlack-Tree? [closed]","l":"http://stackoverflow.com/questions/647537/b-tree-faster-than-avl-or-redblack-tree","q":"\n\n<p>I know that performance never is black and white, often one implementation is faster in case X and slower in case Y, etc. but in general - are B-trees faster then AVL or RedBlack-Trees? They are considerably more complex to implement then AVL trees (and maybe even RedBlack-trees?), but are they <em>faster</em> (does their complexity pay off) ?</p>\n\n<p><strong>Edit:</strong> I should also like to add that if they are faster then the equivalent AVL/RedBlack tree (in terms of nodes/content) - <em>why</em> are they faster?</p>\n    ","a":"\n<p>Sean's post (the currently accepted one) is full of nonsense.  Sorry Sean, I don't mean to be rude; I hope I can convince you that my statement is based in fact.</p>\n\n<blockquote>\n  <p>They're totally different in their use cases, so it's not possible to make a comparison.</p>\n</blockquote>\n\n<p>They're both used for maintaining a set of totally ordered items with fast lookup, insertion and deletion.  They have the same interface and the same intention.</p>\n\n<blockquote>\n  <p>RB trees are typically in-memory structures used to provide fast access (ideally O(logN)) to data. [...]</p>\n</blockquote>\n\n<p><em>always</em> O(log n)</p>\n\n<blockquote>\n  <p>B-trees are typically disk-based structures, and so are inherently slower than in-memory data.</p>\n</blockquote>\n\n<p>Nonsense.  When you store search trees on disk, you typically use B-trees.  That much is true.  When you store data on disk, it's slower to access than data in memory.  But a red-black tree stored on disk is <em>also</em> slower than a red-black tree stored in memory.</p>\n\n<p>You're comparing apples and oranges here.  What is really interesting is a comparison of in-memory B-trees and in-memory red-black trees.</p>\n\n<p>[As an aside: B-trees, as opposed to red-black trees, are theoretically efficient in the I/O-model.  I have experimentally tested (and validated) the I/O-model for sorting; I'd expect it to work for B-trees as well.]</p>\n\n<blockquote>\n  <p>B-trees are rarely binary trees, the number of children a node can have is typically a large number.</p>\n</blockquote>\n\n<p>To be clear, the size range of B-tree nodes is a parameter of the tree (in C++, you may want to use an integer value as a template parameter).</p>\n\n<blockquote>\n  <p>The management of the B-tree structure can be quite complicated when the data changes.</p>\n</blockquote>\n\n<p>I remember them to be much simpler to understand (and implement) than red-black trees.</p>\n\n<blockquote>\n  <p>B-tree try to minimize the number of disk accesses so that data retrieval is reasonably deterministic.</p>\n</blockquote>\n\n<p>That much is true.</p>\n\n<blockquote>\n  <p>It's not uncommon to see something like 4 B-tree access necessary to lookup a bit of data in a very database.</p>\n</blockquote>\n\n<p>Got data?</p>\n\n<blockquote>\n  <p>In most cases I'd say that in-memory RB trees are faster.</p>\n</blockquote>\n\n<p>Got data?</p>\n\n<blockquote>\n  <p>Because the lookup is binary it's very easy to find something. B-tree can have multiple children per node, so on each node you have to scan the node to look for the appropriate child. This is an O(N) operation.</p>\n</blockquote>\n\n<p>The size of each node is a fixed parameter, so even if you do a linear scan, it's O(1).  If we big-oh over the size of each node, note that you typically keep the array sorted so it's O(log n).</p>\n\n<blockquote>\n  <p>On a RB-tree it'd be O(logN) since you're doing one comparison and then branching.</p>\n</blockquote>\n\n<p>You're comparing apples and oranges.  The O(log n) is because the height of the tree is at most O(log n), just as it is for a B-tree.</p>\n\n<p>Also, unless you play nasty allocation tricks with the red-black trees, it seems reasonable to conjecture that B-trees have better caching behavior (it accesses an array, not pointers strewn about all over the place, and has less allocation overhead increasing memory locality even more), which might help it in the speed race.</p>\n\n<p>I can point to experimental evidence that B-trees (with size parameters 32 and 64, specifically) are very competitive with red-black trees for small sizes, and outperforms it hands down for even moderately large values of n.  See <a href=\"http://idlebox.net/2007/stx-btree/stx-btree-0.8.3/doxygen-html/speedtest.html\">http://idlebox.net/2007/stx-btree/stx-btree-0.8.3/doxygen-html/speedtest.html</a></p>\n\n<p>B-trees are faster.  Why?  I conjecture that it's due to memory locality, better caching behavior and less pointer chasing (which are, if not the same things, overlapping to some degree).</p>\n    "},{"t":"Reverse the ordering of words in a string","l":"http://stackoverflow.com/questions/1009160/reverse-the-ordering-of-words-in-a-string","q":"\n\n<p>I have this <code>string s1 = \"My name is X Y Z\"</code> and I want to reverse the order of the words so that <code>s1 = \"Z Y X is name My\"</code>. </p>\n\n<p>I can do it using an additional array. I thought hard but is it possible to do it inplace (without using additional data structures) and with the time complexity being O(n)?</p>\n    ","a":"\n<p>Reverse the entire string, then reverse the letters of each individual word.</p>\n\n<p>After the first pass the string will be</p>\n\n<pre><code>s1 = \"Z Y X si eman yM\"\n</code></pre>\n\n<p>and after the second pass it will be</p>\n\n<pre><code>s1 = \"Z Y X is name My\"\n</code></pre>\n    "},{"t":"Best way to find an intersection between two arrays?","l":"http://stackoverflow.com/questions/13270491/best-way-to-find-an-intersection-between-two-arrays","q":"\n\n<p>I faced this problem many times during various situations. It is generic to all programming languages although I am comfortable with C or Java.</p>\n\n<p>Let us consider two arrays (or collections):</p>\n\n<pre><code>char[] A = {'a', 'b', 'c', 'd'};\nchar[] B = {'c', 'd', 'e', 'f'};\n</code></pre>\n\n<p>How do I get the common elements between the two arrays as a new array?\nIn this case, the intersection of array A and B is <code>char[] c = {'c', 'd'}</code>.</p>\n\n<p>I want to avoid the repeated iteration of one array inside the other array which will\nincrease the execution time by (length of A times length of B) which is too much in the case of huge arrays.</p>\n\n<p>Is there any way we could do a single pass in each array to get the common elements?</p>\n    ","a":"\n<p>Since this looks to me like a string algorithm, I'll assume for a moment that its not possible to sort this sequence (hence string) then you can use <a href=\"http://en.wikipedia.org/wiki/Longest_common_subsequence_problem\">Longest Common Sequence algorithm (LCS)</a></p>\n\n<p>Assuming the input size is constant, then the problem has a complexity of O(nxm), (length of the two inputs)</p>\n    "},{"t":"Matrix multiplication: Small difference in matrix size, large difference in timings","l":"http://stackoverflow.com/questions/7905760/matrix-multiplication-small-difference-in-matrix-size-large-difference-in-timi","q":"\n\n<p>I have a matrix multiply code that looks like this:</p>\n\n<pre><code>for(i = 0; i &lt; dimension; i++)\n    for(j = 0; j &lt; dimension; j++)\n        for(k = 0; k &lt; dimension; k++)\n            C[dimension*i+j] += A[dimension*i+k] * B[dimension*k+j];\n</code></pre>\n\n<p>Here, the size of the  matrix is represented by <code>dimension</code>.\nNow, if the size of the matrices is 2000, it takes 147 seconds to run this piece of code, whereas if the size of the matrices is 2048, it takes 447 seconds. So while the difference in no. of multiplications is (2048*2048*2048)/(2000*2000*2000) = 1.073, the difference in the timings is 447/147 = 3. Can someone explain why this happens? I expected it to scale linearly, which does not happen. I am not trying to make the fastest matrix multiply code, simply trying to understand why it happens. </p>\n\n<p>Specs: AMD Opteron dual core node (2.2GHz), 2G RAM, gcc v 4.5.0</p>\n\n<p>Program compiled as <code>gcc -O3 simple.c</code></p>\n\n<p>I have run this on Intel's icc compiler as well, and seen similar results.</p>\n\n<p>EDIT:</p>\n\n<p>As suggested in the comments/answers, I ran the code with dimension=2060 and it takes 145 seconds.</p>\n\n<p>Heres the complete program:</p>\n\n<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;sys/time.h&gt;\n\n/* change dimension size as needed */\nconst int dimension = 2048;\nstruct timeval tv; \n\ndouble timestamp()\n{\n        double t;\n        gettimeofday(&amp;tv, NULL);\n        t = tv.tv_sec + (tv.tv_usec/1000000.0);\n        return t;\n}\n\nint main(int argc, char *argv[])\n{\n        int i, j, k;\n        double *A, *B, *C, start, end;\n\n        A = (double*)malloc(dimension*dimension*sizeof(double));\n        B = (double*)malloc(dimension*dimension*sizeof(double));\n        C = (double*)malloc(dimension*dimension*sizeof(double));\n\n        srand(292);\n\n        for(i = 0; i &lt; dimension; i++)\n                for(j = 0; j &lt; dimension; j++)\n                {   \n                        A[dimension*i+j] = (rand()/(RAND_MAX + 1.0));\n                        B[dimension*i+j] = (rand()/(RAND_MAX + 1.0));\n                        C[dimension*i+j] = 0.0;\n                }   \n\n        start = timestamp();\n        for(i = 0; i &lt; dimension; i++)\n                for(j = 0; j &lt; dimension; j++)\n                        for(k = 0; k &lt; dimension; k++)\n                                C[dimension*i+j] += A[dimension*i+k] *\n                                        B[dimension*k+j];\n\n        end = timestamp();\n        printf(\"\\nsecs:%f\\n\", end-start);\n\n        free(A);\n        free(B);\n        free(C);\n\n        return 0;\n}\n</code></pre>\n    ","a":"\n<p>Here's my wild guess: <strong>cache</strong></p>\n\n<p>It could be that you can fit 2 rows of 2000 <code>double</code>s into the cache. Which is slighly less than the 32kb L1 cache. (while leaving room other necessary things)</p>\n\n<p>But when you bump it up to 2048, it uses the <strong><em>entire</em></strong> cache (and you spill some because you need room for other things)</p>\n\n<p>Assuming the cache policy is LRU, spilling the cache just a tiny bit will cause the entire row to be repeatedly flushed and reloaded into the L1 cache.</p>\n\n<p>The other possibility is cache associativity due to the power-of-two. Though I think that processor is 2-way L1 associative so I don't think it matters in this case. (but I'll throw the idea out there anyway)</p>\n\n<p><strong>Possible Explanation 2:</strong> Conflict cache misses due to super-alignment on the L2 cache.</p>\n\n<p>Your <code>B</code> array is being iterated on the column. So the access is strided. Your total data size is <code>2k x 2k</code> which is about 32 MB per matrix. That's much larger than your L2 cache.</p>\n\n<p>When the data is not aligned perfectly, you will have decent spatial locality on B. Although you are hopping rows and only using one element per cacheline, the cacheline stays in the L2 cache to be reused by the next iteration of the middle loop.</p>\n\n<p>However, when the data is aligned perfectly (2048), these hops will all land on the same \"cache way\" and will far exceed your L2 cache associativity. Therefore, the accessed cache lines of <code>B</code> will not stay in cache for the next iteration. <strong><em>Instead, they will need to be pulled in all the way from ram.</em></strong></p>\n    "},{"t":"What is the correct way to obtain (-1)^n?","l":"http://stackoverflow.com/questions/29110752/what-is-the-correct-way-to-obtain-1n","q":"\n\n<p>Many algorithms require to compute <code>(-1)^n</code> (both integer), usually as a factor in a series. That is, a factor that is <code>-1</code> for odd n and <code>1</code> for even n. In a C or C++ environment, one often sees:</p>\n\n<pre><code>#include&lt;iostream&gt;\n#include&lt;cmath&gt;\nint main(){\n   int n = 13;\n   std::cout &lt;&lt; std::pow(-1, n) &lt;&lt; std::endl;\n}\n</code></pre>\n\n<p><strong>What is better or the usual convention?</strong> (or something else),</p>\n\n<pre><code>std::pow(-1, n)\nstd::pow(-1, n%2)\n(n%2?-1:1)\n(1-2*(n%2))  // (gives incorrect value for negative n)\n</code></pre>\n\n<p>EDIT:\nIn addition, user @SeverinPappadeux proposed another alternative based on (a global?) array lookups. My version of it is:</p>\n\n<pre><code>const int res[] {-1, 1, -1}; // three elements are needed for negative modulo results\nconst int* const m1pow = res + 1; \n...\nm1pow[n%2]\n</code></pre>\n    ","a":"\n<p>You can use <code>(n &amp; 1)</code> instead of <code>n % 2</code> and <code>&lt;&lt; 1</code> instead of <code>* 2</code> if you want to be super-pedantic, er I mean optimized.<br>\nSo the fastest way to compute in an 8086 processor is:</p>\n\n<p><code>1 - ((n &amp; 1) &lt;&lt; 1)</code></p>\n\n<p>I just want to clarify where this answer is coming from. The original poster alfC did an excellent job of posting a lot of different ways to compute (-1)^n some being faster than others.<br> \nNowadays with processors being as fast as they are and optimizing compilers being as good as they are we <em>usually</em> value readability over the slight (even negligible) improvements from shaving a few CPU cycles from an operation.<br>\nThere was a time when one pass compilers ruled the earth and MUL operations were new and decadent; in those days a power of 2 operation was an invitation for gratuitous optimization. </p>\n    "},{"t":"What is O(log* N)?","l":"http://stackoverflow.com/questions/2387656/what-is-olog-n","q":"\n\n<p>What is <code>O(log* N)</code>?  I found it online with no description.</p>\n\n<p>I know big-Oh, the <code>log*</code> is the question</p>\n    ","a":"\n<p><code>O( log* N )</code> is \"<a href=\"http://en.wikipedia.org/wiki/Iterated_logarithm\">iterated logarithm</a>\":</p>\n\n<blockquote>\n  <p>In computer science, the iterated logarithm of n, written log* n (usually read \"log star\"), is the number of times the logarithm function must be iteratively applied before the result is less than or equal to 1.</p>\n</blockquote>\n    "},{"t":"Manacher's algorithm (algorithm to find longest palindrome substring in linear time)","l":"http://stackoverflow.com/questions/10468208/manachers-algorithm-algorithm-to-find-longest-palindrome-substring-in-linear-t","q":"\n\n<p>After spending about 6-8 hours trying to digest the Manacher's algorithm, I am ready to throw in the towel. But before I do, here is one last shot in the dark: can anyone explain it? I don't care about the code. I want somebody to explain the <strong>ALGORITHM</strong>.</p>\n\n<p>Here seems to be a place that others seemed to enjoy in explaining the algorithm:\n<a href=\"http://www.leetcode.com/2011/11/longest-palindromic-substring-part-ii.html\">http://www.leetcode.com/2011/11/longest-palindromic-substring-part-ii.html</a></p>\n\n<p>I understand why you would want to transform the string, say, 'abba' to #a#b#b#a#\nAfter than I'm lost. For example, the author of the previously mentioned website says the key part of the algorithm is: </p>\n\n<pre><code>                      if P[ i' ] ≤ R – i,\n                      then P[ i ] ← P[ i' ]\n                      else P[ i ] ≥ P[ i' ]. (Which we have to expand past \n                      the right edge (R) to find P[ i ])\n</code></pre>\n\n<p>This seems wrong, because he/she says at one point that P[i] equals 5 when P[i'] = 7 and P[i] is not less or equal to R - i. </p>\n\n<p>If you are not familiar with the algorithm, here are some more links: <a href=\"http://tristan-interview.blogspot.com/2011/11/longest-palindrome-substring-manachers.html\">http://tristan-interview.blogspot.com/2011/11/longest-palindrome-substring-manachers.html</a> (I've tried this one, but the terminology is awful and confusing. First, some things are not defined. Also, too many variables. You need a checklist to recall what variable is referring to what.)</p>\n\n<p>Another is: <a href=\"http://www.akalin.cx/longest-palindrome-linear-time\">http://www.akalin.cx/longest-palindrome-linear-time</a> (good luck)</p>\n\n<p>The basic gist of the algorithm is to find the longest palindrome in linear time. It can be done in O(n^2) with a minimum to medium amount of effort. This algorithm is supposed to be quite \"clever\" to get it down to O(n). </p>\n    ","a":"\n<p>I agree that the logic isn't quite right in the explanation of the link.  I give some details below.</p>\n\n<p>Manacher's algorithm fills in a table P[i] which contains how far the palindrome centered at i extends.  If P[5]=3, then three characters on either side of position five are part of the palindrome.  The algorithm takes advantage of the fact that if you've found a long palindrome, you can fill in values of P on the right side of the palindrome quickly by looking at the values of P on the left side, since they should mostly be the same.</p>\n\n<p>I'll start by explaining the case you were talking about, and then I'll expand this answer as needed.</p>\n\n<p>R indicates the index of the right side of the palindrome centered at C.  Here is the state at the place you indicated:</p>\n\n<pre><code>C=11\nR=20\ni=15\ni'=7\nP[i']=7\nR-i=5\n</code></pre>\n\n<p>and the logic is like this:</p>\n\n<pre><code>if P[i']&lt;=R-i:  // not true\nelse: // P[i] is at least 5, but may be greater\n</code></pre>\n\n<p>The pseudo-code in the link indicates that P[i] should be greater than or equal to P[i'] if the test fails, but I believe it should be greater than or equal to R-i, and the explanation backs that up.</p>\n\n<p>Since P[i'] is greater than R-i, the palindrome centered at i' extends past the palindrome centered at C.  We know the palindrome centered at i will be at least R-i characters wide, because we still have symmetry up to that point, but we have to search explicitly beyond that.</p>\n\n<p>If P[i'] had been no greater than R-i, then the largest palindrome centered at i' is within the largest palindrome centered at C, so we would have known that P[i] couldn't be any larger than P[i'].  If it was, we would have a contradiction.  It would mean that we would be able to extend the palindrome centered at i beyond P[i'], but if we could, then we would also be able to extend the palindrome centered at i' due to the symmetry, but it was already supposed to be as large as possible.</p>\n\n<p>This case is illustrated previously:</p>\n\n<pre><code>C=11\nR=20\ni=13\ni'=9\nP[i']=1\nR-i=7\n</code></pre>\n\n<p>In this case, P[i']&lt;=R-i. Since we are still 7 characters away from the edge of the palindrome centered at C, we know that at least 7 characters around i are the same as the 7 characters around i'.  Since there was only a one character palindrome around i', there is a one character palindrome around i as well.</p>\n\n<p>j_random_hacker noticed that the logic should be more like this:</p>\n\n<pre><code>if P[i']&lt;R-i then\n  P[i]=P[i']\nelse if P[i']&gt;R-i then\n  P[i]=R-i\nelse P[i]=R-i + expansion\n</code></pre>\n\n<p>If P[i'] &lt; R-i, then we know that P[i]==P[i'], since we're still inside the palindrome centered at C.</p>\n\n<p>If P[i'] &gt; R-i, then we know that P[i]==R-i, because otherwise the palindrome centered at C would have extended past R.</p>\n\n<p>So the expansion is really only necessary in the special case where P[i']==R-i, so we don't know if the palindrome at P[i] may be longer.</p>\n\n<p>This is handled in the actual code by setting P[i]=min(P[i'],R-i) and then always expanding.  This way of doing it doesn't increase the time complexity, because if no expansion is necessary, the time taken to do the expansion is constant.</p>\n    "},{"t":"Smart progress bar ETA computation","l":"http://stackoverflow.com/questions/933242/smart-progress-bar-eta-computation","q":"\n\n<p>In many applications, we have some progress bar for a file download, for a compression task, for a search, etc. We all often use progress bars to let users know something is happening. And if we know some details like just how much work has been done and how much is left to do, we can even give a time estimate, often by extrapolating from how much time it's taken to get to the current progress level.</p>\n\n<p><img src=\"http://jameslao.com/wp-content/uploads/2008/01/winrar-progress-bar.png\" alt=\"compression ETA screenshot\"></p>\n\n<p>But we've also seen programs which this Time Left \"ETA\" display is just comically bad. It claims a file copy will be done in 20 seconds, then one second later it says it's going to take 4 days, then it flickers again to be 20 minutes.  It's not only unhelpful, it's confusing!\nThe reason the ETA varies so much is that the progress rate itself can vary and the programmer's math can be overly sensitive.</p>\n\n<p>Apple sidesteps this by just avoiding any accurate prediction and just giving vague estimates! \n<img src=\"http://download.autodesk.com/esd/mudbox/help2009/images/MED/DaliSP1/English/Install%5Flicensing/install%5Fprogress%5FMAC.png\" alt=\"Apple's vague evasion\"></p>\n\n<p>That's annoying too, do I have time for a quick break, or is my task going to be done in 2 more seconds? If the prediction is too fuzzy, it's pointless to make any prediction at all.</p>\n\n<p><strong>Easy but wrong methods</strong></p>\n\n<p>As a first pass ETA computation, probably we all just make a function like if p is the fractional percentage that's done already, and t is the time it's taken so far, we output t*(1-p)/p as the estimate of how long it's going to take to finish.  This simple ratio works \"OK\" but it's also terrible especially at the end of computation. If your slow download speed keeps a copy slowly advancing happening overnight, and finally in the morning, something kicks in and the copy starts going at full speed at 100X faster, your ETA at 90% done may say \"1 hour\", and 10 seconds later you're at 95% and the ETA will say \"30 minutes\" which is clearly an embarassingly poor guess.. in this case \"10 seconds\" is a much, much, much better estimate.</p>\n\n<p>When this happens you may think to change the computation to use <strong>recent</strong> speed, not average speed, to estimate ETA. You take the average download rate or completion rate over the last 10 seconds, and use that rate to project how long completion will be. That performs quite well in the previous overnight-download-which-sped-up-at-the-end example, since it will give very good final completion estimates at the end.  But this still has big problems.. it causes your ETA to bounce wildly when your rate varies quickly over a short period of time, and you get the \"done in 20 seconds, done in 2 hours, done in 2 seconds, done in 30 minutes\" rapid display of programming shame.</p>\n\n<p><strong>The actual question:</strong></p>\n\n<p>What is the best way to compute an estimated time of completion of a task, given the time history of the computation?  I am not looking for links to GUI toolkits or Qt libraries.  I'm asking about the <strong>algorithm</strong> to generate the most sane and accurate completion time estimates.</p>\n\n<p>Have you had success with math formulas?  Some kind of averaging, maybe by using the mean of the rate over 10 seconds with the rate over 1 minute with the rate over 1 hour?  Some kind of artificial filtering like \"if my new estimate varies too much from the previous estimate, tone it down, don't let it bounce too much\"? Some kind of fancy history analysis where you integrate progress versus time advancement to find standard deviation of rate to give statistical error metrics on completion? </p>\n\n<p>What have you tried, and what works best?</p>\n    ","a":"\n<h2> Original Answer</h2>\n\n<p>The company that created this site <a href=\"http://www.joelonsoftware.com/items/2007/10/26.html\">apparently makes</a> a scheduling system that answers this question in the context of employees writing code. The way it works is with Monte Carlo simulation of future based on the past.</p>\n\n<h2>Appendix: Explanation of Monte Carlo</h2> \n\n<p>This is how this algorithm would work in your situation:</p>\n\n<p>You model your task as a sequence of microtasks, say 1000 of them. Suppose an hour later you completed 100 of them. Now you run the simulation for the remaining 900 steps by randomly selecting 90 completed microtasks, adding their times and multiplying by 10. Here you have an estimate; repeat N times and you have N estimates for the time remaining. Note the average  between these estimates will be about 9 hours -- no surprises here. But by presenting the resulting distribution to the user you'll honestly communicate to him the odds, e.g. 'with the probability 90% this will take another 3-15 hours'</p>\n\n<p>This algorithm, by definition, produces complete result if the task in question can be modeled as a bunch of <i>independent, random</i> microtasks. You can gain a better answer only if you know how the task deviates from this model: for example, installers typically have a download/unpacking/installing tasklist and the speed for one cannot predict the other. </p>\n\n<h2>Appendix: Simplifying Monte Carlo</h2>\n\n<p>I'm not a statistics guru, but I think if you look closer into the simulation in this method, it will always return a normal distribution as a sum of large number of independent random variables. Therefore, you don't need to perform it at all. In fact, you don't even need to store all the completed times, since you'll only need their sum and sum of their squares.</p>\n\n<p>In maybe not very standard notation,</p>\n\n<pre><code>sigma = sqrt ( sum_of_times_squared-sum_of_times^2 )\nscaling = 900/100          // that is (totalSteps - elapsedSteps) / elapsedSteps\nlowerBound = sum_of_times*scaling - 3*sigma*sqrt(scaling)\nupperBound = sum_of_times*scaling + 3*sigma*sqrt(scaling)\n</code></pre>\n\n<p>With this, you can output the message saying that the thing will end between [lowerBound, upperBound] from now with some fixed probability (should be about 95%, but I probably missed some constant factor).</p>\n    "},{"t":"Find the Smallest Integer Not in a List","l":"http://stackoverflow.com/questions/1586858/find-the-smallest-integer-not-in-a-list","q":"\n\n<p>An interesting interview question that a colleague of mine uses:</p>\n\n<p>Suppose that you are given a very long, unsorted list of unsigned 64-bit integers. How would you find the smallest non-negative integer that <em>does not</em> occur in the list?</p>\n\n<p>FOLLOW-UP: Now that the obvious solution by sorting has been proposed, can you do it faster than O(n log n)?</p>\n\n<p>FOLLOW-UP: Your algorithm has to run on a computer with, say, 1GB of memory</p>\n\n<p>CLARIFICATION: The list is in RAM, though it might consume a large amount of it. You are given the size of the list, say N, in advance.</p>\n    ","a":"\n<p>If the datastructure can be mutated in place and supports random access then you can do it in O(N) time and O(1) additional space. Just go through the array sequentially and for every index write the value at the index to the index specified by value, recursively placing any value at that location to its place and throwing away values &gt; N. Then go again through the array looking for the spot where value doesn't match the index - that's the smallest value not in the array. This results in at most 3N comparisons and only uses a few values worth of temporary space.</p>\n\n<pre><code># Pass 1, move every value to the position of its value\nfor cursor in range(N):\n    target = array[cursor]\n    while target &lt; N and target != array[target]:\n        new_target = array[target]\n        array[target] = target\n        target = new_target\n\n# Pass 2, find first location where the index doesn't match the value\nfor cursor in range(N):\n    if array[cursor] != cursor:\n        return cursor\nreturn N\n</code></pre>\n    "},{"t":"Most efficient way to see if an ArrayList contains an object in Java","l":"http://stackoverflow.com/questions/558978/most-efficient-way-to-see-if-an-arraylist-contains-an-object-in-java","q":"\n\n<p>I have an ArrayList of objects in Java.  The objects have four fields, two of which I'd use to consider the object equal to another.  I'm looking for the most efficient way, given those two fields, to see if the array contains that object.</p>\n\n<p>The wrench is that these classes are generated based on XSD objects, so I can't modify the classes themselves to overwrite the <code>.equals</code>.  </p>\n\n<p>Is there any better way than just looping through and manually comparing the two fields for each object and then breaking when found?  That just seems so messy, looking for a better way.</p>\n\n<p><strong>Edit:</strong> the ArrayList comes from a SOAP response that is unmarshalled into objects.</p>\n    ","a":"\n<p>It depends on how efficient you need things to be. Simply iterating over the list looking for the element which satisfies a certain condition is O(n), but so is ArrayList.Contains if you could implement the Equals method. If you're not doing this in loops or inner loops this approach is probably just fine.</p>\n\n<p>If you really need very efficient look-up speeds at all cost, you'll need to do two things:</p>\n\n<ol>\n<li>Work around the fact that the class\nis generated: Write an adapter class which\ncan wrap the generated class and\nwhich implement <a href=\"http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html#equals(java.lang.Object)\">equals()</a> based\non those two fields (assuming they\nare public). Don't forget to also\nimplement <a href=\"http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html#hashCode()\">hashCode()</a> (*)</li>\n<li>Wrap each object with that adapter and\nput it in a HashSet.\n<a href=\"http://docs.oracle.com/javase/7/docs/api/java/util/AbstractCollection.html#contains(java.lang.Object)\">HashSet.contains()</a> has constant\naccess time, i.e. O(1) instead of O(n).</li>\n</ol>\n\n<p>Of course, building this HashSet still has a O(n) cost. You are only going to gain anything if the cost of building the HashSet is negligible compared to the total cost of all the contains() checks that you need to do. Trying to build a list without duplicates is such a case.</p>\n\n<p></p><hr>*\n(<em>) Implementing hashCode() is best done by XOR'ing (^ operator) the hashCodes of the same fields you are using for the equals implementation (but <a href=\"http://stackoverflow.com/questions/299304/why-does-javas-hashcode-in-string-use-31-as-a-multiplier/299748\">multiply by 31</a> to reduce the chance of the XOR yielding 0)</em><p></p>\n    "},{"t":"Prove a random generated number is uniform distributed","l":"http://stackoverflow.com/questions/24409639/prove-a-random-generated-number-is-uniform-distributed","q":"\n\n<p>I was asked this question in an interview.</p>\n\n<blockquote>\n  <p>Given a random number generator to generate a number between [0,N), how\n  to prove this number is uniform distributed.</p>\n</blockquote>\n\n<p>I am not sure how to approach this problem, any suggestion?</p>\n    ","a":"\n<p>To <strong>prove</strong> it, you need to know the algorithm being used and show in graph terms that the set of all states constitutes a cycle, that there are no subcycles, and that the cardinality of the state space modulo N is zero so that there is no set of states that occur more/less frequently than others.  This is how we know that Mersenne Twister, for instance, is uniformly distributed even though the 64 bit version has a cycle length of 2<sup>19937</sup>-1 and could never be enumerated within the lifetime of the universe.</p>\n\n<p>Otherwise you use statistical tests to test the hypothesis of uniformity. Statistics can't prove a result, it fails to disprove the hypothesis. The larger your sample size is, the more compelling the failure to disprove a hypothesis is, but it is never proof. (This perspective causes more communications problems with non-statisticians/non-scientists than anything else I know.) There are many tests for uniformity, including chi-square tests, Anderson-Darling, and Kolmogorov-Smirnov to name just a few.</p>\n\n<p>All of the uniformity tests will pass sequences of values such as 0,1,2,...,N-1,0,1,... so uniformity is not sufficient to say you have a good generator. You should also be testing for serial correlation with tests such as spacings tests, runs-up/runs-down, runs above/below the mean, \"birthday\" tests, and so on.</p>\n\n<p>A pretty comprehensive suite of tests for uniformity and serial correlation was created by George Marsaglia over the course of his career, and published in 1995 as what he jokingly called the \"<a href=\"http://en.wikipedia.org/wiki/Diehard_tests\">Diehard tests</a>\" (because it's a heavy duty battery of tests).</p>\n    "},{"t":"Why java Arrays use two different sort algorithms for different types?","l":"http://stackoverflow.com/questions/3707190/why-java-arrays-use-two-different-sort-algorithms-for-different-types","q":"\n\n<p><code>Arrays.java</code>'s sort method uses quicksort for arrays of primitives and merge sort for arrays of objects. I believe that most of time quicksort is faster than merge sort and costs less memory. My experiments support that, although both algorithms are O(nlog(n)). So why are different algorithms used for different types?</p>\n    ","a":"\n<p>The most likely reason: quicksort is not <em>stable</em>, i.e. equal entries can change their relative position during the sort; among other things, this means that if you sort an already sorted array, it may not stay unchanged.</p>\n\n<p>Since primitive types have no identity (there is no way to distinguish two ints with the same value), this does not matter for them. But for reference types, it could cause problems for some applications. Therefore, a stable merge sort is used for those.</p>\n\n<p>OTOH, a reason not to use the (guaranteed n*log(n)) merge sort for primitive types might be that it requires making a clone of the array. For reference types, where the referred objects usually take up far more memory than the array of references, this generally does not matter. But for primitive types, cloning the array outright doubles the memory usage.</p>\n    "},{"t":"How to prove that a problem is NP complete?","l":"http://stackoverflow.com/questions/4294270/how-to-prove-that-a-problem-is-np-complete","q":"\n\n<p>I have problem with scheduling. I need to prove that the problem is NP complete. What can be the methods to prove it NP complete?</p>\n    ","a":"\n<p>To show NP-completeness:</p>\n\n<p><strong>1) show it's in NP.</strong> </p>\n\n<p>That is, given some information C, you can create an algorithm V that will verify for every possible input X whether X is in your domain or not. </p>\n\n<p>The algorithm V must run in polynomial time.</p>\n\n<p>For example:</p>\n\n<p>Prove that the problem of vertex covers (that is, for some graph G, does it have a vertex cover set of size k such that every edge in G has at least one vertex in the cover set?) is in NP:</p>\n\n<ul>\n<li>our input X is some graph G and some number k (this is from the problem definition)</li>\n<li>Take our information C to be \"any possible subset of vertices in graph G of size k\"</li>\n<li>Then we can write an algorithm V that, given G, k and C, will return whether that set of vertices is a vertex cover for G or not, in polynomial time.</li>\n</ul>\n\n<p>Then for every graph G, if there exists some \"possible subset of vertices in G of size k\" which is a vertex cover, then G is in NP.</p>\n\n<p><strong>Note:</strong> we DON'T need to find C in polynomial time. If we could, the problem would be in P.</p>\n\n<p><strong>Note:</strong> Algorithm V should work for EVERY G, for SOME C. For every input there should EXIST information that could help us verify whether the input is in the problem domain or not. That is, there should not be an input where the information doesn't exist.</p>\n\n<p><strong>2) Prove it's NP-hard.</strong></p>\n\n<p>This involves getting a known NP-complete problem like SAT (the set of boolean expressions in the form \"(A OR B OR C) AND (D OR E OR F) AND ...\" where the expression is satisfiable (ie there exists some setting for these booleans which makes the expression true).</p>\n\n<p>Then reduce the NP-complete problem to your problem in polynomial time.</p>\n\n<p>That is, given some input X for SAT (or whatever NP-complete problem you are using), create some input Y for your problem such that X is in SAT if and only if Y is in your problem. The function f:X -&gt; Y must run in polynomial time.</p>\n\n<p>In the example above the input Y would be the graph G and the size of the vertex cover k.</p>\n\n<p>For a full proof, you'd have to prove both:</p>\n\n<ul>\n<li>that X is in SAT =&gt; Y in your problem</li>\n<li>and Y in your problem =&gt; X in SAT.</li>\n</ul>\n\n<p><strong>marcog's</strong> answer has a link with several other NP-complete problems you could reduce to your problem.</p>\n    "}]