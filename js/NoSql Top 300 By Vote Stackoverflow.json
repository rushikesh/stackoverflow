[{"t":"What NoSQL solutions are out there for .NET? [closed]","l":"http://stackoverflow.com/questions/1777103/what-nosql-solutions-are-out-there-for-net","q":"\n\n<p>What <a href=\"http://stackoverflow.com/questions/tagged/nosql\">NoSQL</a> solutions are out there for .NET?</p>\n    ","a":"\n<p>You also should consider using <a href=\"http://redis.io/\" rel=\"nofollow\">Redis</a>. It's an advanced NoSQL database with support for rich server-side data structures such as lists, sets, sorted sets and hashes. It runs in memory but supports both a \"snapshot\" persistence mode as well as an Append only file journal for integrity in-between snapshots.</p>\n\n<h3>Redis is very fast</h3>\n\n<p>It is also one of the fastest NoSQL databases in existence: 110000 SETs/second, 81000 GETs/second in an entry level Linux box. <a href=\"http://code.google.com/p/redis/wiki/Benchmarks\" rel=\"nofollow\">Check the benchmarks</a>. </p>\n\n<p>In wanting to stay true to Redis I've developed a very fast C# Redis Client with a strong-focus on performance. <a href=\"http://www.servicestack.net/mythz_blog/?p=474\" rel=\"nofollow\">Running Raven DB's own benchmarks</a> the default redis-server configuration is <strong>16.9x</strong> faster than Raven DB. When adjusted to use the <code>appendfsync always</code> option (though not recommended) it is still <strong>11.75x faster</strong>.</p>\n\n<p><a href=\"http://twitter.com/marcgravell\" rel=\"nofollow\">@marcgravell</a> has developed a very fast <a href=\"http://code.google.com/p/booksleeve/\" rel=\"nofollow\">async C# Redis Client</a> used to handle Stack Overflows and other Stack Exchange sites distributed caching.</p>\n\n<h3>Simple and Elegant</h3>\n\n<p>Like most NoSQL data stores, Redis is schema-less allowing you to use it straight away without having to define any schemas upfront - providing a major productivity boost.\nThe beauty of Redis is that it's like accessing .NET's generic collection classes you're used to in C# so there is very little projection that needs to be done since its API is already a close match to the Lists and in-memory data structures your program already uses. </p>\n\n<p>It's this elegance of offering fundamental comp-sci data structures that sets Redis apart, it allows it to be extremely versatile whilst only supporting a limited operation set. i.e. Redis has been commonly seen as a distributed cache, message queue, IPC broker between languages/processes, Pub/Sub, push/event-based comms, distributed-locking, CQRS Event Source, Unique Id generator, etc. Even if you're not using it as your primary data store, its speed and simplicity gives you access to a versatile swiss-army toolbox that can fulfil a wealth of use-cases.</p>\n\n<h3>Full featured Redis Client</h3>\n\n<p>There are a number of <a href=\"http://redis.io/clients\" rel=\"nofollow\">C#/.NET Redis clients</a> available, and <a href=\"http://www.nuget.org/packages?q=redis\" rel=\"nofollow\">on NuGet</a>. The Example below uses my <a href=\"http://github.com/ServiceStack/ServiceStack.Redis/\" rel=\"nofollow\">ServiceStack's open source C# client</a>.</p>\n\n<p>All the documentation and tutorials for the ServiceStack's C# Redis Client is available at:\n<a href=\"http://www.servicestack.net/docs/redis-client/redis-client\" rel=\"nofollow\">http://www.servicestack.net/docs/redis-client/redis-client</a></p>\n\n<p>The client provides a rich interface providing wrappers for .NET's generic IList, IDictionary and ICollection mapping to Redis's rich server side data structures. For a quick overview of its features <a href=\"http://servicestack.net/img/Redis-annotated.png\" rel=\"nofollow\">check out the API image map</a>.</p>\n\n<p>A good tutorial showing how you to develop a real-world applications is at:\n<a href=\"http://www.servicestack.net/docs/redis-client/designing-nosql-database\" rel=\"nofollow\">http://www.servicestack.net/docs/redis-client/designing-nosql-database</a></p>\n\n<p>See the <a href=\"https://github.com/ServiceStack/ServiceStack.Examples/blob/master/src/RedisStackOverflow/RedisStackOverflow.ServiceInterface/IRepository.cs\" rel=\"nofollow\">source code</a> of <a href=\"http://www.servicestack.net/RedisStackOverflow/\" rel=\"nofollow\">Redis StackOverflow</a> for another example of a real-world app built entirely with Redis.</p>\n\n<p>Sample code showing a complete CRUD app:</p>\n\n<pre><code>public class Todo\n{\n    public long Id { get; set; }\n    public string Content { get; set; }\n    public int Order { get; set; }\n    public bool Done { get; set; }\n}\n\n//Thread-safe client factory\nvar redisManager = new PooledRedisClientManager(\"localhost:6379\");\n\nredisManager.ExecAs&lt;Todo&gt;(redisTodos =&gt; {\n    var todo = new Todo {\n        Id = redisTodos.GetNextSequence(),\n        Content = \"Learn Redis\",\n        Order = 1,\n    };\n\n    redisTodos.Store(todo);\n\n    Todo savedTodo = redisTodos.GetById(todo.Id);\n    savedTodo.Done = true;\n\n    redisTodos.Store(savedTodo);\n\n    redisTodos.DeleteById(savedTodo.Id);\n\n    var allTodos = redisTodos.GetAll();\n\n    Assert.That(allTodos.Count, Is.EqualTo(0));\n});\n</code></pre>\n\n<p>To play with this example live see ServiceStack's <a href=\"http://www.servicestack.net/Backbone.Todos/\" rel=\"nofollow\">Backbone's TODO application</a> retro-fitted with a <a href=\"https://github.com/ServiceStack/ServiceStack.Examples/blob/master/src/Backbone.Todos/Global.asax.cs\" rel=\"nofollow\">C# Redis back-end</a>.</p>\n\n<h3>Runs on all languages and platforms</h3>\n\n<p>As a result of its simplicity it has language bindings for nearly every language in use today:\n<a href=\"http://redis.io/clients\" rel=\"nofollow\">http://redis.io/clients</a></p>\n\n<p>It's even more terse in dynamic languages, here's how easy it is to create a <a href=\"https://gist.github.com/1144866\" rel=\"nofollow\">non-blocking web server hosting named counters</a> in node.js/CoffeeScript:</p>\n\n<pre><code>app.get '/counters/:id', (request, response) -&gt;\n    id = request.params.id\n    redis.incr id, (err, val) -&gt;\n        response.send \"#{id}: #{val}\", 'Content-Type': 'text/plain', 201\n</code></pre>\n\n<p>One of the benefits of NoSQL is that you get to <a href=\"https://groups.google.com/forum/#!forum/redis-db\" rel=\"nofollow\">mix with developers from different language backgrounds</a> coming together to work out how best we can leverage redis to meet our demanding needs.</p>\n\n<h3>Widely used by the biggest companies</h3>\n\n<p>Because of its simplicity, stability and speed it's used by <a href=\"http://redis.io/topics/whos-using-redis\" rel=\"nofollow\">many large companies</a> including:</p>\n\n<ul>\n<li>StackOverflow</li>\n<li>GitHub</li>\n<li>Twitter</li>\n<li>Blizzard</li>\n<li>Flickr</li>\n<li>Digg</li>\n<li>Instagram</li>\n</ul>\n\n<h3>Downloading Redis for Windows</h3>\n\n<p>I've provided <a href=\"https://github.com/mythz/redis-windows\" rel=\"nofollow\">installation instructions</a> and downloads for the 2 most popular ways of running Redis on windows at: <a href=\"https://github.com/mythz/redis-windows\" rel=\"nofollow\">https://github.com/mythz/redis-windows</a> that shows how to:</p>\n\n<ol>\n<li>Use <a href=\"http://www.vagrantup.com/\" rel=\"nofollow\">Vagrant</a> to run the latest stable version of Redis inside a <a href=\"https://www.virtualbox.org/\" rel=\"nofollow\">VirtualBox VM</a>.</li>\n<li>Download and run <a href=\"https://github.com/MSOpenTech/Redis\" rel=\"nofollow\">Microsoft's Native port of Redis</a></li>\n</ol>\n    "},{"t":"When to use CouchDB over MongoDB and vice versa","l":"http://stackoverflow.com/questions/12437790/when-to-use-couchdb-over-mongodb-and-vice-versa","q":"\n\n<p>I am stuck between these two NoSQL databases. In my project I will be creating a database within a database. For example, I need a solution to create dynamic tables. So users can create tables with columns and rows. I think either MongoDB or CouchDB will be good for this, but I am not sure which one. I will also need efficient paging as well.</p>\n    ","a":"\n<p>Of C,A &amp; P which 2 are more important to you? Quick reference, the <a href=\"http://blog.nahurst.com/visual-guide-to-nosql-systems\"><em>Visual Guide To NoSQL Systems</em></a></p>\n\n<ul>\n<li>MongodB : Availability and Partition Tolerance</li>\n<li>CouchDB : Consistency and Partition Tolerance</li>\n</ul>\n\n<p>A blog post, <a href=\"http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis\"><em>Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase vs Membase vs Neo4j comparison</em></a> has '<em>Best used</em>' scenarios for each NoSQL database compared. Quoting the link, </p>\n\n<ul>\n<li>MongoDB: If you need dynamic queries. If you prefer to define indexes, not map/reduce functions. If you need good performance on a big DB. If you wanted CouchDB, but your data changes too much, filling up disks.</li>\n<li>CouchDB : For accumulating, occasionally changing data, on which pre-defined queries are to be run. Places where versioning is important.</li>\n</ul>\n\n<p>A recent (Feb 2012) and more <a href=\"https://plus.google.com/107397941677313236670/posts/LFBB233PKQ1\">comprehensive comparison</a> by Riyad Kalla, </p>\n\n<ul>\n<li>MongoDB : Master-Slave Replication ONLY</li>\n<li>CouchDB : Master-Master Replication</li>\n</ul>\n\n<p>A blog post (Oct 2011) by someone who tried both, <em><a href=\"http://openmymind.net/2011/10/27/A-MongoDB-Guy-Learns-CouchDB/\">A MongoDB Guy Learns CouchDB</a></em> commented on the CouchDB's paging being not as useful.</p>\n\n<p>A dated (Jun 2009) <a href=\"http://www.kchodorow.com/blog/2009/06/29/couchdb-vs-mongodb-benchmark/\">benchmark</a> by <a href=\"http://www.kchodorow.com/blog/about/\">Kristina Chodorow</a> (<em>part of team behind MongoDB</em>),</p>\n\n<p>I'd go for MongoDB.</p>\n\n<p>Hope it helps.</p>\n    "},{"t":"When to Redis? When to MongoDB?","l":"http://stackoverflow.com/questions/5400163/when-to-redis-when-to-mongodb","q":"\n\n<p>What I want is not a comparison between Redis and MongoDB. I know they are different; the performance and the API is totally different.</p>\n\n<p>Redis is very fast, but the API is very 'atomic'. MongoDB will eat more resources, but the API is very very easy to use, and I am very happy with it.</p>\n\n<p>They're both awesome, and I want to use Redis in deployment as much as I can, but it is hard to code. I want to use MongoDB in development as much as I can, but it needs an expensive machine.</p>\n\n<p>So what do you think about the use of both of them? When to pick Redis? When to pick MongoDB?</p>\n    ","a":"\n<p>I would say, it depends on kind of dev team you are and your application needs.</p>\n\n<p>For example if there are more <strong>querying</strong> required, it mostly means more work in Redis, where you can use different data structures to suit your queries. Same is easier in MongoDB. On other hand this is often extra work in Redis would most likely to pay off with <strong>sheer speed</strong>. </p>\n\n<p>MongoDB offers simplicity, much smaller learning for guys with SQL experience. Whereas Redis offers non-traditional approach hence more learning but huge flexibility.</p>\n\n<p>Eg. A <strong>cache</strong> layer can probably be better implemented in Redis, and as for a more schema-able data MongoDB might be better.  <em>[Note: mongodb is schemaless]</em></p>\n\n<p>If you ask me my personal choice is Redis for most requirements.</p>\n\n<p>Lastly, I hope by now you have seen <a href=\"http://antirez.com/post/MongoDB-and-Redis.html\">http://antirez.com/post/MongoDB-and-Redis.html</a> </p>\n    "},{"t":"Is there any NoSQL database as simple as SQLite?","l":"http://stackoverflow.com/questions/2403174/is-there-any-nosql-database-as-simple-as-sqlite","q":"\n\n<p>Is there any NoSQL database as simple as SQLite? I'm looking for a lightweight database to persist a small set of data for a simple desktop application. I still can use SQLite but prefer a more OO approach since my app doesn't handle much data.</p>\n    ","a":"\n<p><a href=\"http://unqlite.org/\">UnQLite</a> is a in-process software library which implements a self-contained, serverless, zero-configuration, transactional NoSQL database engine.</p>\n    "},{"t":"NoSQL (MongoDB) vs Lucene (or Solr) as your database","l":"http://stackoverflow.com/questions/3215029/nosql-mongodb-vs-lucene-or-solr-as-your-database","q":"\n\n<p>With the NoSQL movement growing based on document-based databases, I've looked at MongoDB lately.  I have noticed a striking similarity with how to treat items as \"Documents\", just like Lucene does (and users of Solr).</p>\n\n<p>So, the question: <strong>Why would you want to use NoSQL (MongoDB, Cassandra, CouchDB, etc) over Lucene (or Solr) as your \"database\"?</strong></p>\n\n<p>What I am (and I am sure others are) looking for in an answer is some deep-dive comparisons of them.  Let's skip over relational database discussions all together, as they serve a different purpose. </p>\n\n<p>Lucene gives some serious advantages, such as powerful searching and weight systems.  Not to mention facets in Solr (which Solr is being integrated into Lucene soon, yay!).  You can use Lucene documents to store IDs, and access the documents as such just like MongoDB.  Mix it with Solr, and you now get a WebService-based, load balanced solution.</p>\n\n<p>You can even throw in a comparison of out-of-proc cache providers such as Velocity or MemCached when talking about similar data storing and scalability of MongoDB.</p>\n\n<p>The restrictions around MongoDB reminds me of using MemCached, but I can use Microsoft's Velocity and have more grouping and list collection power over MongoDB (I think).  Can't get any faster or scalable than caching data in memory.  Even Lucene has a memory provider.</p>\n\n<p>MongoDB (and others) do have some advantages, such as the ease of use of their API.  New up a document, create an id, and store it.  Done.  Nice and easy.</p>\n    ","a":"\n<p>This is a great question, something I have pondered over quite a bit. I will summarize my lessons learned:</p>\n\n<ol>\n<li><p>You can easily use Lucene/Solr in lieu of MongoDB for pretty much all situations, but not vice versa. Grant Ingersoll's <a href=\"http://lucidworks.com/blog/nosql-lucene-and-solr/\">post sums it up here.</a></p></li>\n<li><p>MongoDB etc. seem to serve a purpose where there is no requirement of searching and/or faceting. It appears to be a simpler and arguably easier transition for programmers detoxing from the RDBMS world. Unless one's used to it Lucene &amp; Solr have a steeper learning curve.</p></li>\n<li><p>There aren't many examples of using Lucene/Solr as a datastore, but Guardian has made some headway and summarize this in an excellent <a href=\"http://lucidworks.com/blog/for-the-guardian-solr-is-the-new-database/\">slide-deck</a>, but they too are non-committal on totally jumping on Solr bandwagon and \"investigating\"  combining Solr with CouchDB.</p></li>\n<li><p>Finally, I will offer our experience, unfortunately cannot reveal much about the business-case. We work on the scale of several TB of data, a near real-time application. After investigating various combinations, decided to stick with Solr. No regrets thus far (6-months &amp; counting) and see no reason to switch to some other.</p></li>\n</ol>\n\n<p>Summary: if you do not have a search requirement, Mongo offers a simple &amp; powerful approach. However if search is key to your offering, you are likely better off sticking to one tech (Solr/Lucene) and optimizing the heck out of it - fewer moving parts.</p>\n\n<p>My 2 cents, hope that helped.</p>\n    "},{"t":"What scalability problems have you encountered using a NoSQL data store?","l":"http://stackoverflow.com/questions/2285045/what-scalability-problems-have-you-encountered-using-a-nosql-data-store","q":"\n\n<p>NoSQL refers to non-relational data stores that break with the history of relational databases and ACID guarantees. Popular open source NoSQL data stores include:</p>\n\n<ul>\n<li><a href=\"http://incubator.apache.org/cassandra/\">Cassandra</a> (tabular, written in Java, used by Cisco, WebEx, Digg, Facebook, IBM, Mahalo, Rackspace, Reddit and Twitter)</li>\n<li><a href=\"http://couchdb.apache.org/\">CouchDB</a> (document, written in Erlang, used by BBC and Engine Yard)</li>\n<li><a href=\"http://github.com/cliffmoon/dynomite\">Dynomite</a> (key-value, written in Erlang, used by Powerset)</li>\n<li><a href=\"http://hadoop.apache.org/hbase/\">HBase</a> (key-value, written in Java, used by Bing)</li>\n<li><a href=\"http://www.hypertable.org/\">Hypertable</a> (tabular, written in C++, used by Baidu)</li>\n<li><a href=\"http://sourceforge.net/projects/kai/\">Kai</a> (key-value, written in Erlang)</li>\n<li><a href=\"http://memcachedb.org/\">MemcacheDB</a> (key-value, written in C, used by Reddit)</li>\n<li><a href=\"http://www.mongodb.org/\">MongoDB</a> (document, written in C++, used by Electronic Arts, Github, NY Times and Sourceforge)</li>\n<li><a href=\"http://neo4j.org/\">Neo4j</a> (graph, written in Java, used by some Swedish universities)</li>\n<li><a href=\"http://project-voldemort.com/\">Project Voldemort</a> (key-value, written in Java, used by LinkedIn)</li>\n<li><a href=\"http://code.google.com/p/redis/\">Redis</a> (key-value, written in C, used by Craigslist, Engine Yard and Github)</li>\n<li><a href=\"http://riak.basho.com/\">Riak</a> (key-value, written in Erlang, used by Comcast and Mochi Media)</li>\n<li><a href=\"http://github.com/tuulos/ringo\">Ringo</a> (key-value, written in Erlang, used by Nokia)</li>\n<li><a href=\"http://code.google.com/p/scalaris/\">Scalaris</a> (key-value, written in Erlang, used by OnScale)</li>\n<li><a href=\"http://code.google.com/p/terrastore/\">Terrastore</a> (document, written in Java)</li>\n<li><a href=\"http://code.google.com/p/thrudb/\">ThruDB</a> (document, written in C++, used by JunkDepot.com)</li>\n<li><a href=\"http://1978th.net/tokyocabinet/\">Tokyo Cabinet/Tokyo Tyrant</a> (key-value, written in C, used by Mixi.jp (Japanese social networking site))</li>\n</ul>\n\n<p>I'd like to know about specific problems you - the SO reader - have solved using data stores and what NoSQL data store you used.</p>\n\n<p>Questions:</p>\n\n<ul>\n<li>What scalability problems have you used NoSQL data stores to solve?</li>\n<li>What NoSQL data store did you use? </li>\n<li>What database did you use prior to switching to a NoSQL data store?</li>\n</ul>\n\n<p><b>I'm looking for first-hand experiences, so please do not answer unless you have that.</b></p>\n    ","a":"\n<p>I've switched a small subproject from MySQL to CouchDB, to be able to handle the load. The result was amazing.</p>\n\n<p>About 2 years ago, we've released a self written software on <a href=\"http://www.ubuntuusers.de/\">http://www.ubuntuusers.de/</a> (which is probably the biggest German Linux community website). The site is written in Python and we've added a WSGI middleware which was able to catch all exceptions and send them to another small MySQL powered website. This small website used a hash to determine different bugs and stored the number of occurrences and the last occurrence as well.</p>\n\n<p>Unfortunately, shortly after the release, the traceback-logger website wasn't responding anymore. We had some locking issues with the production db of our main site which was throwing exceptions nearly every request, as well as several other bugs, which we haven't explored during the testing stage. The server cluster of our main site, called the traceback-logger submit page several k times per second. And that was a way too much for the small server which hosted the traceback logger (it was already an old server, which was only used for development purposes).</p>\n\n<p>At this time CouchDB was rather popular, and so I decided to try it out and write a small traceback-logger with it. The new logger only consisted of a single python file, which provided a bug list with sorting and filter options and a submit page. And in the background I've started a CouchDB process. The new software responded extremely quickly to all requests and we were able to view the massive amount of automatic bug reports.</p>\n\n<p>One interesting thing is, that the solution before, was running on an old dedicated server, where the new CouchDB based site on the other hand was only running on a shared xen instance with very limited resources. And I haven't even used the strength of key-values stores to scale horizontally. The ability of CouchDB / Erlang OTP to handle concurrent requests without locking anything was already enough to serve the needs.</p>\n\n<p>Now, the quickly written CouchDB-traceback logger is still running and is a helpful way to explore bugs on the main website. Anyway, about once a month the database becomes too big and the CouchDB process gets killed. But then, the compact-db command of CouchDB reduces the size from several GBs to some KBs again and the database is up and running again (maybe i should consider adding a cronjob there... 0o).</p>\n\n<p>In a summary, CouchDB was surely the best choice (or at least a better choice than MySQL) for this subproject and it does its job well.</p>\n    "},{"t":"Good reasons NOT to use a relational database?","l":"http://stackoverflow.com/questions/37823/good-reasons-not-to-use-a-relational-database","q":"\n\n<p>Can you please point to alternative data storage tools and give good reasons to use them instead of good-old relational databases? In my opinion, most applications rarely use the full power of SQL--it would be interesting to see how to build an SQL-free application.</p>\n    ","a":"\n<p>Plain text files in a filesystem</p>\n\n<ul>\n<li>Very simple to create and edit</li>\n<li>Easy for users to manipulate with simple tools (i.e. text editors, grep etc)</li>\n<li>Efficient storage of binary documents</li>\n</ul>\n\n<hr>\n\n<p>XML or JSON files on disk</p>\n\n<ul>\n<li>As above, but with a bit more ability to validate the structure.</li>\n</ul>\n\n<hr>\n\n<p>Spreadsheet / CSV file</p>\n\n<ul>\n<li>Very easy model for business users to understand</li>\n</ul>\n\n<hr>\n\n<p>Subversion (or similar disk based version control system)</p>\n\n<ul>\n<li>Very good support for versioning of data</li>\n</ul>\n\n<hr>\n\n<p><a href=\"http://www.oracle.com/technology/products/berkeley-db/index.html\">Berkeley DB</a> (Basically, a disk based hashtable)</p>\n\n<ul>\n<li>Very simple conceptually (just un-typed key/value)</li>\n<li>Quite fast</li>\n<li>No administration overhead</li>\n<li>Supports transactions I believe</li>\n</ul>\n\n<hr>\n\n<p><a href=\"http://www.amazon.com/SimpleDB-AWS-Service-Pricing/b/ref=sc_fe_l_2?ie=UTF8&amp;node=342335011&amp;no=3435361&amp;me=A36L942TSJ2AJA\">Amazon's Simple DB</a></p>\n\n<ul>\n<li>Much like Berkeley DB I believe, but hosted</li>\n</ul>\n\n<hr>\n\n<p><a href=\"http://code.google.com/appengine/docs/datastore/\">Google's App Engine Datastore</a></p>\n\n<ul>\n<li>Hosted and highly scalable</li>\n<li>Per document key-value storage (i.e. flexible data model)</li>\n</ul>\n\n<hr>\n\n<p><a href=\"http://incubator.apache.org/couchdb/\">CouchDB</a></p>\n\n<ul>\n<li>Document focus</li>\n<li>Simple storage of semi-structured / document based data</li>\n</ul>\n\n<hr>\n\n<p>Native language collections (stored in memory or serialised on disk)</p>\n\n<ul>\n<li>Very tight language integration</li>\n</ul>\n\n<hr>\n\n<p>Custom (hand-written) storage engine</p>\n\n<ul>\n<li>Potentially very high performance in required uses cases</li>\n</ul>\n\n<hr>\n\n<p>I can't claim to know anything much about them, but you might also like to look into <a href=\"http://en.wikipedia.org/wiki/Object_database\">object database systems</a>.</p>\n    "},{"t":"Is there a query language for JSON?","l":"http://stackoverflow.com/questions/777455/is-there-a-query-language-for-json","q":"\n\n<p>Is there a (roughly) SQL or XQuery-like language for querying JSON?</p>\n\n<p>I'm thinking of very small datasets that map nicely to JSON where it would be nice to easily answer queries such as \"what are all the values of X where Y &gt; 3\" or to do the usual SUM / COUNT type operations.</p>\n\n<p>As completely made-up example, something like this:</p>\n\n<pre><code>[{\"x\": 2, \"y\", 0}}, {\"x\": 3, \"y\", 1}, {\"x\": 4, \"y\": 1}]\n\nSUM(X) WHERE Y &gt; 0     (would equate to 7)\nLIST(X) WHERE Y &gt; 0    (would equate to [3,4])\n</code></pre>\n\n<p>I'm thinking this would work both client-side and server-side with results being converted to the appropriate language-specific data structure (or perhaps kept as JSON)</p>\n\n<p>A quick Googling suggests that people have thought about it and implemented a few things (<a href=\"http://www.jaql.org/release/0.1/jaql-overview.html\">JAQL</a>), but it doesn't seem like a standard usage or set of libraries has emerged yet. While each function is fairly trivial to implement on its own, if someone has already done it right I don't want to re-invent the wheel.</p>\n\n<p>Any suggestions?</p>\n\n<p>Edit: Thanks for the suggestions folks. I appreciate it. This may indeed be a bad idea or JSON may be too generic a format for what I'm thinking.. The reason for wanting a query language instead of just doing the summing/etc functions directly as needed is that I hope to build the queries dynamically based on user-input. Kinda like the argument that \"we don't need SQL, we can just write the functions we need\". Eventually that either gets out of hand or you end up writing your own version of SQL as you push it further and further. (Okay, I know that is a bit of a silly argument, but you get the idea..)</p>\n    ","a":"\n<p>Sure, how about:</p>\n\n<ul>\n<li><a href=\"http://code.google.com/p/jaql/\">JaQL</a></li>\n<li><a href=\"http://goessner.net/articles/JsonPath/\">JsonPath</a>.</li>\n<li><a href=\"http://tech.groups.yahoo.com/group/json/message/1205\">Json Query</a></li>\n</ul>\n\n<p>They all seem to be a bit work in progress, but work to some degree. They are also similar to XPath and XQuery conceptually; even though XML and JSON have different conceptual models (hierarchic vs object/struct).</p>\n    "},{"t":"Difference between CouchDB and Couchbase","l":"http://stackoverflow.com/questions/5578608/difference-between-couchdb-and-couchbase","q":"\n\n<p>Are there any essential differences between <a href=\"http://couchdb.apache.org/\">CouchDB</a> and <a href=\"http://www.couchbase.com/\">Couchbase</a> which makes one or the other preferable to use?</p>\n    ","a":"\n<p>We've written up in more detail the <a href=\"http://www.couchbase.com/couchdb\">relationship between CouchDB and Couchbase</a>. In a nutshell, <a href=\"http://www.couchbase.com/downloads\">Couchbase Server</a> takes all the chewy NoSQL goodness of CouchDB, and gives it the crisp hard edge of a memcache frosting. </p>\n\n<p>If you are new to NoSQL, basically, you store your JSON in Couchbase. It's very fast and can <a href=\"http://nosql.mypopescu.com/post/22117090558/the-story-of-scaling-draw-something-from-an-amazon-s3\">scale up to traffic loads as large you're likely to see.</a></p>\n\n<p>Like a relational database, it gives you back what you stored, when you need it. <a href=\"http://damienkatz.net/2012/06/why_database_technology_matter.html\">Yay databases!</a></p>\n\n<p>Unlike a relational database, instead of you having to specify up-front what you are going to store, you can just store it (we are a drop-in upgrade if you are using memcached) and we'll help you sort it out later.</p>\n    "},{"t":"What does MongoDB not being ACID compliant really mean?","l":"http://stackoverflow.com/questions/7149890/what-does-mongodb-not-being-acid-compliant-really-mean","q":"\n\n<p>I am not a database expert and have no formal computer science background, so bear with me.  I want to know the kinds of <em>real world</em> negative things that can happen if you use MongoDB, which is not <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a> compliant.  This applies to any ACID noncompliant database.</p>\n\n<p>I understand that MongoDB can perform <a href=\"http://www.mongodb.org/display/DOCS/Atomic+Operations\">Atomic Operations</a>, but that they don't \"support traditional locking and complex transactions\", mostly for performance reasons.  I also understand the importance of database transactions, and the example of when your database is for a bank, and you're updating several records that all need to be in sync, you want the transaction to revert back to the initial state if there's a power outage so credit equals purchase, etc.</p>\n\n<p>But when I get into conversations about MongoDB, those of us that don't know the technical details of how databases are actually implemented start throwing around statements like:</p>\n\n<blockquote>\n  <p>MongoDB is way faster than MySQL and Postgres, but there's a tiny chance, like 1 in a million, that it \"won't save correctly\".</p>\n</blockquote>\n\n<p>That \"won't save correctly\" part is referring to this understanding: If there's a power outage right at the instant you're writing to MongoDB, there's a chance for a particular record (say you're tracking pageviews in documents with 10 attributes each), that one of the documents only saved 5 of the attributesâ€¦ which means over time your pageview counters are going to be \"slightly\" off.  You'll never know by how much, you know they'll be 99.999% correct, but not 100%.  This is because, unless you specifically made this a <a href=\"http://www.mongodb.org/display/DOCS/Atomic+Operations\">mongodb atomic operation</a>, the operation is not guaranteed to have been atomic.</p>\n\n<p>So my question is, what is the correct interpretation of when and why MongoDB may not \"save correctly\"?  What parts of ACID does it not satisfy, and under what circumstances, and how do you know when that 0.001% of your data is off?  Can't this be fixed somehow?  If not, this seems to mean that you shouldn't store things like your <code>users</code> table in MongoDB, because a record might not save.  But then again, that 1/1,000,000 user might just need to \"try signing up again\", no?</p>\n\n<p>I am just looking for maybe a list of when/why negative things happen with an ACID noncompliant database like MongoDB, and ideally if there's a standard workaround (like run a background job to cleanup data, or only use SQL for this, etc.).</p>\n    ","a":"\n<p>One thing you lose with MongoDB is multi-collection (table) transactions.  Atomic modifiers in MongoDB can only work against a single document.</p>\n\n<p>If you need to remove an item from inventory and add it to someone's order at the same time - you cant.  Unless those two things - inventory and orders - exist in the same document (which they probably do not).</p>\n\n<p>I encountered this very issue in an application I am working on and two possible solutions exist:</p>\n\n<p>1) Structure your documents as best you can and use atomic modifiers as best you can and for he remaining bit, use a background process to cleanup records that may be out of sync.  For example, I remove items from inventory and add them to a reservedInventory array of the same document using atomic modifiers.  </p>\n\n<p>This lets me always know that items are NOT available in the inventory (because they are reserved by a customer).  When the customer check's out, I then remove the items from the reservedInventory.  Its not a standard transaction and since the customer could abandon the cart, I need some background process to go through and find abandoned carts and move the reserved inventory back into the available inventory pool.</p>\n\n<p>This is obviously less than ideal, but its the only part of a large application where mongodb does not fit the need perfectly.  Plus, it works flawlessly thus far.  This may not be possible for many scenarios, but because of the document structure I am using, it fits well.</p>\n\n<p>2) Use a transactional database in conjunction with MongoDB.  It is common to use MySQL to provide transactions for the things that absolutely need them while letting MongoDB (or any other NoSQL) do what it does best.</p>\n\n<p>If my solution from #1 does not work in the long run, I will investigate further into combining MongoDB with MySQL but for now #1 suits my needs well.</p>\n    "},{"t":"NoSQL - MongoDB vs CouchDB [on hold]","l":"http://stackoverflow.com/questions/3375494/nosql-mongodb-vs-couchdb","q":"\n\n<p>I am a complete noob when it comes to the NoSQL movement.  I have heard lots about MongoDB and CouchDB.  I know there are differences between the two.  Which do you recommend learning as a first step into the NoSQL world?</p>\n    ","a":"\n<p>See following links</p>\n\n<ul>\n<li><a href=\"http://www.slideshare.net/gabriele.lana/couchdb-vs-mongodb-2982288\">CouchDB Vs MongoDB</a></li>\n<li><a href=\"http://www.mongodb.org/display/DOCS/MongoDB,+CouchDB,+MySQL+Compare+Grid\">MongoDB, CouchDB, MySQL Compare Grid</a></li>\n<li><a href=\"http://stackoverflow.com/questions/895762/mongodb-or-couchdb-fit-for-production\">MongoDB or CouchDB - fit for production?</a></li>\n</ul>\n\n<p><strong>Update</strong>: I found great <a href=\"http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis\">comparison of NoSQL</a> databases.</p>\n\n<p><strong>MongoDB</strong></p>\n\n<ul>\n<li>Written in: C++</li>\n<li>Main point: Retains some friendly properties of SQL. (Query, index)</li>\n<li>License: AGPL (Drivers: Apache)</li>\n<li>Protocol: Custom, binary (BSON)</li>\n<li>Master/slave replication (auto failover with replica sets)</li>\n<li>Sharding built-in</li>\n<li>Queries are javascript expressions</li>\n<li>Run arbitrary javascript functions server-side</li>\n<li>Better update-in-place than CouchDB</li>\n<li>Uses memory mapped files for data storage</li>\n<li>Performance over features</li>\n<li>Journaling (with --journal) is best turned on</li>\n<li>On 32bit systems, limited to ~2.5Gb</li>\n<li>An empty database takes up 192Mb</li>\n<li>GridFS to store big data + metadata (not actually an FS)</li>\n<li>Has geospatial indexing</li>\n</ul>\n\n<p><strong>Best used</strong>: If you need dynamic queries. If you prefer to define indexes, not map/reduce functions. If you need good performance on a big DB. If you wanted CouchDB, but your data changes too much, filling up disks.</p>\n\n<p><strong>For example</strong>: For most things that you would do with MySQL or PostgreSQL, but having predefined columns really holds you back.</p>\n\n<p><strong>CouchDB (V1.1.1)</strong></p>\n\n<ul>\n<li>Written in: Erlang</li>\n<li>Main point: DB consistency, ease of use</li>\n<li>License: Apache</li>\n<li>Protocol: HTTP/REST</li>\n<li>Bi-directional (!) replication,</li>\n<li>continuous or ad-hoc,</li>\n<li>with conflict detection,</li>\n<li>thus, master-master replication. (!)</li>\n<li>MVCC - write operations do not block reads</li>\n<li>Previous versions of documents are available</li>\n<li>Crash-only (reliable) design</li>\n<li>Needs compacting from time to time</li>\n<li>Views: embedded map/reduce</li>\n<li>Formatting views: lists &amp; shows</li>\n<li>Server-side document validation possible</li>\n<li>Authentication possible</li>\n<li>Real-time updates via _changes (!)</li>\n<li>Attachment handling</li>\n<li>thus, <a href=\"http://couchapp.org\">CouchApps</a> (standalone js apps)</li>\n<li>jQuery library included</li>\n</ul>\n\n<p><strong>Best used</strong>: For accumulating, occasionally changing data, on which pre-defined queries are to be run. Places where versioning is important.</p>\n\n<p><strong>For example</strong>: CRM, CMS systems. Master-master replication is an especially interesting feature, allowing easy multi-site deployments.</p>\n    "},{"t":"Use cases for NoSQL","l":"http://stackoverflow.com/questions/2875432/use-cases-for-nosql","q":"\n\n<p>NoSQL has been getting a lot of attention in our industry recently. I'm really interested in what peoples thoughts are on the best use-cases for its use over relational database storage. What should trigger a developer into thinking that particular datasets are more suited to a NoSQL solution. I'm particularly interested in <a href=\"http://www.mongodb.org/\">MongoDB</a> and <a href=\"http://couchdb.apache.org/\">CouchDB</a> as they seem to be getting the most coverage with regard to PHP development and that is my focus.</p>\n    ","a":"\n<p>Some great use-cases - for MongoDB anyway - are mentioned on the MongoDB site. The examples given are real-time analytics, Logging and Full Text search. These articles are all well worth a read <a href=\"http://www.mongodb.com/use-cases\" rel=\"nofollow\">http://www.mongodb.com/use-cases</a></p>\n\n<p>There's also a great write-up on which NoSQL database is best suited to which type of project: <a href=\"http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis\" rel=\"nofollow\">http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis</a></p>\n    "},{"t":"What is NoSQL, how does it work, and what benefits does it provide? [closed]","l":"http://stackoverflow.com/questions/1145726/what-is-nosql-how-does-it-work-and-what-benefits-does-it-provide","q":"\n\n<p>I've been hearing things about NoSQL and that it may eventually become the replacement for SQL DB storage methods due to the fact that DB interaction is often a bottle neck for speed on the web.</p>\n\n<p>So I just have a few questions:</p>\n\n<ol>\n<li><p>What exactly is it?</p></li>\n<li><p>How does it work?</p></li>\n<li><p>Why would it be better than using a SQL Database? And how much better is it?</p></li>\n<li><p>Is the technology too new to start implementing yet or is it worth taking a look into?</p></li>\n</ol>\n    ","a":"\n<ol>\n<li><p><strong>What exactly is it?</strong></p>\n\n<p>On one hand, a <a href=\"http://www.strozzi.it/cgi-bin/CSA/tw7/I/en%5FUS/nosql/Home%20Page\">specific system</a>, but it has also become a generic word for a <a href=\"http://www.eflorenzano.com/blog/post/my-thoughts-nosql/\">variety of new data storage backends</a> that do not follow the relational DB model.</p></li>\n<li><p><strong>How does it work?</strong></p>\n\n<p>Each of the systems labelled with the generic name works differently, but the basic idea is to offer better scalability and performance by using DB models that don't support all the functionality of a generic RDBMS, but still enough functionality to be useful. In a way it's like MySQL, which at one time lacked support for transactions but, exactly <em>because</em> of that, managed to outperform other DB systems. If you could write your app in a way that didn't require transactions, it was great.</p></li>\n<li><p><strong>Why would it be better than using a SQL Database? And how much better is it?</strong></p>\n\n<p>It would be better when your site needs to scale so massively that the best RDBMS running on the best hardware you can afford and optimized as much as possible simply can't keep up with the load. How much better it is depends on the specific use case (lots of update activity combined with lots of joins is very hard on \"traditional\" RDBMSs) - could well be a factor of 1000 in extreme cases.</p></li>\n<li><p><strong>Is the technology too new to start implementing yet or is it worth taking a look into?</strong></p>\n\n<p>Depends mainly on what you're trying to achieve. It's certainly mature enough to use. But few applications really need to scale that massively. For most, a traditional RDBMS is sufficient. However, with internet usage becoming more ubiquitous all the time, it's quite likely that applications that do will become more common (though probably not dominant).</p></li>\n</ol>\n    "},{"t":"Non-Relational Database Design [closed]","l":"http://stackoverflow.com/questions/1189911/non-relational-database-design","q":"\n\n<p>I'm interested in hearing about design strategies you have used with <strong>non-relational \"nosql\" databases</strong> - that is, the (mostly new) class of data stores that don't use traditional relational design or SQL (such as Hypertable, CouchDB, SimpleDB, Google App Engine datastore, Voldemort, Cassandra, SQL Data Services, etc.). They're also often referred to as \"key/value stores\", and at base they act like giant distributed persistent hash tables.</p>\n\n<p>Specifically, I want to learn about the differences in <em>conceptual data design</em> with these new databases. What's easier, what's harder, what can't be done at all?</p>\n\n<ul>\n<li><p>Have you come up with alternate designs that work much better in the non-relational world?</p></li>\n<li><p>Have you hit your head against anything that seems impossible? </p></li>\n<li><p>Have you bridged the gap with any design patterns, e.g. to translate from one to the other? </p></li>\n<li><p>Do you even do explicit data models at all now (e.g. in UML) or have you chucked them entirely in favor of semi-structured / document-oriented data blobs?</p></li>\n<li><p>Do you miss any of the major extra services that RDBMSes provide, like relational integrity, arbitrarily complex transaction support, triggers, etc?</p></li>\n</ul>\n\n<p>I come from a SQL relational DB background, so normalization is in my blood. That said, I get the advantages of non-relational databases for simplicity and scaling, and my gut tells me that there has to be a richer overlap of design capabilities. What have you done?</p>\n\n<p>FYI, there have been StackOverflow discussions on similar topics here: </p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/282783/the-next-gen-databases\">the next generation of databases</a></li>\n<li><a href=\"http://stackoverflow.com/questions/731147/what-changes-do-i-need-for-my-tables-to-work-on-appengines-bigtable\">changing schemas to work with Google App Engine</a></li>\n<li><a href=\"http://stackoverflow.com/questions/337344/pros-cons-of-document-based-database-vs-relational-database\">choosing a document-oriented database</a></li>\n</ul>\n    ","a":"\n<p>I think you have to consider that the non-relational DBMS differ a lot regarding their data model and therefore the conceptual data design will also differ a lot. In the thread <a href=\"http://groups.google.com/group/nosql-discussion/browse_thread/thread/bbe3aa69071fd7b9/b5bb363c32f598c9\">Data Design in Non-Relational Databases</a> of the <a href=\"http://groups.google.com/group/nosql-discussion/\">NOSQL Google group</a> the different paradigms are categorized like this:</p>\n\n<ol>\n<li>Bigtable-like systems (HBase,\nHypertable, etc)</li>\n<li>Key-value stores (Tokyo, Voldemort,\netc)</li>\n<li>Document databases (CouchDB,\nMongoDB, etc)</li>\n<li>Graph databases (AllegroGraph,\nNeo4j, Sesame, etc)</li>\n</ol>\n\n<p>I'm mostly into <a href=\"http://stackoverflow.com/questions/1000162/have-anyone-used-graph-based-databases-http-neo4j-org\">graph databases</a>, and the elegance of data design using this paradigm was what brought me there, tired of the shortcomings of <a href=\"http://en.wikipedia.org/wiki/Relational_database_management_system\">RDBMS</a>. I have put a few examples of data design using a graph database on this <a href=\"http://wiki.neo4j.org/content/Domain_Modeling_Gallery\">wiki page</a> and there's an <a href=\"http://wiki.neo4j.org/content/IMDB_The_Domain\">example of how to model</a> the basic <a href=\"http://en.wikipedia.org/wiki/Internet_Movie_Database\">IMDB</a> movie/actor/role data too.</p>\n\n<p>The presentation slides (slideshare) <a href=\"http://www.slideshare.net/adorepump/graph-databases-and-the-future-of-largescale-knowledge-management\">Graph Databases and the Future of Large-Scale Knowledge Management</a> by <a href=\"http://markorodriguez.com/\">Marko Rodriguez</a> contains a very nice introduction to data design using a graph database as well.</p>\n\n<p><em>Answering the specific questions from a graphdb point of view:</em></p>\n\n<p>Alternate design: adding relationships between many different kinds of entities without any worries or a need to predefine which entities can get connected.</p>\n\n<p>Bridging the gap: I tend to do this different for every case, based on the domain itself, as I don't want a \"table-oriented graph\" and the like. However, <a href=\"http://wiki.neo4j.org/content/SQL_Importer\">here's</a> some information on automatic translation from RDBMS to graphdb.</p>\n\n<p>Explicit data models: I do these all the time (whiteboard style), and then use the model as it is in the DB as well.</p>\n\n<p>Miss from RDBMS world: easy ways to create reports. Update: maybe it's not <em>that</em> hard to create reports from a graph database, see <a href=\"http://www.reportsanywhere.com/pebble/2010/05/30/creating_a_report_for_a_neo4j_sample_database.html\">Creating a Report for a Neo4J Sample Database</a>.</p>\n    "},{"t":"Storing time-series data, relational or non?","l":"http://stackoverflow.com/questions/4814167/storing-time-series-data-relational-or-non","q":"\n\n<p>I am creating a system which polls devices for data on varying metrics such as CPU utilisation, disk utilisation, temperature etc. at (probably) 5 minute intervals using SNMP. The ultimate goal is to provide visualisations to a user of the system in the form of time-series graphs.</p>\n\n<p>I have looked at using RRDTool in the past, but rejected it as storing the captured data indefinitely is important to my project, and I want higher level and more flexible access to the captured data. So my quesiton is really:</p>\n\n<p><em>What is better, a relational database (such as MySQL or PostgreSQL) or a non-relational or NoSQL database (such as MongoDB or Redis) with regard to performance when querying data for graphing.</em></p>\n\n<h2>Relational</h2>\n\n<p>Given a relational database, I would use a <code>data_instances</code> table, in which would be stored every instance of data captured for every metric being measured for all devices, with the following fields:</p>\n\n<p>Fields: <code>id</code> <code>fk_to_device</code> <code>fk_to_metric</code> <code>metric_value</code> <code>timestamp</code></p>\n\n<p>When I want to draw a graph for a particular metric on a particular device, I must query this singular table <em>filtering out</em> the other devices, and the other metrics being analysed for this device:</p>\n\n<pre><code>SELECT metric_value, timestamp FROM data_instances\n    WHERE fk_to_device=1 AND fk_to_metric=2\n</code></pre>\n\n<p>The number of rows in this table would be:</p>\n\n<pre><code>d * m_d * f * t\n</code></pre>\n\n<p>where <code>d</code> is the number of <strong>devices</strong>, <code>m_d</code> is the accumulative <strong>number of metrics</strong> being recorded for all devices, <code>f</code> is the <strong>frequency</strong> at which data is polled for and <code>t</code> is the total amount of <strong>time</strong> the system has been collecting data.</p>\n\n<p>For a user recording 10 metrics for 3 devices every 5 minutes for a year, we would have just under <strong>5 million</strong> records. </p>\n\n<h3>Indexes</h3>\n\n<p>Without indexes on <code>fk_to_device</code> and <code>fk_to_metric</code> scanning this continuously expanding table would take too much time. So indexing the aforementioned fields and also <code>timestamp</code> (for creating graphs with localised periods) is a requirement.</p>\n\n<h2>Non-Relational (NoSQL)</h2>\n\n<p>MongoDB has the concept of a <em>collection</em>, unlike tables these can be created programmatically without setup. With these I could partition the storage of data for each device, or even each metric recorded for each device.</p>\n\n<p>I have no experience with NoSQL and do not know if they provide any query performance enhancing features such as indexing, however the previous paragraph proposes doing most of the traditional relational query work in the structure by which the data is stored under NoSQL.</p>\n\n<h2>Undecided</h2>\n\n<p>Would a relational solution with correct indexing reduce to a crawl within the year? Or does the collection based structure of NoSQL approaches (which matches my mental model of the stored data) provide a noticeable benefit?</p>\n    ","a":"\n<p>Definitely Relational.  Unlimited flexibility and expansion.</p>\n\n<p>Two corrections, both in concept and application, followed by an elevation.</p>\n\n<ol>\n<li><p>It is not \"filtering out the un-needed data\"; it is <strong>selecting only the needed data</strong>.  Yes, of course, if you have an Index to support the columns identified in the WHERE clause, it is very fast, and the query does not depend on the size of the table (grabbing 1,000 rows from a 16 billion row table is instantaneous).</p></li>\n<li><p>Your table has one serious impediment.  Given your description, the actual PK is (Device, Metric, DateTime).  (Please don't call it TimeStamp, that means something else, but that is a minor issue.)  The <code>Id</code> column is totally and completely redundant.  The uniqueness of the <strong>row</strong> is identified by:</p>\n\n<pre><code>   `(Device, Metric, DateTime)`\n</code></pre>\n\n<p>The <code>Id</code> column does nothing, it is superfluous (not redundant).  The additional Index to support the <code>Id</code> column obviously impedes the speed of INSERTs, and adds to the disk space used, you can get rid of it.</p></li>\n<li><p>Now that you have removed the impediment, you may not have recognised it, but your table is in Sixth Normal Form.  Very high speed, with just one Index on the PK. For understanding, read <a href=\"http://stackoverflow.com/questions/4394183/should-not-olap-database-be-denormalized-for-reading-performance/4731664#4731664\"><strong>this answer</strong></a> from the <strong>What is Sixth Normal Form ?</strong> heading onwards.</p>\n\n<ul>\n<li>(I have one index only, not three; on the Non-SQLs you may need three indices).  </li>\n</ul>\n\n<p>I have the exact same table (without the <code>Id</code> key, of course). I have an additional column <code>Server</code>. I support multiple customers remotely.  </p>\n\n<pre><code>   `(Server, Device, Metric, DateTime)`\n</code></pre>\n\n<p>The table can be used to Pivot the data (ie. <code>Devices</code> across the top and <code>Metrics</code> down the side, or pivoted) using exactly the same SQL code (yes, switch the cells).  I use the table to erect an unlimited variety of graphs and charts for customers re their server performance.  </p>\n\n<ul>\n<li><p><a href=\"http://www.softwaregems.com.au/Documents/Documentary%20Examples/sysmon%20Public.pdf\"><strong>Monitor Statistics Data Model</strong></a>.<br>\n(Too large for inline; some browsers cannot load inline; click the link.  Also that is the obsolete demo version, for obvious reasons, I cannot show you commercial product DM.)</p></li>\n<li><p>It allows me to produce <a href=\"http://www.softwaregems.com.au/Documents/Documentary%20Examples/sequoia%20091019%20Server%20Public.pdf\"><strong>Charts Like This</strong></a>, six keystrokes after receiving a raw monitoring stats file from the customer, using a <strong>single SELECT command</strong>. Notice the mix-and-match; OS and server on the same chart; a variety of Pivots.  Of course, there is no limit to the number of stats matrices, and thus the charts. (Used with the customer's kind permission.)</p></li>\n<li><p>Readers who are unfamiliar with the Standard for Modelling Relational Databases may find the <a href=\"http://www.softwaregems.com.au/Documents/Documentary%20Examples/IDEF1X%20Notation.pdf\"><strong>IDEF1X Notation</strong></a> helpful.</p></li>\n</ul></li>\n</ol>\n\n<p>Last but not least, SQL is a IEC/ISO/ANSI Standard.  The freeware is actually Non-SQL; it is fraudulent to use the term SQL if they do not provide the Standard.   They may provide \"extras\", but they are absent the basics.</p>\n    "},{"t":"How to empty a redis database?","l":"http://stackoverflow.com/questions/5756067/how-to-empty-a-redis-database","q":"\n\n<p>I've been playing with redis (and add some fun with it) during the last fews days and I'd like to know if there is a way to empty the db (remove the sets, the existing key....) easily.<br>\nDuring my tests, I created several sets with a lot of members, even created sets that I do not remember the name (how can I list those guys though ?).<br>\nAny idea about how to get rid of all of them ?</p>\n    ","a":"\n<p>You have two options:</p>\n\n<ul>\n<li><a href=\"http://redis.io/commands/flushdb\">FLUSHDB</a> - clears currently active database</li>\n<li><a href=\"http://redis.io/commands/flushall\">FLUSHALL</a> - clears all the existing databases</li>\n</ul>\n    "},{"t":"Amazon SimpleDB vs Amazon DynamoDB","l":"http://stackoverflow.com/questions/8961333/amazon-simpledb-vs-amazon-dynamodb","q":"\n\n<p>I have some basic understanding what <a href=\"http://aws.amazon.com/simpledb/\">Amazon SimpleDB</a> is, but according to the <a href=\"http://aws.amazon.com/dynamodb/\">Amazon DynamoDB</a> description it seems to be almost the same: a <a href=\"http://en.wikipedia.org/wiki/NoSQL#Key-value_store\">NoSQL Key-value store</a> service.</p>\n\n<p>Can someone simply explain the main differences between them and tell in which cases to choose one over the other.</p>\n    ","a":"\n<p>This is addressed by the respective FAQ <a href=\"http://aws.amazon.com/dynamodb/faqs/#How_does_Amazon_DynamoDB_differ_from_Amazon_SimpleDB_Which_should_I_use\">Q: How does Amazon DynamoDB differ from Amazon SimpleDB? Which should I use?</a> (hash link no longer works, but use in-page Find to locate question within page) to some extent already, with the most compact summary at the end of the paragraph:</p>\n\n<blockquote>\n  <p>While SimpleDB has scaling limitations, it may be a good fit for\n  smaller workloads that require query flexibility. Amazon SimpleDB\n  automatically indexes all item attributes and thus supports query\n  flexibility at the cost of performance and scale.</p>\n</blockquote>\n\n<p>So it's a trade off between performance/scalability and simplicity/flexibility, i.e. for simpler scenarios it might still be easier getting started with SimpleDB to avoid the complexities of architecturing your application for DynamoDB (see below for a different perspective).</p>\n\n<p>The linked FAQ entry references Werner Vogel's <a href=\"http://www.allthingsdistributed.com/2012/01/amazon-dynamodb.html\">Amazon DynamoDB â€“ a Fast and Scalable NoSQL Database Service Designed for Internet Scale Applications</a> as well, which is indeed an elaborate and thus highly recommended read concerning the <em>History of NoSQL at Amazon</em> in general and Dynamo in particular; it contains many more insights addressing your question as well, e.g.</p>\n\n<blockquote>\n  <p>It became obvious that developers <em>[even Amazon engineers]</em> strongly preferred simplicity to\n  fine-grained control as they voted \"with their feet\" and adopted\n  cloud-based AWS solutions, like Amazon S3 and Amazon SimpleDB, over\n  Dynamo. <em>[addition mine]</em></p>\n</blockquote>\n\n<p>Obviously DynamoDB has been introduced to address this and could thus be qualified as a successor of SimpleDB rather than 'just' amending their existing NoSQL offering:</p>\n\n<blockquote>\n  <p>We concluded that an ideal solution would combine the best parts of\n  the original Dynamo design (incremental scalability, predictable high\n  performance) with the best parts of SimpleDB (ease of administration\n  of a cloud service, consistency, and a table-based data model that is\n  richer than a pure key-value store).</p>\n</blockquote>\n\n<p>Werner's <em>Summary</em> suggests DynamoDB to be a good fit for applications of any size now accordingly:</p>\n\n<blockquote>\n  <p>Amazon DynamoDB is designed to maintain predictably high performance\n  and to be highly cost efficient for workloads of any scale, from the\n  smallest to the largest internet-scale applications.</p>\n</blockquote>\n    "},{"t":"Difference between scaling horizontally and vertically for databases","l":"http://stackoverflow.com/questions/11707879/difference-between-scaling-horizontally-and-vertically-for-databases","q":"\n\n<p>I have come across many NoSQL databases and SQL databases. There are varying parameters to measure the strength and weaknesses of these databases and scalability is one of them. What is the difference between horizontally and vertically scaling these databases?</p>\n    ","a":"\n<p>Horizontal scaling means that you scale by adding more machines into your pool of resources where Vertical scaling means that you scale by adding more power (CPU, RAM) to your existing machine.</p>\n\n<p>In a database world horizontal-scaling is often based on partitioning of the data i.e. each node contains only part of the data , in vertical-scaling the data resides on a single node and scaling is done through multi-core i.e. spreading the load between the CPU and RAM resources of that machine.</p>\n\n<p>With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool - Vertical-scaling is often limited to the capacity of a single machine, scaling beyond that capacity often involves downtime and comes with an upper limit.</p>\n\n<p>A good example for horizontal scaling is Cassandra , MongoDB .. and a good example for vertical scaling is MySQL - Amazon RDS (The cloud version of MySQL) provides an easy way to scale vertically by switching from small to bigger machines this process often involves downtime.</p>\n\n<p>In-Memory Data Grids such as <a href=\"http://www.gigaspaces.com/datagrid\">GigaSpaces XAP</a>, <a href=\"http://www.oracle.com/technetwork/middleware/coherence/overview/index.html\">Coherence</a> etc.. are often optimized for both horizontal and vertical scaling simply because they're not bound to disk. Horizontal-scaling through partitioning and vertical-scaling through multi-core support.  </p>\n\n<p>You can read more on this subject on my earlier posts:\n<a href=\"http://ht.ly/cAhPe\">Scale-out vs Scale-up</a> and <a href=\"http://ht.ly/cAhY6\">The Common Principles Behind the NOSQL Alternatives</a> </p>\n    "},{"t":"How do you query this in Mongo? (is not null)","l":"http://stackoverflow.com/questions/4057196/how-do-you-query-this-in-mongo-is-not-null","q":"\n\n<pre><code>db.mycollection.find(HAS IMAGE URL)\n</code></pre>\n    ","a":"\n<p>This will return all documents with a key called \"IMAGE URL\", but they may still have a null value.</p>\n\n<pre><code>db.mycollection.find({\"IMAGE URL\":{$exists:true}});\n</code></pre>\n\n<p>This will return all documents with both a key called \"IMAGE URL\" <em>and</em> a non-null value.</p>\n\n<pre><code>db.mycollection.find({\"IMAGE URL\":{$ne:null}});\n</code></pre>\n\n<p>Also, according to the docs, $exists currently can't use an index, but $ne can.</p>\n\n<p><strong>Edit: Adding some examples due to interest in this answer</strong></p>\n\n<p>Given these inserts:</p>\n\n<pre><code>db.test.insert({\"num\":1, \"check\":\"check value\"});\ndb.test.insert({\"num\":2, \"check\":null});\ndb.test.insert({\"num\":3});\n</code></pre>\n\n<p>This will return all three documents:</p>\n\n<pre><code>db.test.find();\n</code></pre>\n\n<p>This will return the first and second documents only:</p>\n\n<pre><code>db.test.find({\"check\":{$exists:true}});\n</code></pre>\n\n<p>This will return the first document only:</p>\n\n<pre><code>db.test.find({\"check\":{$ne:null}});\n</code></pre>\n\n<p>This will return the second and third documents only:</p>\n\n<pre><code>db.test.find({\"check\":null})\n</code></pre>\n    "},{"t":"NoSql Crash Course/Tutorial [closed]","l":"http://stackoverflow.com/questions/2328169/nosql-crash-course-tutorial","q":"\n\n<p>I've seen NoSQL pop up quite a bit on SO and I have a solid understanding of <strong>why</strong> you would use it (from here, Wikipedia, etc).  This could be due to the lack of concrete and uniform definition of what it is (more of a paradigm than concrete implementation), but I'm struggling to wrap my head around how I would go about designing a system that would use it or how I would implement it in my system.  I'm really stuck in a relational-db mindset thinking of things in terms of tables and joins...</p>\n\n<p>At any rate, does anybody know of a crash course/tutorial on a system that would use it (kind of a \"hello world\" for a NoSQL-based system) or a tutorial that takes an existing \"Hello World\" app based on SQL and converts it to NoSQL (not necessarily in code, but just a high-level explanation).</p>\n    ","a":"\n<p><a href=\"http://www.slideshare.net/drumwurzel/intro-to-mongodb\">Here is a decent slide show</a> introducing MongoDB. I think some of the big differences is that most of the systems rely on Active Record or some similar database abstraction.</p>\n\n<p>Also I found a wonderful <a href=\"http://books.couchdb.org/relax/\">free orlys book on Couch DB here</a>, which is pretty awesome. </p>\n    "},{"t":"Is there any NoSQL that is ACID compliant?","l":"http://stackoverflow.com/questions/2608103/is-there-any-nosql-that-is-acid-compliant","q":"\n\n<p>Is there any <a href=\"http://en.wikipedia.org/wiki/NoSQL\">NoSQL</a> that is <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a> compliant? </p>\n\n<p>(Or is that even possible with NoSQL given it's just a bunch of loosely coupled key-value pairs.)</p>\n    ","a":"\n<p>I'll post this as an answer purely to support the conversation - <a href=\"http://stackoverflow.com/questions/2608103/is-there-any-nosql-that-is-acid-compliant/2608318#2608318\">Tim Mahy</a> , <a href=\"http://stackoverflow.com/questions/2608103/is-there-any-nosql-that-is-acid-compliant/2612878#2612878\">nawroth</a> , and <a href=\"http://stackoverflow.com/questions/2608103/is-there-any-nosql-that-is-acid-compliant/2608223#2608223\">CraigTP</a> have suggested viable databases. <a href=\"http://couchdb.apache.org/\">CouchDB</a> would be my preferred due to the use of <a href=\"http://erlang.org/\">Erlang</a>, but there are others out there.</p>\n\n<p>I'd say <strong>ACID</strong> does not contradict or negate the concept of <strong>NoSQL</strong>... While there seems to be a trend following the opinion expressed by <a href=\"http://stackoverflow.com/questions/2608103/is-there-any-nosql-that-is-acid-compliant/2608121#2608121\">dove</a> , I would argue the concepts are distinct.</p>\n\n<p><strong>NoSQL</strong> is fundamentally about simple key-value (e.g. Redis) or document-style schema (collected key-value pairs in a \"document\" model, e.g. MongoDB) as a direct alternative to the explicit schema in classical RDBMSs. It allows the developer to treat <em>things</em> asymmetrically, whereas traditional engines have enforced rigid <em>same-ness</em> across the data model. The reason this is so interesting is because <em>it provides a different way to deal with change</em>, and for larger data sets it provides interesting opportunities to deal with volumes and performance.</p>\n\n<p><strong>ACID</strong> provides principles governing how changes are applied to a database. In a very simplified way, it states (my own version):</p>\n\n<ul>\n<li>(A) when you do something to change a database the change should work or fail as a whole</li>\n<li>(C) the database should remain consistent (this is a pretty broad topic)</li>\n<li>(I) if other things are going on at the same time they shouldn't be able to see things mid-update</li>\n<li>(D) if the system blows up (hardware or software) the database needs to be able to pick itself back up; and if it says it finished applying an update, it needs to be certain</li>\n</ul>\n\n<p>The conversation gets a little more excitable when it comes to the idea of <a href=\"http://en.wikipedia.org/wiki/Propagation_constraint\">propagation and constraints</a>. Some RDBMS engines provide the ability to enforce constraints (e.g. foreign keys) which may have propagation elements (a la <em>cascade</em>). In simpler terms, one \"thing\" may have a relationship with another \"thing\" in the database, and if you change an attribute of one it may require the other be changed (updated, deleted, ... lots of options). <strong>NoSQL</strong> databases, being predominantly (at the moment) focused on high data volumes and high traffic, seem to be tackling the idea of distributed updates which take place within (from a consumer perspective) arbitrary time frames. This is basically a specialized form of <a href=\"http://en.wikipedia.org/wiki/Database_replication\">replication</a> managed via <a href=\"http://en.wikipedia.org/wiki/Database_transaction\">transaction</a> - so I would say that if a traditional distributed database can support ACID, so can a NoSQL database.</p>\n\n<p>Some resources for further reading:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/ACID\">Wikipedia article on ACID</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Propagation_constraint\">Wikipedia on propagation constraints</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Database_normalization#Normal_forms\">Wikipedia (yeah, I like the site, ok?) on database normalization</a></li>\n<li><a href=\"http://couchdb.apache.org/docs/overview.html\">Apache documentation on CouchDB with a good overview of how it applies ACID</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Cluster_%28computing%29\">Wikipedia on</a> Cluster Computing</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Database_transaction\">Wikipedia (again...) on database transactions</a></li>\n</ul>\n    "},{"t":"MongoDB with redis","l":"http://stackoverflow.com/questions/10696463/mongodb-with-redis","q":"\n\n<p>Can anyone give example use cases of when you would benefit from using Redis and MongoDB in conjunction with each other?</p>\n    ","a":"\n<p>Redis and MongoDB can be used together with good results. A company well-known for running MongoDB and Redis (along with MySQL and Sphinx) is Craiglist. See <a href=\"http://www.slideshare.net/jzawodn/living-with-sql-and-nosql-at-craigslist-a-pragmatic-approach\">this presentation</a> from Jeremy Zawodny.</p>\n\n<p>MongoDB is interesting for persistent, document oriented, data indexed in various ways. Redis is more interesting for volatile data, or latency sensitive semi-persistent data.</p>\n\n<p>Here are a few examples of concrete usage of Redis on top of MongoDB.</p>\n\n<ul>\n<li><p>Pre-2.2 MongoDB does not have yet an expiration mechanism. Capped collections cannot really be used to implement a real TTL. Redis has a TTL-based expiration mechanism, making it convenient to store volatile data. For instance, user sessions are commonly stored in Redis, while user data will be stored and indexed in MongoDB. Note that MongoDB 2.2 has introduced a low accuracy expiration mechanism at the collection level (to be used for purging data for instance).</p></li>\n<li><p>Redis provides a convenient set datatype and its associated operations (union, intersection, difference on multiple sets, etc ...). It is quite easy to implement a basic faceted search or tagging engine on top of this feature, which is an interesting addition to MongoDB more traditional indexing capabilities.</p></li>\n<li><p>Redis supports efficient blocking pop operations on lists. This can be used to implement an ad-hoc distributed queuing system. It is more flexible than MongoDB tailable cursors IMO, since a backend application can listen to several queues with a timeout, transfer items to another queue atomically, etc ... If the application requires some queuing, it makes sense to store the queue in Redis, and keep the persistent functional data in MongoDB.</p></li>\n<li><p>Redis also offers a pub/sub mechanism. In a distributed application, an event propagation system may be useful. This is again an excellent use case for Redis, while the persistent data are kept in MongoDB.</p></li>\n</ul>\n\n<p>Because it is much easier to design a data model with MongoDB than with Redis (Redis is more low-level), it is interesting to benefit from the flexibility of MongoDB for main persistent data, and from the extra features provided by Redis (low latency, item expiration, queues, pub/sub, atomic blocks, etc ...). It is indeed a good combination.</p>\n\n<p>Please note you should never run a Redis and MongoDB server on the same machine. MongoDB memory is designed to be swapped out, Redis is not. If MongoDB triggers some swapping activity, the performance of Redis will be catastrophic. They should be isolated on different nodes.</p>\n    "},{"t":"NoSQL Use Case Scenarios or WHEN to use NoSQL [closed]","l":"http://stackoverflow.com/questions/10553860/nosql-use-case-scenarios-or-when-to-use-nosql","q":"\n\n<p>With all the hype I'm find it really hard to find reliable information on when to use this. So I pose the following questions, and I'm sorry if these are really dumb questions in advance:</p>\n\n<ol>\n<li>Should I use NoSQL for user data? E.g. profiles, usernames + passwords, etc.</li>\n<li>Should I use NoSQL for important content? E.g. articles, blog posts, product inventory, etc.</li>\n</ol>\n\n<p>I'm assuming no? And I feel like NoSQL is just for quickly accessible things from which it's OK to lose data. But I also read that NoSQL apps have built-in redundancy so that I don't lose data?</p>\n\n<p>Also if the above 2 examples are bad, could you give me specific business use cases where I would use NoSQL? I see a lot of general descriptions but not a lot of real-world examples. The only things I can think of are user-to-user messaging and analytics.</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p>It really is an \"it depends\" kinda question. Some <strong><em>general</em></strong> points:</p>\n\n<ul>\n<li>NoSQL is typically good for unstructured/\"schemaless\" data - usually, you don't need to explicitly define your schema up front and can just include new fields without any ceremony</li>\n<li>NoSQL typically favours a denormalised schema due to no support for JOINs per the RDBMS world. So you would usually have a flattened, denormalized representation of your data.</li>\n<li>Using NoSQL doesn't mean you could lose data. Different dbs have different strategies. e.g. MongoDB - you can essentially choose what level to trade off performance vs potential for data loss - best performance = greater scope for data loss.</li>\n<li>It's often very easy to scale out NoSQL solutions. Adding more nodes to replicate data to is one way to a) offer more scalability and b) offer more protection against data loss if one node goes down. But again, depends on the NoSQL db/configuration. NoSQL does not necessarily mean \"data loss\" like you infer.</li>\n<li>IMHO, complex/dynamic queries/reporting are best served from an RDBMS. Often the query functionality for a NoSQL db is limited.</li>\n<li>It doesn't have to be a 1 or the other choice. My experience has been using RDBMS in conjunction with NoSQL for certain use cases.</li>\n<li>NoSQL db's often lack the ability to perform atomic operations across multiple \"tables\".</li>\n</ul>\n\n<p>You really need to look at and understand what the various types of NoSQL stores are, and how they go about providing scalability/data security etc. It's difficult to give an across-the-board answer as they really are all different and tackle things differently.</p>\n\n<p>For MongoDb as an example, check out their <a href=\"http://www.mongodb.org/display/DOCS/Use+Cases\">Use Cases</a> to see what they suggest as being \"well suited\" and \"less well suited\" uses of MongoDb. </p>\n    "},{"t":"Update MongoDB field using value of another field","l":"http://stackoverflow.com/questions/3974985/update-mongodb-field-using-value-of-another-field","q":"\n\n<p>In MongoDB, is it possible to update the value of a field using the value from another field?  The equivalent SQL would be something like:</p>\n\n<pre><code>UPDATE Person SET Name = FirstName + ' ' + LastName\n</code></pre>\n\n<p>And the MongoDB pseudo-code would be:</p>\n\n<pre><code>db.person.update( {}, { $set : { name : firstName + ' ' + lastName } );\n</code></pre>\n    ","a":"\n<p>You cannot refer to the document itself in an update (yet). You'll need to iterate through the documents and update each document using a function. See <a href=\"http://stackoverflow.com/questions/3788256/mongodb-updating-documents-using-data-from-the-same-document/3792958#3792958\">this answer</a> for an example, or <a href=\"http://stackoverflow.com/a/8343147/1269037\">this one</a> for server-side <code>eval()</code>.</p>\n    "},{"t":"NoSql vs Relational database","l":"http://stackoverflow.com/questions/4160732/nosql-vs-relational-database","q":"\n\n<p>Recently NoSQL get popular. I want to know what's the edge of the NoSQL over traditional RDBMS.</p>\n    ","a":"\n<p>Not all data is relational.  For those situations, NoSQL can be helpful.</p>\n\n<p>With that said, NoSQL stands for \"Not Only SQL\".  It's not intended to knock SQL or supplant it.  </p>\n\n<p>SQL has several very big advantages: </p>\n\n<ol>\n<li>Strong mathematical basis.</li>\n<li>Declarative syntax.</li>\n<li>A well-known language in Structured Query Language (SQL).</li>\n</ol>\n\n<p>Those haven't gone away.</p>\n\n<p>It's a mistake to think about this as an either/or argument.  NoSQL is an alternative that people need to consider when it fits, that's all.</p>\n\n<p>Documents can be stored in non-relational databases, like CouchDB.</p>\n\n<p>Maybe reading <a href=\"http://nosql-database.org/\">this</a> will help.</p>\n    "},{"t":"Large scale data processing Hbase vs Cassandra [closed]","l":"http://stackoverflow.com/questions/7237271/large-scale-data-processing-hbase-vs-cassandra","q":"\n\n<p>I am nearly landed at Cassandra after my research on large scale data storage solutions. But its generally said that Hbase is better solution for large scale data processing and analysis. </p>\n\n<p>While both are same key/value storage and both are/can run (Cassandra recently) Hadoop layer then what makes Hadoop a better candidate when processing/analysis is required on large data.</p>\n\n<p>I also found good details about both at\n<a href=\"http://ria101.wordpress.com/2010/02/24/hbase-vs-cassandra-why-we-moved/\">http://ria101.wordpress.com/2010/02/24/hbase-vs-cassandra-why-we-moved/</a> </p>\n\n<p>but I'm still looking for concrete advantages of Hbase.</p>\n\n<p>While I am more convinced about Cassandra because its simplicity for adding nodes and seamless replication and no point of failure features. And it also keeps secondary index feature so its a good plus.</p>\n    ","a":"\n<p>Trying to determine which is best for you really depends on what you are going to use it for, they each have their advantages and without any more details it becomes more of a religious war. That post you referenced is also more than a year old and both have gone through many changes since then. Please also keep in mind I am not familiar with the more recent Cassandra developments.</p>\n\n<p>Having said that, I'll paraphrase HBase committer Andrew Purtell and add some of my own experiences:</p>\n\n<ul>\n<li><p>HBase is in larger production environments (1000 nodes) although that is still in the ballpark of Cassandra's ~400 node installs so its really a marginal difference.</p></li>\n<li><p>HBase and Cassandra both supports replication between clusters/datacenters. I believe HBase's exposes more to the user so it appears more complicated but then you also get more flexibility.</p></li>\n<li><p>If strong consistency is what your application needs then HBase is likely a better fit. It is designed from the ground up to be consistent. For example it allows for simpler implementation of atomic counters (I think Cassandra just got them) as well as Check and Put operations.</p></li>\n<li><p>Write performance is great, from what I understand that was one of the reasons Facebook went with HBase for their messenger.</p></li>\n<li><p>I'm not sure of the current state of Cassandra's ordered partitioner, but in the past it required manual rebalancing. HBase handles that for you if you want. The ordered partitioner is important for Hadoop style processing.</p></li>\n<li><p>Cassandra and HBase are both complex, Cassandra just hides it better. HBase exposes it more via using HDFS for its storage, if you look at the codebase Cassandra is just as layered. If you compare the Dynamo and Bigtable papers you can see that Cassandra's theory of operation is actually more complex.</p></li>\n<li><p>HBase has more unit tests FWIW.</p></li>\n<li><p>All Cassandra RPC is Thrift, HBase has a Thrift, REST and native Java. The Thrift and REST do only offer a subset of the total client API but if you want pure speed the native Java client is there.</p></li>\n<li><p>There are advantages to both peer to peer and master to slave. The master - slave setup generally makes it easier to debug and reduces quite a bit of complexity.</p></li>\n<li><p>HBase is not tied to only traditional HDFS, you can change out your underlying storage depending on your needs. <a href=\"http://www.mapr.com/\">MapR</a> looks quite interesting and I have heard good things although I have not used it myself.</p></li>\n</ul>\n    "},{"t":"Can I do transactions and locks in CouchDB?","l":"http://stackoverflow.com/questions/299723/can-i-do-transactions-and-locks-in-couchdb","q":"\n\n<p>I need to do transactions (begin, commit or rollback), locks (select for update).\nHow can I do it in a document model db?</p>\n\n<p>Edit:</p>\n\n<p>The case is this:</p>\n\n<ul>\n<li>I want to run an auctions site.</li>\n<li>And I think how to direct purchase as well.</li>\n<li>In a direct purchase I have to decrement the quantity field in the item record, but only if the quantity is greater than zero. That is why I need locks and transactions.</li>\n<li>I don't know how to address that without locks and/or transactions.</li>\n</ul>\n\n<p>Can I solve this with CouchDB?</p>\n    ","a":"\n<p>No.  CouchDB uses an \"optimistic concurrency\" model.  In the simplest terms, this just means that you send a document version along with your update, and CouchDB rejects the change if the current document version doesn't match what you've sent.</p>\n\n<p>It's deceptively simple, really.  You can reframe many normal transaction based scenarios for CouchDB. You do need to sort of throw out your RDBMS domain knowledge when learning CouchDB, though.  It's helpful to approach problems from a higher level, rather than attempting to mold Couch to a SQL based world.</p>\n\n<p><strong>Keeping track of inventory</strong></p>\n\n<p>The problem you outlined is primarily an inventory issue.  If you have a document describing an item, and it includes a field for \"quantity available\", you can handle concurrency issues like this:</p>\n\n<ol>\n<li>Retrieve the document, take note of the <code>_rev</code> property that CouchDB sends along</li>\n<li>Decrement the quantity field, if it's greater than zero</li>\n<li>Send the updated document back, using the <code>_rev</code> property</li>\n<li>If the <code>_rev</code> matches the currently stored number, be done!</li>\n<li>If there's a conflict (when <code>_rev</code> doesn't match), retrieve the newest document version</li>\n</ol>\n\n<p>In this instance, there are two possible failure scenarios to think about.  If the most recent document version has a quantity of 0, you handle it just like you would in a RDBMS and alert the user that they can't actually buy what they wanted to purchase.  If the most recent document version has a quantity greater than 0, you simply repeat the operation with the updated data, and start back at the beginning.  This forces you to do a bit more work than an RDBMS would, and could get a little annoying if there are frequent, conflicting updates.</p>\n\n<p>Now, the answer I just gave presupposes that you're going to do things in CouchDB in much the same way that you would in an RDBMS.  I might approach this problem a bit differently:</p>\n\n<p>I'd start with a \"master product\" document that includes all the descriptor data (name, picture, description, price, etc).  Then I'd add an \"inventory ticket\" document for each specific instance, with fields for <code>product_key</code> and <code>claimed_by</code>.  If you're selling a model of hammer, and have 20 of them to sell, you might have documents with keys like <code>hammer-1</code>, <code>hammer-2</code>, etc, to represent each available hammer.</p>\n\n<p>Then, I'd create a view that gives me a list of available hammers, with a reduce function that lets me see a \"total\".  These are completely off the cuff, but should give you an idea of what a working view would look like.</p>\n\n<p><strong>Map</strong></p>\n\n<pre><code>function(doc) \n{ \n    if (doc.type == 'inventory_ticket' &amp;&amp; doc.claimed_by == null ) { \n        emit(doc.product_key, { 'inventory_ticket' :doc.id, '_rev' : doc._rev }); \n    } \n}\n</code></pre>\n\n<p>This gives me a list of available \"tickets\", by product key.  I could grab a group of these when someone wants to buy a hammer, then iterate through sending updates (using the <code>id</code> and <code>_rev</code>) until I successfully claim one (previously claimed tickets will result in an update error).</p>\n\n<p><strong>Reduce</strong></p>\n\n<pre><code>function (keys, values, combine) {\n    return values.length;\n}\n</code></pre>\n\n<p>This reduce function simply returns the total number of unclaimed <code>inventory_ticket</code> items, so you can tell how many \"hammers\" are available for purchase.</p>\n\n<p><strong>Caveats</strong></p>\n\n<p>This solution represents roughly 3.5 minutes of total thinking for the particular problem you've presented.  There may be better ways of doing this!  That said, it does substantially reduce conflicting updates, and cuts down on the need to respond to a conflict with a new update.  Under this model, you won't have multiple users attempting to change data in primary product entry.  At the very worst, you'll have multiple users attempting to claim a single ticket, and if you've grabbed several of those from your view, you simply move on to the next ticket and try again.</p>\n\n<p>Reference: <a href=\"https://wiki.apache.org/couchdb/Frequently_asked_questions#How_do_I_use_transactions_with_CouchDB.3F\" rel=\"nofollow\">https://wiki.apache.org/couchdb/Frequently_asked_questions#How_do_I_use_transactions_with_CouchDB.3F</a></p>\n    "},{"t":"Add new field to a collection in MongoDB [closed]","l":"http://stackoverflow.com/questions/7714216/add-new-field-to-a-collection-in-mongodb","q":"\n\n<p>I can't find how to add a new field to an existent collection.</p>\n\n<p>I know how to update an existent collection's field, but not how to add a new not-existent field in a collection. Also, how do I add new fields to all the collection's documents?</p>\n    ","a":"\n<p>Same as the updating existing collection field, <a href=\"http://www.mongodb.org/display/DOCS/Updating#Updating-%24set\">$set</a> will add a new fields if the specified field does not exist.</p>\n\n<p>Check out the sample</p>\n\n<pre><code>&gt; db.foo.find()\n&gt; db.foo.insert({\"test\":\"a\"})\n&gt; db.foo.find()\n{ \"_id\" : ObjectId(\"4e93037bbf6f1dd3a0a9541a\"), \"test\" : \"a\" }\n&gt; item = db.foo.findOne()\n{ \"_id\" : ObjectId(\"4e93037bbf6f1dd3a0a9541a\"), \"test\" : \"a\" }\n&gt; db.foo.update({\"_id\" :ObjectId(\"4e93037bbf6f1dd3a0a9541a\") },{$set : {\"new_field\":1}})\n&gt; db.foo.find()\n{ \"_id\" : ObjectId(\"4e93037bbf6f1dd3a0a9541a\"), \"new_field\" : 1, \"test\" : \"a\" }\n</code></pre>\n\n<p><strong>EDIT:</strong></p>\n\n<p>In case you want to add a new_field to all your collection, you have to use empty selector, and set multi flag to true (last param) to update all the documents</p>\n\n<blockquote>\n  <p>db.your_collection.update({},{$set : {\"new_field\":1}},false,true)</p>\n</blockquote>\n\n<p><strong>EDIT:</strong></p>\n\n<p>In the above example last 2 fields <code>false, true</code> specifies the <code>upsert</code> and <code>multi</code> flags.  </p>\n\n<p><strong>Upsert:</strong>  If set to true, creates a new document when no document matches the query criteria. </p>\n\n<p><strong>Multi:</strong>   If set to true, updates multiple documents that meet the query criteria. If set to false, updates one document.</p>\n\n<p>This is for Mongo <code>versions</code> prior to <code>2.2</code>. For latest versions the query is changed a bit</p>\n\n<pre><code>db.your_collection.update({},\n                          {$set : {\"new_field\":1}},\n                          {upsert:false,\n                          multi:true}) \n</code></pre>\n    "},{"t":"Possibility of duplicate Mongo ObjectId's being generated in two different collections?","l":"http://stackoverflow.com/questions/4677237/possibility-of-duplicate-mongo-objectids-being-generated-in-two-different-colle","q":"\n\n<p>Is it possible for the same exact Mongo ObjectId to be generated for a document in two different collections?  I realize that it's definitely very unlikely, but is it possible?  </p>\n\n<p>Without getting too specific, the reason I ask is that with an application that I'm working on we show public profiles of elected officials who we hope to convert into full fledged users of our site.  We have separate collections for users and the elected officials who aren't currently members of our site.  There are various other documents containing various pieces of data about the elected officials that all map back to the person using their elected official ObjectId.</p>\n\n<p>After creating the account we still highlight the data that's associated to the elected official but they now also are a part of the users collection with a corresponding users ObjectId to map their profile to interactions with our application.</p>\n\n<p>We had begun converting our application from MySql to Mongo a few months ago and while we're in transition we store the legacy MySql id for both of these data types and we're also starting to now store the elected official Mongo ObjectId in the users document to map back to the elected official data.</p>\n\n<p>I was pondering just specifying the new user ObjectId as the previous elected official ObjectId to make things simpler but wanted to make sure that it wasn't possible to have a collision with any existing user ObjectId.</p>\n\n<p>Thanks for your insight.</p>\n\n<p>Edit:  Shortly after posting this question, I realized that my proposed solution wasn't a very good idea.  It would be better to just keep the current schema that we have in place and just link to the elected official '_id' in the users document.</p>\n    ","a":"\n<p><strong>Short Answer</strong></p>\n\n<p>Just to add a direct response to your initial question: YES, if you use BSON Object ID generation, then <em>for most drivers</em> the IDs are almost certainly going to be unique across collections. See below for what \"almost certainly\" means.</p>\n\n<p><strong>Long Answer</strong></p>\n\n<p>The BSON Object ID's generated by Mongo DB drivers are highly likely to be unique across collections. This is mainly because of the last 3 bytes of the ID, which <em>for most drivers</em> is generated via a static incrementing counter. That counter is collection-independent; it's global. The Java driver, for example, uses a randomly initialized, static AtomicInteger. </p>\n\n<p>So why, in the Mongo docs, do they say that the IDs are \"highly likely\" to be unique, instead of outright saying that they WILL be unique? Three possibilities can occur where you won't get a unique ID (please let me know if there are more):</p>\n\n<p>Before this discussion, recall that the BSON Object ID consists of:</p>\n\n<p>[4 bytes seconds since epoch, 3 bytes machine hash, 2 bytes process ID, 3 bytes counter]</p>\n\n<p>Here are the three possibilities, so you judge for yourself how likely it is to get a dupe:</p>\n\n<p>1) Counter overflow: there are 3 bytes in the counter. If you happen to insert over 16,777,216 (2^24) documents in a single second, on the same machine, in the same process, then you may overflow the incrementing counter bytes and end up with two Object IDs that share the same time, machine, process, and counter values.</p>\n\n<p>2) Counter non-incrementing: some Mongo drivers use random numbers instead of incrementing numbers for the counter bytes. In these cases, there is a 1/16,777,216 chance of generating a non-unique ID, but only if those two IDs are generated in the same second (i.e. before the time section of the ID updates to the next second), on the same machine, in the same process.</p>\n\n<p>3) Machine and process hash to the same values. The machine ID and process ID values may, in some highly unlikely scenario, map to the same values for two different machines. If this occurs, and at the same time the two counters on the two different machines, during the same second, generate the same value, then you'll end up with a duplicate ID.</p>\n\n<p>These are the three scenarios to watch out for. Scenario 1 and 3 seem highly unlikely, and scenario 2 is totally avoidable if you're using the right driver. You'll have to check the source of the driver to know for sure. </p>\n    "},{"t":"When shouldn't you use a relational database? [closed]","l":"http://stackoverflow.com/questions/667141/when-shouldnt-you-use-a-relational-database","q":"\n\n<p>Apart from the google/bigtable scenario, when shouldn't you use a relational database? Why not, and what should you use? (did you learn 'the hard way'?) </p>\n    ","a":"\n<p>In my experience, you shouldn't use a relational database when any one of these criteria are true:</p>\n\n<ul>\n<li>your data is structured as a hierarchy or a graph (network) of arbitrary depth,</li>\n<li>the typical access pattern emphasizes reading over writing, or</li>\n<li>thereâ€™s no requirement for ad-hoc queries.</li>\n</ul>\n\n<p>Deep hierarchies and graphs do not translate well to relational tables. Even with the assistance of proprietary extensions like Oracle's <code>CONNECT BY</code>, chasing down trees is a mighty pain using SQL.</p>\n\n<p>Relational databases add a lot of overhead for simple read access. Transactional and referential integrity are powerful, but overkill for some applications. So for read-mostly applications, a file metaphor is good enough.</p>\n\n<p>Finally, you simply donâ€™t need a relational database with its full-blown query language if there are no unexpected queries anticipated. If there are no suits asking questions like \"how many 5%-discounted blue widgets did we sell in on the east coast grouped by salesperson?\", and there never will be, then you, sir, can live free of DB.</p>\n    "},{"t":"Best data store for billions of rows","l":"http://stackoverflow.com/questions/2794736/best-data-store-for-billions-of-rows","q":"\n\n<p>I need to be able to store small bits of data (approximately 50-75 bytes) for billions of records (~3 billion/month for a year).</p>\n\n<p>The only requirement is fast inserts and fast lookups for all records with the same GUID and the ability to access the data store from .net.</p>\n\n<p>I'm a SQL server guy and I think SQL Server <em>can</em> do this, but with all the talk about BigTable, CouchDB, and other nosql solutions, it's sounding more and more like an alternative to a traditional RDBS may be best due to optimizations for distributed queries and scaling. I tried cassandra and the .net libraries don't currently compile or are all <em>subject to change</em> (along with cassandra itself).</p>\n\n<p>I've looked into many nosql data stores available, but can't find one that meets my needs as a robust production-ready platform. </p>\n\n<p>If you had to store 36 billion small, flat records so that they're accessible from .net, what would choose and why?</p>\n    ","a":"\n<p>Storing ~3.5TB of data and inserting about 1K/sec 24x7, and also querying at a rate not specified, it is possible with SQL Server, but there are more questions:</p>\n\n<ul>\n<li>what availability requirement you have for this? 99.999% uptime, or is 95% enough? </li>\n<li>what reliability requirement you have? Does missing an insert cost you $1M? </li>\n<li>what recoverability requirement you have? If you loose one day of data, does it matter?</li>\n<li>what consistency requirement you have? Does a write need to be guaranteed to be visible on the next read?</li>\n</ul>\n\n<p>If you need all these requirements I highlighted, the load you propose is going to cost millions in hardware and licensing on an relational system, any system, no matter what gimmicks you try (sharding, partitioning etc). A nosql system would, by their very definition, not meet <em>all</em> these requirements.</p>\n\n<p>So obviously you have already relaxed some of these requirements. There is a nice visual guide comparing the nosql offerings based on the 'pick 2 out of 3' paradigm at <a href=\"http://blog.nahurst.com/visual-guide-to-nosql-systems\">Visual Guide to NoSQL Systems</a>:</p>\n\n<p><img src=\"http://i.stack.imgur.com/3NAOs.png\" alt=\"nosql comparisson\"></p>\n\n<p><em>After OP comment update</em></p>\n\n<p>With SQL Server this would e straight forward implementation: </p>\n\n<ul>\n<li>one single table clustered (GUID, time) key. Yes, is going to get <a href=\"http://www.sqlskills.com/BLOGS/KIMBERLY/post/GUIDs-as-PRIMARY-KEYs-andor-the-clustering-key.aspx\">fragmented</a>, but is fragmentation affect read-aheads and read-aheads are needed only for significant range scans. Since you only query for specific GUID and date range, fragmentation won't matter much. Yes, is a wide key, so non-leaf pages will have poor key density. Yes, it will lead to poor fill factor. And yes, page splits may occur. Despite these problems, given the requirements, is still the best clustered key choice. </li>\n<li>partition the table by time so you can implement efficient deletion of the expired records, via an <a href=\"http://msdn.microsoft.com/en-us/library/aa964122%28SQL.90%29.aspx\">automatic sliding window</a>. Augment this with an online index partition rebuild of the last month to eliminate the poor fill factor and fragmentation introduced by the GUID clustering.</li>\n<li>enable page compression. Since the clustered key groups by GUID first, all records of a GUID will be next to each other, giving <a href=\"http://msdn.microsoft.com/en-us/library/cc280464.aspx\">page compression</a> a good chance to deploy dictionary compression.</li>\n<li>you'll need a fast IO path for log file. You're interested in high throughput, not on low latency for a log to keep up with 1K inserts/sec, so <a href=\"http://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_0\">stripping</a> is a must.</li>\n</ul>\n\n<p>Partitioning and page compression each require an Enterprise Edition SQL Server, they will not work on Standard Edition and both are quite important to meet the requirements.</p>\n\n<p>As a side note, if the records come from a front-end Web servers farm, I would put Express on each web server and instead of INSERT on the back end, I would <a href=\"http://msdn.microsoft.com/en-us/library/ms188407.aspx\"><code>SEND</code></a> the info to the back end, using a local connection/transaction on the Express co-located with the web server. This gives a much much better availability story to the solution.</p>\n\n<p>So this is how I would do it in SQL Server. The good news is that the problems you'll face are well understood and solutions are known. that doesn't necessarily mean this is a better than what you could achieve with Cassandra, BigTable or Dynamo. I'll let someone more knowleageable in things no-sql-ish to argument their case.</p>\n\n<p>Note that I never mentioned the programming model, .Net support and such. I honestly think they're irrelevant in large deployments. They make huge difference in the development process, but once deployed it doesn't matter how fast the development was, if the ORM overhead kills performance :)</p>\n    "},{"t":"SQL (MySQL) vs NoSQL (CouchDB)","l":"http://stackoverflow.com/questions/2559411/sql-mysql-vs-nosql-couchdb","q":"\n\n<p>I am in the middle of designing a highly-scalable application which must store a lot of data. Just for example it will store lots about users and then things like a lot of their messages, comments etc. I have always used MySQL before but now I am minded to try something new like couchdb or similar which is not SQL.</p>\n\n<p>Does anyone have any thoughts or guidance on this?</p>\n    ","a":"\n<p>Here's a quote from a recent <a href=\"http://www.25hoursaday.com/weblog/2010/03/29/TheNoSQLDebateAutomaticVsManualTransmission.aspx\">blog post from Dare Obasanjo</a>.</p>\n\n<blockquote>\n  <p>SQL databases are like automatic\n  transmission and NoSQL databases are\n  like manual transmission. Once you\n  switch to NoSQL, you become\n  responsible for a lot of work that the\n  system takes care of automatically in\n  a relational database system. Similar\n  to what happens when you pick manual\n  over automatic transmission. Secondly,\n  NoSQL allows you to eke more\n  performance out of the system by\n  eliminating a lot of integrity checks\n  done by relational databases from the\n  database tier. Again, this is similar\n  to how you can get more performance\n  out of your car by driving a manual\n  transmission versus an automatic\n  transmission vehicle.</p>\n  \n  <p>However the most notable similarity is\n  that just like most of us canâ€™t really\n  take advantage of the benefits of a\n  manual transmission vehicle because\n  the majority of our driving is sitting\n  in traffic on the way to and from\n  work, there is a similar harsh reality\n  in that most sites arenâ€™t at Google or\n  Facebookâ€™s scale and thus have no need\n  for a Bigtable or Cassandra.</p>\n</blockquote>\n\n<p>To which I can add only that switching from MySQL, where you have at least some experience, to CouchDB, where you have no experience, means you will have to deal with a whole new set of problems and learn different concepts and best practices. While by itself this is wonderful (I am playing at home with MongoDB and like it a lot), it will be a cost that you need to calculate when estimating the work for that project, and brings unknown risks while promising unknown benefits. It will be very hard to judge if you can do the project on time and with the quality you want/need to be successful, if it's based on a technology you don't know.</p>\n\n<p>Now, if you have on the team an expert in the NoSQL field, then by all means take a good look at it. But without any expertise on the team, don't jump on NoSQL for a new commercial project.</p>\n\n<p><strong>Update</strong>: Just to throw some gasoline in the open fire you started, here are two interesting articles from people on the SQL camp. :-)</p>\n\n<p><a href=\"http://teddziuba.com/2010/03/i-cant-wait-for-nosql-to-die.html\">I Can't Wait for NoSQL to Die</a> (original article is gone, here's a <a href=\"http://www.memonic.com/user/dorian/folder/internet-tidbits/id/1HiG\">copy</a>)<br> \n<a href=\"http://www.yafla.com/dforbes/The_Impact_of_SSDs_on_Database_Performance_and_the_Performance_Paradox_of_Data_Explodification\">Fighting The NoSQL Mindset, Though This Isn't an anti-NoSQL Piece</a><br>\n<strong>Update</strong>: Well here is an interesting article about NoSQL<br>\n<a href=\"http://hebrayeem.blogspot.com/2014/01/making-sense-of-nosql.html\">Making Sense of NoSQL</a></p>\n    "},{"t":"What's The Best Practice In Designing A Cassandra Data Model?","l":"http://stackoverflow.com/questions/1502735/whats-the-best-practice-in-designing-a-cassandra-data-model","q":"\n\n<p>And what are the pitfalls to avoid? Are there any deal breaks for you? E.g., I've heard that exporting/importing the Cassandra data is very difficult, making me wonder if that's going to hinder syncing production data to development environment.</p>\n\n<p>BTW, it's very hard to find good tutorials on Cassandra, the only one I have <a href=\"http://arin.me/code/wtf-is-a-supercolumn-cassandra-data-model\">http://arin.me/code/wtf-is-a-supercolumn-cassandra-data-model</a> is still pretty basic.</p>\n\n<p>Thanks.</p>\n    ","a":"\n<p>For me, the main thing is a decision whether to use the OrderedPartitioner or RandomPartitioner.</p>\n\n<p>If you use the RandomPartitioner, range scans are not possible. This means that you must know the exact key for any activity, INCLUDING CLEANING UP OLD DATA.</p>\n\n<p>So if you've got a lot of churn, unless you have some magic way of knowing exactly which keys you've inserted stuff for, using the random partitioner you can easily \"lose\" stuff, which causes a disc space leak and will eventually consume all storage.</p>\n\n<p>On the other hand, you can ask the ordered partitioner \"what keys do I have in Column Family X between A and B\" ? - and it'll tell you. You can then clean them up.</p>\n\n<p>However, there is a downside as well. As Cassandra doesn't do automatic load balancing, if you use the ordered partitioner, in all likelihood all your data will end up in just one or two nodes and none in the others, which means you'll waste resources.</p>\n\n<p>I don't have any easy answer for this, except you can get \"best of both worlds\" in some cases by putting a short hash value (of something you can enumerate easily from other data sources) on the beginning of your keys - for example a 16-bit hex hash of the user ID - which will give you 4 hex digits, followed by whatever the key is you really wanted to use.</p>\n\n<p>Then if you had a list of recently-deleted users, you can just hash their IDs and range scan to clean up anything related to them.</p>\n\n<p><del>The next tricky bit is secondary indexes - Cassandra doesn't have any - so if you need to look up X by Y, you need to insert the data under both keys, or have a pointer. Likewise, these pointers may need to be cleaned up when the thing they point to doesn't exist, but there's no easy way of querying stuff on this basis, so your app needs to Just Remember. </del></p>\n\n<p>And application bugs may leave orphaned keys that you've forgotten about, and you'll have no way of easily detecting them, unless you write some garbage collector which periodically scans every single key in the db (this is going to take a while - but you can do it in chunks) to check for ones which aren't needed any more.</p>\n\n<p>None of this is based on real usage, just what I've figured out during research. We don't use Cassandra in production.</p>\n\n<p>EDIT: Cassandra now does have secondary indexes in trunk.</p>\n    "},{"t":"Explanation of JSONB introduced by PostgreSQL","l":"http://stackoverflow.com/questions/22654170/explanation-of-jsonb-introduced-by-postgresql","q":"\n\n<p>PostgreSQL just introduced <a href=\"http://www.depesz.com/2014/03/25/waiting-for-9-4-introduce-jsonb-a-structured-format-for-storing-json/\">JSONB</a> and it's already trending on <a href=\"https://news.ycombinator.com/item?id=7457197\">hacker news</a>.  It would be great if someone could explain how it's different from Hstore and JSON previously present in PostgreSQL. What are it's advantages and limitations and when should someone consider using it?</p>\n    ","a":"\n<p>First, <a href=\"http://www.postgresql.org/docs/9.3/static/hstore.html\"><code>hstore</code></a> is a contrib module, which only allows you to store key =&gt; value pairs, where keys and values can only be <code>text</code>s (however values can be sql <code>NULL</code>s too).</p>\n\n<p>Both <code>json</code> &amp; <code>jsonb</code> allows you to store a valid JSON <em>value</em> (defined in its <a href=\"http://json.org\">spec</a>).</p>\n\n<p>F.ex. these are valid JSON representations: <code>null</code>, <code>true</code>, <code>[1,false,\"string\",{\"foo\":\"bar\"}]</code>, <code>{\"foo\":\"bar\",\"baz\":[null]}</code> - <code>hstore</code> is just a little subset compared to what JSON is capable (but if you only need this subset, it's fine).</p>\n\n<p>The only difference between <code>json</code> &amp; <code>jsonb</code> is their storage:</p>\n\n<ul>\n<li><code>json</code> is stored in its plain text format, while</li>\n<li><code>jsonb</code> is stored in some binary representation</li>\n</ul>\n\n<p>There are 3 major consequences of this:</p>\n\n<ul>\n<li><code>jsonb</code> usually takes more disk space to store than <code>json</code> (sometimes not)</li>\n<li><code>jsonb</code> takes more time to build from its input representation than <code>json</code></li>\n<li><code>json</code> operations take <em>significantly</em> more time than <code>jsonb</code> (&amp; parsing also needs to be done each time you do some operation at a <code>json</code> typed value)</li>\n</ul>\n\n<p>When <code>jsonb</code> will be available with a stable release, there will be two major use cases, when you can easily select between them:</p>\n\n<ol>\n<li>If you only work with the JSON representation in your application, PostgreSQL is only used to store &amp; retrieve this representation, you should use <code>json</code>.</li>\n<li>If you do a lot of operations on the JSON value in PostgreSQL, or use indexing on some JSON field, you should use <code>jsonb</code>.</li>\n</ol>\n    "},{"t":"What does â€œDocument-orientedâ€ vs. Key-Value mean when talking about MongoDB vs Cassandra?","l":"http://stackoverflow.com/questions/3046001/what-does-document-oriented-vs-key-value-mean-when-talking-about-mongodb-vs-c","q":"\n\n<p>What does going with a document based NoSQL option buy you over a KV store, and vice-versa?</p>\n    ","a":"\n<p>A <em>key-value store</em> provides the simplest possible data model and is exactly what the name suggests: it's a storage system that stores values indexed by a key. You're limited to query by key and the values are <strong>opaque</strong>, the store doesn't know <em>anything</em> about them. This allows very fast read and write operations (a simple disk access) and I see this model as a kind of non volatile cache (i.e. well suited if you need fast accesses by key to long-lived data).</p>\n\n<p>A <em>document-oriented database</em> extends the previous model and values are stored in a <em>structured</em> format (a document, hence the name) that the database can understand. For example, a document could be a blog post <strong>and</strong> the comments <strong>and</strong> the tags stored in a denormalized way. Since the data are <strong>transparent</strong>, the store can do more work (like indexing fields of the document) and you're not limited to query by key. As I hinted, such databases allows to fetch an entire page's data with a single query and are well suited for content oriented applications (which is why big sites like Facebook or Amazon like them).</p>\n\n<p>Other kinds of NoSQL databases include <em>column-oriented stores</em>, <em>graph databases</em> and even <em>object databases</em>. But this goes beyond the question.</p>\n\n<h3>See also</h3>\n\n<ul>\n<li><a href=\"http://nosql.mypopescu.com/post/659390374/comparing-document-databases-to-key-value-stores\">Comparing Document Databases to Key-Value Stores</a></li>\n<li><a href=\"http://blog.knuthaugen.no/2010/03/the-nosql-landscape.html\">Analysis of the NoSQL Landscape</a></li>\n<li><a href=\"http://www.jroller.com/dmdevito/entry/thinking_about_nosql_databases_classification\">Thinking about NoSQL databases (classification and use cases)</a></li>\n</ul>\n    "},{"t":"Life without JOINsâ€¦ understanding, and common practices","l":"http://stackoverflow.com/questions/1532218/life-without-joins-understanding-and-common-practices","q":"\n\n<p>Lots of \"BAW\"s (big ass-websites) are using data storage and retrieval techniques that rely on huge tables with indexes, and using queries that won't/can't use JOINs in their queries (BigTable, HQL, etc) to deal with scalability and sharding databases. How does that work when you have lots and lots of data that is <em>very</em> related?</p>\n\n<p>I can only speculate that much of this joining has to be done on the application side of things, but doesn't that start to get expensive? What if you have to make several queries to several different tables to get information to compile? Isn't hitting the database that many times starting to get more expensive than just using joins in the first place? I guess it depends on how much data you've got?</p>\n\n<p>And for commonly available ORMs, how do they tend to deal with the inability to use joins? Is there support for this in ORMs that are in heavy usage today? Or do most projects that have to approach this level of data tend to roll their own anyways?</p>\n\n<p>So this is not applicable to any current project I'm doing, but it's something that's been in my head for several months now that I can only speculate as to what \"best practices\" are. I've never had a need to address this in any of my projects because they've never reached a scale where it is required. Hopefully this question helps other people as well..</p>\n\n<p>As someone said below, ORMs \"don't work\" without joins. Are there other data access layers that are already available to developers working with data on this level?</p>\n\n<p><strong>EDIT:</strong>\nFor some clarification, <a href=\"http://stackoverflow.com/users/5190/vinko-vrsalovic\">Vinko Vrsalovic</a> said:</p>\n\n<blockquote>\n  <p>\"I believe snicker is wants to talk\n  about NO-SQL, where transactional data\n  is denormalized and used in Hadoop or\n  BigTable or Cassandra schemes.\"</p>\n</blockquote>\n\n<p>This is indeed what I'm talking about.</p>\n\n<p>Bonus points for those who catch the xkcd reference.</p>\n    ","a":"\n<p>The way I look at it, a relational database is a general purpose tool to hedge your bets.  Modern computers are fast enough, and RDBMS' are well-optimized enough that you can grow to quite a respectable size on a single box.  By choosing an RDBMS you are giving yourself very flexible access to your data, and the ability to have powerful correctness constraints that make it much easier to code against the data.  However the RDBMS is not going to represent a good optimization for any particular problem, it just gives you the flexibility to change problems easily.</p>\n\n<p>If you start growing rapidly and realize you are going to have to scale beyond the size of a single DB server, you suddenly have much harder choices to make.  You will need to start identifying bottlenecks and removing them.  The RDBMS is going to be one nasty snarled knot of codependency that you'll have to tease apart.  The more interconnected your data the more work you'll have to do, but maybe you won't have to completely disentangle the whole thing.\nIf you're read-heavy maybe you can get by with simple replication.  If you're saturating your market and growth is leveling off maybe you can partially denormalize and shard to fixed number of DB servers.  Maybe you just have a handful of problem tables that can be moved to a more scalable data store.  Maybe your usage profile is very cache friendly and you can just migrate the load to a giant memcached cluster.</p>\n\n<p>Where scalable key-value stores like BigTable come in is when none of the above can work, and you have so much data of a single type that even when it's denormalized a single table is too much for one server.  At this point you need to be able to partition it arbitrarily and still have a clean API to access it. Naturally when the data is spread out across so many machines you can't have algorithms that require these machines to talk to each other much, which many of the standard relational algorithms would require.  As you suggest, these distributed querying algorithms have the potential to require more total processing power than the equivalent JOIN in a properly indexed relational database, but because they are parallelized the real time performance is orders of magnitude better than any single machine could do (assuming a machine that could hold the entire index even exists).</p>\n\n<p>Now once you can scale your massive data set horizontally (by just plugging in more servers), the hard part of scalability is done.  Well I shouldn't say <em>done</em>, because ongoing operations and development at this scale are a lot harder than the single-server app, but the point is application servers are typically trivial to scale via a share-nothing architecture as long as they can get the data they need in a timely fashion.</p>\n\n<p>To answer your question about how commonly used ORMs handle the inability to use JOINs, the short answer is <strong>they don't</strong>.  ORM stands for Object Relational Mapping, and most of the job of an ORM is just translating the powerful relational paradigm of predicate logic simple object-oriented data structures.  Most of the value of what they give you is simply not going to be possible from a key-value store.  In practice you will probably need to build up and maintain your own data-access layer that's suited to your particular needs, because data profiles at these scales are going to vary dramatically and I believe there are too many tradeoffs for a general purpose tool to emerge and become dominant the way RDBMSs have.  In short, you'll always have to do more legwork at this scale.</p>\n\n<p>That said, it will definitely be interesting to see what kind of relational or other aggregate functionality can be built on top of the key-value store primitives.  I don't really have enough experience here to comment specifically, but there is a lot of knowledge in enterprise computing about this going back many years (eg. Oracle), a lot of untapped theoretical knowledge in academia, a lot of practical knowledge at Google, Amazon, Facebook, et al, but the knowledge that has filtered out into the wider development community is still fairly limited.</p>\n\n<p>However now that a lot of applications are moving to the web, and more and more of the world's population is online, inevitably more and more applications will have to scale, and best practices will begin to crystallize.  The knowledge gap will be whittled down from both sides by cloud services like AppEngine and EC2, as well as open source databases like Cassandra.  In some sense this goes hand in hand with parallel and asynchronous computation which is also in its infancy.  Definitely a fascinating time to be a programmer.</p>\n    "},{"t":"Are there any e-commerce websites that use NoSQL databases [closed]","l":"http://stackoverflow.com/questions/2581738/are-there-any-e-commerce-websites-that-use-nosql-databases","q":"\n\n<p>I have read a lot lately about 'NoSQL' databases such as CouchDB, MongoDB etc. Most of the websites I have seen using this are mainly text based websites such as The New York Times and Source forge.</p>\n\n<p>I was wondering if you could apply this to websites where payment is a huge issue. I am thinking of the following issues:</p>\n\n<ul>\n<li>How well can you secure the data</li>\n<li>Do these system provide an easy backup/restore machanism</li>\n<li>How are transactions handled commit/rollback</li>\n</ul>\n\n<p>I have read the following articles that cover some aspects:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/299723/can-i-do-transactions-and-locks-in-couchdb\">Can I do transactions and locks in CouchDB?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/337344/pros-cons-of-document-based-database-vs-relational-database\">Pros/Cons of document based database vs relational database</a></li>\n</ul>\n\n<p>In these posts the aspect of transactions if covered. However the questions of security and backups is not covered. Can someone shed some light on this subject?</p>\n\n<p>And if possible, does anyone know of some e-commerce websites that have successfully implemented the document based database.</p>\n    ","a":"\n<p>EDIT: March, 2013</p>\n\n<p>I had originally posted a link to an article I wrote on MongoDB and e-commerce, but I no longer agree with all the points I made there. I still believe that MongoDB's document data model can work very well for the catalog management side of an e-commerce site and perhaps for shopping carts. But there are clearly cases where transactions are important, and MongoDB doesn't give you these. The answer to this question with the next-highest number of votes makes a lot of points worth considering.</p>\n\n<p>Here's the original article for those interested:</p>\n\n<blockquote>\n  <p><a href=\"http://web.archive.org/web/20110713174947/http://kylebanker.com/blog/2010/04/30/mongodb-and-ecommerce/\">http://kylebanker.com/blog/2010/04/30/mongodb-and-ecommerce/ (archive.org link)</a></p>\n</blockquote>\n    "},{"t":"How to run MongoDB as windows service?","l":"http://stackoverflow.com/questions/2438055/how-to-run-mongodb-as-windows-service","q":"\n\n<p>How to setup MongoDB so it can run as windows service?</p>\n\n<p>Thanks</p>\n    ","a":"\n<p>I think if you run it with the <code>--install</code> command line switch, it installs it as a Windows Service.</p>\n\n<pre><code>mongod --install\n</code></pre>\n\n<p>It might be worth reading <a href=\"http://groups.google.com/group/mongodb-user/browse_thread/thread/fde13e7ceb64ac44/a39a7e1b711a6b00\">this thread</a> first though. There seems to be some problems with relative/absolute paths when the relevant registry key gets written.</p>\n    "},{"t":"When should I use a NoSQL database instead of a relational database? Is it okay to use both on the same site?","l":"http://stackoverflow.com/questions/3713313/when-should-i-use-a-nosql-database-instead-of-a-relational-database-is-it-okay","q":"\n\n<p>What are the advantages of using NoSQL databases? I've read a lot about them lately, but I'm still unsure why I would want to implement one, and under what circumstances I would want to use one.</p>\n    ","a":"\n<p>Relational databases enforces <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a>. So, you will have schema based transaction oriented data stores. It's proven and suitable for 99% of the real world applications. You can practically do anything with relational databases.</p>\n\n<p>But, there are limitations on speed and scaling when it comes to massive high availability data stores. For example, Google and Amazon have terabytes of data stored in big data centers. Querying and inserting is not performant in these scenarios because of the  blocking/schema/transaction nature of the RDBMs. That's the reason they have implemented their own databases (actually, key-value stores) for massive performance gain and  scalability.</p>\n\n<p>NoSQL databases have been around for a long time - just the term is new. Some examples are graph, object, column, XML and document databases.</p>\n\n<p><strong>For your 2nd question:</strong> Is it okay to use both on the same site?</p>\n\n<p>Why not? Both serves different purposes right?</p>\n    "},{"t":"When NOT to use Cassandra?","l":"http://stackoverflow.com/questions/2634955/when-not-to-use-cassandra","q":"\n\n<p>There has been a lot of talk related to <a href=\"http://cassandra.apache.org/\">Cassandra</a> lately.</p>\n\n<p>Twitter, Digg, Facebook, etc all use it.</p>\n\n<p>When does it make sense to:</p>\n\n<ul>\n<li>use Cassandra,</li>\n<li>not use Cassandra, and</li>\n<li>use a RDMS instead of Cassandra.</li>\n</ul>\n    ","a":"\n<p>The general idea of NoSQL is that you should use whichever data store is the best fit for your application. If you have a table of financial data, use SQL. If you have objects that would require complex/slow queries to map to a relational schema, use an object or key/value store. </p>\n\n<p>Of course just about any real world problem you run into is somewhere in between those two extremes and neither solution will be perfect. You need to consider the capabilities of each store and the consequences of using one over the other, which will be very much specific to the problem you are trying to solve.</p>\n    "},{"t":"Pros/cons of document-based databases vs. relational databases","l":"http://stackoverflow.com/questions/337344/pros-cons-of-document-based-databases-vs-relational-databases","q":"\n\n<p>I've been trying to see if I can accomplish some requirements with a document based database, in this case CouchDB.  Two generic requirements:  </p>\n\n<ul>\n<li>CRUD of entities with some fields which have unique index on it  </li>\n<li>ecommerce web app like eBay (<a href=\"http://stackoverflow.com/questions/299723/can-i-do-transactions-and-locks-in-couchdb\">better description here</a>).</li>\n</ul>\n\n<p>And I'm begining to think that a Document-based database isn't the best choice to address these requirements. Furthermore, I can't imagine a use for a Document based database (maybe my imagination is too limited).  </p>\n\n<p>Can you explain to me if <em>I am asking pears from an elm</em> when I try to use a Document oriented database for these requirements?</p>\n    ","a":"\n<p>You need to think of how you approach the application in a document oriented way.  If you simply try to replicate how you would model the problem in an RDBMS then you will fail.  There are also different trade-offs that you might want to make.  ([ed: not sure how this ties into the argument but:] Remember that CouchDB's design assumes you will have an active cluster of many nodes that could fail at any time.  How is your app going to handle one of the database nodes disappearing from under it?)</p>\n\n<p>One way to think about it is to imagine you didn't have any computers, just paper documents.  How would you create an efficient business process using bits of paper being passed around?  How can you avoid bottlenecks?  What if something goes wrong?</p>\n\n<p>Another angle you should think about is eventual consistency, where you will get into a consistent state eventually, but you may be inconsistent for some period of time.  This is anathema in RDBMS land, but extremely common in the real world.  The canonical transaction example is of transferring money from bank accounts.  How does this actually happen in the real world - through a single atomic transactions or through different banks issuing credit and debit notices to each other?  What happens when you write a cheque?</p>\n\n<p>So lets look at your examples:</p>\n\n<ul>\n<li>CRUD of entities with some fields with unique index on it.</li>\n</ul>\n\n<p>If I understand this correctly in CouchDB terms, you want to have a collection of documents where some named value is guaranteed to be unique across all those documents?  That case isn't generally supportable because documents may be created on different replicas.</p>\n\n<p>So we need to look at the real world problem and see if we can model that.  Do you really need them to be unique?  Can your application handle multiple docs with the same value?  Do you need to assign a unique identifier?  Can you do that deterministically?  A common scenario where this is required is where you need a unique sequential identifier.  This is tough to solve in a replicated environment.  In fact if the unique id is required to be strictly sequential with respect to time created it's impossible <em>if</em> you need the id straight away.  You need to relax at least one of those constraints.</p>\n\n<ul>\n<li>ecommerce web app like ebay</li>\n</ul>\n\n<p>I'm not sure what to add here as the last comment you made on that post was to say \"very useful! thanks\".  Was there something missing from the approach outlined there that is still causing you a problem?  I thought MrKurt's answer was pretty full and I added a little enhancement that would reduce contention.</p>\n    "},{"t":"Explain Merkle Trees for use in Eventual Consistency","l":"http://stackoverflow.com/questions/5486304/explain-merkle-trees-for-use-in-eventual-consistency","q":"\n\n<p><a href=\"http://en.wikipedia.org/wiki/Hash_tree\">Merkle Trees</a> are used as an anti-entropy mechanism in several distributed, replicated key/value stores:</p>\n\n<ul>\n<li><a href=\"http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Dynamo</a></li>\n<li><a href=\"http://doc.erlagner.org/riak_core/merkerl.html\">Riak</a></li>\n<li><a href=\"http://wiki.apache.org/cassandra/AntiEntropy\">Cassandra</a></li>\n</ul>\n\n<p>No doubt an anti-entropy mechanism is A Good Thing - transient failures just happen, in production.\nI'm just not sure I understand why Merkle <em>Trees</em> are the popular approach.</p>\n\n<ul>\n<li><p>Sending a complete Merkle tree to a peer involves sending the local key-space to that peer, along with\nhashes of each key value, stored in the lowest levels of the tree.</p></li>\n<li><p>Diffing a Merkle tree sent from a peer requires having a Merkle tree of your own.</p></li>\n</ul>\n\n<p>Since both peers must already have a sorted key / value-hash space on hand, why not do a linear merge to detect discrepancies?</p>\n\n<p>I'm just not convinced that the tree structure provides any kind of savings when you factor in upkeep costs, and the fact\nthat <em>linear passes over the tree leaves are already being done just to serialize the representation over the wire</em>.</p>\n\n<p>To ground this out, a straw-man alternative might be to have nodes exchange arrays of hash digests,\nwhich are incrementally updated and bucketed by modulo ring-position.</p>\n\n<p>What am I missing?</p>\n    ","a":"\n<p>Merkle trees limit the amount of data transferred when synchronizing. The general assumptions are:</p>\n\n<ol>\n<li>Network I/O is more expensive than local I/O + computing the hashes.</li>\n<li>Transferring the entire sorted key space is more expensive than progressively limiting the comparison over several steps.</li>\n<li>The key spaces have fewer discrepancies than similarities.</li>\n</ol>\n\n<p>A Merkle Tree exchange would look like this:</p>\n\n<ol>\n<li>Start with the root of the tree (a list of one hash value).</li>\n<li>The origin sends the list of hashes at the current level.</li>\n<li>The destination diffs the list of hashes against its own and then\n requests subtrees that are different. If there are no\n differences, the request can terminate.</li>\n<li>Repeat steps 2 and 3 until leaf nodes are reached.</li>\n<li>The origin sends the values of the keys in the resulting set.</li>\n</ol>\n\n<p>In the typical case, the complexity of synchronizing the key spaces will be log(N). Yes, at the extreme, where there are no keys in common, the operation will be equivalent to sending the entire sorted list of hashes, O(N). One could amortize the expense of building Merkle trees by building them dynamically as writes come in and keeping the serialized form on disk.</p>\n\n<p>I can't speak to how Dynamo or Cassandra use Merkle trees, but Riak stopped using them for intra-cluster synchronization (hinted handoff and read-repair are sufficient in most cases). We have plans to add them back later after some internal architectural bits have changed.</p>\n\n<p>For more information about Riak, we encourage you to join the mailing list: <a href=\"http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com\">http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com</a></p>\n    "},{"t":"MongoDB/NoSQL: Keeping Document Change History","l":"http://stackoverflow.com/questions/3507624/mongodb-nosql-keeping-document-change-history","q":"\n\n<p>A fairly common requirement in database applications is to track changes to one or more specific entities in a database.  I've heard this called row versioning, a log table or a history table (I'm sure there are other names for it).  There are a number of ways to approach it in an RDBMS--you can write all changes from all source tables to a single table (more of a log) or have a separate history table for each source table.  You also have the option to either manage the logging in application code or via database triggers.</p>\n\n<p>I'm trying to think through what a solution to the same problem would look like in a NoSQL/document database (specifically MongoDB), and how it would be solved in a uniform way.  Would it be as simple as creating version numbers for documents, and never overwriting them?  Creating separate collections for \"real\" vs. \"logged\" documents?  How would this affect querying and performance?</p>\n\n<p>Anyway, is this a common scenario with NoSQL databases, and if so, is there a common solution?</p>\n    ","a":"\n<p>Good question, I was looking into this myself as well.</p>\n\n<h3>Create a new version on each change</h3>\n\n<p>I came across the <a href=\"http://mongoid.org/en/mongoid/docs/extras.html\">Versioning module</a> of the Mongoid driver for Ruby. I haven't used it myself, but from <a href=\"http://neovintage.blogspot.com/2010/06/mongoid-versioning-run-down.html\">what I could find</a>, it adds a version number to each document. Older versions are embedded in the document itself. The major drawback is that the <strong>entire document is duplicated on each change</strong>, which will result in a lot of duplicate content being stored when you're dealing with large documents. This approach is fine though when you're dealing with small-sized documents and/or don't update documents very often.</p>\n\n<h3>Only store changes in a new version</h3>\n\n<p>Another approach would be to <strong>store only the changed fields in a new version</strong>. Then you can 'flatten' your history to reconstruct any version of the document. This is rather complex though, as you need to track changes in your model and store updates and deletes in a way that your application can reconstruct the up-to-date document. This might be tricky, as you're dealing with structured documents rather than flat SQL tables.</p>\n\n<h3>Store changes within the document</h3>\n\n<p>Each field can also have an individual history. Reconstructing documents to a given version is much easier this way. In your application you don't have to explicitly track changes, but just create a new version of the property when you change its value. A document could look something like this:</p>\n\n<pre><code>{\n  _id: \"4c6b9456f61f000000007ba6\"\n  title: [\n    { version: 1, value: \"Hello world\" },\n    { version: 6, value: \"Foo\" }\n  ],\n  body: [\n    { version: 1, value: \"Is this thing on?\" },\n    { version: 2, value: \"What should I write?\" },\n    { version: 6, value: \"This is the new body\" }\n  ],\n  tags: [\n    { version: 1, value: [ \"test\", \"trivial\" ] },\n    { version: 6, value: [ \"foo\", \"test\" ] }\n  ],\n  comments: [\n    {\n      author: \"joe\", // Unversioned field\n      body: [\n        { version: 3, value: \"Something cool\" }\n      ]\n    },\n    {\n      author: \"xxx\",\n      body: [\n        { version: 4, value: \"Spam\" },\n        { version: 5, deleted: true }\n      ]\n    },\n    {\n      author: \"jim\",\n      body: [\n        { version: 7, value: \"Not bad\" },\n        { version: 8, value: \"Not bad at all\" }\n      ]\n    }\n  ]\n}\n</code></pre>\n\n<p>Marking part of the document as deleted in a version is still somewhat awkward though. You could introduce a <code>state</code> field for parts that can be deleted/restored from your application:</p>\n\n<pre><code>{\n  author: \"xxx\",\n  body: [\n    { version: 4, value: \"Spam\" }\n  ],\n  state: [\n    { version: 4, deleted: false },\n    { version: 5, deleted: true }\n  ]\n}\n</code></pre>\n\n<p>With each of these approaches you can store an up-to-date and flattened version in one collection and the history data in a separate collection. This should improve query times if you're only interested in the latest version of a document. But when you need both the latest version and historical data, you'll need to perform two queries, rather than one. So the choice of using a single collection vs. two separate collections should depend on <strong>how often your application needs the historical versions</strong>.</p>\n\n<p>Most of this answer is just a brain dump of my thoughts, I haven't actually tried any of this yet. Looking back on it, the first option is probably the easiest and best solution, unless the overhead of duplicate data is very significant for your application. The second option is quite complex and probably isn't worth the effort. The third option is basically an optimization of option two and should be easier to implement, but probably isn't worth the implementation effort unless you really can't go with option one.</p>\n\n<p>Looking forward to feedback on this, and other people's solutions to the problem :)</p>\n    "},{"t":"The Next-gen Databases","l":"http://stackoverflow.com/questions/282783/the-next-gen-databases","q":"\n\n<p>I'm learning traditional Relational Databases (with <a href=\"http://www.postgresql.org/\">PostgreSQL</a>) and doing some research I've come across some new types of databases. <a href=\"http://couchdb.apache.org/\">CouchDB</a>, <a href=\"https://launchpad.net/drizzle\">Drizzle</a>, and <a href=\"http://www.zib.de/CSR/Projects/scalaris/\">Scalaris</a> to name a few, what is going to be the next database technologies to deal with?</p>\n    ","a":"\n<p>I would say next-gen <em>database</em>, not next-gen SQL.</p>\n\n<p>SQL is a language for querying and manipulating relational databases.  SQL is dictated by an international standard.  While the standard is revised, it seems to always work within the relational database paradigm.</p>\n\n<p>Here are a few new data storage technologies that are getting attention currently:</p>\n\n<ul>\n<li><a href=\"http://couchdb.apache.org/\"><strong>CouchDB</strong></a> is a non-relational database.  They call it a document-oriented database.</li>\n<li><a href=\"http://aws.amazon.com/simpledb/\"><strong>Amazon SimpleDB</strong></a> is also a non-relational database accessed in a distributed manner through a web service. Amazon also has a distributed key-value store called <strong>Dynamo</strong>, which powers some of its S3 services.</li>\n<li><a href=\"http://github.com/cliffmoon/dynomite/tree/master\"><strong>Dynomite</strong></a> and <a href=\"http://kai.wiki.sourceforge.net/\"><strong>Kai</strong></a> are open source solutions inspired by Amazon Dynamo. </li>\n<li><a href=\"http://research.google.com/archive/bigtable.html\"><strong>BigTable</strong></a> is a proprietary data storage solution used by Google, and implemented using their Google File System technology.  Google's MapReduce framework uses BigTable.</li>\n<li><a href=\"http://hadoop.apache.org/core/\"><strong>Hadoop</strong></a> is an open-source technology inspired by Google's MapReduce, and serving a similar need, to distribute the work of very large scale data stores.</li>\n<li><a href=\"http://www.zib.de/CSR/Projects/scalaris/\"><strong>Scalaris</strong></a> is a distributed transactional key/value store. Also not relational, and does not use SQL. It's a research project from the Zuse Institute in Berlin, Germany.</li>\n<li><a href=\"http://www.w3.org/RDF/\"><strong>RDF</strong></a> is a standard for storing semantic data, in which data and metadata are interchangeable.  It has its own query language SPARQL, which resembles SQL superficially, but is actually totally different.</li>\n<li><a href=\"http://www.vertica.com/\"><strong>Vertica</strong></a> is a highly scalable column-oriented analytic database designed for distributed (grid) architecture.  It does claim to be relational and SQL-compliant.  It can be used through Amazon's Elastic Compute Cloud.</li>\n<li><a href=\"http://www.greenplum.com/\"><strong>Greenplum</strong></a> is a high-scale data warehousing DBMS, which implements both MapReduce and SQL.</li>\n<li><a href=\"http://www.w3.org/XML/\"><strong>XML</strong></a> isn't a DBMS at all, it's an interchange format.  But some DBMS products work with data in XML format.</li>\n<li><a href=\"http://www.odbms.org/\"><strong>ODBMS</strong></a>, or Object Databases, are for managing complex data.  There don't seem to be any dominant ODBMS products in the mainstream, perhaps because of lack of standardization.  Standard SQL is gradually gaining some OO features (e.g. extensible data types and tables).</li>\n<li><a href=\"https://launchpad.net/drizzle\"><strong>Drizzle</strong></a> is a relational database, drawing a lot of its code from MySQL.  It includes various architectural changes designed to manage data in a scalable \"cloud computing\" system architecture.  Presumably it will continue to use standard SQL with some MySQL enhancements.</li>\n<li><a href=\"http://incubator.apache.org/cassandra/\"><strong>Cassandra</strong></a> is a highly scalable, eventually consistent, distributed, structured key-value store, developed at Facebook by one of the authors of Amazon Dynamo, and contributed to the Apache project.</li>\n<li><a href=\"http://project-voldemort.com/\"><strong>Project Voldemort</strong></a> is a non-relational, distributed, key-value storage system.  It is used at LinkedIn.com </li>\n<li><a href=\"http://www.oracle.com/technology/products/berkeley-db/index.html\"><strong>Berkeley DB</strong></a> deserves some mention too.  It's not \"next-gen\" because it dates back to the early 1990's.  It's a popular key-value store that is easy to embed in a variety of applications.  The technology is currently owned by Oracle Corp.</li>\n</ul>\n\n<p>Also see this nice article by Richard Jones: \"<a href=\"http://www.metabrew.com/article/anti-rdbms-a-list-of-distributed-key-value-stores/\">Anti-RDBMS: A list of distributed key-value stores</a>.\"  He goes into more detail describing some of these technologies.</p>\n\n<p>Relational databases have weaknesses, to be sure.  People have been arguing that they don't handle all data modeling requirements since the day it was first introduced.  </p>\n\n<p>Year after year, researchers come up with new ways of managing data to satisfy special requirements:  either requirements to handle data relationships that don't fit into the relational model, or else requirements of high-scale volume or speed that demand data processing be done on distributed collections of servers, instead of central database servers.</p>\n\n<p>Even though these advanced technologies do great things to solve the specialized problem they were designed for, relational databases are still a good general-purpose solution for most business needs.  SQL isn't going away.</p>\n\n<hr>\n\n<p>I've written an article in php|Architect magazine about the innovation of non-relational databases, and data modeling in relational vs. non-relational databases. <a href=\"http://www.phparch.com/magazine/2010-2/september/\">http://www.phparch.com/magazine/2010-2/september/</a></p>\n    "},{"t":"Switching from MySQL to Cassandra - Pros/Cons?","l":"http://stackoverflow.com/questions/2332113/switching-from-mysql-to-cassandra-pros-cons","q":"\n\n<p>For a bit of background - this question deals with a project running on a single small EC2 instance, and is about to migrate to a medium one. The main components are Django, MySQL and a large number of custom analysis tools written in python and java, which do the heavy\nlifting. The same machine is running Apache as well.</p>\n\n<p>The data model looks like the following - a large amount of real time data comes in streamed from various networked sensors, and ideally, I'd like to establish a long-poll approach rather than the current poll every 15 minutes approach (a limitation of computing stats and writing into the database itself). Once the data comes in, I store the raw version in\nMySQL, let the analysis tools loose on this data, and store statistics in another few tables. All of this is rendered using Django.</p>\n\n<p>Relational features I would need -</p>\n\n<ul>\n<li>Order by <em>[SliceRange in Cassandra's API seems to satisy this]</em></li>\n<li>Group by</li>\n<li>Manytomany relations between multiple tables <em>[Cassandra SuperColumns seem to do well for one to many]</em></li>\n<li>Sphinx on this gives me a nice full text engine, so thats a necessity too. <em>[On Cassandra, the Lucandra project seems to satisfy this need]</em></li>\n</ul>\n\n<p>My major problem is that data reads are extremely slow (and writes aren't that hot either). I don't want to throw a lot of money and hardware on it right now, and I'd prefer something that can scale easily with time. Vertically scaling MySQL is not trivial in that sense (or cheap).</p>\n\n<p>So essentially, after having read a lot about NOSQL and experimented with things like MongoDB, Cassandra and Voldemort, my questions are,</p>\n\n<ul>\n<li><p>On a medium EC2 instance, <strong><em>would I gain any benefits in reads/writes by shifting to something like Cassandra</em></strong>? <a href=\"http://static.last.fm/johan/nosql-20090611/cassandra_nosql.pdf\">This article</a> (pdf) definitely seems to suggest that. Currently, I'd say a few hundred writes per minute would be the norm. For reads - since the data changes every 5 minutes or so, cache invalidation has to happen pretty quickly. At some point, it should be able to handle a large number of concurrent users as well. The app performance currently gets killed on MySQL doing some joins on large tables even if indexes are created - something to the order of 32k rows takes more than a minute to render. (This may be an artifact of EC2 virtualized I/O as well). Size of tables is around 4-5 million rows, and there are about 5 such tables.</p></li>\n<li><p>Everyone talks about using Cassandra on multiple nodes, given the CAP theorem and eventual consistency. But, for a project that is just beginning to grow, <strong><em>does it make sense\nto deploy a one node cassandra server</em></strong>? Are there any caveats? For instance, can it replace MySQL as a backend for Django? [Is this recommended?]</p></li>\n<li><p>If I do shift, I'm guessing I'll have to rewrite parts of the app to do a lot more \"administrivia\" since I'd have to do multiple lookups to fetch rows.</p></li>\n<li><p><em><strong>Would it make any sense to just use MySQL as a key value store</strong></em> rather than a relational engine, and go with that? That way I could utilize a large number of stable APIs available, as well as a stable engine (and go relational as needed). (Brett Taylor's post from Friendfeed on this - <a href=\"http://bret.appspot.com/entry/how-friendfeed-uses-mysql\">http://bret.appspot.com/entry/how-friendfeed-uses-mysql</a>)</p></li>\n</ul>\n\n<p>Any insights from people who've done a shift would be greatly appreciated!</p>\n\n<p>Thanks.</p>\n    ","a":"\n<p>Cassandra and the other distributed databases available today do not provide the kind of ad-hoc query support you are used to from sql.  This is because you can't distribute queries with joins performantly, so the emphasis is on denormalization instead.</p>\n\n<p>However, Cassandra 0.6 (beta officially out tomorrow, but you can build from the 0.6 branch yourself if you're impatient) supports Hadoop map/reduce for analytics, which actually sounds like a good fit for you.</p>\n\n<p>Cassandra provides excellent support for adding new nodes painlessly, even to an initial group of one.</p>\n\n<p>That said, at a few hundred writes/minute you're going to be fine on mysql for a long, long time.  Cassandra is much better at being a key/value store (even better, key/columnfamily) but MySQL is much better at being a relational database. :)</p>\n\n<p>There is no django support for Cassandra (or other nosql database) yet.  They are talking about doing something for the next version after 1.2, but based on talking to django devs at pycon, nobody is really sure what that will look like yet.</p>\n    "},{"t":"Why are document stores like Lucene / Solr not included in NoSQL conversations?","l":"http://stackoverflow.com/questions/3339793/why-are-document-stores-like-lucene-solr-not-included-in-nosql-conversations","q":"\n\n<p>All of us have come across the recent hype of no-SQL solutions lately. MongoDB, CouchDB, BigTable, Cassandra, and others have been listed as no-SQL options. Here's an example:</p>\n\n<p><a href=\"http://architects.dzone.com/articles/what-nosql-store-should-i-use\">http://architects.dzone.com/articles/what-nosql-store-should-i-use</a></p>\n\n<p>However, three years ago a co-worker and I were using Lucene.NET as what seem to fit the description of no-SQL. We did not use it just for user-inputted search queries; we used it to make a few reindexed RDBMS table data extremely performant. We implemented our own .NET sort-of-equivalent-to-Solr service to manage these indexes and make them callable. When I left the company, the team switched to Solr itself. (For those not in the know, Solr is a web service that wraps Lucene with REST-callable queries and index dumps.)</p>\n\n<p>What I don't understand is, why is Solr not counted in the typical lists of no-SQL solution options? Am I missing something here? I assume that there are technical reasons why Solr is not comparable to the likes of CouchDB, etc., and in fact I understand that CouchDB uses Lucene as its data store (yes?), but what disqualifies Solr? </p>\n\n<p>I'm not asking as some kind of Solr fanboy or anything, I just don't understand why Solr and the like don't fit the definition of no-SQL, and if Solr technically does fit the definition then what about it likely makes people pooh-pooh it? I'm asking because I'm having difficulty determining whether I should continue using Lucene-based solutions (like Solr) for solutions that I build or if I should really do more research with these other options.</p>\n    ","a":"\n<p>I once listened to an interview with author Ursula K. LeGuin about fiction writing.  The interviewer asked her about authors who work in different <em>genre</em> of writing.  What makes one author a romance writer, and another a mystery writer, and another a science fiction writer?  LeGuin responded by explaining: </p>\n\n<blockquote>\n  <p>Genre is about marketing, not about content.</p>\n</blockquote>\n\n<p>It was an eye-opening statement.</p>\n\n<p>I think the same applies to technology solutions.  The NoSQL movement is attracting attention because it's full of marketing energy right now.  NoSQL data stores like Hadoop, CouchDB, MongoDB, have commercial ventures backing them, pushing their solutions as new and innovative and exciting so they can grow their business.  The term \"NoSQL\" is a <strong>marketing brand</strong> that helps them to explain their value.</p>\n\n<p>You're right that Lucene/Solr is technically very similar to a NoSQL document store:  it's a denormalized bag of documents (their term) with fields that aren't necessarily consistent across the collection of documents.  It's indexed in a sophisticated way to allow you to search across all fields or by specific fields.  </p>\n\n<p>But that's not the genre Lucene uses to explain its value.  They don't have the same mission to grow a market and a business, since they're managed by the Apache Foundation.  They're happy to focus on the use case of fulltext search, even though the technology could be used in other ways.  They're following a tenet of software success:  do one thing, and do it well.</p>\n    "},{"t":"Explanation of BASE terminology","l":"http://stackoverflow.com/questions/3342497/explanation-of-base-terminology","q":"\n\n<p>The <strong>BASE</strong> acronym is used to describe the properties of certain databases, usually NoSQL databases. It's often referred to as the opposite of <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a>.</p>\n\n<p>There are only few articles that touch upon the details of BASE, whereas ACID has plenty of articles that elaborate on each of the atomicity, consistency, isolation and durability properties. Wikipedia only devotes <a href=\"http://en.wikipedia.org/wiki/Eventual_consistency\">a few lines</a> to the term.</p>\n\n<p>This leaves me with some questions about <strong>the definition</strong>:</p>\n\n<blockquote>\n  <p><b>B</b>asically <b>A</b>vailable, <b>S</b>oft state, <b>E</b>ventual consistency</p>\n</blockquote>\n\n<p>I have interpreted these properties as follows, using <a href=\"http://queue.acm.org/detail.cfm?id=1394128\">this article</a> and my imagination:</p>\n\n<p><strong>Basically available</strong> could refer to the perceived availability of the data. If a single node fails, part of the data won't be available, but the entire data layer stays operational.</p>\n\n<ul>\n<li>Is this interpretation correct, or does it refer to something else?</li>\n<li><strong>Update:</strong> deducing from <a href=\"http://stackoverflow.com/questions/3342497/explanation-of-base-terminology/3342749#3342749\">Mau's answer</a>, could it mean the entire data layer is always accepting new data, i.e. there are no locking scenarios that prevent data from being inserted immediately?</li>\n</ul>\n\n<p><strong>Soft state</strong>: All I could find was the concept of data needing a period refresh. Without a refresh, the data will expire or be deleted.</p>\n\n<ul>\n<li>Automatic deletion of data in a database seems strange to me.</li>\n<li>Expired or stale data makes more sense. But this concept would apply to any type of redundant data storage, not just NoSQL. Does it describe something else then?</li>\n</ul>\n\n<p><strong>Eventual consistency</strong> means that updates will eventually ripple through to all servers, given enough time.</p>\n\n<ul>\n<li>This property is clear to me.</li>\n</ul>\n\n<hr>\n\n<p>Can someone explain these properties in detail?</p>\n\n<p>Or is it just a far-fetched and meaningless acronym that refers to the concepts of acids and bases as found in chemistry?</p>\n    ","a":"\n<p>The BASE acronym was defined by <a href=\"http://en.wikipedia.org/wiki/Eric_Brewer_%28scientist%29\">Eric Brewer</a>, who is also known for formulating the <a href=\"http://en.wikipedia.org/wiki/CAP_theorem\">CAP theorem</a>.</p>\n\n<p>The CAP theorem states that a distributed computer system cannot guarantee all of the following three properties at the same time:</p>\n\n<ul>\n<li>Consistency</li>\n<li>Availability</li>\n<li>Partition tolerance</li>\n</ul>\n\n<p>A BASE system gives up on consistency.</p>\n\n<ul>\n<li><strong>Basically available</strong> indicates that the system <em>does</em> guarantee availability, in terms of the CAP theorem.</li>\n<li><strong>Soft state</strong> indicates that the state of the system may change over time, even without input. This is because of the eventual consistency model.</li>\n<li><strong>Eventual consistency</strong> indicates that the system will become consistent over time, given that the system doesn't receive input during that time.</li>\n</ul>\n\n<p>Brewer does admit that <a href=\"http://www.julianbrowne.com/article/viewer/brewers-cap-theorem\">the acronym is contrived</a>:</p>\n\n<blockquote>\n  <p>I came up with [the BASE] acronym with my students in their office earlier that year. I agree it is contrived a bit, but so is \"ACID\" -- much more than people realize, so we figured it was good enough.</p>\n</blockquote>\n    "},{"t":"NoSQL CAP theorem - Availability and Partition Tolerance","l":"http://stackoverflow.com/questions/12346326/nosql-cap-theorem-availability-and-partition-tolerance","q":"\n\n<p>While I try to understand the \"Availability\" (A) and \"Partition tolerance\" (P) in CAP, I found it difficult to understand the explanations from various articles. </p>\n\n<p>I get a feeling that A and P can go together (I know this is not the case, and that's why I fail to understand!).</p>\n\n<p>Explaining in simple terms, what is A and P and the difference between them? </p>\n    ","a":"\n<p>Consistency means that data is the same across the cluster, so you can read or write to/from any node and get the same data.</p>\n\n<p>Availability means the ability to access the cluster even if a node in the cluster goes down.</p>\n\n<p>Partition Tolerance means that the cluster continues to function even if there is a \"partition\" (communications break) between two nodes (both nodes are up, but can't communicate).</p>\n\n<p>In order to get both availability and partition tolerance, you have to give up consistency. Consider if you have two nodes, X and Y, in a master-master setup. Now, there is a break between network comms in X and Y, so they can't synch updates. At this point you can either:</p>\n\n<p>A) Allow the nodes to get out of sync (giving up consistency), or</p>\n\n<p>B) Consider the cluster to be \"down\" (giving up availability)</p>\n\n<p>All the combinations available are:</p>\n\n<ul>\n<li><strong>CA</strong> - data is consistent between all nodes - as long as all nodes are online - and you can read/write from any node and be sure that the data is the same, but if you ever develop a partition between nodes, the data will be out of sync (and won't re-sync once the partition is resolved).</li>\n<li><strong>CP</strong> - data is consistent between all nodes, and maintains partition tolerance (preventing data desync) by becoming unavailable when a node goes down.</li>\n<li><strong>AP</strong> - nodes remain online even if they can't communicate with each other and will resync data once the partition is resolved, but you aren't guaranteed that all nodes will have the same data (either during or after the partition)</li>\n</ul>\n\n<p>You should note that <a href=\"http://codahale.com/you-cant-sacrifice-partition-tolerance/\">CA systems don't practically exist</a> (even if some systems claim to be so).</p>\n    "},{"t":"Azure Table Vs MongoDB on Azure","l":"http://stackoverflow.com/questions/7989465/azure-table-vs-mongodb-on-azure","q":"\n\n<p>I want to use a NoSQL database on Windows Azure and the data volume will be very large. Whether a Azure Table storage or a MongoDB database running using a Worker role can offer better performance and scalability? Has anyone used MongoDB on Azure using a Worker role? Please share your thoughts on using MongoDB on Azure over the Azure table storage.</p>\n    ","a":"\n<p>Table Storage is a core Windows Azure storage feature, designed to be scalable (<strike>100TB</strike>  <strike>200TB</strike> 500TB per account), durable (triple-replicated in the data center, optionally georeplicated to another data center), and schemaless (each row may contain any properties you want). A row is located by partition key + row key, providing very fast lookup. All Table Storage access is via a well-defined REST API usable through any language (with SDKs, built on top of the REST APIs, already in place for .NET, PHP, Java, Python &amp; Ruby).</p>\n\n<p>MongoDB is a document-oriented database. To run it in Azure, you need to install MongoDB onto a web/worker roles or Virtual Machine, point it to a cloud drive (thereby providing a drive letter) or attached disk (for Windows/Linux Virtual Machines), optionally turn on journaling (which I'd recommend), and optionally define an external endpoint for your use (or access it via virtual network). The Cloud Drive / attached disk, by the way, is actually stored in an Azure Blob, giving you the same durability and georeplication as Azure Tables.</p>\n\n<p>When comparing the two, remember that Table Storage is Storage-as-a-Service: you simply access a well-known REST endpoint. With MongoDB, you're responsible for maintaining the database (e.g. whenever MongoDB Inc (formerly 10gen) pushes out a new version of MongoDB, you'll need to update your server accordingly).</p>\n\n<p>Regarding MongoDB Inc's alpha version pointed to by jtoberon: If you take a close look at it, you'll see a few key things:</p>\n\n<ul>\n<li>The setup is for a Standalone mongodb instance, without replica-sets or shards. Regarding replica-sets, you still get several benefits using the Standalone version, due to the way Blob storage works.</li>\n<li>To provide high-availability, you can run with multiple instances. In this case, only one instance serves the database, and one is a 'warm-standby' that launches the mongod process as soon as the other instance fails (for maintenance reboot, hardware failure, etc.).</li>\n</ul>\n\n<p>While 10gen's Windows Azure wrapper is still considered 'alpha,' mongod.exe is not. You can launch the mongod exe just like you'd launch any other Windows exe. It's just the management code around the launching, and that's what the alpa implementation is demonstrating.</p>\n\n<p><strong>EDIT 2011-12-8: This is no longer in an alpha state. You can download the latest MongoDB+Windows Azure project <a href=\"https://github.com/mongodb/mongo-azure\">here</a>, which provides replica-set support.</strong></p>\n\n<p>For performance, I think you'll need to do some benchmarking. Having said that, consider the following:</p>\n\n<ul>\n<li>When accessing either Table Storage or MongoDB from, say, a Web Role, you're still reaching out to the Windows Azure Storage system.</li>\n<li>MongoDB uses lots of memory for its own cache. For this reason, lots of high-scale MongoDB systems are deployed to larger instance sizes. For Table Storage access, you won't have the same memory-size consideration.</li>\n</ul>\n\n<p><strong>EDIT April 7, 2015</strong>\nIf you want to use a document-based database as-a-service, Azure now offers DocumentDB.</p>\n    "},{"t":"Soâ€¦ this NoSQL thing","l":"http://stackoverflow.com/questions/3183067/so-this-nosql-thing","q":"\n\n<p>I've been looking at MongoDB and I'm fascinated. It appears (although I have to be suspicious) that in exchange for organizing my database in a slightly different way, I get as much performance as I have CPUs and RAM for free? It seems elegant, and flexible, but I'm not trading that for fast like I am with Rails. So what's the catch? What does a relational database give me that I can't do as well or at all with Mongo? In other words, why (other than immaturity of existing NoSQL systems and resistence to change) doesn't the entire industry jump ship from MySQL? </p>\n\n<p>As I understood it, as you scale, you get MySQL to feed Memcache. Now it appears I can start with something equally performant from the beginning.</p>\n\n<p>I know I can't do transactions across relationships... when would this be a big deal?</p>\n\n<p>I read <a href=\"http://teddziuba.com/2010/03/i-cant-wait-for-nosql-to-die.html\">http://teddziuba.com/2010/03/i-cant-wait-for-nosql-to-die.html</a> but as I understand it, his argument is basically that real businesses which use real tools don't need to avoid SQL, so people who feel a need to ditch it are doing it wrong. But no \"enterprise\" has to deal with nearly as many concurrent users as Facebook or Google, so I don't really see his point. (Walmart has 1.8 million employees; Facebook has 300 million users).</p>\n\n<p>I'm genuinely curious about this... I promise I'm not trolling.</p>\n    ","a":"\n<p>I am also a big fan of MongoDB. That having been said, it is absolutely not a wholesale replacement for RDBMS. Facebook has 300 million users but if some of your friends don't show up in the list one time, or one of the photo albums is missing on the occasional request, would you notice? Probably not. If your status update doesn't trickle down to all of your friends for a few minutes, does it matter? Hardly. If Wal-Mart's balance sheets are out of sync, would someone lose their head? Definitely.</p>\n\n<p>NoSQL databases are great in \"fuzzy\" environments where relationships are not strict and data integrity can afford to be out of sync. RDBMS are still important when data sets are extremely complex and relational (hence the name), and they need to be kept pure.</p>\n\n<p>The big push to NoSQL comes from the fact for the last 30 years, we have been using RDMBS systems for both scenarios. We now have a more appropriate tool for many situations. Some would argue most, in fact. But no one would argue all.</p>\n    "},{"t":"NoSQL best practices [closed]","l":"http://stackoverflow.com/questions/2170152/nosql-best-practices","q":"\n\n<p>What are the best practices for NoSQL Databases, OODBs or whatever other acronyms may exist for them?</p>\n\n<p>For example, I've often seen a field \"type\" being used for deciding how the DB document (in couchDB/mongoDB terms) should be interpreted by the client, the application.</p>\n\n<p>Where applicable, use PHP as a reference language. Read: I'm also interested in how such data can be best handled on the client side, not only strictly the DB structure. This means practically that I'm also looking for patterns like \"ORM\"s for SQL DBs (active record, data mapper, etc).</p>\n\n<p>Don't hesitate making statements about how such a DB and the new features of PHP 5.3 could best work together.</p>\n    ","a":"\n<p>I think that currently, the whole idea of NoSQL data stores and the concept of document databases is so new and different from the established ideas which drive relational storage that there are currently very few (if any) best practices.</p>\n\n<p>We know at this point that the rules for storing your data within say CouchDB (or any other document database) are rather different to those for a relational one. For example, it is pretty much a fact that normalisation and aiming for 3NF is not something one should strive for. One of the common examples would be that of a simple blog.</p>\n\n<p>In a relational store, you'd have a table each for \"Posts\", \"Comments\" and \"Authors\". Each Author would have many Posts, and each Post would have many Comments. This is a model which works well enough, and maps fine over any relational DB. However, storing the same data within a docDB would most likely be rather different. You'd probably have something like a collection of Post documents, each of which would have its own Author and collection of Comments embedded right in. Of course that's probably not the only way you could do it, and it is somewhat a compromise (now querying for a single post is fast - you only do one operation and get everything back), but you have no way of maintaining the relationship between authors and posts (since it all becomes part of the post document).</p>\n\n<p>I too have seen examples making use of a \"type\" attribute (in a CouchDB example). Sure, that sounds like a viable approach. Is it the best one? I haven't got a clue. Certainly in MongoDB you'd use seperate collections within a database, making the type attribute total nonsense. In CouchDB though... perhaps that <em>is</em> best. The other alternatives? Separate databases for each type of document? This seems a bit loopy, so I'd lean towards the \"type\" solution myself. But that's just me. Perhaps there's something better.</p>\n\n<p>I realise I've rambled on quite a bit here and said very little, most likely nothing you didn't already know. My point is this though - I think its up to us to experiment with the tools we've got and the data we're working with and over time the good ideas will be spread and <em>become</em> the best-practices. I just think you're asking a little too early in the game.</p>\n    "},{"t":"Dumping all key/value pairs in a Redis db","l":"http://stackoverflow.com/questions/3798874/dumping-all-key-value-pairs-in-a-redis-db","q":"\n\n<p>I'm using an ORM called Ohm in Ruby that works on top of Redis and am curious to find out how the data is actually stored. I was wondering if there is way to list all the keys/values in a Redis db.</p>\n\n<p>Any lead will go a long way in helping me out (I'm basically stuck atm). Thanks in advance!</p>\n\n<p><strong>Update:</strong><br>\nA note for others trying this out using redis-cli, use this:</p>\n\n<pre><code>$ redis-cli keys\n* (press * followed by Ctrl-D)\n... (prints a list of keys and exits)\n$\n</code></pre>\n\n<p>Thanks @antirez and @hellvinz!</p>\n    ","a":"\n<p>You can explore the Redis dataset using the 'redis-cli' tool included in the Redis distribution.</p>\n\n<p>Just start the tool without arguments, then type commands to explore the dataset.</p>\n\n<p>For instance KEYS  will list all the keys matching a glob-style pattern, for instance with: <em>keys *</em> you'll see all the keys available.</p>\n\n<p>Then you can use the TYPE command to check what type is a given key, if it's a list you can retrieve the elements inside using <em>LRANGE mykey 0 -1</em>, if It is a Set you'll use instead <em>SMEMBERS mykey</em> and so forth. Check the Redis documentation for a list of all the available commands and how they works.</p>\n    "},{"t":"What does Redis do when it runs out of memory?","l":"http://stackoverflow.com/questions/5068518/what-does-redis-do-when-it-runs-out-of-memory","q":"\n\n<p>This might be easy question but I am having a hard time finding the answer. How does Redis 2.0 handle running out of maximum allocated memory? How does it decide which data to remove or which data to keep in memory? </p>\n\n<p>Thanks </p>\n    ","a":"\n<p>If you have virtual memory functionality turned on (new in version 2.0 or 2.2, I think), then Redis starts to store the \"not-so-frequently-used\" data to disk when memory runs out.</p>\n\n<p>If virtual memory in Redis is disabled, it appears as if the operating system's virtual memory starts to get used up (i.e. swap), and performance drops tremendously.</p>\n\n<p>Now, you can also configure Redis with a maxmemory parameter, which prevents Redis from using any more memory (the default).</p>\n\n<p>Newer versions of Redis have various policies when maxmemory is reached:</p>\n\n<ul>\n<li>volatile-lru remove a key among the\nones with an expire set, trying to\nremove keys not recently used.</li>\n<li>volatile-ttl remove a key among the\nones with an expire set, trying to\nremove keys with short remaining time\nto live.</li>\n<li>volatile-random remove a\nrandom key among the ones with an\nexpire set.</li>\n<li>allkeys-lru like\nvolatile-lru, but will remove every\nkind of key, both normal keys or keys\nwith an expire set.</li>\n<li>allkeys-random\nlike volatile-random, but will remove\nevery kind of keys, both normal keys\nand keys with an expire set.</li>\n</ul>\n\n<p>If you pick a policy that only removes keys with an EXPIRE set, then when Redis runs out of memory, it looks like the program just aborts the malloc() operation.  That is, if you try to store more data, the operation just fails miserably.</p>\n\n<p>Some links for more info (since you shouldn't just take my word for it):</p>\n\n<ul>\n<li><a href=\"http://antirez.com/post/redis-as-LRU-cache.html\">http://antirez.com/post/redis-as-LRU-cache.html</a></li>\n<li><a href=\"http://eli.thegreenplace.net/2009/10/30/handling-out-of-memory-conditions-in-c/\">http://eli.thegreenplace.net/2009/10/30/handling-out-of-memory-conditions-in-c/</a></li>\n</ul>\n    "},{"t":"What is an SSTable?","l":"http://stackoverflow.com/questions/2576012/what-is-an-sstable","q":"\n\n<p>In BigTable/GFS and Cassandra terminology, what is the definition of a SSTable?</p>\n    ","a":"\n<p>Sorted Strings Table (borrowed from google) is a file of key/value string pairs, sorted by keys</p>\n    "},{"t":"How do you track record relations in NoSQL?","l":"http://stackoverflow.com/questions/4126811/how-do-you-track-record-relations-in-nosql","q":"\n\n<p>I am trying to figure out the equivalent of foreign keys and indexes in NoSQL KVP or Document databases. Since there are no pivotal tables (to add keys marking a relation between two objects) I am really stumped as to how you would be able to retrieve data in a way that would be useful for normal web pages.</p>\n\n<p>Say I have a user, and this user leaves many comments all over the site. The only way I can think of to keep track of that users comments is to 1) Embed them in the user object (which seems quite useless) 2) create and maintain a <code>user_id:comments</code> value that contains a list of each comment's key [comment:34, comment:197, etc...] so that that I can fetch them as needed.</p>\n\n<p>However, taking the second example you will soon hit a brick wall when you use it for tracking other things like a key called \"active_comments\" which might contain 30 million ids in it making it <em>cost a TON</em> to query each page just to know some recent active comments. It also would be very prone to <em>race-conditions</em> as many pages might try to update it at the same time.</p>\n\n<p><strong>How can I track relations like the following in a NoSQL database?</strong></p>\n\n<ul>\n<li>All of a user's comments</li>\n<li>All active comments</li>\n<li>All posts tagged with [keyword]</li>\n<li>All students in a club - or all clubs a student is in</li>\n</ul>\n\n<p><em>Or am I thinking about this incorrectly?</em></p>\n    ","a":"\n<p>All the answers for how to store many-to-many associations in the \"NoSQL way\" reduce to the same thing: <strong>storing data redundantly.</strong></p>\n\n<p>In NoSQL, you don't design your database based on the relationships between data entities.  You design your database based on the queries you will run against it.  Use the same criteria you would use to denormalize a relational database:  if it's more important for data to have cohesion (think of values in a comma-separated list instead of a normalized table), then do it that way.</p>\n\n<p>But this inevitably optimizes for one type of query (e.g. comments by any user for a given article) at the expense of other types of queries (comments for any article by a given user).  If your application has the need for both types of queries to be equally optimized, you should not denormalize.  And likewise, you should not use a NoSQL solution if you need to use the data in a relational way.</p>\n\n<p>There is a risk with denormalization and redundancy that redundant sets of data will get out of sync with one another.  This is called an <em>anomaly</em>.  When you use a normalized relational database, the RDBMS can prevent anomalies.  In a denormalized database or in NoSQL, it becomes your responsibility to write application code to prevent anomalies.</p>\n\n<p>One might think that it'd be great for a NoSQL database to do the hard work of preventing anomalies for you.  There is a paradigm that can do this -- the relational paradigm.</p>\n    "},{"t":"Reasons for and against moving from SQL server to MongoDB","l":"http://stackoverflow.com/questions/3287966/reasons-for-and-against-moving-from-sql-server-to-mongodb","q":"\n\n<p>I know this is a big question and it's not a yes or no answer but we develop web apps and are looking into using MongoDB for our persistence solution. Combining MongoDB with NoRM for object storage.</p>\n\n<p>What I want to ask is what pitfalls have you experienced with switching from SQL to mongo? When is mongo simply not the right solution and is mongodb's advantages enough to move development from SQL?</p>\n\n<p>Many thanks in advance for any advice you may have.</p>\n    ","a":"\n<p>In my opinion the format of your data should be the primary concern when choosing a storage backend. Do you have data that is relational in nature? If so, can it and is it a good idea to model the data in documents? Data modeling is as important in a document database as in an relational database, it just done differently. How many types of objects do you have and how are they related? Can DBrefs in Mongodb do the trick or will you miss foreign keys so much it will be painful? What are your access patterns for the data? Are you just fetching data of one type filtered by a field value, or do you have intricate fetching modes? </p>\n\n<p>Do you need ACID transactional integrity? Does the domain enforce a lot of constraints on the data? Do you need the scalability factor of a document database or is that just a \"cool\" thing to have?</p>\n\n<p>What are your consistency and data integrity requirements? Some NoSQL solutions and MongoDB in particular are quite loose on the write consistency in order to get performance. NoSQL is no uniform landscape and other products, e.g. CouchDB has other characteristics in this department. Some are tunable too.</p>\n\n<p>These are all questions that should go into the choice of storage. </p>\n\n<p><strong>Some Experiences</strong> </p>\n\n<ul>\n<li>Doing extensive reporting on the stored data can be harder when using MongoDB or any document database and some use cases have been combining RDBMS and document-db for that purpose.</li>\n<li>(Very) Different query model. MongoDB differs from other document-dbs too.</li>\n<li>Flexible to change data format/schema during development</li>\n<li>Unknown territory</li>\n<li>varying degree of maturity in drivers and frameworks</li>\n<li>Fast</li>\n<li>Simpler (in many ways) product and management tools (compared to many RDBMS products)</li>\n<li>No more impedance mismatch. The storage fits the data, not the other way around. </li>\n<li>Less friction and more direct access to data. </li>\n<li>Domain more tied to persistence (depending on the ORM \"level\" of NoRM, on how much it abstract away the backend. I haven't used NoRM so I can't answer that.)</li>\n</ul>\n    "},{"t":"Transactions in NoSQL?","l":"http://stackoverflow.com/questions/2212230/transactions-in-nosql","q":"\n\n<p>I'm looking into NoSQL for scaling alternatives to a database.  What do I do if I want transaction-based things that are sensitive to these kind of things?</p>\n    ","a":"\n<p>Generally speaking, NoSQL solutions have lighter weight transactional semantics than relational databases, but still have facilities for atomic operations at some level.</p>\n\n<p>Generally, the ones which do master-master replication provide less in the way of consistency, and more availability.  So one should choose the right tool for the right problem.</p>\n\n<p>Many offer transactions at the single document (or row etc.) level.  For example with MongoDB there is atomicity at the single document - but documents can be fairly rich so this usually works pretty well -- more info <a href=\"http://www.mongodb.org/display/DOCS/Atomic+Operations\">here</a>.</p>\n    "},{"t":"Cassandra port usage - how are the ports used?","l":"http://stackoverflow.com/questions/2359159/cassandra-port-usage-how-are-the-ports-used","q":"\n\n<p>When experimenting with Cassandra I've observed that Cassandra listens to the following ports:</p>\n\n<ul>\n<li>TCP *:8080</li>\n<li>TCP *:8888</li>\n<li>TCP *:57311</li>\n<li>TCP *:57312</li>\n<li>TCP 127.0.0.1:7000</li>\n<li>TCP 127.0.0.1:9160</li>\n<li>UDP 127.0.0.1:700</li>\n</ul>\n\n<p>How does Cassandra use each of the ports listed?</p>\n    ","a":"\n<p>@Schildmeijer is largely right, however port 7001 is still used when using <a href=\"http://wiki.apache.org/cassandra/InternodeEncryption\">TLS Encrypted Internode communication</a></p>\n\n<p>So my complete list would be for current versions of Cassandra:</p>\n\n<ul>\n<li>7199 - JMX (was 8080 pre Cassandra 0.8.xx)</li>\n<li>7000 - Internode communication (not used if TLS enabled)</li>\n<li>7001 - TLS Internode communication (used if TLS enabled)</li>\n<li>9160 - Thift client API</li>\n<li>9042 - CQL native transport port</li>\n</ul>\n    "},{"t":"How to choose between Cassandra, Membase, Hadoop, MongoDB, RDBMS etc.? [closed]","l":"http://stackoverflow.com/questions/8120036/how-to-choose-between-cassandra-membase-hadoop-mongodb-rdbms-etc","q":"\n\n<p>Is there a paper/blog-post on when to use Cassandra or Membase or Hadoop or plain old relational databases ? Is there a paper discussing the strengths/weaknesses of each, and on what scenarios either of these technologies should be chosen ?</p>\n\n<p>I am thinking of writing a new webservice which will have about a million hits per day and data spanning about a few terabytes.</p>\n    ","a":"\n<p><strong>EDIT</strong> <em>The NoSQL Ecosystem</em> by Adam Marcus (from the book The Architecture of open source applications): <a href=\"http://www.aosabook.org/en/nosql.html\">http://www.aosabook.org/en/nosql.html</a></p>\n\n<p>general thoughts and comparison <a href=\"http://www.thoughtworks.com/articles/nosql-comparison\">http://www.thoughtworks.com/articles/nosql-comparison</a></p>\n\n<p>technical comparison <a href=\"http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis\">http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis</a></p>\n\n<p>a Master's Thesis - Analysis and Classification of NoSQL Databases  <a href=\"http://scholar.googleusercontent.com/scholar?q=cache:rQlbiz6bojAJ:scholar.google.com/+comparison+of+nosql&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1\">http://scholar.googleusercontent.com/scholar?q=cache:rQlbiz6bojAJ:scholar.google.com/+comparison+of+nosql&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1</a></p>\n    "},{"t":"Difference between HBase and Hadoop/HDFS","l":"http://stackoverflow.com/questions/16929832/difference-between-hbase-and-hadoop-hdfs","q":"\n\n<p>This is kind of naive question but I am new to NoSQL paradigm and don't know much about it. So if somebody can help me clearly understand difference between the HBase and Hadoop or if give some pointers which might help me understand the difference.</p>\n\n<p>Till now, I did some research and acc. to my understanding Hadoop provides framework to work with raw chunk of data(files) in HDFS and HBase is database engine above Hadoop, which basically works with structured data instead of raw data chunk. Hbase provides a logical layer over HDFS just as SQL does. Is it correct?</p>\n\n<p>Pls feel free to correct me.</p>\n\n<p>Thanks.</p>\n    ","a":"\n<p>Hadoop is basically 2 things, a FS (Hadoop Distributed File System) and a computation framework (MapReduce). HDFS allows you store huge amounts of data in a distributed (provides faster read/write access) and redundant (provides better availability) manner. And MapReduce allows you to process this huge data in a distributed and parallel manner. But MapReduce is not limited to just HDFS. Being a FS, HDFS lacks the random read/write capability. It is good for sequential data access. And this is where HBase comes into picture. It is a NoSQL database that runs on top your Hadoop cluster and provides you random real-time read/write access to your data.</p>\n\n<p>You can store both structured and unstructured data in Hadoop, and HBase as well. Both of them provide you multiple mechanisms to access the data, like the shell and other APIs.\nAnd, HBase stores data as key/value pairs in a columnar fashion while HDFS stores data as flat files. Some of the salient features of both the systems are :</p>\n\n<p><strong>Hadoop</strong></p>\n\n<ol>\n<li>Optimized for streaming access of large files.</li>\n<li>Follows write-once read-many ideology.</li>\n<li>Doesn't support random read/write.</li>\n</ol>\n\n<p><strong>HBase</strong></p>\n\n<ol>\n<li>Stores key/value pairs in columnar fashion (columns are clubbed together as column families).</li>\n<li>Provides low latency access to small amounts of data from within a large data set.</li>\n<li>Provides flexible data model.</li>\n</ol>\n\n<p>Hadoop is most suited for offline batch-processing kinda stuff while HBase is used when you have real-time needs.</p>\n\n<p>An analogous comparison would be between MySQL and Ext4.</p>\n    "},{"t":"DynamoDB vs MongoDB NoSQL","l":"http://stackoverflow.com/questions/17931073/dynamodb-vs-mongodb-nosql","q":"\n\n<p>I'm trying to figure it out what can I use for a future project, we plan to store about from 500k records per month in the first year and maybe more for the next years this is a vertical application so there's no need to use a database for this, that's the reason why I decided to choose a noSQL data storage.</p>\n\n<p>The first option that came to my mind was mongo db since is a very mature product with a lot of support from the community but in the other hand we got a brand new product that offers a managed service at top performance, I'll develop this applciation but there's no maintenance plan (at least for now) so I think that will be a huge advantage since amazon provides a elastic way to escale.</p>\n\n<p>my major concern is about the query structure, I haven't looked at the dynamoDB query capabilities yet but since is a k/v data storage I feel that this could be more limitated than mongo db.</p>\n\n<p>if someone had the experience moving on a project from mongoDB to DynamoDB, any advice will be totally appreciated.</p>\n\n<p>I plan to use mvc4/c# as a development stack!</p>\n    ","a":"\n<p>With 500k documents, there is no reason to scale whatsoever. A typical laptop with an SSD and 8GB of ram can easily do 10s of millions of records, so if you are trying to pick because of scaling your choice doesn't really matter. I would suggest you pick when you like most, and perhaps where you can find the most online support with.</p>\n    "},{"t":"Choosing MongoDb/CouchDb/RavenDb - performance and scalability advice [closed]","l":"http://stackoverflow.com/questions/5258360/choosing-mongodb-couchdb-ravendb-performance-and-scalability-advice","q":"\n\n<p>We are looking at a document db storage solution with fail over clustering, for some read/write intensive application.</p>\n\n<p>We will be having an average of 40K concurrent writes per second written to the db (with peak can go up to 70,000 during) - and may have around almost similiar number of reads happening.</p>\n\n<p>We also need a mechanism for the db to notify about the newly written records (some kind of trigger at db level).</p>\n\n<p>What will be a good option in terms of a proper choice of document db and related capacity planning?</p>\n\n<p><strong>Updated</strong></p>\n\n<p>More details on the expectation.</p>\n\n<ul>\n<li>On an average, we are expecting 40,000 (40K) Number of inserts (new documents) per second across 3-4 databases/document collections.</li>\n<li>The peak may go up to 120,000 (120K) Inserts</li>\n<li>The Inserts should be readable right away - almost realtime</li>\n<li>Along with this, we expect around 5000 updates or deletes per second</li>\n<li>Along with this, we also expect 500-600 concurrent queries accessing data. These queries and execution plans are somewhat known, though this might have to be updated, like say, once in a week or so.</li>\n<li>The system should support failover clustering on the storage side</li>\n</ul>\n    ","a":"\n<p>if \"20,000 concurrent writes\" means inserts then I would go for CouchDB and use \"_changes\" api for triggers. But with 20.000 writes you would need a stable sharding aswell. Then you would better take a look at <a href=\"https://github.com/cloudant/bigcouch/\">bigcouch</a></p>\n\n<p>And if \"20.000\" concurrent writes consist \"mostly\" updates I would go for MongoDB for sure, since Its \"update in place\" is pretty awesome. But then you should handle triggers manually, but using another collection to update in place a general document can be a handy solution. Again be careful about sharding.</p>\n\n<p>Finally I think you cannot select a database with just concurrency, you need to plan the api (how you would retrieve data) then look at options in hand.</p>\n    "},{"t":"Redis, CouchDB or Cassandra? [closed]","l":"http://stackoverflow.com/questions/4720508/redis-couchdb-or-cassandra","q":"\n\n<p>What are the strengths and weaknesses of the various NoSQL databases available?</p>\n\n<p>In particular, it seems like Redis is weak when it comes to distributing write load over multiple servers. Is that the case? Is it a big problem? How big does a service have to grow before that could be a significant problem?</p>\n    ","a":"\n<p>The strengths and weaknesses of the NoSQL databases (and also SQL databases) is <em>highly</em> dependent on your use case. For very large projects, performance is king; but for brand new projects, or projects where time and money are limited, simplicity and time-to-market are probably the most important. For teaching yourself (broadening your perspective, becoming a better, more valuable programmer), perhaps the most important thing is simple, solid fundamental concepts.</p>\n\n<p>What kind of project do you have in mind?</p>\n\n<p>Some strengths and weaknesses, off the top of my head:</p>\n\n<ul>\n<li><strong>Redis</strong>\n<ul>\n<li>Very simple key-value \"global variable server\"</li>\n<li>Very simple (some would say \"non-existent\") query system</li>\n<li>Easily the fastest in this list</li>\n<li>Transactions</li>\n<li>Data set must fit in memory</li>\n<li>Immature clustering, with unclear future (I'm sure it'll be great, but it's not yet decided.)</li>\n</ul></li>\n<li><strong>Cassandra</strong>\n<ul>\n<li>Arguably the most community momentum of the BigTable-like databases</li>\n<li>Probably the easiest of this list to manage in big/growing clusters</li>\n<li>Support for map/reduce, good for analytics, data warehousing</li>\n<li>MUlti-datacenter replication</li>\n<li>Tunable consistency/availability</li>\n<li>No single point of failure</li>\n<li>You must know what queries you will run early in the project, to prepare the data shape and indexes</li>\n</ul></li>\n<li><strong>CouchDB</strong>\n<ul>\n<li>Hands-down the best sync (replication) support, supporting master/slave, master/master, and more exotic architectures</li>\n<li>HTTP protocol, browsers/apps can interact directly with the DB partially or entirely. (Sync is also done over HTTP)</li>\n<li>After a brief learning curve, pretty sophisticated query system using Javascript and map/reduce</li>\n<li>Clustered operation (no SPOF, tunable consistency/availability) is currently a significant fork (BigCouch). It will probably merge into Couch but there is no roadmap.</li>\n<li>Similarly, clustering and multi-datacenter are theoretically possible (the \"exotic\" thing I mentioned) however you must write all that tooling yourself at this time.</li>\n<li>Append only file format (both databases and indexes) consumes disk surprisingly quickly, and you must manually run compaction (vacuuming) which makes a full copy of all records in the database. The same is required for each index file. Again, you have to be your own toolsmith.</li>\n</ul></li>\n</ul>\n    "},{"t":"MongoDB: Terrible MapReduce Performance","l":"http://stackoverflow.com/questions/3947889/mongodb-terrible-mapreduce-performance","q":"\n\n<p>I have a long history with relational databases, but I'm new to MongoDB and MapReduce, so I'm almost positive I must be doing something wrong. I'll jump right into the question. Sorry if it's long.</p>\n\n<p>I have a database table in MySQL that tracks the number of member profile views for each day. For testing it has 10,000,000 rows.</p>\n\n<pre><code>CREATE TABLE `profile_views` (\n  `id` int(10) unsigned NOT NULL auto_increment,\n  `username` varchar(20) NOT NULL,\n  `day` date NOT NULL,\n  `views` int(10) unsigned default '0',\n  PRIMARY KEY  (`id`),\n  UNIQUE KEY `username` (`username`,`day`),\n  KEY `day` (`day`)\n) ENGINE=InnoDB;\n</code></pre>\n\n<p>Typical data might look like this.</p>\n\n<pre><code>+--------+----------+------------+------+\n| id     | username | day        | hits |\n+--------+----------+------------+------+\n| 650001 | Joe      | 2010-07-10 |    1 |\n| 650002 | Jane     | 2010-07-10 |    2 |\n| 650003 | Jack     | 2010-07-10 |    3 |\n| 650004 | Jerry    | 2010-07-10 |    4 |\n+--------+----------+------------+------+\n</code></pre>\n\n<p>I use this query to get the top 5 most viewed profiles since 2010-07-16.</p>\n\n<pre><code>SELECT username, SUM(hits)\nFROM profile_views\nWHERE day &gt; '2010-07-16'\nGROUP BY username\nORDER BY hits DESC\nLIMIT 5\\G\n</code></pre>\n\n<p>This query completes in under a minute. Not bad!</p>\n\n<p>Now moving onto the world of MongoDB. I setup a sharded environment using 3 servers. Servers M, S1, and S2. I used the following commands to set the rig up (Note: I've obscured the IP addys).</p>\n\n<pre><code>S1 =&gt; 127.20.90.1\n./mongod --fork --shardsvr --port 10000 --dbpath=/data/db --logpath=/data/log\n\nS2 =&gt; 127.20.90.7\n./mongod --fork --shardsvr --port 10000 --dbpath=/data/db --logpath=/data/log\n\nM =&gt; 127.20.4.1\n./mongod --fork --configsvr --dbpath=/data/db --logpath=/data/log\n./mongos --fork --configdb 127.20.4.1 --chunkSize 1 --logpath=/data/slog\n</code></pre>\n\n<p>Once those were up and running, I hopped on server M, and launched mongo. I issued the following commands:</p>\n\n<pre><code>use admin\ndb.runCommand( { addshard : \"127.20.90.1:10000\", name: \"M1\" } );\ndb.runCommand( { addshard : \"127.20.90.7:10000\", name: \"M2\" } );\ndb.runCommand( { enablesharding : \"profiles\" } );\ndb.runCommand( { shardcollection : \"profiles.views\", key : {day : 1} } );\nuse profiles\ndb.views.ensureIndex({ hits: -1 });\n</code></pre>\n\n<p>I then imported the same 10,000,000 rows from MySQL, which gave me documents that look like this:</p>\n\n<pre><code>{\n    \"_id\" : ObjectId(\"4cb8fc285582125055295600\"),\n    \"username\" : \"Joe\",\n    \"day\" : \"Fri May 21 2010 00:00:00 GMT-0400 (EDT)\",\n    \"hits\" : 16\n}\n</code></pre>\n\n<p>Now comes the real meat and potatoes here... My map and reduce functions. Back on server M in the shell I setup the query and execute it like this.</p>\n\n<pre><code>use profiles;\nvar start = new Date(2010, 7, 16);\nvar map = function() {\n    emit(this.username, this.hits);\n}\nvar reduce = function(key, values) {\n    var sum = 0;\n    for(var i in values) sum += values[i];\n    return sum;\n}\nres = db.views.mapReduce(\n    map,\n    reduce,\n    {\n        query : { day: { $gt: start }}\n    }\n);\n</code></pre>\n\n<p>And here's were I run into problems. <strong><em>This query took over 15 minutes to complete!</em></strong> The MySQL query took under a minute. Here's the output:</p>\n\n<pre><code>{\n        \"result\" : \"tmp.mr.mapreduce_1287207199_6\",\n        \"shardCounts\" : {\n                \"127.20.90.7:10000\" : {\n                        \"input\" : 4917653,\n                        \"emit\" : 4917653,\n                        \"output\" : 1105648\n                },\n                \"127.20.90.1:10000\" : {\n                        \"input\" : 5082347,\n                        \"emit\" : 5082347,\n                        \"output\" : 1150547\n                }\n        },\n        \"counts\" : {\n                \"emit\" : NumberLong(10000000),\n                \"input\" : NumberLong(10000000),\n                \"output\" : NumberLong(2256195)\n        },\n        \"ok\" : 1,\n        \"timeMillis\" : 811207,\n        \"timing\" : {\n                \"shards\" : 651467,\n                \"final\" : 159740\n        },\n}\n</code></pre>\n\n<p>Not only did it take forever to run, but the results don't even seem to be correct.</p>\n\n<pre><code>db[res.result].find().sort({ hits: -1 }).limit(5);\n{ \"_id\" : \"Joe\", \"value\" : 128 }\n{ \"_id\" : \"Jane\", \"value\" : 2 }\n{ \"_id\" : \"Jerry\", \"value\" : 2 }\n{ \"_id\" : \"Jack\", \"value\" : 2 }\n{ \"_id\" : \"Jessy\", \"value\" : 3 }\n</code></pre>\n\n<p>I know those value numbers should be much higher.</p>\n\n<p>My understanding of the whole MapReduce paradigm is the task of performing this query should be split between all shard members, which should increase performance. I waited till Mongo was done distributing the documents between the two shard servers after the import. Each had almost exactly 5,000,000 documents when I started this query.</p>\n\n<p>So I must be doing something wrong. Can anyone give me any pointers?</p>\n\n<p>Edit: Someone on IRC mentioned adding an index on the day field, but as far as I can tell that was done automatically by MongoDB.</p>\n    ","a":"\n<p>excerpts from MongoDB Definitive Guide from O'Reilly:</p>\n\n<blockquote>\n  <p>The price of using MapReduce is speed:\n  group is not particularly speedy, but \n  MapReduce is slower and is not\n  supposed to be used in â€œreal time.â€\n  You run  MapReduce as a background\n  job, it creates a collection of\n  results, and then you can  query that\n  collection in real time.</p>\n</blockquote>\n\n<pre><code>options for map/reduce:\n\n\"keeptemp\" : boolean \nIf the temporary result collection should be saved when the connection is closed. \n\n\"output\" : string \nName for the output collection. Setting this option implies keeptemp : true. \n</code></pre>\n    "},{"t":"Is mongodb running?","l":"http://stackoverflow.com/questions/5091624/is-mongodb-running","q":"\n\n<p>I have installed mongodb and the php drivers on my unix server.</p>\n\n<p>My question is how can I tell if mongodb is running? Is there a simple command line query to check status? If I start it once from the shell will it keep running if I exit the shell (this doesn't seem to be the case). How can I make the mongodb connection persistent and auto start on server reboot?</p>\n\n<p>I can run:</p>\n\n<blockquote>\n  <p>-bash-3.2$ su<br>\n  Password:<br>\n  [root@xxx]# cd /var/lib<br>\n  [root@xxx]# ./mongodb-linux-i686-1.6.5/bin/mongod<br>\n  ./mongodb-linux-i686-1.6.5/bin/mongod --help for help and startup options<br>\n  Wed Feb 23 08:06:54 MongoDB starting : pid=7271 port=27017 dbpath=/data/db/ 32-bit  </p>\n  \n  <p>** NOTE: when using MongoDB 32 bit, you are limited to about 2 gigabytes of data<br>\n  **       see <a href=\"http://blog.mongodb.org/post/137788967/32-bit-limitations\">http://blog.mongodb.org/post/137788967/32-bit-limitations</a>  </p>\n  \n  <p>** WARNING: You are running in OpenVZ. This is known to be broken!!!  </p>\n  \n  <p>Wed Feb 23 08:06:54 db version v1.6.5, pdfile version 4.5<br>\n  Wed Feb 23 08:06:54 git version: 0eb017e9b2828155a67c5612183337b89e12e291<br>\n  Wed Feb 23 08:06:54 sys info: Linux domU-12-31-39-01-70-B4 2.6.21.7-2.fc8xen #1 SMP Fri<br>\n  Feb 15 12:39:36 EST 2008 i686 BOOST_LIB_VERSION=1_37<br>\n  Wed Feb 23 08:06:54 [initandlisten] waiting for connections on port 27017<br>\n  Wed Feb 23 08:06:54 [websvr] web admin interface listening on port 28017  </p>\n</blockquote>\n\n<p>If I open a seperate shell I can then then connect to mongodb:</p>\n\n<blockquote>\n  <p>-bash-3.2$ cd /var/lib<br>\n  -bash-3.2$ ./mongodb-linux-i686-1.6.5/bin/mongo<br>\n  MongoDB shell version: 1.6.5<br>\n  connecting to: test<br>\n  db.foo.find()<br>\n  { \"_id\" : ObjectId(\"4d63d7d3eb95985ab19c8feb\"), \"a\" : 1 }</p>\n</blockquote>\n\n<p>However if I close the initial shell I can't connect:</p>\n\n<blockquote>\n  <p>-bash-3.2$ cd /var/lib<br>\n  -bash-3.2$ ./mongodb-linux-i686-1.6.5/bin/mongo<br>\n  MongoDB shell version: 1.6.5<br>\n  connecting to: test<br>\n  Wed Feb 23 08:25:10 Error: couldn't connect to server 127.0.0.1 (anon):1154<br>\n  exception: connect failed  </p>\n</blockquote>\n    ","a":"\n<p>check with either:</p>\n\n<pre><code>   ps -edaf | grep mongo   # \"ps\" flags may differ on your OS\n</code></pre>\n\n<p>or </p>\n\n<pre><code>   /etc/init.d/mongod status\n</code></pre>\n\n<p>or </p>\n\n<pre><code>   service mongod status\n</code></pre>\n\n<p>to see if mongod is running (you need to be root to do this, or prefix everything with <code>sudo</code>)</p>\n\n<p>check the log file /var/log/mongo/mongo.log  to see if there are any problems reported</p>\n    "},{"t":"MongoDB normalization, foreign key and joining","l":"http://stackoverflow.com/questions/5841681/mongodb-normalization-foreign-key-and-joining","q":"\n\n<p>Before I dive really deep into MongoDB for days, I thought I'd ask a pretty basic question as to whether I should dive into it at all or not. I have basically no experience with nosql.</p>\n\n<p>I did read a little about some of the benefits of document databases, and I think for this new application, they will be really great. It is always a hassle to do favourites, comments, etc. for many types of objects (lots of m-to-m relationships) and subclasses - it's kind of a pain to deal with.</p>\n\n<p>I also have a structure that will be a pain to define in SQL because it's extremely nested and translates to a document a lot better than 15 different tables.</p>\n\n<p>But I am confused about a few things. </p>\n\n<ol>\n<li><p>Is it desirable to keep your database normalized still? I really don't want to be updating multiple records. Is that still how people approach the design of the database in MongoDB?</p></li>\n<li><p>What happens when a user favourites a book and this selection is still stored in a user document, but then the book is deleted? How does the relationship get detached without foreign keys? Am I manually responsible for deleting all of the links myself?</p></li>\n<li><p>What happens if a user favourited a book that no longer exists and I query it (some kind of join)? Do I have to do any fault-tolerance here?</p></li>\n</ol>\n    ","a":"\n<p>MongoDB doesn't support server side foreign key relationships, normalization is also discouraged. You should embed your child object within parent objects if possible, this will increase performance and make foreign keys totally unnecessary. That said it is not always possible, so there is a special construct called DBRef which allows to reference objects in a different collection. This may be then not so speedy because DB has to make additional queries to read objects but allows for kind of foreign key reference.</p>\n\n<p>Still you will have to handle your references manually. Only while looking up your DBRef you will see if it exists, the DB will not go through all the documents to look for the references and remove them if the target of the reference doesn't exist any more. But I think removing all the references after deleting the book would require a single query per collection, no more, so not that difficult really.</p>\n\n<p>If your schema is more complex then probably your should choose a relational database and not nosql.</p>\n\n<p>There is also a book about designing MongoDB databases: <a href=\"http://oreilly.com/catalog/0636920018391/\">Document Design for MongoDB</a></p>\n\n<p><strong>UPDATE</strong> The book above is not available anymore, yet because of popularity of MongoDB there are quite a lot of others. I won't link them all, since such links are likely to change, a simple search on Amazon shows multiple pages so it shouldn't be a problem to find some.</p>\n\n<p>See the MongoDB manual page for <a href=\"http://docs.mongodb.org/manual/applications/database-references/\">'Manual references' and DBRefs</a> for further specifics and examples</p>\n    "},{"t":"What exactly is NoSQL?","l":"http://stackoverflow.com/questions/2173082/what-exactly-is-nosql","q":"\n\n<p>What exactly is NoSQL? Is it database systems that only work with {key:value} pairs?</p>\n\n<p>As far as I know <code>MemCache</code> is one of such database systems, am I right?</p>\n\n<p>What other popular NoSQL databases are there and where exactly are they useful?</p>\n\n<p>Thanks, Boda Cydo.</p>\n    ","a":"\n<p>From wikipedia:</p>\n\n<blockquote>\n  <p>NoSQL is an umbrella term for a loosely defined class of non-relational data stores that break with a long history of relational databases and ACID guarantees. Data stores that fall under this term may not require fixed table schemas, and usually avoid join operations. The term was first popularised in early 2009.</p>\n</blockquote>\n\n<p>The motivation for such an architecture was high scalability, to support sites such as Facebook, advertising.com, etc...</p>\n    "},{"t":"How to search over huge non-text based data sets?","l":"http://stackoverflow.com/questions/5987242/how-to-search-over-huge-non-text-based-data-sets","q":"\n\n<p>In a project I am working, the client has a an old and massive(terabyte range) RDBMS. Queries of all kinds are slow and there is no time to fix/refactor the schema. I've identified the sets of common queries that need to be optimized. This set is divided in two: full-text and metadata queries.</p>\n\n<p>My plan is to extract the data from their database and partition it across two different storage systems each optimized for a particular query set.</p>\n\n<p>For full-text search, Solr is the engine that makes most sense. It's sharding and replication features make it a great fit for half of the problem.</p>\n\n<p>For metadata queries, I am not sure what route to take. Currently, I'm thinking of using an RDBMS with an extremely de-normalized schema that represents a particular subset of the data from the \"Authoritative\" RDBMS. However, my client is concerned about the lack of sharding and replication of such subsystem and difficulty/complications of setting such features as compared with Solr that already includes them. Metadata in this case takes the form of integers, dates, bools, bits, and strings(with max size of 10chars).</p>\n\n<p>Is there a database storage system that features built-in sharding and replication that may be particular useful to query said metadata? Maybe a no-sql solution out there that provides a good query engine? </p>\n\n<p>Illuminate please.</p>\n\n<p>Additions/Responses:</p>\n\n<p><em>Solr can be used for metadata, however, the metadata is volatile. Therefore, I would have to commit often to the indexes. This would cause search to degrade pretty fast.</em></p>\n    ","a":"\n<p>Use <strong>MongoDB</strong> for your metadata store:</p>\n\n<ul>\n<li>Built-in <a href=\"http://www.mongodb.org/display/DOCS/Sharding\" rel=\"nofollow\">sharding</a></li>\n<li>Built-in replication</li>\n<li>Failover &amp; high availability</li>\n<li><a href=\"http://www.mongodb.org/display/DOCS/Querying\" rel=\"nofollow\">Simple query engine</a> that should work for most common cases</li>\n</ul>\n\n<p><strong>However</strong>, the downside is that you can not perform joins. Be smart about denormalizing your data so that you can avoid this.</p>\n    "},{"t":"NoSQL and spatial data","l":"http://stackoverflow.com/questions/2041622/nosql-and-spatial-data","q":"\n\n<p>Has any of you had any experience with using NoSQL (non-relational) databases to store spatial data? Are there any potential benefits (speed, space, ...) of using such databases to hold data for, say, a desktop application (compared to using SpatiaLite or PostGIS)?</p>\n\n<p>I've seen <a href=\"http://gissolved.blogspot.com/2009/06/spatial-indexing-mongodb-with-rtree.html\">posts about using MongoDB for spatial data</a>, but I'm interested in some performance comparison.</p>\n    ","a":"\n<p>graphs databases like <a href=\"http://www.neo4j.org\">Neo4j</a> are a very good fit, especially as you can add different indexing schemes dynamically as you go. Typical stuff you can do on your base data is of course 1D indexing (e.g. Timline or B-Trees) or funkier stuff like Hilbert Curves etc, see <a href=\"http://blog.notdot.net/2009/11/Damn-Cool-Algorithms-Spatial-indexing-with-Quadtrees-and-Hilbert-Curves\">Nick's blog</a>. Also, for some live demonstration, look at the AWE open source GIS desktop tool <a href=\"http://www.youtube.com/craigtaverner\">here</a>, the underlying indexed graph being visible around time 07:00 .</p>\n    "},{"t":"Real alternatives to Windows Azure PaaS (web role)? [closed]","l":"http://stackoverflow.com/questions/7244038/real-alternatives-to-windows-azure-paas-web-role","q":"\n\n<p>I am looking for alternatives to the Windows Azure PaaS (Platform as a Service) offering, meeting the following requirements:</p>\n\n<ol>\n<li>The platform should provide high availability and scalability.</li>\n<li>The platform should <strong>manage/update</strong> the application server and OS for me.</li>\n<li>The platform should provide some form of <strong>NoSQL database</strong> (like Azure Table Storage or SimpleDB).</li>\n<li>The platform should natively support <strong>ASP.NET MVC</strong>, i.e. IIS &amp; runtime environment should be automatically managed/patched for me.</li>\n</ol>\n\n<p>While Amazon satisfies item 1 and 3, it provides only IaaS (Infrastructure as a Service), meaning that I still have a large administration overhead.</p>\n\n<p>So that question is, what alternatives are there to Windows Azure, that provide a roughly equivalent feature set? Any suggestions are welcome!</p>\n    ","a":"\n<p>Have you tried <a href=\"http://AppHarbor.com\"><strong>AppHarbor</strong></a>?</p>\n\n<p>It's just amazing!</p>\n\n<ul>\n<li>Runs on top of Amazon EC2</li>\n<li>The entire platform, including application servers, is managed and updated by AppHarbor.</li>\n<li>Applications run inside IIS on fully updated Windows Server 2008 R2 instances.</li>\n<li>.NET 2.0-4.0 is natively supported along with ASP.NET MVC 1, 2 and 3, ASP.NET WebPages, WCF services. Updates are installed as they are released.</li>\n<li>Scaling is <a href=\"http://blog.appharbor.com/2011/08/29/application-scaling-now-in-beta\">really easy</a> and it takes seconds to deploy. Scaling to multiple instances increases availability and redundancy.</li>\n<li>All website administration including rollback to a previous version and <a href=\"http://support.appharbor.com/kb/getting-started/managing-environments\">environment configuration</a> can be handled through the appharbor.com web interface.</li>\n<li>Builds and tests .NET projects right out of GIT Repository (or Mercurial through <a href=\"https://bitbucket.org/\">BitBucket</a>).</li>\n<li>MySQL and MSSQL are available as shared and dedicated offerings and optional redundancy for high availability.</li>\n<li>NoSQL databases are supported through <a href=\"http://appharbor.com/addon\">add-ons</a>. Add-ons are also available for e-mail, indexing, caching and performance testing.</li>\n<li>You can use AWS SimpleDB and any other AWS service through your own AWS account. Servers are currently located in US-EAST (Virginia), so put your services there if possible for lower latency.</li>\n<li><a href=\"http://support.appharbor.com/\">Excellent support</a></li>\n<li><a href=\"http://support.appharbor.com/discussions/problems/900-background-jobs\">Create background jobs easily</a></li>\n</ul>\n\n<p>This is <strong>Heroku for .NET</strong> :o)</p>\n    "},{"t":"What are the pros and cons of DynamoDB with respect to other NoSQL databases?","l":"http://stackoverflow.com/questions/8925719/what-are-the-pros-and-cons-of-dynamodb-with-respect-to-other-nosql-databases","q":"\n\n<p>We use MongoDB database add-on on Heroku for our SaaS product. Now that Amazon launched DynamoDB, a cloud database service, I was wondering how that changes the NoSQL offerings landscape?</p>\n\n<p>Specifically for cloud based services or SaaS vendors, how will using DynamoDB be better or worse as compared to say MongoDB? Are there any cost, performance, scalability, reliability, drivers, community etc. benefits of using one versus the other?</p>\n    ","a":"\n<p>For starters, it will be fully managed by Amazon's expert team, so you can bet that it will scale very well with virtually no input from the end user (developer).</p>\n\n<p>Also, since its built and managed by Amazon, you can assume that they have designed it to work very well with their infrastructure so you can can assume that performance will be top notch.  In addition to being specifically built for their infrastructure, they have chosen to use SSD's as storage so right from the start, disk throughput will be significantly higher than other data stores on AWS that are HDD backed.</p>\n\n<p>I havent seen any drivers yet and I think its too early to tell how the community will react to this, but I suspect that Amazon will have drivers for all of the most popular languages and the community will likely receive this well - and in turn create additional drivers and tools.</p>\n    "},{"t":"NoSQL for mobile apps? [closed]","l":"http://stackoverflow.com/questions/5924924/nosql-for-mobile-apps","q":"\n\n<p>Is there any established noSQL database solution to be used for developing native mobile applications (Android and/or iOs)?</p>\n    ","a":"\n<p>I don't think there's an <em>established</em> NoSQL backend for native mobile apps, but CouchDB is a great NoSQL database with implementations for both iOS and Android.</p>\n\n<p>iOS: <a href=\"http://www.couchbase.com/products-and-services/mobile-couchbase\" rel=\"nofollow\">http://www.couchbase.com/products-and-services/mobile-couchbase</a></p>\n\n<p>Android: <a href=\"https://github.com/couchbase/couchbase-lite-android\" rel=\"nofollow\">https://github.com/couchbase/couchbase-lite-android</a></p>\n    "},{"t":"Foreign keys in mongo?","l":"http://stackoverflow.com/questions/6334048/foreign-keys-in-mongo","q":"\n\n<p><img src=\"http://i.stack.imgur.com/6ejPD.png\" alt=\"enter image description here\"></p>\n\n<p>How do I design a scheme such this in MongoDB? I think there are no foreign keys!</p>\n    ","a":"\n<p>You may be interested in using a ORM like Mongoid or MongoMapper.</p>\n\n<p><a href=\"http://mongoid.org/docs/relations/referenced/1-n.html\">http://mongoid.org/docs/relations/referenced/1-n.html</a></p>\n\n<p>In a NoSQL database like MongoDB there are not 'tables' but documents. Documents are grouped inside Collections. You can have any kind of document â€“ with any kind of data â€“ in a single collection. Basically, in a NoSQL database it is up to you to decide how to organise the data and its relations, if there are any.</p>\n\n<p>What Mongoid and MongoMapper do is to provide you with convenient methods to set up relations quite easily. Check out the link I gave you and ask any thing.</p>\n\n<p>Edit:</p>\n\n<p>In mongoid you will write your scheme like this:</p>\n\n<pre><code>class Student\n  include Mongoid::Document\n\n    field :name\n    embeds_many :addresses\n    embeds_many :scores    \nend\n\nclass Address\n  include Mongoid::Document\n\n    field :address\n    field :city\n    field :state\n    field :postalCode\n    embedded_in :student\nend\n\nclass Score\n  include Mongoid::Document\n\n    belongs_to :course\n    field :grade, type: Float\n    embedded_in :student\nend\n\n\nclass Course\n  include Mongoid::Document\n\n  field :name\n  has_many :scores  \nend\n</code></pre>\n\n<p>Edit:</p>\n\n<pre><code>&gt; db.foo.insert({group:\"phones\"})\n&gt; db.foo.find()                  \n{ \"_id\" : ObjectId(\"4df6539ae90592692ccc9940\"), \"group\" : \"phones\" }\n{ \"_id\" : ObjectId(\"4df6540fe90592692ccc9941\"), \"group\" : \"phones\" }\n&gt;db.foo.find({'_id':ObjectId(\"4df6539ae90592692ccc9940\")}) \n{ \"_id\" : ObjectId(\"4df6539ae90592692ccc9940\"), \"group\" : \"phones\" }\n</code></pre>\n\n<p>You can use that ObjectId in order to do relations between documents.</p>\n    "},{"t":"anybody tried neo4j vs titan - pros and cons","l":"http://stackoverflow.com/questions/17269306/anybody-tried-neo4j-vs-titan-pros-and-cons","q":"\n\n<p>Can anybody please provide or point out to a good comparison between Neo4j and Titan? \nOne thing i can see is in terms of scale - Titan is scaleout and requires an underlying scalable datastore like cassandra. Neo4j is only for HA and has its own embedded database. Any other pros and cons? Any specific usecases. (Is Titan being used anywhere currently?)</p>\n\n<p>I also have the following link: <a href=\"http://architects.dzone.com/articles/16-graph-databases-compared\">http://architects.dzone.com/articles/16-graph-databases-compared</a> that gives a objective compare for graph databases but not much on pros and cons between Neo4j and Titan.</p>\n    ","a":"\n<p>You may have a look @ <a href=\"https://groups.google.com/d/msg/aureliusgraphs/vkQkzjN8fo0/9YYgqI4TA0QJ\"> \nTitan vs Neo4j</a>, it may help you..</p>\n    "},{"t":"Scan with filter using HBase shell","l":"http://stackoverflow.com/questions/7256100/scan-with-filter-using-hbase-shell","q":"\n\n<p>Does anybody know how to scan records based on some scan filter i.e.:</p>\n\n<p><code>column:something = \"somevalue\"</code></p>\n\n<p>Something like <a href=\"http://stackoverflow.com/questions/2431387/how-to-read-data-from-hbase\">this</a>, but from HBase shell?</p>\n    ","a":"\n<p>Try this. It's kind of ugly, but it works for me.</p>\n\n<pre><code>import org.apache.hadoop.hbase.filter.CompareFilter\nimport org.apache.hadoop.hbase.filter.SingleColumnValueFilter\nimport org.apache.hadoop.hbase.filter.SubstringComparator\nimport org.apache.hadoop.hbase.util.Bytes\nscan 't1', { COLUMNS =&gt; 'family:qualifier', FILTER =&gt;\n    SingleColumnValueFilter.new\n        (Bytes.toBytes('family'),\n         Bytes.toBytes('qualifier'),\n         CompareFilter::CompareOp.valueOf('EQUAL'),\n         SubstringComparator.new('somevalue'))\n}\n</code></pre>\n\n<p>The HBase shell will include whatever you have in ~/.irbrc, so you can put something like this in there (I'm no Ruby expert, improvements are welcome):</p>\n\n<pre><code># imports like above\ndef scan_substr(table,family,qualifier,substr,*cols)\n    scan table, { COLUMNS =&gt; cols, FILTER =&gt;\n        SingleColumnValueFilter.new\n            (Bytes.toBytes(family), Bytes.toBytes(qualifier),\n             CompareFilter::CompareOp.valueOf('EQUAL'),\n             SubstringComparator.new(substr)) }\nend\n</code></pre>\n\n<p>and then you can just say in the shell:</p>\n\n<pre><code>scan_substr 't1', 'family', 'qualifier', 'somevalue', 'family:qualifier'\n</code></pre>\n    "},{"t":"PHP-friendly NoSQL solutions [closed]","l":"http://stackoverflow.com/questions/2822021/php-friendly-nosql-solutions","q":"\n\n<p>I'm looking to use a NoSQL solution for my next project, which will be written in PHP. What choices do I have in terms of NoSQL solutions that can easily interfaced via PHP? I haven't done much thinking about the architecture yet, so I'm not sure what my needs will be; I'd simply like to know what my choices are so I don't build something I can't reasonably implement. </p>\n\n<p>For instance, I know Cassandra has Pandra, but that's just a PHP library. MongoDB has a native PECL extension.</p>\n    ","a":"\n<p><a href=\"http://books.couchdb.org/relax/\" rel=\"nofollow\">CouchDB</a> has <a href=\"http://kore-nordmann.de/blog/phpillow_php_couchdb_wrapper.html\" rel=\"nofollow\">PHPillow</a>:</p>\n\n<blockquote>\n  <p>PHPillow is an object orientated wrapper for CouchDB. Releases can be found on the downloads page. Apache CouchDB is a distributed, fault-tolerant and schema-free document-oriented database accessible via a RESTful HTTP/JSON API. Among other features, it provides robust, incremental replication with bi-directional conflict detection and resolution, and is queryable and indexable using a table-oriented view engine with JavaScript acting as the default view definition language.</p>\n</blockquote>\n\n<p>Also see <a href=\"http://nosql-database.org\" rel=\"nofollow\">http://nosql-database.org</a> for a good overview.</p>\n    "},{"t":"What's the attraction of schemaless database systems?","l":"http://stackoverflow.com/questions/3856222/whats-the-attraction-of-schemaless-database-systems","q":"\n\n<p>I've been hearing a lot of talk about schema-less (often distributed) database systems like MongoDB, CouchDB, SimpleDB, etc...</p>\n\n<p>While I can understand they might be valuable for some purposes, in most of my applications I'm trying to persist objects that have a specific number of fields of a specific type, and I just automatically think in the relational model.  I'm always thinking in terms of rows with unique integer ids, null/not null fields, SQL datatypes, and select queries to find sets.</p>\n\n<p>While I'm attracted to the distributed nature and easy JSON/RESTful interfaces of these new systems, I don't understand how loosely typed key/value hashes will help me with my development.  Why would a loose typed, schema-less system be good for keeping clean data sets?  How can I for example, find all items with dates between x and y when they might not have dates?  Is there any concept of a join?</p>\n\n<p>I understand many systems have their own differences and strengths, but I'm wondering at the difference in paradigm.  I suppose this is an open-ended question, but perhaps the community's answers and ways they have personally seen the advantages of these systems will help enlighten me and others about when I would want to make use of these (admittedly more hip) systems instead of the traditional RDBMS.</p>\n    ","a":"\n<p>I'll just call out one or two common reasons  (I'm sure people will be writing essay answers)</p>\n\n<ol>\n<li><p>With highly distributed systems, any given data set may be spread across multiple servers. When that happens, the relational constraints which the DB engine can guarantee are greatly reduced. <em>Some</em> of your referential integrity will need to be handled in application code. When doing so, you will quickly discover several pain points:</p>\n\n<ul>\n<li>your logic is spread across multiple layers (app and db)</li>\n<li>your logic is spread across multiple languages (SQL and your app language of choice)</li>\n</ul>\n\n<p>The outcome is that the logic is less encapsulated, less portable, and MUCH more expensive to change. Many devs find themselves writing more logic in app code and less in the database. Taken to the extreme,  the database schema becomes irrelevant. </p></li>\n<li><p>Schema managementâ€”especially on systems where downtime is not an optionâ€”is difficult. reducing the schema complexity reduces that difficulty.</p></li>\n<li><p>ACID doesn't work very well for distributed systems (<a href=\"http://queue.acm.org/detail.cfm?id=1394128\">BASE</a>, <a href=\"http://en.wikipedia.org/wiki/CAP_theorem\">CAP</a>, etc). The SQL language (and the entire relational model to a certain extent) is optimized for a transactional ACID world. So some of the SQL language features and best practices are useless while others are actually harmful. Some developers feel uncomfortable about  \"against the grain\" and prefer to drop SQL entirely in favor of a language which was designed from the ground up for their requirements.</p></li>\n<li><p>Cost: most RDBMS systems aren't free. The leaders in scaling (Oracle, Sybase, SQL Server) are all commercial products. When dealing with large (\"web scale\") systems,  database licensing costs can meet or exceed the hardware costs! The costs are high enough to change the normal build/buy considerations drastically towards building a custom solution on top of an OSS offering (all the significant NOSQL offerings are OSS)</p></li>\n</ol>\n    "},{"t":"MySQL and NoSQL: Help me to choose the right one","l":"http://stackoverflow.com/questions/4419499/mysql-and-nosql-help-me-to-choose-the-right-one","q":"\n\n<p>There is a big database, 1,000,000,000 rows, called threads (these threads actually exist, I'm not making things harder just because of I enjoy it). Threads has only a few stuff in it, to make things faster: (int id, string hash, int replycount, int dateline (timestamp), int forumid, string title)</p>\n\n<p>Query:</p>\n\n<p><code>select * from thread where forumid = 100 and replycount &gt; 1 order by dateline desc limit 10000, 100</code></p>\n\n<p>Since that there are 1G of records it's quite a slow query. So I thought, let's split this 1G of records in as many tables as many forums(category) I have! That is almost perfect. Having many tables I have less record to search around and it's really faster. The query now becomes:</p>\n\n<p><code>select * from thread_{forum_id} where replycount &gt; 1 order by dateline desc limit 10000, 100</code></p>\n\n<p>This is really faster with 99% of the forums (category) since that most of those have only a few of topics (100k-1M). However because there are some with about 10M of records, some query are still to slow (0.1/.2 seconds, to much for my app!, <strong><em>I'm already using indexes!</em></strong>).</p>\n\n<p>I don't know how to improve this using MySQL. Is there a way?</p>\n\n<p>For this project I will use 10 Servers (12GB ram, 4x7200rpm hard disk on software raid 10, quad core)</p>\n\n<p>The idea was to simply split the databases among the servers, but with the problem explained above that is still not enought.</p>\n\n<p>If I install cassandra on these 10 servers (by supposing I find the time to make it works as it is supposed to) should I be suppose to have a performance boost?</p>\n\n<p><em><strong>What should I do? Keep working with MySQL with distributed database on multiple machines or build a cassandra cluster?</strong></em></p>\n\n<p>I was asked to post what are the indexes, here they are:</p>\n\n<pre><code>mysql&gt; show index in thread;\nPRIMARY id\nforumid\ndateline\nreplycount\n</code></pre>\n\n<p>Select explain:</p>\n\n<pre><code>mysql&gt; explain SELECT * FROM thread WHERE forumid = 655 AND visible = 1 AND open &lt;&gt; 10 ORDER BY dateline ASC LIMIT 268000, 250;\n+----+-------------+--------+------+---------------+---------+---------+-------------+--------+-----------------------------+\n| id | select_type | table  | type | possible_keys | key     | key_len | ref         | rows   | Extra                       |\n+----+-------------+--------+------+---------------+---------+---------+-------------+--------+-----------------------------+\n|  1 | SIMPLE      | thread | ref  | forumid       | forumid | 4       | const,const | 221575 | Using where; Using filesort | \n+----+-------------+--------+------+---------------+---------+---------+-------------+--------+-----------------------------+\n</code></pre>\n    ","a":"\n<p>You should read the following and learn a little bit about the advantages of a well designed innodb table and how best to use clustered indexes - only available with innodb !</p>\n\n<p><a href=\"http://dev.mysql.com/doc/refman/5.0/en/innodb-index-types.html\">http://dev.mysql.com/doc/refman/5.0/en/innodb-index-types.html</a></p>\n\n<p><a href=\"http://www.xaprb.com/blog/2006/07/04/how-to-exploit-mysql-index-optimizations/\">http://www.xaprb.com/blog/2006/07/04/how-to-exploit-mysql-index-optimizations/</a></p>\n\n<p>then design your system something along the lines of the following simplified example:</p>\n\n<h2>Example schema (simplified)</h2>\n\n<p>The important features are that the tables use the innodb engine and the primary key for the threads table is no longer a single auto_incrementing key but a composite <strong>clustered</strong> key based on a combination of forum_id and thread_id. e.g.</p>\n\n<pre><code>threads - primary key (forum_id, thread_id)\n\nforum_id    thread_id\n========    =========\n1                   1\n1                   2\n1                   3\n1                 ...\n1             2058300  \n2                   1\n2                   2\n2                   3\n2                  ...\n2              2352141\n...\n</code></pre>\n\n<p>Each forum row includes a counter called next_thread_id (unsigned int) which is maintained by a trigger and increments every time a thread is added to a given forum. This also means we can store 4 billion threads per forum rather than 4 billion threads in total if using a single auto_increment primary key for thread_id.</p>\n\n<pre><code>forum_id    title   next_thread_id\n========    =====   ==============\n1          forum 1        2058300\n2          forum 2        2352141\n3          forum 3        2482805\n4          forum 4        3740957\n...\n64        forum 64       3243097\n65        forum 65      15000000 -- ooh a big one\n66        forum 66       5038900\n67        forum 67       4449764\n...\n247      forum 247            0 -- still loading data for half the forums !\n248      forum 248            0\n249      forum 249            0\n250      forum 250            0\n</code></pre>\n\n<p>The disadvantage of using a composite key is that you can no longer just select a thread by a single key value as follows:</p>\n\n<pre><code>select * from threads where thread_id = y;\n</code></pre>\n\n<p>you have to do:</p>\n\n<pre><code>select * from threads where forum_id = x and thread_id = y;\n</code></pre>\n\n<p>However, your application code should be aware of which forum a user is browsing so it's not exactly difficult to implement - store the currently viewed forum_id in a session variable or hidden form field etc...</p>\n\n<p>Here's the simplified schema:</p>\n\n<pre><code>drop table if exists forums;\ncreate table forums\n(\nforum_id smallint unsigned not null auto_increment primary key,\ntitle varchar(255) unique not null,\nnext_thread_id int unsigned not null default 0 -- count of threads in each forum\n)engine=innodb;\n\n\ndrop table if exists threads;\ncreate table threads\n(\nforum_id smallint unsigned not null,\nthread_id int unsigned not null default 0,\nreply_count int unsigned not null default 0,\nhash char(32) not null,\ncreated_date datetime not null,\nprimary key (forum_id, thread_id, reply_count) -- composite clustered index\n)engine=innodb;\n\ndelimiter #\n\ncreate trigger threads_before_ins_trig before insert on threads\nfor each row\nbegin\ndeclare v_id int unsigned default 0;\n\n  select next_thread_id + 1 into v_id from forums where forum_id = new.forum_id;\n  set new.thread_id = v_id;\n  update forums set next_thread_id = v_id where forum_id = new.forum_id;\nend#\n\ndelimiter ;\n</code></pre>\n\n<p>You may have noticed I've included reply_count as part of the primary key which is a bit strange as (forum_id, thread_id) composite is unique in itself. This is just an index optimisation which saves some I/O when queries that use reply_count are executed. Please refer to the 2 links above for further info on this.</p>\n\n<h2>Example queries</h2>\n\n<p>I'm still loading data into my example tables and so far I have a loaded approx. 500 million rows (half as many as your system). When the load process is complete I should expect to have approx:</p>\n\n<pre><code>250 forums * 5 million threads = 1250 000 000 (1.2 billion rows)\n</code></pre>\n\n<p>I've deliberately made some of the forums contain more than 5 million threads for example, forum 65 has 15 million threads:</p>\n\n<pre><code>forum_id    title   next_thread_id\n========    =====   ==============\n65        forum 65      15000000 -- ooh a big one\n</code></pre>\n\n<h2>Query runtimes</h2>\n\n<pre><code>select sum(next_thread_id) from forums;\n\nsum(next_thread_id)\n===================\n539,155,433 (500 million threads so far and still growing...)\n</code></pre>\n\n<p>under innodb summing the next_thread_ids to give a total thread count is much faster than the usual:</p>\n\n<pre><code>select count(*) from threads;\n</code></pre>\n\n<p>How many threads does forum 65 have:</p>\n\n<pre><code>select next_thread_id from forums where forum_id = 65\n\nnext_thread_id\n==============\n15,000,000 (15 million)\n</code></pre>\n\n<p>again this is faster than the usual:</p>\n\n<pre><code>select count(*) from threads where forum_id = 65\n</code></pre>\n\n<p>Ok now we know we have about 500 million threads so far and forum 65 has 15 million threads - let's see how the schema performs :)</p>\n\n<pre><code>select forum_id, thread_id from threads where forum_id = 65 and reply_count &gt; 64 order by thread_id desc limit 32;\n\nruntime = 0.022 secs\n\nselect forum_id, thread_id from threads where forum_id = 65 and reply_count &gt; 1 order by thread_id desc limit 10000, 100;\n\nruntime = 0.027 secs\n</code></pre>\n\n<p>Looks pretty performant to me - so that's a single table with 500+ million rows (and growing) with a query that covers 15 million rows in 0.02 seconds (while under load !)</p>\n\n<h2>Further optimisations</h2>\n\n<p>These would include:</p>\n\n<ul>\n<li><p>partitioning by range </p></li>\n<li><p>sharding</p></li>\n<li><p>throwing money and hardware at it</p></li>\n</ul>\n\n<p>etc...</p>\n\n<p>hope you find this answer helpful :)</p>\n    "},{"t":"MongoDB vs. Redis vs. Cassandra for a fast-write, temporary row storage solution","l":"http://stackoverflow.com/questions/3010224/mongodb-vs-redis-vs-cassandra-for-a-fast-write-temporary-row-storage-solution","q":"\n\n<p>I'm building a system that tracks and verifies ad impressions and clicks. This means that there are a lot of insert commands (about 90/second average, peaking at 250) and some read operations, but the focus is on performance and making it blazing-fast.</p>\n\n<p>The system is currently on MongoDB, but I've been introduced to Cassandra and Redis since then. Would it be a good idea to go to one of these two solutions, rather than stay on MongoDB? Why or why not?</p>\n\n<p>Thank you</p>\n    ","a":"\n<p>I currently work for a <em>very</em> large ad network and we write to flat files :)</p>\n\n<p>I'm personally a Mongo fan, but frankly, Redis and Cassandra are unlikely to perform either better or worse. I mean, all you're doing is throwing stuff into memory and then flushing to disk in the background (both Mongo and Redis do this).</p>\n\n<p>If you're looking for blazing fast speed, the other option is to keep several impressions in local memory and then flush them disk every minute or so. Of course, this is basically what Mongo and Redis do for you. Not a real compelling reason to move.</p>\n    "},{"t":"How to load 100 million records into MongoDB with Scala for performance testing?","l":"http://stackoverflow.com/questions/6783212/how-to-load-100-million-records-into-mongodb-with-scala-for-performance-testing","q":"\n\n<p>I have a small script that's written in Scala which is intended to load a MongoDB instance up with 100,000,000 sample records. The idea is to get the DB all loaded, and then do some performance testing (and tune/re-load if necessary).</p>\n\n<p>The problem is that the load-time per 100,000 records increases pretty linearly. At the beginning of my load process it took only 4 seconds to load those records. Now, at nearly 6,000,000 records, it's taking between 300 and 400 seconds to load the same amount (100,000)! That's two orders of magnitude slower! Queries are still snappy, but at this rate, I'll never be able to load the amount of data that I'd like.</p>\n\n<p>Will this work faster if I write a file out with all of my records (all 100,000,000!), and then use <a href=\"http://eliothorowitz.com/post/459890033/streaming-twitter-into-mongodb\">mongoimport</a> to import the whole thing? Or are my expectations too high and I'm using the DB beyond what it's supposed to handle?</p>\n\n<p>Any thoughts? Thanks!</p>\n\n<p>Here's my script:</p>\n\n<pre><code>import java.util.Date\n\nimport com.mongodb.casbah.Imports._\nimport com.mongodb.casbah.commons.MongoDBObject\n\nobject MongoPopulateTest {\n  val ONE_HUNDRED_THOUSAND = 100000\n  val ONE_MILLION          = ONE_HUNDRED_THOUSAND * 10\n\n  val random     = new scala.util.Random(12345)\n  val connection = MongoConnection()\n  val db         = connection(\"mongoVolumeTest\")\n  val collection = db(\"testData\")\n\n  val INDEX_KEYS = List(\"A\", \"G\", \"E\", \"F\")\n\n  def main(args: Array[String]) {\n    populateCoacs(ONE_MILLION * 100)\n  }\n\n  def populateCoacs(count: Int) {\n    println(\"Creating indexes: \" + INDEX_KEYS.mkString(\", \"))\n    INDEX_KEYS.map(key =&gt; collection.ensureIndex(MongoDBObject(key -&gt; 1)))\n\n    println(\"Adding \" + count + \" records to DB.\")\n\n    val start     = (new Date()).getTime()\n    var lastBatch = start\n\n    for(i &lt;- 0 until count) {\n      collection.save(makeCoac())\n      if(i % 100000 == 0 &amp;&amp; i != 0) {\n        println(i + \": \" + (((new Date()).getTime() - lastBatch) / 1000.0) + \" seconds (\" +  (new Date()).toString() + \")\")\n        lastBatch = (new Date()).getTime()\n      }\n    }\n\n    val elapsedSeconds = ((new Date).getTime() - start) / 1000\n\n    println(\"Done. \" + count + \" COAC rows inserted in \" + elapsedSeconds + \" seconds.\")\n  }\n\n  def makeCoac(): MongoDBObject = {\n    MongoDBObject(\n      \"A\" -&gt; random.nextPrintableChar().toString(),\n      \"B\" -&gt; scala.math.abs(random.nextInt()),\n      \"C\" -&gt; makeRandomPrintableString(50),\n      \"D\" -&gt; (if(random.nextBoolean()) { \"Cd\" } else { \"Cc\" }),\n      \"E\" -&gt; makeRandomPrintableString(15),\n      \"F\" -&gt; makeRandomPrintableString(15),\n      \"G\" -&gt; scala.math.abs(random.nextInt()),\n      \"H\" -&gt; random.nextBoolean(),\n      \"I\" -&gt; (if(random.nextBoolean()) { 41 } else { 31 }),\n      \"J\" -&gt; (if(random.nextBoolean()) { \"A\" } else { \"B\" }),\n      \"K\" -&gt; random.nextFloat(),\n      \"L\" -&gt; makeRandomPrintableString(15),\n      \"M\" -&gt; makeRandomPrintableString(15),\n      \"N\" -&gt; scala.math.abs(random.nextInt()),\n      \"O\" -&gt; random.nextFloat(),\n      \"P\" -&gt; (if(random.nextBoolean()) { \"USD\" } else { \"GBP\" }),\n      \"Q\" -&gt; (if(random.nextBoolean()) { \"PROCESSED\" } else { \"UNPROCESSED\" }),\n      \"R\" -&gt; scala.math.abs(random.nextInt())\n    )\n  }\n\n  def makeRandomPrintableString(length: Int): String = {\n    var result = \"\"\n    for(i &lt;- 0 until length) {\n      result += random.nextPrintableChar().toString()\n    }\n    result\n  }\n}\n</code></pre>\n\n<p>Here's the output from my script:</p>\n\n<pre><code>Creating indexes: A, G, E, F\nAdding 100000000 records to DB.\n100000: 4.456 seconds (Thu Jul 21 15:18:57 EDT 2011)\n200000: 4.155 seconds (Thu Jul 21 15:19:01 EDT 2011)\n300000: 4.284 seconds (Thu Jul 21 15:19:05 EDT 2011)\n400000: 4.32 seconds (Thu Jul 21 15:19:10 EDT 2011)\n500000: 4.597 seconds (Thu Jul 21 15:19:14 EDT 2011)\n600000: 4.412 seconds (Thu Jul 21 15:19:19 EDT 2011)\n700000: 4.435 seconds (Thu Jul 21 15:19:23 EDT 2011)\n800000: 5.919 seconds (Thu Jul 21 15:19:29 EDT 2011)\n900000: 4.517 seconds (Thu Jul 21 15:19:33 EDT 2011)\n1000000: 4.483 seconds (Thu Jul 21 15:19:38 EDT 2011)\n1100000: 4.78 seconds (Thu Jul 21 15:19:43 EDT 2011)\n1200000: 9.643 seconds (Thu Jul 21 15:19:52 EDT 2011)\n1300000: 25.479 seconds (Thu Jul 21 15:20:18 EDT 2011)\n1400000: 30.028 seconds (Thu Jul 21 15:20:48 EDT 2011)\n1500000: 24.531 seconds (Thu Jul 21 15:21:12 EDT 2011)\n1600000: 18.562 seconds (Thu Jul 21 15:21:31 EDT 2011)\n1700000: 28.48 seconds (Thu Jul 21 15:21:59 EDT 2011)\n1800000: 29.127 seconds (Thu Jul 21 15:22:29 EDT 2011)\n1900000: 25.814 seconds (Thu Jul 21 15:22:54 EDT 2011)\n2000000: 16.658 seconds (Thu Jul 21 15:23:11 EDT 2011)\n2100000: 24.564 seconds (Thu Jul 21 15:23:36 EDT 2011)\n2200000: 32.542 seconds (Thu Jul 21 15:24:08 EDT 2011)\n2300000: 30.378 seconds (Thu Jul 21 15:24:39 EDT 2011)\n2400000: 21.188 seconds (Thu Jul 21 15:25:00 EDT 2011)\n2500000: 23.923 seconds (Thu Jul 21 15:25:24 EDT 2011)\n2600000: 46.077 seconds (Thu Jul 21 15:26:10 EDT 2011)\n2700000: 104.434 seconds (Thu Jul 21 15:27:54 EDT 2011)\n2800000: 23.344 seconds (Thu Jul 21 15:28:17 EDT 2011)\n2900000: 17.206 seconds (Thu Jul 21 15:28:35 EDT 2011)\n3000000: 19.15 seconds (Thu Jul 21 15:28:54 EDT 2011)\n3100000: 14.488 seconds (Thu Jul 21 15:29:08 EDT 2011)\n3200000: 20.916 seconds (Thu Jul 21 15:29:29 EDT 2011)\n3300000: 69.93 seconds (Thu Jul 21 15:30:39 EDT 2011)\n3400000: 81.178 seconds (Thu Jul 21 15:32:00 EDT 2011)\n3500000: 93.058 seconds (Thu Jul 21 15:33:33 EDT 2011)\n3600000: 168.613 seconds (Thu Jul 21 15:36:22 EDT 2011)\n3700000: 189.917 seconds (Thu Jul 21 15:39:32 EDT 2011)\n3800000: 200.971 seconds (Thu Jul 21 15:42:53 EDT 2011)\n3900000: 207.728 seconds (Thu Jul 21 15:46:21 EDT 2011)\n4000000: 213.778 seconds (Thu Jul 21 15:49:54 EDT 2011)\n4100000: 219.32 seconds (Thu Jul 21 15:53:34 EDT 2011)\n4200000: 241.545 seconds (Thu Jul 21 15:57:35 EDT 2011)\n4300000: 193.555 seconds (Thu Jul 21 16:00:49 EDT 2011)\n4400000: 190.949 seconds (Thu Jul 21 16:04:00 EDT 2011)\n4500000: 184.433 seconds (Thu Jul 21 16:07:04 EDT 2011)\n4600000: 231.709 seconds (Thu Jul 21 16:10:56 EDT 2011)\n4700000: 243.0 seconds (Thu Jul 21 16:14:59 EDT 2011)\n4800000: 310.156 seconds (Thu Jul 21 16:20:09 EDT 2011)\n4900000: 318.421 seconds (Thu Jul 21 16:25:28 EDT 2011)\n5000000: 378.112 seconds (Thu Jul 21 16:31:46 EDT 2011)\n5100000: 265.648 seconds (Thu Jul 21 16:36:11 EDT 2011)\n5200000: 295.086 seconds (Thu Jul 21 16:41:06 EDT 2011)\n5300000: 297.678 seconds (Thu Jul 21 16:46:04 EDT 2011)\n5400000: 329.256 seconds (Thu Jul 21 16:51:33 EDT 2011)\n5500000: 336.571 seconds (Thu Jul 21 16:57:10 EDT 2011)\n5600000: 398.64 seconds (Thu Jul 21 17:03:49 EDT 2011)\n5700000: 351.158 seconds (Thu Jul 21 17:09:40 EDT 2011)\n5800000: 410.561 seconds (Thu Jul 21 17:16:30 EDT 2011)\n5900000: 689.369 seconds (Thu Jul 21 17:28:00 EDT 2011)\n</code></pre>\n    ","a":"\n<p>Some tips :</p>\n\n<ol>\n<li><p><strong>Do not index your collection before inserting</strong>, as inserts modify the index which is an overhead. Insert everything, then create index .</p></li>\n<li><p><strong>instead of \"save\" , use mongoDB \"batchinsert\"</strong> which can insert many records in 1 operation. So have around 5000 documents inserted per batch.\nYou will see remarkable performance gain .</p>\n\n<p>see the method#2 of insert <a href=\"http://api.mongodb.org/java/2.0/com/mongodb/DBCollection.html#insert%28com.mongodb.DBObject%5B%5D%29\">here</a>, it takes array of documents to insert instead of single document.\nAlso see the discussion in <a href=\"https://groups.google.com/forum/?fromgroups#!topic/mongodb-user/1Np12qqZ3-Y\">this thread</a></p>\n\n<p>And if you want to benchmark more -</p></li>\n<li><p>This is just a guess, <strong>try using a capped collection of a predefined large size</strong> to store all your data. Capped collection without index has very good insertion performance.</p></li>\n</ol>\n    "},{"t":"Is MongoDB reliable? [closed]","l":"http://stackoverflow.com/questions/3505141/is-mongodb-reliable","q":"\n\n<p>I am developing a dating website and I am thinking of using a NoSQL database to store the profiles etc. I am currenly looking into the MongoDB and so far I am very pleased. The only worry is that I read on different websites that MongoDB is unreliable and not good.</p>\n\n<p>I looked into the NoSQL alternatives and found no one that fully meets my specific criterias:</p>\n\n<ol>\n<li><p>Easy to learn and use.</p></li>\n<li><p>Fully compatible with PHP out of the box.</p></li>\n<li><p>Fast and well documented.</p></li>\n</ol>\n\n<p>What do you think, am I doing the right thing to go with MongoDB or is it a waste of time?</p>\n\n<p>Thankful for all input in the matter!</p>\n    ","a":"\n<p>I researched MongoDB for my social service startup and it is definitely worth considering. MongoDB has a powerful feature set that makes it a realistic and strong alternative to RDBMS solutions. </p>\n\n<p>Amongst them:</p>\n\n<ol>\n<li><strong>Document Database:</strong> Most of your data is embedded in a document, so in order to get the data about a person, you don't have to join several tables. Thus, better performance for many use cases.</li>\n<li><strong>Strong Query Language:</strong> Despite not being a RDBMS, MongoDB has a very strong query language that allows you to get something very specific or very general from a document or documents. The DB is queried using javascript so you can do many more things beside querying (e.g. functions, calculations).</li>\n<li><strong>Sharding &amp; Replication:</strong> Sharding allows you application to scale horizontally rather than vertically. In other words, more small servers instead of one huge server. And replication gives you fail-over safety in several configurations (e.g. master/slave). </li>\n<li><strong>Powerful Indexing:</strong> I originally got interested in MongoDB because it allows <em>geo-spatial</em> indexing out of the box but it has many other indexing configurations as well. </li>\n<li><strong>Cross-Platform:</strong> MongoDB has many <a href=\"http://www.mongodb.org/display/DOCS/Drivers\" rel=\"nofollow\">drivers</a>. </li>\n</ol>\n\n<p>As for the documentation, there is not deluge but that is because this project only started in 2009; soon there will be plenty more. However there is enough to get started with your project. In addition to that you can check out Kyle Banker's <a href=\"http://www.manning.com/banker/\" rel=\"nofollow\">MongoDB in Action</a>, great resource.</p>\n\n<p>Lastly, I had experience only with RDMBS prior to MongoDB, didn't know javascript or json and still found it to be very simple and elegant.</p>\n    "},{"t":"Sorted String Table (SSTable) or B+ Tree for a Database Index?","l":"http://stackoverflow.com/questions/8651346/sorted-string-table-sstable-or-b-tree-for-a-database-index","q":"\n\n<p>Using two databases to illustrate this example: <a href=\"http://couchdb.apache.org/\">CouchDB</a> and <a href=\"http://cassandra.apache.org/\">Cassandra</a>.</p>\n\n<h2><strong>CouchDB</strong></h2>\n\n<p>CouchDB uses a B+ Tree for document indexes (using <a href=\"https://plus.google.com/u/0/107397941677313236670/posts/CyvwRcvh4vv\">a clever modification</a> to work in their append-only environment) - more specifically as documents are modified (insert/update/delete) they are appended to the running database file as well as a full Leaf -&gt; Node path from the B+ tree of all the nodes effected by the updated revision right after the document.</p>\n\n<p>These piece-mealed index revisions are inlined right alongside the modifications such that the full index is a union of the most recent index modifications appended at the end of the file along with additional pieces further back in the data file that are still relevant and haven't been modified yet.</p>\n\n<blockquote>\n  <p>Searching the <a href=\"http://en.wikipedia.org/wiki/B+_tree\">B+ tree </a> is O(logn).</p>\n</blockquote>\n\n<h2><strong>Cassandra</strong></h2>\n\n<p>Cassandra keeps record keys sorted, in-memory, in tables (let's think of them as arrays for this question) and writes them out as separate (sorted) <a href=\"http://wiki.apache.org/cassandra/ArchitectureSSTable\">sorted-string tables</a> from time to time.</p>\n\n<p>We can think of the collection of all of these tables as the \"index\" (from what I understand).</p>\n\n<p>Cassandra is required to <a href=\"http://www.datastax.com/dev/blog/leveled-compaction-in-apache-cassandra\">compact/combine these sorted-string tables</a> from time to time, creating a more complete file representation of the index.</p>\n\n<blockquote>\n  <p>Searching <a href=\"http://en.wikipedia.org/wiki/Sorted_array\">a sorted array</a> is O(logn).</p>\n</blockquote>\n\n<h2><strong>Question</strong></h2>\n\n<p>Assuming a similar level of complexity between either maintaining partial B+ tree chunks in CouchDB versus partial sorted-string indices in Cassandra and given that both provide O(logn) search time which one do you think would make a better representation of a database index and why?</p>\n\n<p>I am specifically curious if there is an implementation detail about one over the other that makes it <em>particularly</em> attractive or if they are both a wash and you just pick whichever data structure you prefer to work with/makes more sense to the developer.</p>\n\n<p>Thank you for the thoughts.</p>\n    ","a":"\n<p>When comparing a BTree index to an SSTable index, you should consider the write complexity:  </p>\n\n<ul>\n<li><p>When writing randomly to a copy-on-write BTree, you will incur random reads (to do the copy of the leaf node and path).  So while the writes my be sequential on disk, for datasets larger than RAM, these random reads will quickly become the bottle neck.  For a SSTable-like index, no such read occurs on write - there will only be the sequential writes.</p></li>\n<li><p>You should also consider that in the worse case, every update to a BTree could incur log_b N IOs - that is, you could end up writing 3 or 4 blocks for every key.  If key size is much less than block size, this is extremely expensive.  For an SSTable-like index, each write IO will contain as many fresh keys as it can, so the IO cost for each key is more like 1/B.</p></li>\n</ul>\n\n<p>In practice, this make SSTable-like thousands of times faster (for random writes) than BTrees.</p>\n\n<p>When considering implementation details, we have found it a lot easier to implement SSTable-like indexes (almost) lock-free, where as locking strategies for BTrees has become quite complicated.</p>\n\n<p>You should also re-consider you read costs.  You are correct than a BTree is O(log_b N) random IOs for random point reads, but a SSTable-like index is actually O(#sstables . log_b N). Without an decent merge scheme, #sstables is proportional to N. There are various tricks to get round this (using Bloom Filters, for instance), but these don't help with small, random range queries.   This is what we found with Cassandra:</p>\n\n<p><a href=\"http://www.acunu.com/blogs/richard-low/cassandra-under-heavy-write-load-part-ii/\">http://www.acunu.com/blogs/richard-low/cassandra-under-heavy-write-load-part-ii/</a></p>\n\n<p>This is why Castle, our (GPL) storage engine, does merges slightly differently, and can achieve a lot better (O(log^2 N)) range queries performance with a slight trade off in write performance (O(log^2 N / B)).  In practice we find it to be quicker than Cassandra's SSTable index for writes as well.</p>\n\n<p>If you want to know more about this, I've given a talk about how it works:</p>\n\n<ul>\n<li><a href=\"http://skillsmatter.com/podcast/nosql/castle-big-data\">http://skillsmatter.com/podcast/nosql/castle-big-data</a></li>\n<li><a href=\"http://www.slideshare.net/acunu/in-the-brain-of-tom-wilkie\">http://www.slideshare.net/acunu/in-the-brain-of-tom-wilkie</a> </li>\n</ul>\n    "},{"t":"SQL versus noSQL (speed)","l":"http://stackoverflow.com/questions/13397979/sql-versus-nosql-speed","q":"\n\n<p>When people are comparing SQL and noSQL, and concluding the upsides and downsides of each one, what I never hear anyone talking about is the speed. </p>\n\n<p>Isn't performing SQL queries generally <strong>faster</strong> than performing noSQL queries? </p>\n\n<p>I mean, for me this would be a really obvious conclusion, because you should always be able to find something faster if you <strong>know</strong> the structure of your database than if you don't.</p>\n\n<p>But people never seem to mention this, so I want to know if my conclusion is right or wrong.</p>\n    ","a":"\n<p>The definition of noSQL systems is a very broad one -- a database that doesn't use SQL / is not a RDBMS.\nTherefore, the answer to your question is, in short: \"it depends\".</p>\n\n<p>Some noSQL systems are basically just persistent key/value storages (like <a href=\"http://www.project-voldemort.com/\">Project Voldemort</a>). If your queries are of the type \"look up the value for a given key\", such a system will (or at least <em>should be</em>) faster that an RDBMS, because it only needs to have a much smaller feature set.</p>\n\n<p>Another popular type of noSQL system is the document database (like <a href=\"http://couchdb.apache.org/\">CouchDB</a>).\nThese databases have no predefined data structure.\nTheir speed advantage relies heavily on denormalization and creating a data layout that is taylored to the queries that you will run on it. For example, for a blog, you could save a blog post in a document together with its comments. This reduces the need for joins and lookups, making your queries faster, but it also reduces your flexibility regarding queries.</p>\n    "},{"t":"Has anyone worked with Aerospike? How does it compare to MongoDB?","l":"http://stackoverflow.com/questions/25208914/has-anyone-worked-with-aerospike-how-does-it-compare-to-mongodb","q":"\n\n<p>Can anyone say if Aerospike is as good as they claim it to be? I'm a bit skeptical since it's a commercial enterprise. As far as I understand they just released a open source version, but the claims on their website could still be exaggerated.</p>\n\n<p>I'm especially interested on how Aerospike compares to MongoDB.</p>\n    ","a":"\n<p>I have used Aerospike, MongoDB, Redis, and tested many other NoSQL databases, and I would say\nAerospike is very good at what it does, but it is different than MongoDB.  Everything depends on what you are planning on using a database for.  I can give you an example of what I am using my different databases for, and go over the differences between them an Aerospike, and the benefits of Aerospike.</p>\n\n<p><strong>MongoDB</strong></p>\n\n<p>I am using MongoDB as a SQL alternative.  In my MongoDB database I have many different fields, often times, the fields are changing, and I will randomly need to query on various fields, it is a very unstructured database, and MongoDB is amazing at that.  I have also used MongoDB as a standard key-value store as well, it preforms well, but I have had MongoDB preform sub optimally at scale(both transaction scale and database size scale).  Admittedly, the database may have been able to be optimized a little better, however I find it very hard to find documentation on configuring MongoDB correctly in different situations.</p>\n\n<p><strong>Redis</strong></p>\n\n<p>Redis is a pure Key-Value store.  Redis' biggest problem is that it is purely in memory(on disk backups but you cannot store more information the you have memory available).  It is extremely fast for what it is used for.  I personally use it for a small transactional database: I do very simple functions on keys, like counting how many times an event happened for a certain user.  I also do quick in-memory look ups that I need mapped to different values.  Redis is a great tool for a small dataset, and it is extremely fast.  Configuration is very easy as well.</p>\n\n<p><strong>Aerospike</strong></p>\n\n<p>I personally use Aerospike as a solution to scaling a Redis database, although from my understanding, it can be used for more.  Like Redis, Aerospike is a Key-Value store.  I believe the open source edition also supports secondary indexes, which Redis does not (I have not used secondary indexes in production, but I have done a little testing on them).  </p>\n\n<p>Aerospike's best feature is its ability to scale.   The biggest problem I needed to solve when looking into Aerospike was how can I scale my system for for large data sets, and still be extremely fast.  The project I use Aerospike for has very stringent requirements on speed.  I usually make 3-4 database lookups, plus other processing and need to have sub-50ms transaction times.  A few of those look-ups are on data sets which are 300GB+.  I could not find a solution to hold this data and make it accessible in a reasonable amount of time.  Redis obviously won't work unless I had a machine which had 300GB+ of RAM.  MongoDB started to perform extremely poorly at a size much lower than 300GB.  So I gave Aerospike a shot, and it was able to handle everything very well.  The best thing about Aerospike has been as my data set has grown, I have not had to done much more than standing up a new box when needed, the speed has stayed consistent.</p>\n\n<p>I also find Aerospikes documentation very good, it isn't too hard to configure, and it's pretty easy to find answers for any issue that have come up.</p>\n\n<p><strong>Conclusion</strong></p>\n\n<p>So back to you question of if Aerospike is as good as they claim.  Personally, I have not seen anything less than what they have claimed in my uses.  I haven't had to scale to 1 million TPS.  But I do believe with enough hardware that would be possible.  I also believe the numbers of the speed difference between MongoDB.  Aerospike is a much more \"configured\" and \"planed out\" database than MongoDB.  Because of this, at scale Aerospike will be much faster than MongoDB.  It only has to worry about a single(or in case of secondary indices, a few hundred) indexes, rather than MongoDB, which can change dynamically.  The question you really need to be asking is what goal am I trying to accomplish with my database, and then look into what database will fit your needs best.  If you need a scalable, fast key-value store database, I would say Aerospike is probably the best out there.</p>\n\n<p>Let me know if you have any specific questions or need anything clarified, I would probably be able to help you out.</p>\n    "},{"t":"Querying internal array size in MongoDB","l":"http://stackoverflow.com/questions/6722850/querying-internal-array-size-in-mongodb","q":"\n\n<p>Consider a MongoDB document in <code>users</code> collection: </p>\n\n<pre><code>{ username : 'Alex', tags: ['C#', 'Java', 'C++'] }\n</code></pre>\n\n<p>Is there any way, to get the length of the <code>tags</code> array from the server side (without passing the tags to the client) ? </p>\n\n<p>Thank you! </p>\n    ","a":"\n<p>Now MongoDB (2.6 release) <a href=\"http://docs.mongodb.org/manual/reference/operator/aggregation/size/#exp._S_size\" rel=\"nofollow\">supports</a> <code>$size</code> operation in aggregation. </p>\n\n<p>From the documentation:</p>\n\n<pre><code>{ &lt;field&gt;: { $size: &lt;array&gt; } }\n</code></pre>\n\n<p>What you want can be accomplished as following with either by using this:</p>\n\n<pre><code>db.users.aggregate(\n   [\n      {\n         $group: {\n            _id: \"$username\",\n            tags_count:  {$first: {$size: \"$tags\" }}\n         }\n      }\n   ]\n)\n</code></pre>\n\n<p>or</p>\n\n<pre><code>db.users.aggregate(\n   [\n      {\n         $project: {\n            tags_count: {$size: \"$tags\"}\n         }\n      }\n   ]\n)\n</code></pre>\n    "},{"t":"Where does mongodb stand in the CAP theorem?","l":"http://stackoverflow.com/questions/11292215/where-does-mongodb-stand-in-the-cap-theorem","q":"\n\n<p>Everywhere I look, I see that MongoDB is CP.\nBut when I dig in I see it is eventually consistent.\nIs it CP when you use safe=true? If so, does that mean that when I write with safe=true, all replicas will be updated before getting the result?</p>\n    ","a":"\n<p>MongoDB is strongly consistent by default - if you do a write and then do a read, assuming the write was successful you will always be able to read the result of the write you just read.  This is because MongoDB is a single-master system and all reads go to the primary by default.  If you optionally enable reading from the secondaries then MongoDB becomes eventually consistent where it's possible to read out-of-date results.</p>\n\n<p>MongoDB also gets high-availability through automatic failover in replica sets: <a href=\"http://www.mongodb.org/display/DOCS/Replica+Sets\">http://www.mongodb.org/display/DOCS/Replica+Sets</a></p>\n    "},{"t":"How does column-oriented NoSQL differ from document-oriented?","l":"http://stackoverflow.com/questions/7565012/how-does-column-oriented-nosql-differ-from-document-oriented","q":"\n\n<p>The three types of NoSQL databases I've read about is key-value, column-oriented, and document-oriented.</p>\n\n<p>Key-value is pretty straight forward - a key with a plain value.</p>\n\n<p>I've seen document-oriented databases described as like key-value, but the value can be a structure, like a JSON object. Each \"document\" can have all, some, or none of the same keys as another.</p>\n\n<p>Column oriented seems to be very much like document oriented in that you don't specify a structure.</p>\n\n<p>So what is the difference between these two, and why would you use one over the other?</p>\n\n<p>I've specifically looked at MongoDB and Cassandra. I basically need a dynamic structure that can change, but not affect other values. At the same time I need to be able to search/filter specific keys and run reports. With CAP, AP is the most important to me. The data can \"eventually\" be synced across nodes, just as long as there is no conflict or loss of data. Each user would get their own \"table\".</p>\n    ","a":"\n<p>In Cassandra, each row (addressed by a key) contains one or more \"columns\". Columns are themselves key-value pairs. The column names need not be predefined, i.e. the structure isn't fixed. Columns in a row are stored in sorted order according to their keys (names). </p>\n\n<p>In some cases, you may have very large numbers of columns in a row (e.g. to act as an index to enable particular kinds of query). Cassandra can handle such large structures efficiently, and you can retrieve specific ranges of columns.</p>\n\n<p>There is a further level of structure (not so commonly used) called super-columns, where a column contains nested (sub)columns.</p>\n\n<p>You can think of the overall structure as a nested hashtable/dictionary, with 2 or 3 levels of key.</p>\n\n<p><strong>Normal column family:</strong></p>\n\n<pre><code>row\n    col  col  col ...\n    val  val  val ...\n</code></pre>\n\n<p><strong>Super column family:</strong></p>\n\n<pre><code>row\n      supercol                      supercol                     ...\n          (sub)col  (sub)col  ...       (sub)col  (sub)col  ...\n           val       val      ...        val       val      ...\n</code></pre>\n\n<p>There are also higher-level structures  - column families and keyspaces - which can be used to divide up or group together your data. </p>\n\n<p>See also this Question: <a href=\"http://stackoverflow.com/questions/6315124/cassandra-what-is-a-subcolumn\">Cassandra: What is a subcolumn</a></p>\n\n<p>And <a href=\"http://www.datastax.com/docs/0.8/data_model/index\">http://www.datastax.com/docs/0.8/data_model/index</a></p>\n\n<p>Or the data modelling links from <a href=\"http://wiki.apache.org/cassandra/ArticlesAndPresentations\">http://wiki.apache.org/cassandra/ArticlesAndPresentations</a></p>\n\n<p>Re: comparison with document-oriented databases - the latter usually insert whole documents (typically JSON), whereas in Cassandra you can address individual columns or supercolumns, and update these individually, i.e. they work at a different level of granularity. Each column has its own separate timestamp/version (used to reconcile updates across the distributed cluster).</p>\n\n<p>The Cassandra column values are just bytes, but can be typed as ASCII, UTF8 text, numbers, dates etc.</p>\n\n<p>Of course, you could use Cassandra as a primitive document store by inserting columns containing JSON - but you wouldn't get all the features of a real document-oriented store.</p>\n    "},{"t":"What .NET-compatible graph database solution(s) have a proven track record?","l":"http://stackoverflow.com/questions/11299179/what-net-compatible-graph-database-solutions-have-a-proven-track-record","q":"\n\n<h2>I am looking for a generic graph database solution that has existing .NET-compatible infrastructure and a proven track record.</h2>\n\n<p>I've found links to several options on Google and SO, but not a lot of information on existing implementations and usages in real-world applications. </p>\n\n<p>I've also considered using a hybrid between a document DB (like <a href=\"http://ravendb.net/\">RavenDB</a> or <a href=\"http://www.MongoDB.org/display/DOCS/CSharp+Language+Center\">MongoDB</a>) and a dedicated Triple Store or RDBMS (like SQL), and augmenting the data store in order to support the functionality I want. However, this is probably quite a bit of work, and my hope is that someone else has done it already.</p>\n\n<h2>What I've looked at:</h2>\n\n<ul>\n<li><p><a href=\"http://research.microsoft.com/en-us/projects/trinity/default.aspx\">Trinity</a> - This one is made by Microsoft and the literature makes it sound great, but I couldn't find a download link, and the Release page says \"The Trinity package is currently for intranet access only.\".</p></li>\n<li><p><a href=\"http://www.db4o.com/about/productinformation/db4o/\">db4o</a> - This one is an Object-Oriented DB with native support for both .NET and Java. It seems to be marketed as a graph DB but I'm not sure if the 'graph' structure/operations are implicit or explicit (or if it offers more than any other document db).</p></li>\n<li><p><a href=\"http://www.tinkerpop.com/\">TinkerPop</a> - This project looks like exactly what I'm looking for, but the github sources seem to be only in Java. This <a href=\"http://www.slideshare.net/ahzf/fosdem-2011-a-common-graph-database-access-layer-for-net-and-mono\">slideshare from graph-database.org</a> discusses .NET versions, but I haven't been able to find them. </p></li>\n<li><p><a href=\"http://www.cloudgraph.com/index.html\">CloudGraph</a> - This sounds great, but appears to not exist.</p></li>\n<li><p><a href=\"https://launchpad.net/giraffedb\">GiraffeDB</a> - \"GiraffeDB is a powerful graph database system for the .NET framework 4.0, capable of representing complex semantics in an efficient and accessible way\" is \"currently undergoing planning\".</p></li>\n<li><p><a href=\"http://www.franz.com/agraph/allegrograph/\">AllegroGraph 4.7</a> - This appears to be pretty mature (supporting SPARQL and Prolog with a number of Client Interfaces), but is closed source. I'm obviously going to be skeptical of a closed-source project that I haven't heard anything about.</p></li>\n</ul>\n\n<p>There are also a few Java projects that look pretty promising (<a href=\"http://www.hypergraphdb.org/index\">HyperGraphDB</a> and <a href=\"http://neo4j.org/\">Neo4j</a>, but I haven't seen any existing .NET integration of either. I'm not completely opposed to using a Java solution and doing that legwork myself, but once again, I'd prefer a proven solution that saves me the most time.</p>\n    ","a":"\n<p>there are several capable .NET client libs for www.neo4j.org, see <a href=\"http://docs.neo4j.org/chunked/snapshot/tutorials-rest.html\" rel=\"nofollow\">http://docs.neo4j.org/chunked/snapshot/tutorials-rest.html</a> that let you access it from inside .NET. You might want to ask more questions on groups.google.com/group/neo4j/</p>\n\n<p>/peter</p>\n    "},{"t":"How can I resize a mongodb capped collection without losing data?","l":"http://stackoverflow.com/questions/4225535/how-can-i-resize-a-mongodb-capped-collection-without-losing-data","q":"\n\n<p>How can I resize a mongodb capped collection without losing data?</p>\n\n<p>Is there a command for that, or could somebody provide a script?</p>\n    ","a":"\n<p>You basically need to create a new capped collection and copy the docs to it. This can be done very easily in the javascript (shell), or your language of choice.</p>\n\n<pre><code>db.createCollection(\"new\", {capped:true, size:1073741824}); /* size in bytes */\ndb.old.find().forEach(function (d) {db.new.insert(d)});\ndb.old.renameCollection(\"bak\", true);\ndb.new.renameCollection(\"old\", true);\n</code></pre>\n\n<p><strong>Note</strong>: Just make sure nobody is inserting/updating the old collection when you switch. If you run that code in a db.eval(\"....\") it will lock the server while it runs.</p>\n\n<p>There is a feature request for increasing the size of an existing collection: <a href=\"http://jira.mongodb.org/browse/SERVER-1864\">http://jira.mongodb.org/browse/SERVER-1864</a></p>\n    "},{"t":"SQL Server vs. NoSQL","l":"http://stackoverflow.com/questions/6257860/sql-server-vs-nosql","q":"\n\n<p>So I have a website that could eventually get some pretty high traffic. My DB implementation is in SQL Server 2008 at the moment. I really only have 2 tables and a few stored procs. Most of the DB could be re-designed to work without joining (although it wouldn't make sense when I can join so easily within SQL Server).</p>\n\n<p>I heard that sites like Digg and Facebook use NoSQL databases for a lot of their basic data access. Is this something worth looking into, or will SQL Server not really slow me down that bad?</p>\n\n<p>I use paging on my site (although this might change in the future), and I also use AJAX'd data access for most of the \"live\" stuff, so it doesn't really seem to be a performance hindrance at the moment, but I'm afraid it will be as the data starts expanding exponentially.</p>\n\n<p>Am I going to gain a lot of performance my moving to NoSQL? Honestly, right now I don't even completely understand NoSQL, so any tips on <em>how</em> this will help me improve the better.</p>\n\n<p>Thanks guys.</p>\n    ","a":"\n<p>Actually Facebook use a relational database at its core, see <a href=\"http://hosted.mediasite.com/mediasite/Viewer/?peid=459a9f2cb92d4830b40d3e64003a50e61d\">SOCC Keynote Address: Building Facebook: Performance at Massive Scale</a>. And so do many other web-scale sites, see <a href=\"http://www.quora.com/Quora-Infrastructure/Why-does-Quora-use-MySQL-as-the-data-store-instead-of-NoSQLs-such-as-Cassandra-MongoDB-CouchDB-etc?srid=kBS\">Why does Quora use MySQL as the data store instead of NoSQLs such as Cassandra, MongoDB, CouchDB etc?</a>. There is also a discussion of how to scale SQL Server to web-scale size, see <a href=\"http://cacm.acm.org/magazines/2011/6/108663-scalable-sql/fulltext\">How do large-scale sites and applications remain SQL-based?</a> which is based on MySpace's architecture (more details at <a href=\"http://rusanu.com/2011/06/01/scale-out-sql-server-by-using-reliable-messaging/\">Scale out SQL Server by using Reliable Messaging</a>). I'm not saying that NoSQL doesn't have its use cases, I just want to point out that there are many shades of gray between white and black.</p>\n\n<p>If you're afraid that your current solution will not scale then perhaps you should look at what are the factors that prevent scalability with your current solution. Test data is cheap to produce, load the 'exponentially increased' data volume and run your test harness, see where it cracks. None of the NoSQL solutions will bring magic off-the-shelf scalability, they all require you to understand how to use them effectively and deploy them correctly. And they also require you to test with large volumes if you want to ensure success at scale. Same for traditional relational solutions. </p>\n    "},{"t":"CouchDB sorting and filtering in the same view","l":"http://stackoverflow.com/questions/3311225/couchdb-sorting-and-filtering-in-the-same-view","q":"\n\n<p>I'm trying to use CouchDB for a new app, and I need to create a view that sorts by multiple fields and also filters by multiple fields. Here is an example document, I've left out the _id and _rev to save myself some typing.</p>\n\n<pre><code>{\n    \"title\": \"My Document\",\n    \"date\": 1279816057,\n    \"ranking\": 5,\n    \"category\": \"fun\",\n    \"tags\": [\n        \"couchdb\",\n        \"technology\"\n    ],\n}\n</code></pre>\n\n<p>From the documentation, I've learned that I can easily create a view that sorts by a field such as ranking.</p>\n\n<pre><code>function(doc) {\n    emit(doc.ranking, doc);\n}\n</code></pre>\n\n<p>I've also learned that I can easily filter by fields such as category</p>\n\n<pre><code>function(doc) {\n    emit(doc.category, doc);\n}\n\nhttp://127.0.0.1:5984/database/_design/filter/_view/filter?key=%22fun%22\n</code></pre>\n\n<p>My problem is that I need to do a bunch of these things all at the same time. I want to filter based on category and also tag. I should be able to filter down to only documents with category of \"fun\" and tag of \"couchdb\". I want to sort those filtered results by ranking in descending order, then by date in ascending order, then by title in alphabetical order. </p>\n\n<p>How can I create one view that does all of that sorting and filtering combined?</p>\n    ","a":"\n<p>For emitting more than one piece of data in a key, you'll want to read up on Complex Keys <a href=\"http://wiki.apache.org/couchdb/View_collation\">http://wiki.apache.org/couchdb/View_collation</a> You'll most likely end up emit()'ing a key that is an array made up of the category and tag. For example...</p>\n\n<pre><code>function(doc) {\n  for(var i = 0; i &lt; doc.tags.length; i++)\n    emit([doc.category, doc.tags[i]], doc);\n}\n</code></pre>\n\n<p>Now when you query <code>?key=[\"fun\", \"couchdb\"]</code> you'll get all the items in the fun category tagged as couchdb. Or if you want all of the items in the fun category, regardless of their tag, then you can query with a range: <code>?startkey=[\"fun\"]&amp;endkey=[\"fun\", {}]</code> Just remember, if your item has multiple tags that you'll get it multiple times in the results (because you emit()'d the doc once per tag).</p>\n\n<p>To go the extra step of sorting by rating, date, and title you'll add two more elements to your array: an integer and either the ranking, date, or title. Remember, you can emit() more than once per map function. An example map function...</p>\n\n<pre><code>function(doc) {\n  for(var i = 0; i &lt; doc.tags.length; i++)\n  {\n     emit([doc.category, doc.tags[i], 0, doc.ranking], doc);\n     emit([doc.category, doc.tags[i], 1, doc.title], doc);\n     emit([doc.category, doc.tags[i], 2, doc.date], doc);\n  }\n}\n</code></pre>\n\n<p>Now your key structure is: [\"category\", \"tag\", 0 ... 2, rank/title/date]</p>\n\n<p>You're basically grouping all of the rankings under 0, titles under 1, and dates under 2. Of course, you're transmitting a lot of data, so you could either break each of these groupings out into a separate view in your design document, or only return the doc's _id as the value (<code>emit([ ...], doc._id);</code>). </p>\n\n<p>Get everything in the fun category with the couchdb tag (ascending):</p>\n\n<p><code>?startkey=[\"fun\", \"couchdb\"]&amp;endkey=[\"fun\", \"couchdb\", {}, {}]</code></p>\n\n<p>Get everything in the fun category with the couchdb tag (descending):</p>\n\n<p><code>?startkey=[\"fun\", \"couchdb\", {}, {}]&amp;endkey=[\"fun\", \"couchdb\"]&amp;descending=true</code></p>\n\n<p>Get only rankings in the fun category with the couchdb tag (ascending):</p>\n\n<p><code>?startkey=[\"fun\", \"couchdb\", 0]&amp;endkey=[\"fun\", \"couchdb\", 0, {}]</code></p>\n\n<p>Get only rankings in the fun category with the couchdb tag (descending):</p>\n\n<p><code>?startkey=[\"fun\", \"couchdb\", 0, {}]&amp;endkey=[\"fun\", \"couchdb\", 0]&amp;descending=true</code></p>\n\n<p>I hope this helps. Complex keys start to really show how powerful Map/Reduce is at slicing and dicing data.</p>\n\n<p>Cheers.</p>\n    "},{"t":"NoSQL Database for hierarchical data","l":"http://stackoverflow.com/questions/4767944/nosql-database-for-hierarchical-data","q":"\n\n<p>What type of NoSQL database is best suited to store hierarchical data?</p>\n\n<p>Say for example I want to store posts of a forum with a tree structure:</p>\n\n<pre><code>original post\n + re: original post\n + re: original post\n   + re2: original post\n     + re3: original post\n   + re2: original post\n</code></pre>\n    ","a":"\n<p>MongoDB and CouchDB offer solutions, but not built in functionality. See this SO question on <a href=\"http://stackoverflow.com/questions/4048151/what-are-the-options-for-storing-hierarchical-data-in-a-relational-database\">representing hierarchy in a relational database</a> as most other NoSQL solutions I've seen are similar in this regard; where you have to write your own algorithms for recalculating that information as nodes are added, deleted and moved. Generally speaking you're making a decision between fast read times (e.g. <a href=\"http://en.wikipedia.org/wiki/Nested_set_model\">nested set</a>) or fast write times (<a href=\"http://en.wikipedia.org/wiki/Adjacency_list\">adjacency list</a>). See aforementioned SO question for more options along these lines - the <a href=\"http://evolt.org/node/4047/\">flat table approach</a> appears most aligned with your question. </p>\n\n<p>One standard that does abstract away these considerations is the <a href=\"http://en.wikipedia.org/wiki/Content_repository_API_for_Java\">Java Content Repository</a> (JCR), both <a href=\"http://jackrabbit.apache.org/\">Apache JackRabbit</a> and <a href=\"http://www.jboss.org/exojcr.html\">JBoss eXo</a> are implementations. Note, behind the scenes both are still doing some sort of algorithmic calculations to maintain hierarchy as described above. In addition, the JCR also handles permissions, file storage, and several other aspects - so it may be overkill for your project. </p>\n    "},{"t":"How can I access Amazon DynamoDB via Python?","l":"http://stackoverflow.com/questions/8935130/how-can-i-access-amazon-dynamodb-via-python","q":"\n\n<p>I'm currently using hbase with my Python apps and wanted to try out Amazon <a href=\"http://aws.amazon.com/dynamodb/\">DynamoDB</a>. Is there a way to use Python to read, write and query data?</p>\n    ","a":"\n<p>You can use boto: <a href=\"https://github.com/boto/boto\">https://github.com/boto/boto</a></p>\n\n<p>docs: <a href=\"https://boto.readthedocs.org/en/latest/dynamodb2_tut.html\">https://boto.readthedocs.org/en/latest/dynamodb2_tut.html</a></p>\n\n<p>api reference: <a href=\"https://boto.readthedocs.org/en/latest/ref/dynamodb2.html\">https://boto.readthedocs.org/en/latest/ref/dynamodb2.html</a></p>\n    "},{"t":"Practical example for each type of database (real cases)","l":"http://stackoverflow.com/questions/18198960/practical-example-for-each-type-of-database-real-cases","q":"\n\n<p>There are several types of database for different purposes, however normally MySQL is used to everything, because is the most well know Database. Just to give an example in my company an application of big data has a MySQL database at an initial stage, what is unbelievable and will bring serious consequences to the company. Why MySQL? Just because no one know how (and when) should use another DBMS. </p>\n\n<p>So, my question is not about vendors, but type of databases. Can you give me an practical example of specific situations (or apps) for each type of database where is highly recommended to use it? </p>\n\n<p><strong>Example:</strong></p>\n\n<p>â€¢ A social network should use the type X because of Y.</p>\n\n<p>â€¢ MongoDB or couch DB can't support transactions, so Document DB is not good to an app for a bank or auctions site.</p>\n\n<p>And so on...</p>\n\n<hr>\n\n<p><strong>Relational:</strong> <a href=\"http://www.mysql.com/\">MySQL</a>, <a href=\"http://www.postgresql.org/\">PostgreSQL</a>, <a href=\"http://www.sqlite.org/\">SQLite</a>, <a href=\"http://www.firebirdsql.org/\">Firebird</a>, <a href=\"http://mariadb.org\">MariaDB</a>, <a href=\"http://www.oracle.com/technetwork/indexes/downloads/index.html#database\">Oracle DB</a>, <a href=\"http://www.microsoft.com/sqlserver/pt/br/default.aspx\">SQL server</a>, <a href=\"http://www-01.ibm.com/software/data/db2/\">IBM DB2</a>, <a href=\"http://www-01.ibm.com/software/data/informix/\">IBM Informix</a>, <a href=\"http://www.teradata.com/products-and-services/Teradata-Database/#tabbable=0&amp;tab1=0&amp;tab2=0&amp;tab3=0&amp;tab4=0\">Teradata</a></p>\n\n<p><strong>Object:</strong> <a href=\"http://zodb.org/\">ZODB</a>, <a href=\"http://www.db4o.com/\">DB4O</a>, <a href=\"http://eloquera.com\">Eloquera</a>, <a href=\"http://actian.com/products/versant\">Versant</a> ,  <a href=\"http://www.objectivity.com/\">Objectivity DB</a>, <a href=\"http://www.velocitydb.com/\">VelocityDB</a></p>\n\n<p><strong>Graph databases:</strong> <a href=\"http://www.franz.com/agraph/allegrograph/\">AllegroGraph</a>, <a href=\"http://www.neo4j.org/\">Neo4j</a>, <a href=\"http://www.orientdb.org\">OrientDB</a>, <a href=\"http://www.objectivity.com/infinitegraph\">InfiniteGraph</a>, <a href=\"http://graphbase.net/\">graphbase</a>, <a href=\"http://www.sparkledb.net/\">sparkledb</a>, <a href=\"https://github.com/twitter/flockdb\">flockdb</a>, <a href=\"http://www.brightstardb.com/\">BrightstarDB</a></p>\n\n<p><strong>Key value-stores:</strong> <a href=\"http://aws.amazon.com/dynamodb/\">Amazon DynamoDB</a>, <a href=\"http://redis.io/\">Redis</a>, <a href=\"http://basho.com/riak/\">Riak</a>, <a href=\"http://www.project-voldemort.com/\">Voldemort</a>, <a href=\"http://foundationdb.com/\">FoundationDB</a>, <a href=\"http://code.google.com/p/leveldb/\">leveldb</a>, <a href=\"http://www.iqlect.com/\">BangDB</a>, <a href=\"http://sourceforge.net/projects/kai/\">KAI</a>, <a href=\"http://hamsterdb.com/\">hamsterdb</a>, <a href=\"http://www.tarantool.org/\">Tarantool</a>, <a href=\"http://code.google.com/p/maxtable/\">Maxtable</a>, <a href=\"http://hyperdex.org/\">HyperDex</a>, <a href=\"http://hyperdex.org/\">Genomu</a>, <a href=\"http://code.google.com/p/memcachedb/downloads/list\">Memcachedb</a></p>\n\n<p><strong>Column family:</strong> <a href=\"http://research.google.com/archive/bigtable.html\">Big table</a>, <a href=\"http://hbase.apache.org/\">Hbase</a>, <a href=\"http://hypertable.org/\">hyper table</a>, <a href=\"http://cassandra.apache.org\">Cassandra</a>, <a href=\"http://accumulo.apache.org/\">Apache Accumulo</a></p>\n\n<p><strong>RDF Stores:</strong> <a href=\"http://jena.apache.org/\">Apache Jena</a>, <a href=\"http://www.openrdf.org/\">Sesame</a> </p>\n\n<p><strong>Multimodel Databases:</strong> <a href=\"http://www.arangodb.org/\">arangodb</a>, <a href=\"http://www.datomic.com/\">Datomic</a>, <a href=\"http://www.orientechnologies.com/\">Orient DB</a>, <a href=\"http://fatcloud.com/net_nosql_database.html\">FatDB</a>, <a href=\"http://code.google.com/p/alchemydatabase/\">AlchemyDB</a></p>\n\n<p><strong>Document:</strong> <a href=\"http://www.mongodb.org/\">Mongo DB</a>, <a href=\"http://couchdb.apache.org\">Couch DB</a>, <a href=\"http://www.rethinkdb.com/\">Rethink DB</a>, <a href=\"https://github.com/ravendb/ravendb\">Raven DB</a>, <a href=\"http://code.google.com/p/terrastore/\">terrastore</a>, <a href=\"http://www.oberasoftware.com/\">Jas DB</a>, <a href=\"http://www.codeproject.com/Articles/375413/RaptorDB-the-Document-Store\">Raptor DB</a>, <a href=\"http://djondb.com/\">djon DB</a>, <a href=\"http://ejdb.org/\">EJDB</a>, <a href=\"http://www.densodb.net/\">denso DB</a>, <a href=\"http://www.couchbase.com/\">Couchbase</a> </p>\n\n<p><strong>XML Databases:</strong> <a href=\"http://basex.org/\">BaseX</a>, <a href=\"http://www.sedna.org/\">Sedna</a>, <a href=\"http://exist-db.org/exist/apps/homepage/index.html\">eXist</a></p>\n\n<p><strong>Hierarchical:</strong> <a href=\"http://www.intersystems.com/cache/\">InterSystems CachÃ©</a>, <a href=\"http://sourceforge.net/projects/fis-gtm/\">GT.M</a> <em>thanks to @Laurent Parenteau</em></p>\n    ","a":"\n<p>I found two impressive articles about this subject.</p>\n\n<p>All credits to <strong><a href=\"http://highscalability.com/\">highscalability.com</a></strong>. The information is transcribed from these urls.</p>\n\n<p><a href=\"http://highscalability.com/blog/2011/6/20/35-use-cases-for-choosing-your-next-nosql-database.html\">http://highscalability.com/blog/2011/6/20/35-use-cases-for-choosing-your-next-nosql-database.html</a></p>\n\n<p><a href=\"http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html\">http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html</a></p>\n\n<hr>\n\n<p><em><strong>If Your Application Needs...</strong></em></p>\n\n<p>â€¢ <strong>complex transactions</strong> because you can't afford to lose data or if you would like a simple transaction programming model then look at a Relational or Grid database.</p>\n\n<p>â€¢ <strong>Example:</strong> an inventory system that might want full ACID. I was very unhappy when I bought a product and they said later they were out of stock. I did not want a compensated transaction. I wanted my item!</p>\n\n<p>â€¢ <strong>to scale</strong> then NoSQL or SQL can work. Look for systems that support scale-out, partitioning, live addition and removal of machines, load balancing, automatic sharding and rebalancing, and fault tolerance.</p>\n\n<p>â€¢ to <strong>always</strong> be able to <strong>write</strong> to a database because you need high availability then look at Bigtable Clones which feature eventual consistency.</p>\n\n<p>â€¢ to handle lots of <strong>small continuous reads and writes</strong>, that may be volatile, then look at Document or Key-value or databases offering fast in-memory access. Also consider SSD.</p>\n\n<p>â€¢ to implement <strong>social network operations</strong> then you first may want a Graph database or second, a database like Riak that supports relationships. An in- memory relational database with simple SQL joins might suffice for small data sets. Redis' set and list operations could work too.</p>\n\n<p>â€¢ to operate over <strong>a wide variety of access patterns and data types</strong> then look at a Document database, they generally are flexible and perform well.</p>\n\n<p>â€¢ powerful <strong>offline reporting with large datasets</strong> then look at Hadoop first and second, products that support MapReduce. Supporting MapReduce isn't the same as being good at it.</p>\n\n<p>â€¢ to <strong>span multiple data-centers</strong> then look at Bigtable Clones and other products that offer a distributed option that can handle the long latencies and are partition tolerant.</p>\n\n<p>â€¢ to build <strong>CRUD</strong> apps then look at a Document database, they make it easy to access complex data without joins. </p>\n\n<p>â€¢ <strong>built-in search</strong> then look at Riak.</p>\n\n<p>â€¢ to operate on <strong>data structures</strong> like lists, sets, queues, publish-subscribe then look at Redis. Useful for distributed locking, capped logs, and a lot more.</p>\n\n<p>â€¢ <strong>programmer friendliness</strong> in the form of programmer friendly data types like JSON, HTTP, REST, Javascript then first look at Document databases and then Key-value Databases.</p>\n\n<p>â€¢ <strong>transactions</strong> combined with <strong>materialized views</strong> for <strong>real-time</strong> data feeds then look at VoltDB. Great for data-rollups and time windowing.</p>\n\n<p>â€¢ <strong>enterprise level support and SLAs</strong> then look for a product that makes a point of catering to that market. Membase is an example.</p>\n\n<p>â€¢ to log <strong>continuous streams</strong> of data that may have no consistency guarantees necessary at all then look at Bigtable Clones because they generally work on distributed file systems that can handle a lot of writes.</p>\n\n<p>â€¢ to be <strong>as simple as possible</strong> to operate then look for a hosted or PaaS solution because they will do all the work for you.</p>\n\n<p>â€¢ to be sold to <strong>enterprise customers</strong> then consider a Relational Database because they are used to relational technology.</p>\n\n<p>â€¢ to <strong>dynamically build relationships</strong> between objects that have <strong>dynamic properties</strong> then consider a Graph Database because often they will not require a schema and models can be built incrementally through programming.</p>\n\n<p>â€¢ to support <strong>large media</strong> then look storage services like S3. NoSQL systems tend not to handle large BLOBS, though MongoDB has a file service.</p>\n\n<p>â€¢ to <strong>bulk upload</strong> lots of data quickly and efficiently then look for a product supports that scenario. Most will not because they don't support bulk operations.</p>\n\n<p>â€¢ an <strong>easier upgrade path</strong> then use a fluid schema system like a Document Database or a Key-value Database because it supports optional fields, adding fields, and field deletions without the need to build an entire schema migration framework.</p>\n\n<p>â€¢ to implement <strong>integrity constraints</strong> then pick a database that support SQL DDL, implement them in stored procedures, or implement them in application code.</p>\n\n<p>â€¢ a <strong>very deep join depth</strong> the use a Graph Database because they support blisteringly fast navigation between entities.</p>\n\n<p>â€¢ to move <strong>behavior close to the data</strong> so the data doesn't have to be moved over the network then look at stored procedures of one kind or another. These can be found in Relational, Grid, Document, and even Key-value databases.</p>\n\n<p>â€¢ to <strong>cache or store BLOB</strong> data then look at a Key-value store. Caching can for bits of web pages, or to save complex objects that were expensive to join in a relational database, to reduce latency, and so on.</p>\n\n<p>â€¢ a <strong>proven track record</strong> like not corrupting data and just generally working then pick an established product and when you hit scaling (or other issues) use on of the common workarounds (scale-up, tuning, memcached, sharding, denormalization, etc).</p>\n\n<p>â€¢ <strong>fluid data types</strong> because your data isn't tabular in nature, or requires a flexible number of columns, or has a complex structure, or varies by user (or whatever), then look at Document, Key-value, and Bigtable Clone databases. Each has a lot of flexibility in their data types.</p>\n\n<p>â€¢ other business units to <strong>run quick relational queries</strong> so you don't have to reimplement everything then use a database that supports SQL.</p>\n\n<p>â€¢ to operate in the cloud and automatically take full advantage of cloud features then we may not be there yet. </p>\n\n<p>â€¢ support for <strong>secondary indexes</strong> so you can look up data by different keys then look at relational databases and Cassandra's new secondary index support.</p>\n\n<p>â€¢ creates an <strong>ever-growing set of data</strong> (really BigData) that rarely gets accessed then look at Bigtable Clone which will spread the data over a distributed file system.</p>\n\n<p>â€¢ to <strong>integrate with other services</strong> then check if the database provides some sort of write-behind syncing feature so you can capture database changes and feed them into other systems to ensure consistency.</p>\n\n<p>â€¢ <strong>fault tolerance</strong> check how durable writes are in the face power failures, partitions, and other failure scenarios.</p>\n\n<p>â€¢ to push the technological envelope in a direction nobody seems to be going then build it yourself because that's what it takes to be great sometimes.</p>\n\n<p>â€¢ to work on a <strong>mobile platform</strong> then look at CouchDB/Mobile couchbase.</p>\n\n<hr>\n\n<p><strong>General Use Cases (NoSQL)</strong></p>\n\n<p>â€¢ <strong>Bigness</strong>. NoSQL is seen as a key part of a new data stack supporting: big data, big numbers of users, big numbers of computers, big supply chains, big science, and so on. When something becomes so massive that it must become massively distributed, NoSQL is there, though not all NoSQL systems are targeting big. Bigness can be across many different dimensions, not just using a lot of disk space. </p>\n\n<p>â€¢ <strong>Massive write performance.</strong> This is probably the canonical usage based on Google's influence. High volume. Facebook needs to store 135 billion messages a month. Twitter, for example, has the problem of storing 7 TB/data per day with the prospect of this requirement doubling multiple times per year. This is the data is too big to fit on one node problem. At 80 MB/s it takes a day to store 7TB so writes need to be distributed over a cluster, which implies key-value access, MapReduce, replication, fault tolerance, consistency issues, and all the rest. For faster writes in-memory systems can be used.</p>\n\n<p>â€¢ <strong>Fast key-value access.</strong> This is probably the second most cited virtue of NoSQL in the general mind set.  When latency is important it's hard to beat hashing on a key and reading the value directly from memory or in as little as one disk seek. Not every NoSQL product is about fast access, some are more about reliability, for example. but what people have wanted for a long time was a better memcached and many NoSQL systems offer that.</p>\n\n<p>â€¢ <strong>Flexible schema and flexible datatypes.</strong>  NoSQL products support a whole range of new data types, and this is a major area of innovation in NoSQL. We have: column-oriented, graph, advanced data structures, document-oriented, and key-value. Complex objects can be easily stored without a lot of mapping. Developers love avoiding complex schemas and ORM frameworks. Lack of structure allows for much more flexibility. We also have program and programmer friendly compatible datatypes likes JSON. </p>\n\n<p>â€¢ <strong>Schema migration.</strong> Schemalessness makes it easier to deal with schema migrations without so much worrying. Schemas are in a sense dynamic, because they are imposed by the application at run-time, so different parts of an application can have a different view of the schema.</p>\n\n<p>â€¢ <strong>Write availability.</strong> Do your writes need to succeed no mater what? Then we can get into partitioning, CAP, eventual consistency and all that jazz.</p>\n\n<p>â€¢ <strong>Easier maintainability, administration and operations.</strong> This is very product specific, but many NoSQL vendors are trying to gain adoption by making it easy for developers to adopt them. They are spending a lot of effort on ease of use, minimal administration, and automated operations. This can lead to lower operations costs as special code doesn't have to be written to scale a system that was never intended to be used that way.</p>\n\n<p>â€¢ <strong>No single point of failure.</strong> Not every product is delivering on this, but we are seeing a definite convergence on relatively easy to configure and manage high availability with automatic load balancing and cluster sizing. A perfect cloud partner.</p>\n\n<p>â€¢ <strong>Generally available parallel computing.</strong> We are seeing MapReduce baked into products, which makes parallel computing something that will be a normal part of development in the future.</p>\n\n<p>â€¢ <strong>Programmer ease of use.</strong> Accessing your data should be easy. While the relational model is intuitive for end users, like accountants, it's not very intuitive for developers. Programmers grok keys, values, JSON, Javascript stored procedures, HTTP, and so on. NoSQL is for programmers. This is a developer led coup. The response to a database problem can't always be to hire a really knowledgeable DBA, get your schema right, denormalize a little, etc., programmers would prefer a system that they can make work for themselves. It shouldn't be so hard to make a product perform. Money is part of the issue. If it costs a lot to scale a product then won't you go with the cheaper product, that you control, that's easier to use, and that's easier to scale?</p>\n\n<p>â€¢ <strong>Use the right data model for the right problem.</strong> Different data models are used to solve different problems. Much effort has been put into, for example, wedging graph operations into a relational model, but it doesn't work. Isn't it better to solve a graph problem in a graph database? We are now seeing a general strategy of trying find the best fit between a problem and solution.</p>\n\n<p>â€¢ <strong>Avoid hitting the wall.</strong> Many projects hit some type of wall in their project. They've exhausted all options to make their system scale or perform properly and are wondering what next? It's comforting to select a product and an approach that can jump over the wall by linearly scaling using incrementally added resources.  At one time this wasn't possible. It took custom built everything, but that's changed. We are now seeing usable out-of-the-box products that a project can readily adopt.</p>\n\n<p>â€¢ <strong>Distributed systems support.</strong> Not everyone is worried about scale or performance over and above that which can be achieved by non-NoSQL systems. What they need is a distributed system that can span datacenters while handling failure scenarios without a hiccup. NoSQL systems, because they have focussed on scale, tend to exploit partitions, tend not use heavy strict consistency protocols, and so are well positioned to operate in distributed scenarios.</p>\n\n<p>â€¢ <strong>Tunable CAP tradeoffs.</strong> NoSQL systems are generally the only products with a \"slider\" for choosing where they want to land on the CAP spectrum. Relational databases pick strong consistency which means they can't tolerate a partition failure. In the end this is a business decision and should be decided on a case by case basis. Does your app even care about consistency? Are a few drops OK? Does your app need strong or weak consistency? Is availability more important or is consistency? Will being down be more costly than being wrong? It's nice to have products that give you a choice.</p>\n\n<p>â€¢ <strong>More Specific Use Cases</strong></p>\n\n<p>â€¢ Managing large streams of non-transactional data: Apache logs, application logs, MySQL logs, clickstreams, etc.</p>\n\n<p>â€¢ Syncing online and offline data. This is a niche CouchDB has targeted. </p>\n\n<p>â€¢ Fast response times under all loads.</p>\n\n<p>â€¢ Avoiding heavy joins for when the query load for complex joins become too large for a RDBMS.</p>\n\n<p>â€¢ Soft real-time systems where low latency is critical. Games are one example.</p>\n\n<p>â€¢ Applications where a wide variety of different write, read, query, and consistency patterns need to be supported. There are systems optimized for 50% reads 50% writes, 95% writes, or 95% reads. Read-only applications needing extreme speed and resiliency, simple queries, and can tolerate slightly stale data. Applications requiring moderate performance, read/write access, simple queries, completely authoritative data. Read-only application which complex query requirements.</p>\n\n<p>â€¢ Load balance to accommodate data and usage concentrations and to help keep microprocessors busy.</p>\n\n<p>â€¢ Real-time inserts, updates, and queries.</p>\n\n<p>â€¢ Hierarchical data like threaded discussions and parts explosion.</p>\n\n<p>â€¢ Dynamic table creation.</p>\n\n<p>â€¢ Two tier applications where low latency data is made available through a fast NoSQL interface, but the data itself can be calculated and updated by high latency Hadoop apps or other low priority apps.</p>\n\n<p>â€¢ <strong>Sequential data reading.</strong> The right underlying data storage model needs to be selected. A B-tree may not be the best model for sequential reads.</p>\n\n<p>â€¢ Slicing off part of service that may need better performance/scalability onto it's own system. For example, user logins may need to be high performance and this feature could use a dedicated service to meet those goals.</p>\n\n<p>â€¢ <strong>Caching.</strong> A  high performance caching tier for web sites and other applications. Example is a cache for the Data Aggregation System used by the Large Hadron Collider.\nVoting.</p>\n\n<p>â€¢ Real-time page view counters.</p>\n\n<p>â€¢ User registration, profile, and session data.</p>\n\n<p>â€¢ <strong>Document, catalog management  and content management systems.</strong> These are facilitated by the ability to store complex documents has a whole rather than organized as relational tables. Similar logic applies to inventory, shopping carts, and other structured data types.</p>\n\n<p>â€¢ <strong>Archiving.</strong> Storing a large continual stream of data that is still accessible on-line. Document-oriented databases with a flexible schema that can handle schema changes over time.</p>\n\n<p>â€¢ <strong>Analytics.</strong> Use MapReduce, Hive, or Pig to perform analytical queries and scale-out systems that support high write loads.</p>\n\n<p>â€¢ Working with heterogenous types of data, for example, different media types at a generic level.</p>\n\n<p>â€¢ Embedded systems. They donâ€™t want the overhead of SQL and servers, so they uses something simpler for storage.</p>\n\n<p>â€¢ A \"market\" game, where you own buildings in a town. You want the building list of someone to pop up quickly, so you partition on the owner column of the building table, so that the select is single-partitioned. But when someone buys the building of someone else you update the owner column along with price.</p>\n\n<p>â€¢ JPL is using SimpleDB to store rover plan attributes. References are kept to a full plan blob in S3. </p>\n\n<p>â€¢ Federal law enforcement agencies tracking Americans in real-time using credit cards, loyalty cards and travel reservations.</p>\n\n<p>â€¢ Fraud detection by comparing transactions to known patterns in real-time.</p>\n\n<p>â€¢ Helping diagnose the typology of tumors by integrating the history of every patient.\nIn-memory database for high update situations, like a web site that displays everyone's \"last active\" time (for chat maybe). If users are performing some activity once every 30 sec, then you will be pretty much be at your limit with about 5000 simultaneous users.\nHandling lower-frequency multi-partition queries using materialized views while continuing to process high-frequency streaming data.</p>\n\n<p>â€¢ Priority queues.</p>\n\n<p>â€¢ Running calculations on cached data, using a program friendly interface, without have to go through an ORM.</p>\n\n<p>â€¢ Unique a large dataset using simple key-value columns.</p>\n\n<p>â€¢ To keep querying fast, values can be rolled-up into different time slices.</p>\n\n<p>â€¢ Computing the intersection of two massive sets, where a join would be too slow.</p>\n\n<p>â€¢ A timeline ala Twitter. </p>\n    "},{"t":"NoSql and Data-Warehouse","l":"http://stackoverflow.com/questions/2684462/nosql-and-data-warehouse","q":"\n\n<p>What are the relations between NoSql and Data-Warehouse technologies/theories? </p>\n\n<p>What concepts they share? </p>\n\n<p>What are the basic differences between them? </p>\n\n<p>How do you think each could be benefits/enriches from the other? </p>\n\n<p>I think your ideas should be helpful for the future of both technologies.</p>\n\n<p><strong>UPDATE</strong>:</p>\n\n<p>Some useful links:</p>\n\n<ul>\n<li><a href=\"http://smartdatacollective.com/barry-devlin/33172/integrating-nosql-data-warehouse\">Integrating NoSQL in the Data Warehouse</a></li>\n<li><a href=\"http://ayende.com/blog/4552/nosql-and-data-warehousing\">NoSQL and Data Warehousing</a></li>\n<li>\n<a href=\"http://www.infogain.com/company/perspective-big-data.jsp\">Are You Ready for Big Data?</a></li>\n</ul>\n\n<p><strong>2nd UPDATE:</strong></p>\n\n<ul>\n<li><a href=\"http://nosql.mypopescu.com/post/6715306643/mongodb-bi-and-non-relational-databases\">MongoDB, BI and Non-Relational Databases</a></li>\n</ul>\n    ","a":"\n<p>Data Warehouses have very little in common with NoSQL - the main similarity is that any two data warehouses can have very different philosopohies or conventions just like any two NoSQL systems can be nearly unrelated.</p>\n\n<p>The only concept they share is that they are both used to analyze large amounts of data.</p>\n\n<p>NoSQL solutions usually manage relatively limited schemas with large cardinality in few entities, while data warehouses typically have lots of facts and dimensions (in a dimensional model) or lots of entities in a 3NF model.  DW systems usually manage multiple lines of business and attempt to combine that data.</p>\n\n<p>DW systems typically have reporting abilities in SQL which allows you to access all the data in a standard way.  NoSQL systems are typically more code-based - for instance Map/Reduce.</p>\n    "},{"t":"What are the best uses of document stores?","l":"http://stackoverflow.com/questions/3376636/what-are-the-best-uses-of-document-stores","q":"\n\n<p>I have been hearing a lot about document oriented data stores like CouchDB. I understand the uses of BigTable like stores such as Cassandra. After reading <a href=\"http://stackoverflow.com/questions/1104624/what-is-the-difference-between-cassandra-and-couchdb\">this question</a>, I was wondering what the conditions would be to merit using a document store?</p>\n    ","a":"\n<p>Column-family stores such as Bigtable and Cassandra have very limited querying capabilities. The application is responsible for maintaining indexes in order to query a more complex data model.</p>\n\n<p>Document databases allow you to query the content, not just the key. It will also manage the indexes for you, reducing the complexity of your application.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Domain-driven_design\">Domain-driven design</a> evangelizes the use of aggregates and value objects. <a href=\"http://ayende.com/Blog/archive/2010/04/19/that-no-sql-thing-document-databases-ndash-usages.aspx\">As Ayende points out</a>, (complex) aggregates are very natural candidates to be stored as a single document, instead of normalizing them over multiple tables or column families. This will reduce the complexity of your persistence layer. There's also less chance that related data is scattered across multiple nodes, as all the data is contained in a single document.</p>\n\n<p>If your application needs to store polymorphic objects, document databases are also a good candidate. Of course, this could also be stored in Cassandra, but you won't have as much querying capabilities. At least not out of the box.</p>\n\n<p>Think of a document database as a luxurious sports car. It doesn't need a professional driver (read: complex application) to get you from A to B, it has features such as air conditioning and comfortable seats and it will lap the high-scalability track in an acceptable time. However, if you want to set a lap record on the high-scalability track, you <em>will</em> need a professional driver and a highly optimized car (e.g. Cassandra), which lacks features such as air conditioning.</p>\n    "},{"t":"Is there a good Redis browser?","l":"http://stackoverflow.com/questions/12292351/is-there-a-good-redis-browser","q":"\n\n<p>Are there any good browsers/explorer for viewing Redis out there ?\nAm new to Redis so my expectation is if there is something similar to MongoVUE,Toad or SQLExplorer.</p>\n\n<p>I tried Redis Admin UI from service stack but ran into 500 error when trying on IIS </p>\n    ","a":"\n<p>If you prefer <strong>open-source desktop tools</strong>, also take a look on <a href=\"http://redisdesktop.com/\">Redis Desktop Manager</a>.</p>\n\n<p>It's a cross-platform open source Redis DB management tool (i.e. Admin GUI)</p>\n\n<p><img src=\"http://i.stack.imgur.com/7Bmcw.png\" alt=\"enter image description here\"></p>\n    "},{"t":"How to decide which NoSQL technology to use?","l":"http://stackoverflow.com/questions/3735784/how-to-decide-which-nosql-technology-to-use","q":"\n\n<p>What is the pros and cons of MongoDB (document-based), HBase (column-based) and Neo4j (objects graph)?  </p>\n\n<p>I'm particularly interested to know some of the typical use cases for each one.      </p>\n\n<p><a href=\"http://stackoverflow.com/questions/703999/what-are-some-examples-of-problems-that-are-best-solved-with-graphs\">http://stackoverflow.com/questions/703999/what-are-some-examples-of-problems-that-are-best-solved-with-graphs</a></p>\n\n<p>Maybe any Slideshare or Scribd worthy presentation?</p>\n    ","a":"\n<p><strong>MongoDB</strong> </p>\n\n<p><em>Scalability:</em> Highly available and consistent but sucks at relations and many distributed writes. It's primary benefit is storing and indexing schemaless documents. Document size is capped at 4mb and indexing only makes sense for limited depth. See <a href=\"http://www.paperplanes.de/2010/2/25/notes_on_mongodb.html\">http://www.paperplanes.de/2010/2/25/notes_on_mongodb.html</a></p>\n\n<p><em>Best suited for:</em> Tree structures with limited depth</p>\n\n<p><em>Use Cases:</em> Diverse Type Hierarchies, Biological Systematics, Library Catalogs</p>\n\n<p><strong>Neo4j</strong></p>\n\n<p><em>Scalability:</em> Highly available but not distributed. Powerful traversal framework for high-speed traversals in the node space. Limited to graphs around several billion nodes/relationships. See <a href=\"http://highscalability.com/neo4j-graph-database-kicks-buttox\">http://highscalability.com/neo4j-graph-database-kicks-buttox</a></p>\n\n<p><em>Best suited for:</em> Deep graphs with unlimited depth and cyclical, weighted connections</p>\n\n<p><em>Use Cases:</em> Social Networks, Topological analysis, Semantic Web Data, Inferencing</p>\n\n<p><strong>HBase</strong></p>\n\n<p><em>Scalability:</em> Reliable, consistent storage in the petabytes and beyond. Supports very large numbers of objects with a limited set of sparse attributes. Works in tandem with Hadoop for large data processing jobs. <a href=\"http://www.ibm.com/developerworks/opensource/library/os-hbase/index.html\">http://www.ibm.com/developerworks/opensource/library/os-hbase/index.html</a></p>\n\n<p><em>Best suited for:</em> directed, acyclic graphs</p>\n\n<p><em>Use Cases:</em> Log analysis, Semantic Web Data, Machine Learning</p>\n    "},{"t":"Comparison : Aerospike vs Cassandra [closed]","l":"http://stackoverflow.com/questions/25443933/comparison-aerospike-vs-cassandra","q":"\n\n<p>Both Aerospike and Cassandra says they are better than the other in their own respective benchmarks.  </p>\n\n<p>Reference : <a href=\"http://java.dzone.com/articles/benchmarking-cassandra-right\">http://java.dzone.com/articles/benchmarking-cassandra-right</a>  and a few others.</p>\n\n<p>Has anyone <strong>used both of them</strong>?<br>\nIs Aerospike as good as claimed?<br>\nFinally is it <strong>advisable to replace Cassandra with Aerospike</strong>?</p>\n    ","a":"\n<p>Choosing between Cassandra and Aerospike really depends on your use case more than anything. I have personally used both as a production system for the same project and for me Aerospike was the clear winner but that's because our use case is to have highly concurrent, low latency, transactional, small updates to billions of entries with ~10x more read than write volume. This is what Aerospike excels at, it has the minimal latency I have ever seen in a database of its kind even when using an SSD namespace. For these reasons Aerospike was the clear choice for us.</p>\n\n<p>On the other hand, Cassandra is better for high write volume and can handle larger records. Everything is page based so it operates well on non-SSDs but can never give you the extreme low latency that Aerospike can unless your records fit into the cache. Its also worth noting that Cassandra is much harder to maintain from an operations perspective than Aerospike is. For us personally it was an operations nightmare and I know that Netflix has to employ a sizable team of operations engineers solely to manage their Cassandra clusters. Also while the system may have matured more by now, when we were using it (around the 1.0 version) we would hit strange occasional assert errors and exceptions that stop internal db actions from taking place and typically had to wipe the data from those nodes in order to fix it every time.</p>\n\n<p>Another factor here is cost which may or may not play into your decision depending on your application. The larger the keyspace the more expensive your Aerospike cluster will be from a hardware perspective. All keys need to be stored in memory regardless of whether it is an in memory or ssd namespace. Once you get into the billions of keys range you will need terabytes of ram in your cluster to support that with a replication factor of 2. Cassandra obviously does not have this issue since the keys and values are both stores on disk.</p>\n\n<p>To answer your second 2 questions, yes it is as good as it claims, we store about 5B keys and do ~1M TPS at peak load and it does it without breaking a sweat (although it takes almost 20 nodes per cluster to do this with 120GB ram each). And as for is it advisable to replace Cassandra with Aerospike, for us it was a definite win and the right decision. If your application fits the design of Aerospike and it works out to be cost effective then it is definitely advisable to make the switch. When it comes down to it though its about your use case. If its not clear which one is the better fit for you then try them both and see how they play out. Good luck.</p>\n    "},{"t":"How does Leveldb compare with Redis or Riak or Tokyo Tyrant? [closed]","l":"http://stackoverflow.com/questions/6101402/how-does-leveldb-compare-with-redis-or-riak-or-tokyo-tyrant","q":"\n\n<p><a href=\"http://leveldb.googlecode.com/svn/trunk/doc/index.html\">Leveldb</a> seems to be a new interesting persistent key value store from Google. How does Leveldb differ from Redis or Riak or Tokyo Tyrant? In what specific use cases is one better than the other?</p>\n    ","a":"\n<p>I find I disagree a bit with colum's criteria though the differences between leveldb and Redis he points out are spot on.</p>\n\n<p>Do you need concurrency? I'd go with Redis. I say this because Redis already has the code written to handle it. Any time I can use well-written Other People's Code to handle concurrency, so much the better. I don't simply mean multi-threaded apps, but include in this the notion of multiple processes - be they on the same system or not. Even then, not needing to write and debug locking in a multi-threaded app has a big advantage in my eyes.</p>\n\n<p>Do you want it completely self-contained within the app? Go with leveldb as it is a library. Do need or want more than just a k/v? Go with Redis. </p>\n\n<p>I'm only commenting on the leveldb or Redis aspect as I don't consider myself fluent enough yet in Riak or TT to comment on their better suits. </p>\n\n<p>In short if all you are looking for is persistent key-value store in a single-threaded app then leveldb is the option to choose among your list (another would be Tokyo cabinet or good ole BerkleyDB or even sqlite). But if you want more than that, choose one of the others. </p>\n\n<p>[edit: updated explanation wrt. concurrency]</p>\n    "},{"t":"How to choose which type of NoSQL to use [closed]","l":"http://stackoverflow.com/questions/5689091/how-to-choose-which-type-of-nosql-to-use","q":"\n\n<p>There is a great list of various NoSQL database platforms at <a href=\"http://nosql-database.org\">http://nosql-database.org</a>.  It categorizes each as a \"wide column store\", \"document store\", \"key-value store\", or \"graph store\".  What I'm not finding is guidance on how to choose which of those 3 categories is most appropriate for a given problem.  </p>\n\n<p>What are the pros/cons or strengths/weaknesses of each type?<br>\nWhich classes of problems is each type best suited for?</p>\n\n<p>To be clear, I'm asking about distinctions between these 3 types of NoSQL systems and <em>not</em> specific implementations of them.</p>\n    ","a":"\n<p>There is a good <a href=\"http://www.thoughtworks.com/articles/nosql-comparison\">article</a> (though it doesn't go in depth) on this exact issue on the thoughtworks site.</p>\n\n<p>And <a href=\"http://blog.nahurst.com/visual-guide-to-nosql-systems\">this visual guide</a> is excellent as well</p>\n    "},{"t":"Join operation with NOSQL","l":"http://stackoverflow.com/questions/1995216/join-operation-with-nosql","q":"\n\n<p>I have gone through some articles regarding Bigtable and NOSQL. It is very interesting that they avoid JOIN operations. </p>\n\n<p>As a basic example, let's take Employee and Department table and assume the data is spread across multiple tables / servers.</p>\n\n<p>Just want to know, if data is spread across multiple servers, how do we do JOIN or UNION operations?</p>\n    ","a":"\n<p>When you have extremely large data, you probably want to avoid joins. This is because the overhead of an individual key lookup is relatively large (the service needs to figure out which node(s) to query, and query them in parallel and wait for responses). By overhead, I mean latency, not throughput limitation.</p>\n\n<p>This makes joins suck really badly as you'd need to do a lot of foreign key lookups, which would end up going to many,many different nodes (in many cases). So you'd want to avoid this as a pattern.</p>\n\n<p>If it doesn't happen very often, you could probably take the hit, but if you're going to want to do a lot of them, it may be worth \"denormalising\" the data.</p>\n\n<p>The kind of stuff which gets stored in NoSQL stores is typically pretty \"abnormal\" in the first place. It is not uncommon to duplicate the same data in all sorts of different places to make lookups easier.</p>\n\n<p>Additionally most nosql don't (really) support secondary indexes either, which means you have to duplicate stuff if you want to query by any other criterion.</p>\n\n<p>If you're storing data such as employees and departments, you're really better off with a conventional database.</p>\n    "},{"t":"Mongodb vs Postgres in a Nodejs App [closed]","l":"http://stackoverflow.com/questions/18776088/mongodb-vs-postgres-in-a-nodejs-app","q":"\n\n<p>I'm building a nodejs application and am utterly torn between nosql mongodb vs rmds PostregresSql. My project is to create a open source example project for logging visitors and displaying visitor statistics in real time on a webpage using nodejs. I was planning on using mongodb at first, because lot of nodejs examples and tutorials, albiet mostly older ones, used it and paas hosters with a free teir are abounding. However, I was seeing a lot of bashing on mongodb recently and found that people who tried to use mongodb ended up switching to postgres. <a href=\"http://blog.engineering.kiip.me/post/20988881092/a-year-with-mongodb\">http://blog.engineering.kiip.me/post/20988881092/a-year-with-mongodb</a> <a href=\"http://dieswaytoofast.blogspot.com/2012/09/mysql-vs-postgres-vs-mongodb.html\">http://dieswaytoofast.blogspot.com/2012/09/mysql-vs-postgres-vs-mongodb.html</a> <a href=\"http://www.plotprojects.com/why-we-use-postgresql-and-slick/\">http://www.plotprojects.com/why-we-use-postgresql-and-slick/</a> I also a fan of heroku and have heard a lot about postgress becuase of that and find that sql queries can be nice sometimes.</p>\n\n<p>I'm not a database expert, so I can't tell for the life of me which way to go. I would really apreaciate it if you could give some advice on which one to consider and why.</p>\n\n<p>I have a few criteria:</p>\n\n<ol>\n<li><p>Since I want this to be a example, it would be nice to have a way to host a decently sized amount of data. I know that mongodb defiantly offers this, but postgres paas like heroku seem to have pretty small databases(since I am logging every visitor to the website) </p></li>\n<li><p>A database that is simplistic and easy to explain to others.</p></li>\n<li><p>Performance doesn't really matter, but speed can't hurt</p></li>\n</ol>\n\n<p>Thanks for all of the help!</p>\n\n<p><strong>Note</strong>: Please no flame wars, everyone has their own opinion :)</p>\n    ","a":"\n<p>Choosing between an SQL database and a NoSQL database is certainly being debated heavily right now and there are plenty of good articles on the subject. I'll list a couple at the end.  I have no problem recommending SQL over NOSQL for your particular needs.</p>\n\n<p>NoSQL has a niche group of use cases where data is stored in large tightly coupled packages called documents rather than in a relational model.  In a nutshell, data that is tightly coupled to a single entity (like all the text documents used by a single user) is better stored in a NoSQL document model.  Data that behaves like excel spreadsheets and fits nicely in rows and is subject to aggregate calculations is better stored in a SQL database of which postgresql is only one of several good choices.</p>\n\n<p>A third option that you might consider is redis (<a href=\"http://redis.io/\">http://redis.io/</a>) which is a simple key value data store that is extremely fast when querying like SQL but not as rigidly typed.</p>\n\n<p>The example you cite seems to be a straightforward row/column type problem.  You will find the SQL syntax is much less arcane than the query syntax for MongoDB. Node has many things to recommend it and the toolset has matured significantly in the past year.  I would recommend using the <a href=\"http://mongoosejs.com/\">monogoose</a> npm package as it reduces the amount of boilerplate code that is required with native mongodb and I have not noticed any performance degradation.</p>\n\n<p><a href=\"http://slashdot.org/topic/bi/sql-vs-nosql-which-is-better/\">http://slashdot.org/topic/bi/sql-vs-nosql-which-is-better/</a></p>\n\n<p><a href=\"http://www.infoivy.com/2013/07/nosql-database-comparison-chart-only.html\">http://www.infoivy.com/2013/07/nosql-database-comparison-chart-only.html</a></p>\n    "},{"t":"Simulating relations in MongoDB","l":"http://stackoverflow.com/questions/3013027/simulating-relations-in-mongodb","q":"\n\n<p>Being one of the most popular NoSQL solutions MongoDB has most of the advantages of this approach. But one issue I'm still struggling with is how reflect object relations in NoSQL data store, specifically - MongoDB.</p>\n\n<p>For example, let's consider a simple data model: User, Post and Comment. It is clear to me that comments have no value on their own and thus become embedded objects for Posts. But when it comes to users - that becomes tricky because User is an entity on its own, not coupled with Post. Now in case I need to list posts with user full names and links to profiles on a web page, I would need to have a list of posts and information about posts authors (name and id at least).</p>\n\n<p>I see 2 possible solutions here:</p>\n\n<ol>\n<li>De-normalize the data in a way that each post entry contains its author's ID and full name (and any other user attributes I might need when listing posts). This way I would make querying for the data really simple but wheneve user updates his/her profile I would need to update all the posts by the user as well. But then I need to also store user attributes in the comments objects which means that updating user profile essentially requires me to update all the posts that have at least one comment by the user unless I want to store comments in a separate collections.</li>\n<li>Only store user ID in the post object and run 2 queries: one to get a list of posts and another one to get list of users where user id is in the list of post authors. This requires  2 queries and extra processing in my application code to map users to the posts.</li>\n</ol>\n\n<p>I'm sure I'm not the first one facing this issue but unfortunately I have not found any best practices on this topic so far. Opinions?</p>\n    ","a":"\n<p>Both are valid solutions, the advantage of solution 1 is that you can show a page like this with retrieving only one document from the db. Users don't update their profile very often and you can update all posts and embedded comments async after a user profile is changed. You can index the posts and embedded comments on userid so the update should be fast. Updating in mongodb is very speedy because mongodb does an update-in-place and you can't rollback or commit so mongodb doesn't have to log the changes. </p>\n\n<p>However users on sites like stackoverflow also have a reputation and this reputation changes a lot more than their profile. </p>\n\n<p>Solution 2 requires the retrieving of more documents per page, however you can use the $in operator with a list of userid's (userid of post+userid's of comments) so you only need two \"selects statements\".  </p>\n    "},{"t":"Embedded non-relational (nosql) data store [closed]","l":"http://stackoverflow.com/questions/2081080/embedded-non-relational-nosql-data-store","q":"\n\n<p>I'm thinking about using/implementing some kind of an embedded key-value (or document) store for my Windows desktop application. I want to be able to store various types of data (GPS tracks would be one example) and of course be able to query this data. The amount of data would be such that it couldn't all be loaded into memory at the same time.</p>\n\n<p>I'm thinking about using sqlite as a storage engine for a key-value store, something like <a href=\"http://yserial.sourceforge.net/\">y-serial</a>, but written in .NET. I've also read about <a href=\"http://bret.appspot.com/entry/how-friendfeed-uses-mysql\">FriendFeed's usage of MySQL to store schema-less data</a>, which is a good pointer on how to use RDBMS for non-relational data. sqlite seems to be a good option because of its simplicity, portability and library size. </p>\n\n<p>My question is whether there are any other options for an embedded non-relational store? It doesn't need to be distributable and it doesn't have to support transactions, but it does have to be accessible from .NET and it should have a small download size.</p>\n\n<p>UPDATE: I've found an article titled <a href=\"http://www.sqlite.org/cvstrac/wiki?p=KeyValueDatabase\">SQLite as a Key-Value Database </a> which compares sqlite with Berkeley DB, which is an embedded key-value store library.</p>\n    ","a":"\n<p>Windows has a built-in embedded non-relational store. It is called ESENT and is used by several Windows applications, including the Active Directory and Windows Desktop Search.</p>\n\n<p><a href=\"http://blogs.msdn.com/windowssdk/archive/2008/10/23/esent-extensible-storage-engine-api-in-the-windows-sdk.aspx\">http://blogs.msdn.com/windowssdk/archive/2008/10/23/esent-extensible-storage-engine-api-in-the-windows-sdk.aspx</a></p>\n\n<p>If you want .NET access you can use the ManagedEsent layer on CodePlex.</p>\n\n<p><a href=\"http://managedesent.codeplex.com/\">http://managedesent.codeplex.com/</a></p>\n\n<p>That project has a PersistentDictionary class that implements a key-value store that implements the IDictionary interface, but is backed by a database.</p>\n    "},{"t":"MongoDB vs. Cassandra vs. MySQL for real-time advertising platform","l":"http://stackoverflow.com/questions/6162789/mongodb-vs-cassandra-vs-mysql-for-real-time-advertising-platform","q":"\n\n<p>I'm working on a real-time advertising platform with a heavy emphasis on performance. I've always developed with MySQL, but I'm open to trying something new like MongoDB or Cassandra if significant speed gains can be achieved. I've been reading about both all day, but since both are being rapidly developed, a lot of the information appears somewhat dated.</p>\n\n<p>The main data stored would be entries for each click, incremented rows for views, and information for each campaign (just some basic settings, etc). The speed gains need to be found in inserting clicks, updating view totals, and generating real-time statistic reports. The platform is developed with PHP.</p>\n\n<p>Or maybe none of these?</p>\n    ","a":"\n<p>There are several ways to achieve this with all of the technologies listed. It is more a question of how you use them. Your ideal solution may use a combination of these, with some consideration for usage patterns. I don't feel that the information out there is that dated because the concepts at play are very fundamental. There may be new NoSQL databases and fixes to existing ones, but your question is primarily architectural.</p>\n\n<p>NoSQL solutions like MongoDB and Cassandra get a lot of attention for their insert performance. People tend to complain about the update/insert performance of relational databases but there are ways to mitigate these issues.</p>\n\n<p>Starting with MySQL you could review O'Reilly's <a href=\"http://oreilly.com/catalog/9780596101718\">High Performance MySQL</a>, optimise the schema, add more memory perhaps run this on different hardware from the rest of your app (assuming you used MySQL for that), or partition/shard data. Another area to consider is your application. Can you queue inserts and updates at the application level before insertion into the database? This will give you some flexibility and is probably useful in all cases. Depending on how your final schema looks, MySQL will give you some help with extracting the data as long as you are comfortable with SQL. This is a benefit if you need to use 3rd party reporting tools etc.</p>\n\n<p>MongoDB and Cassandra are different beasts. My understanding is that it was easier to add nodes to the latter but this has changed since MongoDB has replication etc built-in. Inserts for both of these platforms are not constrained in the same manner as a relational database. Pulling data out is pretty quick too, and you have a lot of flexibility with data format changes. The tradeoff is that you can't use SQL (a benefit for some) so getting reports out may be trickier. There is nothing to stop you from collecting data in one of these platforms and then importing it into a MySQL database for further analysis.</p>\n\n<p>Based on your requirements there are tools other than NoSQL databases which you should look at such as <a href=\"http://www.cloudera.com/blog/2010/07/whats-new-in-cdh3b2-flume/\">Flume</a>. These make use of the Hadoop platform which is used extensively for analytics. These may have more flexibility than a database for what you are doing. There is some content from <a href=\"http://www.cloudera.com/company/press-center/hadoop-world-nyc/agenda/\">Hadoop World</a> that you might be interested in.</p>\n    "},{"t":"How to load initial data in MongoDB?","l":"http://stackoverflow.com/questions/3434311/how-to-load-initial-data-in-mongodb","q":"\n\n<p>Does anyone know how to populate mongodb with initial data? For example, with a traditional SQL database, you can put all your SQL statements in a textfile, then load that using a SQL command. This is extremely useful for unit testing.</p>\n\n<p>Is it possible to do this with the mongo shell? For example, write down a list of shell statements into a file, and get the mongo shell to read the file and execute the statements.</p>\n    ","a":"\n<p>You can use the <a href=\"http://www.mongodb.org/display/DOCS/Import+Export+Tools#ImportExportTools-mongoimport\">mongoimport</a> tool that comes with MongoDB to import <strong>raw data</strong>.</p>\n\n<p>To <strong>run scripts</strong> from a file, e.g. to recreate indexes, pass the file name as a command line argument:</p>\n\n<pre><code>mongo file.js \"another file.js\"\n</code></pre>\n    "},{"t":"Is there something like Redis DB, but not limited with RAM size? [closed]","l":"http://stackoverflow.com/questions/18447380/is-there-something-like-redis-db-but-not-limited-with-ram-size","q":"\n\n<p>I'm looking for a database matching these criteria:</p>\n\n<ul>\n<li>May be non-persistent;</li>\n<li>Almost all keys of DB need to be updated once in 3-6 hours (100M+ keys with total size of 100Gb)</li>\n<li>Ability to quickly select data by key (or Primary Key)</li>\n<li>This needs to be a DBMS (so LevelDB doesn't fit)</li>\n<li>When data is written, DB cluster must be able to serve queries (single nodes can be blocked though)</li>\n<li>Not in-memory â€“ our dataset will exceed the RAM limits</li>\n<li>Horizontal scaling and replication</li>\n<li>Support full rewrite of all data (MongoDB doesn't clear space after deleting data)</li>\n<li>C# and Java support</li>\n</ul>\n\n<p>Here's my process of working with such database:\nWe've got an analytics cluster that produces 100M records (50GB) of data every 4-6 hours. The data is a \"key - array[20]\". This data needs to be distributed to users through a front-end system with a rate of 1-10k requests per second. In average, only ~15% of the data is requested, the rest of it will be rewritten in 4-6 hours when the next data set is generated.</p>\n\n<p>What i tried:</p>\n\n<ol>\n<li>MongoDB. Datastorage overhead, high defragmentation costs.</li>\n<li>Redis. Looks perfect, but it's limited with RAM and our data exceeds it.</li>\n</ol>\n\n<p>So the question is: is there anything like Redis, but not limited with RAM size?</p>\n    ","a":"\n<p><strong>Yes, there are two alternatives to Redis that are not limited by RAM size while remaining compatible with Redis protocol:</strong></p>\n\n<p>Ardb (C++), replication(Master-Slave/Master-Master): <a href=\"https://github.com/yinqiwen/ardb\">https://github.com/yinqiwen/ardb</a></p>\n\n<blockquote>\n  <p>A redis-protocol compatible persistent storage server, support\n  LevelDB/KyotoCabinet/LMDB as storage engine.</p>\n</blockquote>\n\n<p>Edis (Erlang): <a href=\"http://inaka.github.io/edis/\">http://inaka.github.io/edis/</a></p>\n\n<blockquote>\n  <p>Edis is a protocol-compatible Server replacement for Redis, written in\n  Erlang. Edis's goal is to be a drop-in replacement for Redis when\n  persistence is more important than holding the dataset in-memory. Edis\n  (currently) uses Google's leveldb as a backend.</p>\n</blockquote>\n\n<p><strong>And for completeness here is another data-structures database:</strong></p>\n\n<p>Hyperdex (Strings, Integers, Floats, Lists, Sets, Maps): <a href=\"http://hyperdex.org/doc/04.datatypes/\">http://hyperdex.org/doc/04.datatypes/</a></p>\n\n<blockquote>\n  <p>HyperDex is:</p>\n  \n  <ul>\n  <li>Fast: HyperDex has lower latency, higher throughput, and lower\n  variance than other key-value stores. </li>\n  <li>Scalable: HyperDex scales as\n  more machines are added to the system. </li>\n  <li>Consistent: HyperDex guarantees\n  linearizability for key-based operations. Thus, a read always returns\n  the latest value inserted into the system. Not just â€œeventually,â€ but\n  immediately and always. </li>\n  <li>Fault Tolerant: HyperDex automatically\n  replicates data on multiple machines so that concurrent failures, up\n  to an application-determined limit, will not cause data loss.\n  Searchable: </li>\n  <li>HyperDex enables efficient lookups of secondary data\n  attributes. </li>\n  <li>Easy-to-Use: HyperDex provides APIs for a variety of\n  scripting and native languages. </li>\n  <li>Self-Maintaining: A HyperDex is\n  self-maintaining and requires little user maintenance.</li>\n  </ul>\n</blockquote>\n    "},{"t":"What are some â€œmental stepsâ€ a developer must take to begin moving from SQL to NO-SQL (CouchDB, FathomDB, MongoDB, etc)?","l":"http://stackoverflow.com/questions/2297565/what-are-some-mental-steps-a-developer-must-take-to-begin-moving-from-sql-to-n","q":"\n\n<p>I have my mind firmly wrapped around relational databases and how to code efficiently against them. Most of my experience is with MySQL and SQL. I like many of the things I'm hearing about document-based databases, especially when someone in a recent podcast mentioned huge performance benefits. So, if I'm going to go down that road, what are some of the mental steps I must take to shift from SQL to NO-SQL?</p>\n\n<p>If it makes any difference in your answer, I'm a C# developer primarily (today, anyhow). I'm used to ORM's like EF and Linq to SQL. Before ORMs, I rolled my own objects with generics and datareaders. Maybe that matters, maybe it doesn't. </p>\n\n<p>Here are some more specific:</p>\n\n<ol>\n<li>How do I need to think about joins?</li>\n<li>How will I query without a SELECT statement?</li>\n<li>What happens to my existing stored objects when I add a property in my code?</li>\n</ol>\n\n<p>(feel free to add questions of your own here)</p>\n    ","a":"\n<p>Firstly, each NoSQL store is different. So it's not like choosing between Oracle or Sql Server or MySQL. The differences between them can be vast.</p>\n\n<p>For example, with CouchDB you cannot execute ad-hoc queries (dynamic queries if you like). It is very good at online - offline scenarios, and is small enough to run on most devices. It has a RESTful interface, so no drivers, no ADO.NET libraries. To query it you use MapReduce (now this is very common across the NoSQL space, but not ubiquitous) to create views, and these are written in a number of languages, though most of the documentation is for Javascript. CouchDB is also designed to crash, which is to say if something goes wrong, it just restarts the process (the Erlang process, or group of linked processes that is, not the entire CouchDB instance typically).</p>\n\n<p>MongoDB is designed to be highly performant, has drivers, and seems like less of a leap for a lot of people in the .NET world because of this. I believe though that in crash situations it is possible to lose data (it doesn't offer the same level of transactional guarantees around writes that CouchDB does).</p>\n\n<p>Now both of these are document databases, and as such they share in common that their data is unstructured. There are no tables, no defined schema - they are schemaless. They are not like a key-value store though, as they do insist that the data you persist is intelligible to them. With CouchDB this means the use of JSON, and with MongoDB this means the use of BSON.</p>\n\n<p>There are many other differences between MongoDB and CouchDB and these are considered in the NoSQL space to be very close in their design!</p>\n\n<p>Other than document databases, their are network oriented solutions like Neo4J, columnar stores (column oriented rather than row oriented in how they persist data), and many others.</p>\n\n<p>Something which is common across most NoSQL solutions, other than MapReduce, is that they are not relational databases, and that the majority do not make use of SQL style syntax. Typcially querying follows an imperative mode of programming rather than the declarative style of SQL.</p>\n\n<p>Another typically common trait is that absolute consistency, as typically provided by relational databases, is traded for eventual models of consistency.</p>\n\n<p>My advice to anyone looking to use a NoSQL solution would be to first really understand the requirements they have, understand the SLAs (what level of latency is required; how consistent must that latency remain as the solutions scales; what scale of load is anticipated; is the load consistent or will it spike; how consistent does a users view of the data need to be, should they always see their own writes when they query, should their writes be immediately visible to all other users; etc...). Understand that you can't have it all, read up on Brewers CAP theorum, which basically says you can't have absolute consistence, 100% availability, and be partition tolerant (cope when nodes can't communicate). Then look into the various NoSQL solutions and start to eliminate those which are not designed to meet your requirements, understand that the move from a relational database is not trivial and has a cost associated with it (I have found the cost of moving an organisation in that direction, in terms of meetings, discussions, etc... itself is very high, preventing focus on other areas of potential benefit). Most of the time you will not need an ORM (the R part of that equation just went missing), sometimes just binary serialisation may be ok (with something like DB4O for example, or a key-value store), things like the Newtonsoft JSON/BSON library may help out, as may automapper. I do find that working with C#3 theere is a definite cost compared to working with a dynamic language like, say Python. With C#4 this may improve a little with things like the ExpandoObject and Dynamic from the DLR.</p>\n\n<p>To look at your 3 specific questions, with all it depends on the NoSQL solution you adopt, so no one answer is possible, however with that caveat, in very general terms:</p>\n\n<ol>\n<li><p>If persisting the object (or aggregate more likely) as a whole, your joins will typically be in code, though you can do some of this through MapReduce.</p></li>\n<li><p>Again, it depends, but with Couch you would execute a GET over HTTP against either a specific resource, or against a MapReduce view.</p></li>\n<li><p>Most likely nothing. Just keep an eye-out for the serialisation, deserialisation scenarios. The difficulty I have found comes in how you manage versions of your code. If the property is purely for pushing to an interface (GUI, web service) then it tends to be less of an issue. If the property is a form of internal state which behaviour will rely on, then this can get more tricky.</p></li>\n</ol>\n\n<p>Hope it helps, good luck!</p>\n    "},{"t":"Any detailed and specific reasons for Why MongoDB is much faster than SQL DBs?","l":"http://stackoverflow.com/questions/11244553/any-detailed-and-specific-reasons-for-why-mongodb-is-much-faster-than-sql-dbs","q":"\n\n<p>Ok, there are questions about <a href=\"http://stackoverflow.com/questions/5186707/why-is-mongodb-so-fast\">Why Is MongoDB So Fast</a></p>\n\n<p>I appreciate those answers, however, they are quite general. Yes, I know: </p>\n\n<ul>\n<li>MongoDB is document-based, then why being document-based can lead to\nmuch higher speed?</li>\n<li>MongoDB is noSQL, but why noSQL means higher performance?</li>\n<li>SQL does a lot more than MongoDB for consistency, ACID, etc, but I believe MongoDB is also doing something similar to keep data safe, maintain indexing, etc, right?</li>\n</ul>\n\n<p>Ok, I write this question just in order to find out </p>\n\n<ol>\n<li><strong>what are the detailed and specific reasons</strong> for MongoDB's high performance?</li>\n<li>What <strong>exactly</strong> SQL does, but MongoDB does not do, so it gains very high performance?</li>\n<li>If an interviewer (a MongoDB and SQL expert) asks you <code>\"Why MongoDB is so fast\"</code>, how would you answer? Obviously just answering: <code>\"because MongoDB is noSQL\"</code> is not enough.</li>\n</ol>\n\n<p>Thanks</p>\n    ","a":"\n<p>First, let's compare apples with apples: <strong>Reads and writes with MongoDB are like single reads and writes by primary key on a table with no non-clustered indexes in an RDBMS.</strong></p>\n\n<p>So lets benchmark exactly that: <a href=\"http://mysqlha.blogspot.de/2010/09/mysql-versus-mongodb-yet-another-silly.html\">http://mysqlha.blogspot.de/2010/09/mysql-versus-mongodb-yet-another-silly.html</a></p>\n\n<p>And it turns out, the speed difference in a fair comparison of exactly the same primitive operation is not big. <strong>In fact, MySQL is slightly faster.</strong> I'd say, they are equivalent.</p>\n\n<p>Why? Because actually, both systems are doing similar things in this particular benchmark. Returning a single row, searched by primary key, is actually not that much work. It is a very fast operation. I suspect that cross-process communication overheads are a big part of it.</p>\n\n<p>My guess is, that the more tuned code in MySQL outweighs the slightly less systematic overheads of MongoDB (no logical locks and probably some other small things).</p>\n\n<p>This leads to an interesting conclusion: <strong>You can use MySQL like a document database and get excellent performance out of it.</strong></p>\n\n<hr>\n\n<p>If the interviewer said: \"We don't care about documents or styles, we just need a much faster database, do you think we should use MySQL or MongoDB?\", what would I answer?</p>\n\n<p>I'd recommend to disregard performance for a moment and look at the relative strength of the two systems. Things like scaling (way up) and replication come to mind for MongoDB. For MySQL, there are a lot more features like rich queries, concurrency models, better tooling and maturity and lots more.</p>\n\n<p>Basically, you can trade features for performance. Are willing to do that? That is a choice that cannot be made generally. If you opt for performance at any cost, consider tuning MySQL first before adding another technology.</p>\n\n<hr>\n\n<p>Here is what happens when a client retrieves a single row/document by primary key. I'll annotate the differences between both systems:</p>\n\n<ol>\n<li>Client builds a binary command (same)</li>\n<li>Client sends it over TCP (same)</li>\n<li>Server parses the command (same)</li>\n<li>Server accesses query plan from cache (SQL only, not MongoDB, not HandlerSocket)</li>\n<li>Server asks B-Tree component to access the row (same)</li>\n<li>Server takes a physical readonly-lock on the B-Tree path leading to the row (same)</li>\n<li>Server takes a logical lock on the row (SQL only, not MongoDB, not HandlerSocket)</li>\n<li>Server serializes the row and sends it over TCP (same)</li>\n<li>Client deserializes it (same)</li>\n</ol>\n\n<p>There are only two additional steps for typical SQL-bases RDBMS'es. <strong>That's why there isn't really a difference.</strong></p>\n    "},{"t":"NoSQL vs. SQL when scalability is irrelevant","l":"http://stackoverflow.com/questions/2534408/nosql-vs-sql-when-scalability-is-irrelevant","q":"\n\n<p>Recently I have read a lot about different NoSQL databases and how they are being effectively deployed by some major websites out there.  I'm starting a project in which I think the schema-free nature of a database such as MongoDB would be tremendously useful.  Everything I have read though seems to indicate that the main advantage of a NoSQL database is scalability.  Is choosing a NoSQL database for the schema-free design just as legitimate a design decision as that of scalability?</p>\n    ","a":"\n<p>Yes, sometimes RDBMS are not the best solution, although there are ways to accomodate user defined fields (see XML Datatype, EAV design pattern, or just have spare generic columns) sometimes a schema free database is a good choice.  </p>\n\n<p>However, you need to nail down your requirements before choosing to go with a document database, as you will loose a lot of the power you may be used to with the relational model </p>\n\n<p>eg...</p>\n\n<p>If you would otherwise have multiple tables in your RDBMS database, you will need to research the features MongoDB affords you to accomodate these needs.  </p>\n\n<p>If you will need to query the data in specific ways, again you need to research what MongoDB offers you.  </p>\n\n<p>I wouldnt think of NoSQL as replacement for RDBMS, rather a slightly different tool that brings its own sets of advantages and disadvantages making it more suitable for some projects than others.  </p>\n\n<p>(Both databases may be used in some circumstances. Also if you decide to go down the route of possibly using MongoDB, once you have researched the websites out there and have more specific questions, you can visit Freenode IRC #mongodb channel)</p>\n    "},{"t":"File Storage for Web Applications: Filesystem vs DB vs NoSQL engines","l":"http://stackoverflow.com/questions/2890452/file-storage-for-web-applications-filesystem-vs-db-vs-nosql-engines","q":"\n\n<p>I have a web application that stores a lot of user generated files.  Currently these are all stored on the server filesystem, which has several downsides for me.</p>\n\n<ul>\n<li>When we move \"folders\" (as defined by our application) we also have to move the files on disk (although this is more due to strange design decisions on the part of the original developers than a requirement of storing things on the filesystem).</li>\n<li>It's hard to write tests for file system actions; I have a mock filesystem class that logs actions like move, delete etc, without performing them, which more or less does the job, but I don't have 100% confidence in the tests.</li>\n<li>I will be adding some other jobs which need to access the files from other service to perform additional tasks (e.g. indexing in Solr, generating thumbnails, movie format conversion), so I need to get at the files remotely.  Doing this over network shares seems dodgy...</li>\n<li>Dealing with permissions on the filesystem as sometimes given us problems in the past, although now that we've moved to a pure Linux environment this should be less of an issue.</li>\n</ul>\n\n<p>So, my main questions are</p>\n\n<ul>\n<li>What are the downsides of storing files as BLOBs in MySQL?</li>\n<li>Do the same problems exist with NoSQL systems like Cassandra?</li>\n<li>Does anyone have any other suggestions that might be appropriate, e.g. MogileFS, etc?</li>\n</ul>\n    ","a":"\n<p>Not a direct answer but some pointers to very interesting and somehow similar questions (yeah, they are about blobs and images but this is IMO comparable).</p>\n\n<blockquote>\n  <p>What are the downsides of storing files as BLOBs in MySQL?</p>\n</blockquote>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3748/storing-images-in-db-yea-or-nay\">Storing Images in DB - Yea or Nay?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/2517752/images-in-database-vs-file-system\">Images in database vs file system</a></li>\n<li><a href=\"http://stackoverflow.com/search?q=images+database+filesystem\">http://stackoverflow.com/search?q=images+database+filesystem</a></li>\n</ul>\n\n<blockquote>\n  <p>Do the same problems exist with NoSQL systems like Cassandra?</p>\n</blockquote>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/2776208/nosql-for-filesystem-storage-organization-and-replication\">NoSQL for filesystem storage organization and replication?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/2278186/storing-images-in-nosql-stores\">Storing images in NoSQL stores</a></li>\n</ul>\n\n<p>PS: I don't want to be the killjoy but I don't think that any NoSQL solution is going to solve your problem (NoSQL is just irrelevant for most businesses). </p>\n    "},{"t":"Mongodb and PostgreSql thoughts","l":"http://stackoverflow.com/questions/4426540/mongodb-and-postgresql-thoughts","q":"\n\n<p>I've got an app fully working with PostgreSql. After reading about Mongodb, I was interested to see how the app would work with it. After a few weeks, I migrated the whole system to Mongodb.</p>\n\n<p>I like a few things with Mongodb. However, I found certain queries I was doing in PostgreSql, I couldn't do efficiently in Mongodb. Especially, when I had to join several tables to calculate some logic. For example, <a href=\"http://stackoverflow.com/questions/4366417/need-suggestions-on-designing-artist-recommendation\">this</a>.</p>\n\n<p>Moreover, I am using Ruby on Rails 3, and an ODM called Mongoid. Mongoid is still in beta release. Documentation was good, but again, at times I found the ODM to be very limiting compared to what Active Record offered with traditional (SQL) database systems.</p>\n\n<p>Even to this date, I feel more comfortable working with PostgreSql than Mongodb. Only because I can join tables and do anything with the data. </p>\n\n<p>I've made two types of backups. One with PostgreSql and the other with Mongodb. Some say, some apps are more suitable with one or the other type of db. Should I continue with Mongodb and eventually hope for its RoR ODM (Mongoid) to fully mature, or should I consider using PostgreSql? </p>\n\n<p>A few more questions:\n1) Which one would be more suitable for developing a social networking site similar to Facebook.\n2) Which one would be more suitable for 4-page standard layout type of website (Home, Products, About, Contact)</p>\n    ","a":"\n<p>You dumped a decades-tested, fully featured RDBMS for a young, beta-quality, feature-thin document store with little community support. Unless you're already running tens of thousands of dollars a month in servers and think MongoDB was a better fit for the nature of your data, you probably wasted a lot of time for negative benefit. MongoDB is fun to toy with, and I've built a few apps using it myself for that reason, but it's almost never a better choice than Postgres/MySQL/SQL Server/etc. for production applications.</p>\n    "},{"t":"Update field in exact element array in MongoDB","l":"http://stackoverflow.com/questions/10432677/update-field-in-exact-element-array-in-mongodb","q":"\n\n<p>I have document structure like this:</p>\n\n<pre><code>{\n_id:\"43434\",\nheroes : [\n    { nickname : \"test\",  items : [\"\", \"\", \"\"] },\n    { nickname : \"test2\", items : [\"\", \"\", \"\"] },\n]\n}\n</code></pre>\n\n<p>Can I <code>$set</code> the second element of the <code>items</code> array of the embedded object in array <code>heros</code> with <code>nickname</code> <code>\"test\"</code> ?</p>\n\n<p><strong>Result:</strong></p>\n\n<pre><code>{\n_id:\"43434\",\nheroes : [\n    { nickname : \"test\",  items : [\"\", \"new_value\", \"\"] }, // modified here\n    { nickname : \"test2\", items : [\"\", \"\", \"\"] },\n]\n}\n</code></pre>\n\n<p>Thanks for help.</p>\n    ","a":"\n<p>You need to make use of 2 concepts: <a href=\"http://docs.mongodb.org/manual/reference/operator/update/positional/\">mongodb's positional operator</a> and simply using the numeric index for the entry you want to update.</p>\n\n<p>The positional operator allows you to use a condition like this:</p>\n\n<pre><code>{\"heros.nickname\": \"test\"}\n</code></pre>\n\n<p>and then reference the found array entry like so:</p>\n\n<pre><code>{\"heros.$  // &lt;- the dollar represents the matching array key index\n</code></pre>\n\n<p>As you want to update the 2nd array entry in \"items\", and array keys are 0 indexed - that's the key 1.</p>\n\n<p>So:</p>\n\n<pre><code>&gt; db.denis.insert({_id:\"43434\", heros : [{ nickname : \"test\",  items : [\"\", \"\", \"\"] }, { nickname : \"test2\", items : [\"\", \"\", \"\"] }]});\n&gt; db.denis.update(\n    {\"heros.nickname\": \"test\"}, \n    {$set: {\n        \"heros.$.items.1\": \"new_value\"\n    }}\n)\n&gt; db.denis.find()\n{\n    \"_id\" : \"43434\", \n    \"heros\" : [\n        {\"nickname\" : \"test\", \"items\" : [\"\", \"new_value\", \"\" ]},\n        {\"nickname\" : \"test2\", \"items\" : [\"\", \"\", \"\" ]}\n    ]\n}\n</code></pre>\n    "},{"t":"Proper way to import json file to mongo","l":"http://stackoverflow.com/questions/10999023/proper-way-to-import-json-file-to-mongo","q":"\n\n<p>I've been trying to use mongo with some data imported, but I'm not able to use it properly with my document description.</p>\n\n<p>This is an example of the .json I import using mongoimport: <a href=\"https://gist.github.com/2917854\">https://gist.github.com/2917854</a> </p>\n\n<pre><code>mongoimport -d test -c example data.json\n</code></pre>\n\n<p>I noticed that all my document it's imported to a unique object in spite of creating one of object for each shop.</p>\n\n<p>That's why when I try to find a shop or anything I want to query, all the document is returned.</p>\n\n<pre><code>db.example.find({\"shops.name\":\"x\"})\n</code></pre>\n\n<p>I want to be able to query the db to obtain products by the id using dot notation something similar to:</p>\n\n<pre><code>db.example.find({\"shops.name\":\"x\",\"categories.type\":\"shirts\",\"clothes.id\":\"1\"}\n</code></pre>\n\n<p>The problem is that all the document is imported like a single object. The question is: How<br>\ndo I need to import the object to obtain my desired result?</p>\n    ","a":"\n<p><a href=\"http://www.mongodb.org/display/DOCS/Import+Export+Tools#ImportExportTools-mongoimport\">Docs</a> note that:</p>\n\n<blockquote>\n  <p>This utility takes a single file that contains 1 JSON/CSV/TSV string per line and inserts it.</p>\n</blockquote>\n\n<p>In the structure you are using -assuming the errors on the gist are fixed- you are essentially importing one document with only <code>shops</code> field.</p>\n\n<p>After breaking the data into separate shop docs, import using something like (shops being the collection name, makes more sense than using <code>example</code>):</p>\n\n<pre><code>mongoimport -d test -c shops data.json\n</code></pre>\n\n<p>and then you can query like:</p>\n\n<pre><code>db.shops.find({\"name\":x,\"categories.type\":\"shirts\"})\n</code></pre>\n    "},{"t":"Full-text search in NoSQL databases","l":"http://stackoverflow.com/questions/5453872/full-text-search-in-nosql-databases","q":"\n\n<ul>\n<li>Has anyone here have any experience deploying a real online system that had a full text search in any of the NoSQL databases? </li>\n<li>For example, how does the full-text search compare in MongoDB, Riak and CouchDB? </li>\n<li>Some of the metric that I am looking for is ease of deployment and maintaince and of course speed.</li>\n<li>How mature are they? Are they any replacement for the Lucene infrastructure?</li>\n</ul>\n\n<p>Thanks.</p>\n    ","a":"\n<p>None of the existing \"NoSQL\" database provides a reasonable implementation of something that could be named \"fulltext search\". MongoDB in particular has barely nothing so far (matching using regular expressions is not fulltext search and searching using $in or $all operators on a keyword word list is just a very poor implementation of a \"fulltext search\"). Using Solr, ElasticSearch or Sphinx is straight forward - an implementation and integration on the application level. Your choice widely depends on you requirements and current setup.</p>\n    "},{"t":"Query on Mongoid Hash Field","l":"http://stackoverflow.com/questions/4242289/query-on-mongoid-hash-field","q":"\n\n<p>I want to query on a Hash field for a Mongoid Class. I'm not sure how I can do this with conditions?</p>\n\n<p>Here is an example:</p>\n\n<pre><code>class Person\n  include Mongoid::Document\n\n  field :things, :type =&gt; Hash\nend\n</code></pre>\n\n<p>So, let's say that I have this:</p>\n\n<pre><code>p = Person.new\np.things = {}\np.things[:tv] = \"Samsung\"\n</code></pre>\n\n<p>I want to query for the first person with a tv that is a Samsung...</p>\n\n<pre><code>People.first(:conditions =&gt; ?????\n</code></pre>\n\n<p>Thanks in advance.</p>\n    ","a":"\n<pre><code>Person.where('things.tv' =&gt; 'Samsung').first\n</code></pre>\n\n<p>This is where Mongoid and MongoDB really shine. Mongoid's Criteria methods (<code>Person.where</code>, <code>Person.any_of</code>, <code>Person.excludes</code>, etc.) will give you much more flexibility than the ActiveRecord-style finders (passing a <code>:conditions</code> hash to <code>Person.find</code>, <code>Person.first</code>, etc.)</p>\n\n<p>Mongoid's site has some great documentation on how to use <code>Criteria</code>:</p>\n\n<p><a href=\"http://mongoid.org/en/mongoid/docs/querying.html\">http://mongoid.org/en/mongoid/docs/querying.html</a></p>\n    "},{"t":"Mongo DB Design, embedding vs relationships","l":"http://stackoverflow.com/questions/3813975/mongo-db-design-embedding-vs-relationships","q":"\n\n<p>I'm building a simple accounting system where a user has many bills.  Now I'm trying to decide if bills should be its own collection, or nested within the user.  I'm leaning towards the former but I've NEVER done any noSQL stuff so I'm just going by trial and error and what I think makes sense to me.  </p>\n\n<p>I understand that Mongo has a 4mb document size limit which is what's making me think that I should have a separate collection for bills, as these will accumulate daily and could eventually take up a large amount of space.</p>\n\n<p>I'm just looking for opinions on the matter.  Basically I'll be querying for bills of a user between different date periods (as you can imagine an accounting system would do).</p>\n\n<p>Not that it really matters but I'm using Mongoid in a Rails3 project.  I figured I'd do something like:</p>\n\n<pre><code>class User\n  references_many :bills\nend\n\nclass Bill\n  referenced_in :user\nend\n</code></pre>\n\n<p>Any comments or design suggestions are greatly appreciated.</p>\n    ","a":"\n<p>1) Regarding the 4MB document limit, this is what the \"MongoDB: The Definitive Guide\" says :</p>\n\n<blockquote>\n  <p>Documents larger than 4MB (when converted to BSON) cannot be saved to the database. This is a somewhat arbitrary limit (and may be raised in the future); it is mostly to prevent bad schema design and ensure consistent performance. To see the BSON size (in bytes) of the document <em>doc</em>, run Object.bsonsize(<em>doc</em>) from the shell.</p>\n  \n  <p>To give you an idea of how much 4MB is, the entire text of <em>War and Peace</em> is just 3.14MB.</p>\n</blockquote>\n\n<p>In the end it depends on how big you expect the bills for a user to grow. I hope the excerpt above gives you an idea of the limits imposed by the document size.</p>\n\n<p>2) De-normalized schema (bills go with the user document) is the way to go if you know that you are never going to run global queries on bills (example of such a query is if you want to retrieve the ten most recent bills entered into the system). You will have to use map-reduce to retrieve results for such queries if you use a denormalized schema. </p>\n\n<p>Normalized schema (user and bills in separate documents) is a better choice if you want flexibility in how the bills are queried. However, since MongoDB doesn't support joins, you will have to run multiple queries every time you want to retrieve the bills corresponding to a user.</p>\n\n<p>Given the use-case you mentioned, I'd go with de-normalized schema.</p>\n\n<p>3) All updates in MongoDB are atomic and serialized. That should answer Steve's concern.</p>\n\n<p>You may find these slides helpful. <a href=\"http://www.slideshare.net/kbanker/mongodb-meetup\">http://www.slideshare.net/kbanker/mongodb-meetup</a> </p>\n\n<p>You may also look at MongoDB's Production Deployments page. You may find the SF.net slides helpful.</p>\n    "},{"t":"Filtering embedded documents in MongoDB","l":"http://stackoverflow.com/questions/2138454/filtering-embedded-documents-in-mongodb","q":"\n\n<p>I am having trouble grasping how to filter embedded documents in MongoDB, and am starting to think I should be using a relational association, but that feels wrong in the document-store context.</p>\n\n<p>Sticking with a typical blog/comment system, I have a collection of <code>blogs</code>, and each <code>blog</code> has many <code>comments</code>.  The comments are stored as embedded documents inside the blog document.</p>\n\n<p>It is very simple to filter my <code>blogs</code> collection, but in order to filter my <code>comments</code> embedded in each <code>blog</code>, I am having to load them all into memory (retrieve all into a Ruby array), and loop through each comment, returning ones that match a specific criteria.</p>\n\n<p>My efforts to filter embedded documents using dot notation is failing, and bringing back all sub documents.</p>\n\n<p>Is there a better way of getting MongoDB to filter these for me, or should I resign myself to relational associations? (Pulling back all embedded documents and manually filtering is going to be too intensive in the long run)</p>\n    ","a":"\n<p>There's currently no way to filter on embedded docs in the way you're describing. Using the dot notation allows you to match on an embedded doc, but the entire document, parent and all, will still be returned. It's also possible to select which fields will be returned, but that doesn't really help your case, either.</p>\n\n<p>We have a \"virtual collections\" case, which would implement the desired functionality; feel free to vote on it:</p>\n\n<p><a href=\"http://jira.mongodb.org/browse/SERVER-142\">http://jira.mongodb.org/browse/SERVER-142</a></p>\n\n<p>In the meantime, you should probably treat comments as their own collection. In general, if you need to work with a given data set on its own, make it a collection. If it's better conceived of as part of some other set, it's better to embed.</p>\n    "},{"t":"Proper NoSQL data schema for web photo gallery","l":"http://stackoverflow.com/questions/8948767/proper-nosql-data-schema-for-web-photo-gallery","q":"\n\n<p>I'm looking to build an appropriate data structure for NoSQL storage of a photo gallery. In my web application, a photo can be part of 1 or more albums. I have experience with MySQL, but almost none with key-value storage.</p>\n\n<p>With MySQL, I would have set up (3) tables as follows:</p>\n\n<pre><code>photos (photo_id, title, date_uploaded, filename)\nalbums (album_id, title, photo_id)\nalbum_photo_map (photo_id, album_id)\n</code></pre>\n\n<p>And then, to retrieve a list of the 5 latest photos (with album data), a query like this:</p>\n\n<pre><code>SELECT *\nFROM albums, photos, album_photo_map\nWHERE albums.album_id = album_photo_map.album_id AND\n                photos.photo_id = album_photo_map.photo_id\nORDER BY photos.date_uploaded DESC LIMIT 5;\n</code></pre>\n\n<p>How would I accomplish a similar query using a NoSQL key-value pair database? (Specifically, Amazon's DynamoDB.) What would the storage look like? How would the indexing work?</p>\n    ","a":"\n<p>Using mongodb lingo, your collections could look like this:</p>\n\n<pre><code>photos = [\n    {\n        _id: ObjectId(...),\n        title: \"...\",\n        date_uploaded: Date(...),\n        albums: [\n            ObjectId(...),\n            ...\n        ]\n    },\n    ...\n]\n\nalbums = [\n    {\n        _id: ObjectId(...),\n        title: \"...\"\n    }\n]\n</code></pre>\n\n<p>Finding the 5 newest photos would be done like this:</p>\n\n<pre><code>&gt; var latest = db.photos.find({}).sort({date_uploaded:1}).limit(5);\n</code></pre>\n\n<p>There's no server-side joins in mongo, so you'd have to fetch all the latest albums like this:</p>\n\n<pre><code>&gt; var latest_albums = latest.find({}, {albums: 1});\n</code></pre>\n\n<p>Of course, then you have to boil this down into a set.</p>\n\n<p>It's actually easier if you just embed the album inside the photo documents, since they're small:</p>\n\n<pre><code>photos = [\n    {\n        _id: ObjectId(...),\n        title: \"...\",\n        date_uploaded: Date(...),\n        albums: [\n            {name: \"family-vacation-2011\", title: \"My family vacation in 2010\"},\n            ...\n        ]\n    },\n    ...\n]\n</code></pre>\n\n<p>Then querying is the same, but you don't have to join.  Finding all photos in an album looks like:</p>\n\n<pre><code>&gt; db.photos.find({albums:{$elemMatch:{name: \"family-vacation-2011\"}}});\n</code></pre>\n    "},{"t":"How to sum the value of a key across all documents in a MongoDB collection","l":"http://stackoverflow.com/questions/4621300/how-to-sum-the-value-of-a-key-across-all-documents-in-a-mongodb-collection","q":"\n\n<p>I have collection in MongoDB:</p>\n\n<pre><code>{ \"_id\" : ObjectId(\"4d2407265ff08824e3000001\"), \"subida\" : 3.95 }\n{ \"_id\" : ObjectId(\"4d2551b4ae9fa739640df821\"), \"subida\" : 6.03 }\n{ \"_id\" : ObjectId(\"4d255b115ff08821c2000001\"), \"subida\" : 5.53 }\n{ \"_id\" : ObjectId(\"4d25e8d55ff08814f8000001\"), \"subida\" : 1.96 }\n</code></pre>\n\n<p>How I can sum the value of a key, e.g., <code>\"subida\"</code>, across all documents? With the documents above, I should receive something along the lines of:</p>\n\n<pre><code>{ \"subida\" : 17.47 }\n</code></pre>\n    ","a":"\n<p>I'd personally perform a mapreduce on the collection :</p>\n\n<p>map is a simple function emitting the \"subida\" field. The key should the same if you need a single sum; the result after reduce will yield the single object <code>{&lt;key&gt;: &lt;sum&gt;}</code>, with <code>&lt;key&gt;</code> being whatever value you supplied in the emit.</p>\n\n<pre><code>map = function() { emit(&lt;key&gt;, this.subida); }\n</code></pre>\n\n<p>reduce is also a simple function summing them :</p>\n\n<pre><code>red = function(k, v) {\n  var i, sum = 0;\n  for (i in v) {\n    sum += v[i];\n  }\n  return sum;\n}\n</code></pre>\n\n<p>You can then call mapreduce on your collection <code>&lt;mycollection&gt;</code>:</p>\n\n<pre><code>res = db.&lt;mycollection&gt;.mapReduce(map, red);\n</code></pre>\n\n<p>Which will create a temporary new collection you can manipulate like any other collection. The value returned by mapReduce holds several values regarding the mapReduce such as the time taken, status..., as well as the temp. collection name created in the \"result\" field.\nTo get the values you need, you have to query that collection :</p>\n\n<pre><code>db[res.result].find()\n</code></pre>\n\n<p>Which should give you the object <code>{&lt;key&gt;: &lt;sum&gt;}</code>.</p>\n\n<p>If you run MongoDB 1.7.4 or higher, you can save you a bit of hassle by asking MongoDB to return the result directly without creating a collection :</p>\n\n<pre><code>db.&lt;mycollection&gt;.mapReduce(map, red, {out : {inline: 1}});\n</code></pre>\n    "},{"t":"Basics of MongoDB Scripts - How to","l":"http://stackoverflow.com/questions/3595265/basics-of-mongodb-scripts-how-to","q":"\n\n<p>What are the basics of MongoDB Scripts?</p>\n\n<p>I think the script will end with <code>.js</code>, and we run it using <code>mongo try.js</code></p>\n\n<p>But if I put</p>\n\n<pre><code>print(db.foo.find())\n</code></pre>\n\n<p>in <code>try.js</code> and use <code>mongo try.js</code></p>\n\n<p>it will say</p>\n\n<pre><code>MongoDB shell version: 1.6.1\nconnecting to: test\nDBQuery: test.foo -&gt; undefined\n</code></pre>\n\n<p>and if I use the interactive shell by typing <code>mongo</code> and type</p>\n\n<pre><code>&gt; db.foo.find()\n{ \"_id\" : ObjectId(\"4c7a73428261000000003a7e\"), \"a\" : 1 }\n&gt; print(db.foo.find())\nDBQuery: test.foo -&gt; undefined\n</code></pre>\n\n<p>the <code>{a : 1}</code> was something I inserted earlier using <code>db.foo.insert({a:1})</code></p>\n\n<p>what are the proper ways of doing MongoDB Scripts and how to print things out like a Ruby <code>irb</code> or Python's <code>IDLE</code>?  thanks.  (Ruby's <code>puts a.inspect</code> or <code>p a</code> can both print out the whole structure of <code>a</code> usually (all the variable names and values in <code>a</code>))</p>\n\n<p><code>alert(db.foo.find())</code> and <code>console.log(db.foo.find())</code> won't work either.</p>\n    ","a":"\n<p>The external script files are executed outside of the shell context.</p>\n\n<p>The <code>db.foo.find()</code> database command only returns a cursor; it <strong>doesn't print anything</strong> by itself. When the command is issued from the shell, the shell will iterate the cursor and print the results. When the command is run from an external script file, nothing is printed.</p>\n\n<p>The <code>print()</code> command will print out the string representation of the object. In your case, it's the cursor:</p>\n\n<pre><code>&gt; print(db.foo.find())\nDBQuery: test.foo -&gt; undefined\n</code></pre>\n\n<p>If you need to print results of the query, you'll have to iterate the cursor in your script file and print each result, similarly to what the shell does:</p>\n\n<pre><code>function printResult (r) {\n  print(tojson(r))\n}\n\ndb.foo.find().forEach(printResult)\n</code></pre>\n    "},{"t":"nosql db for python [closed]","l":"http://stackoverflow.com/questions/5832531/nosql-db-for-python","q":"\n\n<p>what are the popular nosql databases that are used from python ? i know there are a few options as explained at <a href=\"http://nosql-database.org/\">http://nosql-database.org/</a> but what do python programmers use the most ?</p>\n\n<p>thanks</p>\n    ","a":"\n<p>Most of the nosql databases have python clients which are actively supported.  Pick your database based on your usage needs.  Using it from python shouldn't be a problem.<br>\nTo name a few:<br>\nCassandra: <a href=\"https://github.com/datastax/python-driver\">https://github.com/datastax/python-driver</a><br>\nRiak: <a href=\"https://github.com/basho/riak-python-client\">https://github.com/basho/riak-python-client</a><br>\nMongoDB: <a href=\"http://api.mongodb.org/python/current/\">http://api.mongodb.org/python/current/</a><br>\nCouchDB: <a href=\"http://wiki.apache.org/couchdb/Getting_started_with_Python\">http://wiki.apache.org/couchdb/Getting_started_with_Python</a><br>\nRedis: <a href=\"https://github.com/andymccurdy/redis-py\">https://github.com/andymccurdy/redis-py</a>  </p>\n    "},{"t":"NoSQL Solution for Persisting Graphs at Scale","l":"http://stackoverflow.com/questions/9302295/nosql-solution-for-persisting-graphs-at-scale","q":"\n\n<p>I'm hooked on using Python and NetworkX for analyzing graphs and as I learn more I want to use more and more data (guess I'm becoming a data junkie :-).  Eventually I think my NetworkX graph (which is stored as a dict of dict) will exceed the memory on my system.  I know I can probably just add more memory but I was wondering if there was a way to instead integrate NetworkX with Hbase or a similar solution?</p>\n\n<p>I looked around and couldn't really find anything but I also couldn't find anything related to allowing a simple MySQL backend as well.</p>\n\n<p>Is this possible? Does anything exist to allow for connectivity to some kind of persistant storage?</p>\n\n<p>Thanks!</p>\n\n<p>Update: I remember seeing this subject in 'Social Network Analysis for Startups', the author talks about other storage methods(including hbase, s3, etc..) but does not show how to do this or if its possible.</p>\n    ","a":"\n<p>There are two general types of containers for storing graphs:</p>\n\n<ol>\n<li><p><strong>true graph databases:</strong> e.g., <em>Neo4J</em>, <em>agamemnon</em>, <em>GraphDB</em>, and <em>AllegroGraph</em>; these not only store a graph but they also understand that a graph is, so for instance, you can query these\ndatabases e.g., <em>how many nodes are between the shortest path from\nnode X and node Y</em>?</p></li>\n<li><p><strong>static graph containers</strong>: Twitter's MySQL-adapted FlockDB is the most well-known exemplar here. These DBs can store and retrieve\ngraphs just fine; but to query the graph itself, you have to first\nretrieve the graph from the DB then use a library (e.g., Python's\nexcellent Networkx) to query the graph itself.</p></li>\n</ol>\n\n<p>The redis-based graph container i discuss below is in the second category, though apparently redis is also well-suited for containers in the first category as evidenced by <a href=\"http://pypi.python.org/pypi/redis_graph/1.0\">redis-graph</a>, a remarkably small python package for implementing a graph database in redis.</p>\n\n<p><strong>redis</strong> will work beautifully here. </p>\n\n<p><a href=\"http://www.redis.io\">Redis</a> is a heavy-duty, durable data store suitable for production use, yet it's also simple enough to use for command-line analysis.</p>\n\n<p>Redis is different than other databases in that it has multiple data structure types; the one i would recommend here is the <strong><em>hash</em></strong> data type. Using this redis data structure allows you to very closely mimic a \"list of dictionaries\", a conventional schema for storing graphs, in which each item in the list is a dictionary of edges keyed to the node from which those edges originate.</p>\n\n<p>You need to first install <a href=\"http://www.redis.io\">redis</a> and the python client. The <a href=\"http://degizmo.com/2010/03/22/getting-started-redis-and-python/\">DeGizmo Blog</a> has an excellent \"up-and-running\" tutorial which includes a <em>step-by-step</em> guid on installing both.</p>\n\n<p>Once redis and its python client are installed, <em>start a redis server</em>, which you do like so:</p>\n\n<ul>\n<li><p><em>cd</em> to the directory in which you installed redis (<em>/usr/local/bin</em> on 'nix if you installed via <em>make install</em>); next</p></li>\n<li><p>type <em>redis-server</em> at the shell prompt then enter</p></li>\n</ul>\n\n<p>you should now see the server log file tailing on your shell window</p>\n\n<pre><code>&gt;&gt;&gt; import numpy as NP\n&gt;&gt;&gt; import networkx as NX\n\n&gt;&gt;&gt; # start a redis client &amp; connect to the server:\n&gt;&gt;&gt; from redis import StrictRedis as redis\n&gt;&gt;&gt; r1 = redis(db=1, host=\"localhost\", port=6379)\n</code></pre>\n\n<p>In the snippet below, i have stored a four-node graph; each line below calls <em>hmset</em> on the redis client and stores one node and the edges connected to that node (\"0\" =&gt; no edge, \"1\" =&gt; edge). (In practice, of course, you would abstract these repetitive calls in a function; here i'm showing each call because it's likely easier to understand that way.)  </p>\n\n<pre><code>&gt;&gt;&gt; r1.hmset(\"n1\", {\"n1\": 0, \"n2\": 1, \"n3\": 1, \"n4\": 1})\n      True\n\n&gt;&gt;&gt; r1.hmset(\"n2\", {\"n1\": 1, \"n2\": 0, \"n3\": 0, \"n4\": 1})\n      True\n\n&gt;&gt;&gt; r1.hmset(\"n3\", {\"n1\": 1, \"n2\": 0, \"n3\": 0, \"n4\": 1})\n      True\n\n&gt;&gt;&gt; r1.hmset(\"n4\", {\"n1\": 0, \"n2\": 1, \"n3\": 1, \"n4\": 1})\n      True\n\n&gt;&gt;&gt; # retrieve the edges for a given node:\n&gt;&gt;&gt; r1.hgetall(\"n2\")\n      {'n1': '1', 'n2': '0', 'n3': '0', 'n4': '1'}\n</code></pre>\n\n<p>Now that the graph is persisted, retrieve it from the redis DB as a NetworkX graph. </p>\n\n<p>There are many ways to do this, below did it in <em>two</em> *steps*:</p>\n\n<ol>\n<li><p>extract the data from the redis database into an <em>adjacency matrix</em>,\nimplemented as a 2D NumPy array; then</p></li>\n<li><p>convert that directly to a NetworkX graph using a NetworkX\n<em>built-in</em> function:</p></li>\n</ol>\n\n<p>reduced to code, these two steps are:</p>\n\n<pre><code>&gt;&gt;&gt; AM = NP.array([map(int, r1.hgetall(node).values()) for node in r1.keys(\"*\")])\n&gt;&gt;&gt; # now convert this adjacency matrix back to a networkx graph:\n&gt;&gt;&gt; G = NX.from_numpy_matrix(am)\n\n&gt;&gt;&gt; # verify that G in fact holds the original graph:\n&gt;&gt;&gt; type(G)\n      &lt;class 'networkx.classes.graph.Graph'&gt;\n&gt;&gt;&gt; G.nodes()\n      [0, 1, 2, 3]\n&gt;&gt;&gt; G.edges()\n      [(0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (3, 3)]\n</code></pre>\n\n<p>When you end a redis session, you can shut down the server from the client like so:</p>\n\n<pre><code>&gt;&gt;&gt; r1.shutdown()\n</code></pre>\n\n<p>redis saves to disk just before it shuts down so this is a good way to ensure all writes were persisted.</p>\n\n<p>So where is the redis DB? It is stored in the default location with the default file name, which is <strong><em>dump.rdb</em></strong> on your home directory.</p>\n\n<p>To change this, edit the <strong><em>redis.conf</em></strong> file (included with the redis source distribution); go to the line starting with: </p>\n\n<pre><code># The filename where to dump the DB\ndbfilename dump.rdb\n</code></pre>\n\n<p>change dump.rdb to anything you wish, but leave the .rdb extension in place.</p>\n\n<p>Next, to change the file path, find this line in redis.conf:</p>\n\n<pre><code># Note that you must specify a directory here, not a file name\n</code></pre>\n\n<p>The line below that is the directory location for the redis database. Edit it so that it recites the location you want. Save your revisions and rename this file, but keep the .conf extension. You can store this config file anywhere you wish, just provide the full path and name of this custom config file on the same line when you start a redis server:</p>\n\n<p>So the next time you start a redis server, you must do it like so (from the shell prompt:</p>\n\n<pre><code>$&gt; cd /usr/local/bin    # or the directory in which you installed redis \n\n$&gt; redis-server /path/to/redis.conf\n</code></pre>\n\n<p>Finally, the <a href=\"http://pypi.python.org/pypi/redis_graph/1.0\">Python Package Index</a> lists a package specifically for implementing a graph database in redis. The package is called <a href=\"http://amix.dk/blog/post/19592#redis-graph-Graph-database-for-Python\">redis-graph</a> and i have not used it.</p>\n    "},{"t":"Choosing a distributed shared memory solution","l":"http://stackoverflow.com/questions/3045164/choosing-a-distributed-shared-memory-solution","q":"\n\n<p>I have a task to build a prototype for a massively scalable distributed shared memory (DSM) app. The prototype would only serve as a proof-of-concept, but I want to spend my time most effectively by picking the components which would be used in the real solution later on.</p>\n\n<p>The aim of this solution is to take data input from an external source, churn it and make the result available for a number of frontends. Those \"frontends\" would just take the data from the cache and serve it without extra processing. The amount of frontend hits on this data can literally be millions per second.</p>\n\n<p>The data itself is very volatile; it can (and does) change quite rapidly. However the frontends should see \"old\" data until the newest has been processed and cached. The processing and writing is done by a single (redundant) node while other nodes only read the data. In other words: no read-through behaviour.</p>\n\n<p>I was looking into solutions like <a href=\"http://memcached.org\">memcached</a> however this particular one doesn't fulfil <strong>all</strong> our requirements which are listed below:</p>\n\n<ol>\n<li>The solution must at least have <strong>Java client API</strong> which is reasonably well maintained as the rest of app is written in Java and we are seasoned Java developers;</li>\n<li>The solution must be totally <strong>elastic</strong>: it should be possible to add new nodes without restarting other nodes in the cluster;</li>\n<li>The solution must be able to handle <strong>failover</strong>. Yes, I realize this means some overhead, but the overall served data size isn't big (1G max) so this shouldn't be a problem. By \"failover\" I mean seamless execution without hardcoding/changing server IP address(es) like in memcached clients when a node goes down;</li>\n<li>Ideally it should be possible to specify the degree of data overlapping (e.g. how many copies of the same data should be stored in the DSM cluster);</li>\n<li>There is no need to permanently store all the data but there might be a need of post-processing of some of the data (e.g. serialization to the DB).</li>\n<li><strong>Price</strong>. Obviously we prefer free/open source but we're happy to pay a reasonable amount if a solution is worth it. In any way, paid 24hr/day support contract is a must.</li>\n<li>The whole thing has to be hosted in <strong>our data centers</strong> so SaaS offerings like Amazon SimpleDB are out of scope. We would only consider this if no other options would be available.</li>\n<li>Ideally the solution would be <em>strictly consistent</em> (as in CAP); however, <em>eventual consistence</em> can be considered as an option.</li>\n</ol>\n\n<p>Thanks in advance for any ideas.</p>\n    ","a":"\n<p>Have a look at <a href=\"http://www.hazelcast.com\">Hazelcast</a>. It is pure Java, open source (Apache license) highly scalable in-memory data grid product. It does offer 7X24 support. And it does solve all of your problems I tried to explain each of them below:</p>\n\n<ol>\n<li>It has a native Java Client.  </li>\n<li>It is 100% dynamic. Add and remove nodes dynamically. No need to change anything. </li>\n<li>Again everything is dynamic.</li>\n<li>You can configure number of backup nodes. </li>\n<li>Hazelcast support persistency.</li>\n<li>Everything that Hazelcast offers is free(open source) and it does offer enterprise level support.</li>\n<li>Hazelcast is single jar file. super easy to use. Just add jar to your classpath. Have a look at screen cast in main page.</li>\n<li>Hazelcast is strictly consistent. You can never read stale data. </li>\n</ol>\n    "},{"t":"Is an ORM redundant with a NoSQL API?","l":"http://stackoverflow.com/questions/2694044/is-an-orm-redundant-with-a-nosql-api","q":"\n\n<p>with MongoDB (and I assume other NoSQL database APIs worth their salt) the ways of querying the database are much more simplistic than SQL. There is no tedious SQL queries to generate and such. For instance take this from mongodb-csharp:</p>\n\n<pre><code>using MongoDB.Driver; \nMongo db = new Mongo(); \ndb.Connect(); //Connect to localhost on the default port. \nDocument query = new Document(); \nquery[\"field1\"] = 10; \nDocument result = db[\"tests\"][\"reads\"].FindOne(query); \ndb.Disconnect();\n</code></pre>\n\n<p>How could an ORM even simplify that? Is an ORM or other \"database abstraction device\" required on top of a decent NoSQL API? </p>\n    ","a":"\n<p>Well, yes, Object-<strong>Relational</strong> mappers are redundant with MongoDB because MongoDB isn't a <strong>relational</strong> database, it's a Document-Oriented database.</p>\n\n<p>So instead of SQL, you write queries in JSON.  Unless you really, <em>really</em> want to write raw JSON, as opposed to, say, Linq, then you're still going to want to use a mapper.  And if you don't want to create coupling against MongoDB itself, then you don't want to pass actual <code>Document</code> objects around, you want to map them to real POCOs.</p>\n\n<p>The mapping is much <em>easier</em> with a document-oriented DB like MongoDB, because you have nested documents instead of relations, but that doesn't mean it goes away completely.  It just means you've substituted one type of \"impedance mismatch\" for a different, slightly-less-dramatic mismatch.</p>\n    "},{"t":"Confused about Spring-Data DDD repository pattern","l":"http://stackoverflow.com/questions/5975199/confused-about-spring-data-ddd-repository-pattern","q":"\n\n<p>I don't know so much about DDD repository pattern but the implementation in Spring is confusion me.</p>\n\n<pre><code>public interface PersonRepository extends JpaRepository&lt;Person, Long&gt; { â€¦ }\n</code></pre>\n\n<p>As the interface extends JpaRepository (or MongoDBRepository...), if you change from one db to another, you have to change the interface as well.</p>\n\n<p>For me an interface is there to provide some abstraction, but here it's not so much abstract...</p>\n\n<p>Do you know why Spring-Data works like that? </p>\n    ","a":"\n<p>You are right, an Interface is an abstraction about something that works equals for all implementing classes, from an outside point of view. </p>\n\n<p>And that is exactly what happens here:</p>\n\n<ul>\n<li>JpaRepository is a common view of all your JPA Repositories (for all the different Entities), while MongoDBRepository is the same for all MongoDB Entities.</li>\n<li><p>But JpaRepository and MongoDBRepository have nothing in common, except the stuff that is defined in there common super Interfaces: </p>\n\n<ul>\n<li>org.springframework.data.repository.PagingAndSortingRepository</li>\n<li>org.springframework.data.repository.Repository</li>\n</ul></li>\n</ul>\n\n<p>So for me it looks normal.</p>\n\n<p>If you use the classes that implement your Repository then use PagingAndSortingRepository or Repository if you want to be able to switch from an JPA implementation to an Document based implementation (<em>sorry but I can not imagine such a use case - anyway</em>). And of course your Repository implementation should implement the correct interface (JpaRepository, MongoDBRepository) depending on what it is.</p>\n    "},{"t":"Understanding mongo db explain","l":"http://stackoverflow.com/questions/12510974/understanding-mongo-db-explain","q":"\n\n<p>I fired a query and tried to explain it on mongo console and got </p>\n\n<pre><code>\"isMultiKey\" : true,\n    \"n\" : 8,\n    \"nscannedObjects\" : 17272,\n    \"nscanned\" : 17272,\n    \"nscannedObjectsAllPlans\" : 21836,\n    \"nscannedAllPlans\" : 21836,\n    \"scanAndOrder\" : true,\n    \"indexOnly\" : false,\n    \"nYields\" : 0,\n    \"nChunkSkips\" : 0,\n    \"millis\" : 184,\n</code></pre>\n\n<p>Most of the things are explained in <a href=\"http://www.mongodb.org/display/DOCS/Explain\">http://www.mongodb.org/display/DOCS/Explain</a>, but I cannot understand what does nscannedObjectsAllPlans, nscannedAllPlans means. Can anyone help?</p>\n\n<p>Thanks</p>\n    ","a":"\n<p><code>nscanned</code> and <code>nscannedObjects</code> report results for the winning plan.</p>\n\n<p><code>nscannedAllPlans</code> and <code>nscannedObjectsAllPlans</code> report results for all plans</p>\n\n<p>For example:</p>\n\n<pre><code>&gt;t = db.jstests_explainb;\n&gt;t.drop();\n\n&gt;t.ensureIndex( { a:1, b:1 } );\n&gt;t.ensureIndex( { b:1, a:1 } );\n\n&gt;t.save( { a:0, b:1 } );\n&gt;t.save( { a:1, b:0 } );\n\n&gt;t.find( { a:{ $gte:0 }, b:{ $gte:0 } } ).explain( true );\n{\n  \"cursor\": \"BtreeCursor a_1_b_1\",\n  \"isMultiKey\": false,\n  \"n\": 2,\n  \"nscannedObjects\": 2,\n  \"nscanned\": 2,\n  \"nscannedObjectsAllPlans\": 6,\n  \"nscannedAllPlans\": 6,\n  \"scanAndOrder\": false,\n  \"indexOnly\": false,\n  \"nYields\": 0,\n  \"nChunkSkips\": 0,\n  \"millis\": 2,\n...\n}\n</code></pre>\n    "},{"t":"What does 'soft-state' in BASE mean?","l":"http://stackoverflow.com/questions/4851242/what-does-soft-state-in-base-mean","q":"\n\n<p>BASE stands for 'Basically Available, Soft state, Eventually consistent'</p>\n\n<p>So, I've come this far: \"Basically Available: the system is available, but not necessarily all items in it at any given point in time\" and \"Eventually Consistent: after a certain time all nodes are consistent, but at any given time this might not be the case\" (please correct me if I'm wrong).</p>\n\n<p>But, what is meant exactly by 'Soft State'? I haven't been able to find any decent explanations on the internet yet.</p>\n    ","a":"\n<p>This page (originally <a href=\"http://blog.tekelec.com/blog/bid/9608/FAQ-What-does-Soft-State-mean\" rel=\"nofollow\">here</a>, now available only from the <a href=\"http://web.archive.org/web/20120630030359/http://www.tekelec.com/tekelec-blog/index.php/2009/07/faq-what-does-soft-state-mean/\" rel=\"nofollow\">web archive</a>) may help: </p>\n\n<blockquote>\n  <p>[soft state] is information (state) the user put into the system that\n  will go away if the user doesn't maintain it. Stated another way, the\n  information will expire unless it is refreshed. </p>\n  \n  <p>By contrast, the position of a typical simple light-switch is\n  \"hard-state\". If you flip it up, it will stay up, possibly forever. It\n  will only change back to down when you (or some other user) explicitly\n  comes back to manipulate it.</p>\n</blockquote>\n    "},{"t":"Graph DBs vs. Document DBs vs. Triplestores","l":"http://stackoverflow.com/questions/12043086/graph-dbs-vs-document-dbs-vs-triplestores","q":"\n\n<p>This is a somewhat abstract and general question. I'm interested in the inherent (as well as implementation-specific) properties of different approaches to persist unstructured data with both lots of internal references (graph-like) and lots of properties (JSON-like).</p>\n\n<ul>\n<li><p>Since a graph is a superset of a tree, you can look at graph DBs (e.g. Neo4j) as a superset of document DBs (e.g. MongoDB). That is, a graph DB provides all the functionality of a document DB plus additionally also allows loops or has a native pointer type so you don't have to dereference foreign-keys/ids manually.\nSo is there some tipping point that you reach when adding more references to your objects/resources where you're better off with a graph DB but were previously better off with a document store? Are there advantages to document DBs (storage space, performance?) or should you just always go with a graph DB just in case you'll need more references in the future?</p></li>\n<li><p>Similarly, how do graph DBs and triplestores (e.g. RDF stores) compare? Graph DBs (where nodes and edges have properties) seem to be a superset of the simple triplestores. So for what problems (if any) perform triplestores actually better then, say Neo4j? (One advantage of RDF stores is that there is a standardized query language â€“ SPARQL â€“ although there seem to be a lot of people that don't like SPARQL and thus would call it a disadvantage.)</p></li>\n</ul>\n\n<p>I guess my question is: The graph model (with properties) seems to be able to neatly express all kinds of data, what is the catch when you enter reality? I suppose the catch of graph DBs is performance, so I'd love to see some numbers or rules of thumb on what kind of slowdowns to expect when loading, querying and modifying data as well as memory, and persistent storage requirements (compared to document and triple stores). Also what about horizontal scalability? I got the impression that there the playing field is quite level.</p>\n\n<p>Do you think it is possible that graphs with their expressibility will become the new default storage model for projects that have not super-large data, or are we doomed for a decade of <a href=\"http://martinfowler.com/bliki/PolyglotPersistence.html\">Polyglot Persistence</a> with RDBMS, JSON stores and Graph DBs living along each other that have to be integrated with even more glue code?</p>\n    ","a":"\n<p>I'm not sure I would agree with the sentiment that a lot of people don't like SPARQL.  SPARQL 1.0 did have some short comings, but it quite nicely addressed what it was designed for, and the new iteration, SPARQL 1.1, builds upon it adding many constructs from SQL that people expected to see in the original spec including sub-queries, aggregates &amp; update semantics.  I think the fact that it's standard and you can expect to see the same parsing &amp; semantics in every triple store, as opposed to dialects of SQL, is a nice feature.</p>\n\n<p>I would also claim that all triple stores are graph databases; you can put properties on specific edges in RDF, albeit not as nicely as you can w/ Neo4j.  But triple stores have the advantage of a real query language, a w3c standard data representation which makes it trivial to take your data to another triplestore, and for a number of triple stores, the ability to perform reasoning based on OWL.</p>\n\n<p>I dont know anything about the scalability for most graph db's, but generally, the commercial RDF databases scale quite well.  All can scale into the billions of triples, which handles a great many use cases.  Though how they handle scale differs wildly from vendor to vendor wrt to scale up or scale out, clustering, etc.  You'll also see pretty different mem &amp; hardware requirements to match the implementations for each.  For me, I've tended to just go and grab an EC2 instance, usually a 2XL or 4XL, mount an EBS large enough to hold the data, and I'm pretty well set.</p>\n\n<p>Additionally, some triple stores integrate with Lucene or similar technologies to provide inverted indexes over the data, and many now are starting to include geo-spatial and temporal indexes.  These are very useful features that I'm not sure of their availability in something like Neo4j.</p>\n\n<p>With that said, they're not going to scale as well as a relational databases, they're just not as mature.  But you're also not going to get screwed when you have \"real\" amounts of data either.  Of course, one of the advantages of triples stores is reasoning, which performing at scale is tricky, but that's much of the reason why the various OWL profiles were created.  But you can paint yourself into a corner if you don't think ahead.</p>\n\n<p>I think graph databases, triple stores specifically, can be a pretty good match for a lot of applications that are being built, but I dont think that means that everything should be done with them.  Like anything else, they're tools w/ their good points and their bad points, so you kind of have to make the right choice based on your application.  But they probably always merit at least a consideration these days.</p>\n    "},{"t":"Database EAV Pros/Cons and Alternatives","l":"http://stackoverflow.com/questions/2224234/database-eav-pros-cons-and-alternatives","q":"\n\n<p>I have been looking for a database solution to allow user defined fields and values (allowing an unlimited number).  At first glance, EAV seemed like the right fit, but after some reading I am not sure anymore.</p>\n\n<p>What are the pros and cons of EAV?</p>\n\n<p><strong>Is there an alternative database method to allow user defined attributes/fields and values?</strong></p>\n    ","a":"\n<p>This is not to be considered an exhaustive answer, but just a few points on the topic.</p>\n\n<p>Since the question is also tagged with the <code>[sql]</code> tag, let me say that, in general, <a href=\"http://en.wikipedia.org/wiki/Relational_databases\">relational databases</a> aren't particularly suitable for storing data using the <a href=\"http://en.wikipedia.org/wiki/Entity-attribute-value_model\">EAV</a> model. You can still design an EAV model in SQL, but you will have to sacrifice many advantages that a relational database would give. Not only you won't be able to enforce referential integrity, use SQL data types for values and enforce mandatory attributes, but even the very basic queries can become difficult to write. In fact, to overcome this limitation, several EAV solutions rely on data duplication, instead of joining with related tables, which as you can imagine, has plenty of drawbacks.</p>\n\n<p>If you really require a schemaless design, \"allowing an unlimited number of attributes\", your best bet is probably to use a <a href=\"http://en.wikipedia.org/wiki/NoSQL\">NoSQL</a> solution. Even though the weaknesses of EAV relative to relational databases also apply to NoSQL alternatives, you will be offered additional features that are difficult to achieve with conventional SQL databases. For example, usually NoSQL datastores can be scaled much easier than relational databases, simply because they were designed to solve some sort of scalability problem, and they intentionally dropped features that make scaling difficult. </p>\n\n<p>Many cloud computing platforms (such as those offered by <a href=\"http://aws.amazon.com/simpledb/\">Amazon</a>, <a href=\"http://code.google.com/appengine/\">Google</a> and <a href=\"http://www.microsoft.com/windowsazure/windowsazure/\">Microsoft</a>) are featuring datastores based on the EAV model, where an arbitrary number of attributes can be associated with a given entity. If you are considering deploying your application to the cloud, you may consider this both as a business advantage, as well as a technical one, because the strong competition between the big vendors is pushing the value-to-cost ratios to very high levels, by continually pushing up on the features and pushing down the financial and implementation costs.</p>\n    "},{"t":"Data Mining in a Django/Postgres application","l":"http://stackoverflow.com/questions/8317478/data-mining-in-a-django-postgres-application","q":"\n\n<p>I need to build in a analytics (reporting, charting &amp; graphing) system into my Django application. In an ideal world I could just query my Postgres DB and get the data I want but when the amount of data in the DB goes through the roof, I'll hit performance bottlenecks and other issues like index hell.</p>\n\n<p>I'm wondering if you could point me in a right direction to implement this:</p>\n\n<ul>\n<li>Is this a good scenario to use a NoSQL DB like (CouchDB, MongoDB, Redis) and query the data from that?</li>\n<li>Since Postgres and Django have no OLAP/MDX support should I go along with a star-schema in a different databse and query that?</li>\n</ul>\n\n<p>I'm looking to avoid two things:</p>\n\n<ul>\n<li>I don't want to query my actual DB for analytics as it might take a huge performance hit.</li>\n<li>I'd like to keep my analytics as up to date as possible i.e. I'd like to incrementally update my data warehouse to have a the latest data. Every time, there's a CRUD operation on my transactional DB, I'd like to update the data warehouse.</li>\n</ul>\n\n<p>This is yet another scenario that I haven't worked with and am trying to understand the quickest and best way to accomplish.</p>\n\n<p>I hope I've been verbose enough. If not, I'dd gladly explain more.</p>\n\n<p>Thanks everyone</p>\n\n<hr>\n\n<p>After digging around the web and using the knowledge I have, I've come to this solution:</p>\n\n<p>Use the Postgres to store the relational data. On Every CRUD operation, call the analytics code to do the calculations on the data and store the data in a NoSQL DB like Redis/CouchDB.</p>\n\n<p>Looking at this good comparison of the NoSQL DB's (http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis), I think Redis or CouchDB fits in just fine. Targeted for analytics.</p>\n\n<p>I could store calculated Analytics in Redis/CouchDB and update them incrementally when my source data changes.</p>\n\n<p>Is this a good solution?</p>\n    ","a":"\n<p>You might want to consider <a href=\"http://square.github.com/cube/\">Cube</a>. It is not a Django app, but it has a lot of nice features baked in, and Django can communicate to it easily.  Also, it is lovely.</p>\n\n<p><img src=\"http://i.stack.imgur.com/3sUvq.png\" alt=\"Cube screenshot\"></p>\n\n<p>You could have you Django app just blast off events into MongoDB when the occur.  This separation of systems would prevent any additional strain on your Django app.</p>\n    "},{"t":"Hector vs Astyanax for Cassandra [closed]","l":"http://stackoverflow.com/questions/9481578/hector-vs-astyanax-for-cassandra","q":"\n\n<p>We are starting a new java web-project with Cassandra as the database. The team is very well-experienced with RDBMS/JPA/Hibernate/Spring but very new to the world of NoSQL. We want to start the development with as simple setup as possible.\n<a href=\"http://rantav.github.com/hector/build/html/index.html#\">Hector</a> seems to be the most preferred and popular choice for connecting to Cassandra. But, Netflix has recently offered <a href=\"https://github.com/Netflix/astyanax/wiki\">Astyanax</a>, which has its origins in Hector.\nCan anyone who has used both these technologies share their experiences? I am looking for easy setup, good documentation and simple/clean usage.\nSuggestions about other api's are also welcome.</p>\n    ","a":"\n<p>I've tried both and Astyanax is way easier. The API actually makes sense and reflects what your are actually doing. Both Hector or direct Thrift usually results hard to decipher code.</p>\n\n<p>There are some issues yet to be solved in Astyanax (a.o. getColumnByName), but I've decided to build my project using it.</p>\n\n<p>Oh, I used the snapshot version (manually build, since it was not in any maven repo) because of some outdated references. </p>\n    "},{"t":"When to replace RDBMS/ORM with NoSQL [closed]","l":"http://stackoverflow.com/questions/3522069/when-to-replace-rdbms-orm-with-nosql","q":"\n\n<p>What kind of projects benefit from using a NoSQL database instead of rdbms wrapped by an ORM?</p>\n\n<p>Examples:</p>\n\n<ul>\n<li>Stackoverflow similiar sites?</li>\n<li>Social communities?</li>\n<li>forums?</li>\n</ul>\n    ","a":"\n<p>Your question is very general. NoSQL describes a collection of database techniques that are very different from each other. Roughly, there are:</p>\n\n<ul>\n<li>Key-value stores (Redis, Riak)</li>\n<li>Triplestores (AllegroGraph)</li>\n<li>Column-family stores (Bigtable, Cassandra)</li>\n<li>Document-oriented stores (CouchDB, MongoDB)</li>\n<li>Graph databases (Neo4j)</li>\n</ul>\n\n<p>A project can benefit from the use of a document database during the <strong>development phase</strong> of the project, because you won't have to design complex entity-relation diagrams or write complex join queries. I've detailed other uses of document databases in <a href=\"http://stackoverflow.com/questions/3376636/what-are-the-best-uses-of-document-stores/3378288#3378288\">this answer</a>.</p>\n\n<p>If your application needs to handle very large amounts of data, the development phase will likely be longer when you use a specialized NoSQL solution such as Cassandra. However, when your application goes into <strong>production</strong>, it will greatly benefit from the performance and scalability of Cassandra.</p>\n\n<p>Very generally speaking, if an application has the following requirements:</p>\n\n<ul>\n<li>scale horizontally</li>\n<li>work with data model X</li>\n<li>perform Y operations</li>\n</ul>\n\n<p>the application will benefit from using a NoSQL solution that is geared towards storing data model X and perform Y operations on the data. If you need more specific answers regarding a certain type of NoSQL database, you'll need to update your question.</p>\n\n<ol>\n<li>Benefits during development (e.g. easier to use than SQL, no licensing costs)?</li>\n<li>Benefits in terms of performance (e.g. runs like hell with a million concurrent users)?</li>\n<li>What type of NoSQL database?</li>\n</ol>\n\n<hr>\n\n<h3>Update</h3>\n\n<p>Key-value stores can only be queried by key in most cases. They're useful to store simple data, such as user sessions, simple profile data or precomputed values and output. Although it is possible to store more complex data in key-value pairs, it burdens the application with the responsibility of maintaining 'manual' indexes in order to perform more advanced queries.</p>\n\n<p>Triplestores are for storing <a href=\"http://en.wikipedia.org/wiki/Resource_Description_Framework\">Resource Description Metadata</a>. I don't anything about these stores, except for <a href=\"http://en.wikipedia.org/wiki/Triplestore\">what Wikipedia tells me</a>, so you'll have to do some research on that.</p>\n\n<p>Column-family stores are built for storing and processing very large amounts of data. They are used by Google's search engine and <a href=\"http://www.facebook.com/note.php?note_id=24413138919&amp;id=9445547199&amp;index=9\">Facebook's inbox search</a>. The data is queried by <a href=\"http://en.wikipedia.org/wiki/MapReduce\">MapReduce functions</a>. Although MapReduce functions may be hard to grasp in the beginning, the concept is quite simple. Here's an analogy which (hopefully) explains the concept:</p>\n\n<p>Imagine you have multiple shoe-boxes filled with receipts, and you want to calculate your total expenses. You invite some of your friends over and assign a person to each shoe-box. Each person writes down the total of each receipt in his shoe-box. This process of selecting the required data is the Map part.</p>\n\n<p>When a person has written down the totals of (some of) his receipts, he can sum up these totals. This is the Reduce part and can be repeated multiple times until all receipts have been handled. In the end, all of your friends come together and sum up their total sums, giving you your total expenses. That's the final Reduce step.</p>\n\n<p>The advantage of this approach is that you can have any number of shoe-boxes and you can assign any number of people to a shoe-box and still end up with the same result. Each shoe-box can be seen as a server in the database's network. Each friend can be seem as a thread on the server. With MapReduce you can have your data distributed across many servers and have each server handle part of the query, optimizing the performance of your database.</p>\n\n<p>Document-oriented stores are explained in <a href=\"http://stackoverflow.com/questions/3376636/what-are-the-best-uses-of-document-stores\">this question</a>, so I won't discuss them here.</p>\n\n<p>Graph databases are for storing networks of highly connected objects, like the users on a social network for example. These databases are optimized for graph operations, such as finding the shortest path between two nodes, or finding all nodes within three hops from the current node. Such operations are quite expensive on RDBMS systems or other NoSQL databases, but very cheap on graph databases.</p>\n    "},{"t":"MongoDB - will it fit a small hobby web application?","l":"http://stackoverflow.com/questions/3172378/mongodb-will-it-fit-a-small-hobby-web-application","q":"\n\n<p>I'm working on a small web application using the Python Flask framework. For a few happy weeks  SQLAlchemy was a perfect fit for my needs. In the meantime I found out more about MongoDB, played with it locally and started thinking of using it instead of MySql along with SQLAlchemy.</p>\n\n<p>Is this a good idea in terms of:</p>\n\n<ol>\n<li>Suitability. The project probably wont become 2nd Facebook, so I dont really need any scalability at all...</li>\n<li>Deployment. Is it more complicated to deploy a MongoDB enabled app, than a MySQL app?</li>\n<li>Anything else I need to know about Mongo and NoSql??</li>\n</ol>\n    ","a":"\n<p>Hobby web application == Best way to learn MongoDB.</p>\n\n<p>I'll work the questions in order.</p>\n\n<ol>\n<li>Yes, MongoDB is suitable for projects that don't need to scale. In fact, MongoDB is a very good way to get small projects off the ground quickly. You do very little \"DB configuration\" work, especially when using PHP and Rails (though C# NoRM looks pretty simple too).</li>\n<li>Basically every web host will give you \"free MySQL\" databases as part of the package. Providers that offer MongoDB are harder to find, so it depends on your setup. If you have a Virtual Private Server, then setting up MongoDB should be easy. If you're on $20/month shared hosting, then you'll probably have to look at someone else to host the DB. The MongoDB site has a list of <a href=\"http://www.mongodb.org/display/DOCS/Hosting+Center\">hosting providers</a> as well as instructions for configuration on many popular cloud computing providers. You will pay a small premium for using MongoDB.</li>\n<li>Be prepared to \"think different\". If you've used SQL for a long time, you're probably used to thinking of all data as \"relational\" and accessible as \"sets\" (which is SQL's strength). You have to think of MongoDB as a hash table, good for single or small lookups. Be prepared to write some \"client-side\" <code>for</code> loops in your code. You'll need those to process data in arrays within an object.</li>\n</ol>\n    "},{"t":"Eventual consistency in plain English","l":"http://stackoverflow.com/questions/10078540/eventual-consistency-in-plain-english","q":"\n\n<p>I often hear about eventual consistency in different speeches about NoSQL, data grids etc. \nIt seems that definition of eventual consistency varies in many sources (and maybe even depends on a concrete data storage).</p>\n\n<p>Can anyone give a simple explanation what Eventual Consistency is in general terms, not related to any concrete data storage?</p>\n    ","a":"\n<p>Eventual consistency:</p>\n\n<ol>\n<li>I tell you that it's going to rain tomorrow.</li>\n<li>Your neighbor tells his wife that it's going to be sunny tomorrow.</li>\n<li>You tell your neighbor that it is going to rain tomorrow.</li>\n</ol>\n\n<p>Eventually, all of the servers (you, me, your neighbor) know the truth (that it's going to rain tomorrow), but in the meantime the client (his wife) came away thinking it is going to be sunny, even though she asked after one of the servers knew the truth.</p>\n\n<p>As opposed to Strict Consistency / ACID compliance:</p>\n\n<ol>\n<li>Your bank balance is $50.</li>\n<li>You deposit $100.</li>\n<li>Your bank balance, queried from any ATM anywhere, is $150.</li>\n<li>Your daughter withdraws $40 with your ATM card.</li>\n<li>Your bank balance, queried from any ATM anywhere, is $110.</li>\n</ol>\n\n<p>At no time can your balance reflect anything other than the actual sum of all of the transactions made on your account to that exact moment.</p>\n\n<p>The <strong><em>reason</em></strong> why so many NoSQL systems have eventual consistency is that virtually all of them are designed to be distributed, and with fully distributed systems there is super-linear overhead to maintaining strict consistency (meaning you can only scale so far before things start to slow down, and when they do you need to throw exponentially more hardware at the problem to keep scaling).</p>\n    "},{"t":"What deficiencies do you feel exist with SQL and what changes would you make to it? [closed]","l":"http://stackoverflow.com/questions/3076855/what-deficiencies-do-you-feel-exist-with-sql-and-what-changes-would-you-make-to","q":"\n\n<p>Have you encountered deficiencies, limitations or flaws while using SQL?</p>\n\n<p>Tasks which are simple to accomplish with other non-SQL languages are so complicated or impossible to do with SQL!   </p>\n\n<p><a href=\"http://stackoverflow.com/questions/17833851/sp-query-for-round-robin-matrix-permutations\">Here's a good example</a></p>\n\n<p>Can you provide me with case examples of problems you have encountered or cases where for example an SQL query required complex constructs? A trap that people fall into is thinking that the desired solution has to fit within a single SQL statement. </p>\n\n<p>Can you suggest improvements to make SQL more powerful and less complicated? Example: PSM</p>\n\n<p>Which SQL implementation do you feel is the most robust?</p>\n\n<p>What capabilities from a non-SQL environment would you like to see implemented in SQL?..</p>\n\n<p>Let's suppose SQL didn't exist, what would you use for manipulating data?..</p>\n    ","a":"\n<p>SQL in general has some serious deficiencies as a database language. Just a few of the problems are:</p>\n\n<ul>\n<li><p>Duplicate rows (multi-set rather than set-based model)</p></li>\n<li><p>Nulls and three value logic add complexity, ambiguity and inconsistent results without adding any expressive power to the language</p></li>\n<li><p>SELECT statement syntax is much more verbose and complex than the relational algebra </p></li>\n<li><p>Lack of support for multiple assignment means that referential integrity support and support for constraints in general is severely limited</p></li>\n</ul>\n\n<p>Two examples of more specific query problems that are hard in SQL:</p>\n\n<ul>\n<li><p>No simple equivalent for transitive closure in SQL. Selection from adjacency relation structures therefore requires the use of either procedural code, cursors or the non-intuitive and hard to optimise recursive query syntax.</p></li>\n<li><p>Lack of key inheritence means that SQL query interfaces invariably return simple two dimensional tables, which is a major drawback for decision support (OLAP) type of queries that are intrinsically n-dimensional.</p></li>\n</ul>\n\n<p>Improvements? I don't believe there are any useful improvements that would make sense because fixing the above would change the language so radically that there wouldn't be much point in even pretending it was SQL any more. The best way forward I believe is to develop entirely new, truly relational languages. Date and Darwen's D model being the most obvious successor to SQL has already lead to a number of new implementations of relational languages.</p>\n    "},{"t":"NoSQL / RDBMS hybrid with referential integrity (delete cascade)?","l":"http://stackoverflow.com/questions/3388051/nosql-rdbms-hybrid-with-referential-integrity-delete-cascade","q":"\n\n<p>Is there a database out there that gives you the benefit of referential integrity and being able to use a SQL type language for querying, but also lets entities be loosely defined with respect to their data attributes and also the relationships between them?</p>\n\n<p>E.g. take a RBAC type model where you have Permissions, Users, User Groups &amp; Roles. A complex/flexible model could have the following rules:</p>\n\n<ul>\n<li>Roles can have one or more permissions and a permission can belong to one or more Roles</li>\n<li>Users can have one or more permissions and a permission can belong to one or more Users</li>\n<li>Users Groups can have one or more permissions and a permission can belong to one or more Users Groups</li>\n<li>Users can have one or more roles and a role can belong to one or more Users</li>\n<li>User Groups can have one or more roles and a role can belong to one or more User Groups</li>\n<li>Roles can have one or more roles and a role can belong to one or more Roles</li>\n</ul>\n\n<p>To model the above in an RDBMS would involve the creation of lots of intersection tables. Ideally, all I'd like to define in the database is the entities themselves (User, Role, etc) plus some mandatory attributes. Everything else would then be dynamic (i.e. no DDL required), e.g. I could create a User with a new attribute which wasn't pre-defined. I could also create a relationship between entities that hasn't been predefined, though the database would handle referential integrity like a normal RDBMS.</p>\n\n<p>The above can be achieved to some degree in a RDBMS by creating a table to store entities and another one to store relationships etc, but this overly complicates the SQL needed to perform simple queries and may also have performance implications.</p>\n    ","a":"\n<p>Most NoSQL databases are built to scale very well. This is done at the cost of consistency, of which referential integrity is part of. So most NoSQL don't support any type of relational constraints.</p>\n\n<p>There's one type of NoSQL database that does support relations. In fact, it's designed especially for relations: the <a href=\"http://en.wikipedia.org/wiki/Graph_database\">graph database</a>. Graph databases store nodes and explicit relations (edges) between these nodes. Both nodes and edges can contain data in the form of key/value pairs, without being tied to a predefined schema.</p>\n\n<p>Graph databases are optimized for relational queries and nifty graph operations, such as finding the shortest path between two nodes, or finding all nodes within a given distance from the current node. You wouldn't need this in a role/permission scenario, but if you do, it'll be a lot harder to achieve using an RDBMS.</p>\n\n<p>Another option is to make your entire data layer a hybrid, by using a RDBMS to store the relations and a document database to store the actual data. This would complicate your application slightly, but I don't think it's such a bad solution. You'll be using two different technologies, both dealing with the problems they were designed to deal with.</p>\n    "},{"t":"With the recent prevelance of NoSQL databases why would I use a SQL database?","l":"http://stackoverflow.com/questions/3294755/with-the-recent-prevelance-of-nosql-databases-why-would-i-use-a-sql-database","q":"\n\n<p>After developing software for about 5 years now, I have spent probably atleast 20% and perhaps up to 40% of that time simply making a RDBMS able to save and retrieve complex object graphs. Many times this resulted in less than optimal coding solutions in order to make something easier to do from the database side. This eventually ended after a very significant amount of time spent in learning NHibernate and the session management patterns that are part of it. With NHibernate I was able to finally eschew the large majority of 100% wasted time of writing CRUD for the 1000th time and use forward generation of my database from my domain model.</p>\n\n<p>Yet all of this work still results in a flawed model where my database is merely the best attempt by SQL to imitate my actual object. With document databases this is no longer the case as the object becomes the document itself instead of merely emulating the object through tables and columns. </p>\n\n<p><strong>At this point I'm really starting to question why would I ever need SQL again?</strong></p>\n\n<p>What can really be done <em>substantially</em> better with SQL than a document database?</p>\n\n<p>I know this is somewhat of leading into a apples to oranges comparison especially when you factor in the various types of NoSQL databases having widely different feature-sets but for the sake of this argument base it on the notion of NoSQL databases can inherently query objects correctly and not on the limitations of a key value store. Also leave out the reporting aspect as that should generally be handled in a OLAP database unless your answer includes a specific reason you would not use a OLAP database for it.</p>\n    ","a":"\n<p>My key question was where would a SQL database really outshine a document database and from all the responses there really doesn't seem to be much.</p>\n\n<p>Given that NoSQL databases come in just as many variations of types of databases as relational that both match all or some parts of ACID depending on which database you use that at this point they are basically the equitable for solving problems.</p>\n\n<p>After this the key differences would be tooling and maturity which SQL databases have a much larger grasp in for being the established player but this is how it is for all new technology.</p>\n    "},{"t":"What should be the considerations for choosing SQL/NoSQL?","l":"http://stackoverflow.com/questions/2440079/what-should-be-the-considerations-for-choosing-sql-nosql","q":"\n\n<p>Target application is a medium-sized website built to support several hundred to several thousand users an hour, with an option to scale above that. Data model is rather simple, and caching potential is pretty high (~10:1 ratio of read to edit actions).</p>\n\n<p>What should be the considerations when coming to choose between a relational, SQL-based datastore to a NoSQL option (such as HBase and Cassandra)?</p>\n    ","a":"\n<p>To me, you don't have any particular problem to solve. If you need ACIDity, use a database; if you don't, then it doesn't matter. At the end just build your app. And let me quote <a href=\"http://bjclark.me/2009/08/04/nosql-if-only-it-was-that-easy/\">NoSQL: If Only It Was That Easy</a>:</p>\n\n<blockquote>\n  <p>The real thing to point out is that if you are being held back from making something super awesome because you canâ€™t choose a database, you are doing it wrong. If you know mysql, just used it. Optimize when you actually need to. Use it like a k/v store, use it like a rdbms, but for god sake, build your killer app! None of this will matter to most apps. Facebook still uses MySQL, a lot. Wikipedia uses MySQL, a lot. FriendFeed uses MySQL, a lot. NoSQL is a great tool, but itâ€™s certainly not going to be your competitive edge, itâ€™s not going to make your app hot, and most of all, your users wonâ€™t give a shit about any of this.</p>\n</blockquote>\n    "},{"t":"Am I missing something about Document Databases?","l":"http://stackoverflow.com/questions/3440332/am-i-missing-something-about-document-databases","q":"\n\n<p>I've been looking at the rise of the NoSql movement and the accompanying rise in popularity of document databases like mongodb, ravendb, and others.  While there are quite a few things about these that I like, I feel like I'm not understanding something important.</p>\n\n<p>Let's say that you are implementing a store application, and you want to store in the database products, all of which have a single, unique category.  In Relational Databases, this would be accomplished by having two tables, a product and a category table, and the product table would have a field (called perhaps \"category_id\") which would reference the row in the category table holding the correct category entry.  This has several benefits, including non-repetition of data.  </p>\n\n<p>It also means that if you misspelled the category name, for example, you could update the category table and then it's fixed, since that's the only place that value exists.</p>\n\n<p>In document databases, though, this is not how it works.  You completely denormalize, meaning in the \"products\" document, you would actually have a value holding the actual category string, leading to lots of repetition of data, and errors are much more difficult to correct.  Thinking about this more, doesn't it also mean that running queries like \"give me all products with this category\" can lead to result that do not have integrity.</p>\n\n<p>Of course the way around this is to re-implement the whole \"category_id\" thing in the document database, but when I get to that point in my thinking, I realize I should just stay with relational databases instead of re-implementing them.  </p>\n\n<p>This leads me to believe I'm missing some key point about document databases that leads me down this incorrect path.  So I wanted to put it to stack-overflow, what am I missing?</p>\n    ","a":"\n<blockquote>\n  <p>You completely denormalize, meaning in the \"products\" document, you would actually have a value holding the actual category string, leading to lots of repetition of data [...]</p>\n</blockquote>\n\n<p>True, denormalizing means storing additional data. It also means less collections (tables in SQL), thus resulting in less relations between pieces of data. Each single document can contain the information that would otherwise come from multiple SQL tables.</p>\n\n<p>Now, if your database is distributed across multiple servers, it's more efficient to query a single server instead of multiple servers. With the denormalized structure of document databases, it's much more likely that you only need to <strong>query a single server to get all the data you need</strong>. With a SQL database, chances are that your related data is spread across multiple servers, making queries very inefficient.</p>\n\n<blockquote>\n  <p>[...] and errors are much more difficult to correct.</p>\n</blockquote>\n\n<p>Also true. Most NoSQL solutions don't guarantee things such as referential integrity, which are common to SQL databases. As a result, your application is responsible for maintaining relations between data. However, as the amount of relations in a document database is very small, it's not as hard as it may sound.</p>\n\n<p>One of the advantages of a document database is that it is <strong>schema-less</strong>. You're completely free to define the contents of a document at all times; you're not tied to a predefined set of tables and columns as you are with a SQL database.</p>\n\n<h2>Real-world example</h2>\n\n<p>If you're building a CMS on top of a SQL database, you'll either have a separate table for each CMS content type, or a single table with generic columns in which you store all types of content. With separate tables, you'll have <strong>a lot of tables.</strong> Just think of all the join tables you'll need for things like tags and comments <em>for each content type</em>. With a single generic table, your application is responsible for correctly managing all of the data. Also, the raw data in your database is <strong>hard to update and quite meaningless</strong> outside of your CMS application.</p>\n\n<p>With a document database, you can store each type of CMS content in a single collection, while maintaining a strongly defined structure within each document. You could also store all tags and comments within the document, making <strong>data retrieval very efficient</strong>. This efficiency and flexibility comes at a price: your application is more responsible for managing the integrity of the data. On the other hand, the price of scaling out with a document database is much less, compared to a SQL database.</p>\n\n<h2>Advice</h2>\n\n<p>As you can see, both SQL and NoSQL solutions have advantages and disadvantages. As David <a href=\"http://stackoverflow.com/questions/3440332/am-i-missing-something-about-document-databases/3440398#3440398\">already pointed out</a>, each type has its uses. I recommend you to analyze your requirements and create two data models, one for a SQL solution and one for a document database. Then choose the solution that fits best, keeping scalability in mind.</p>\n    "},{"t":"MongoDB database encryption","l":"http://stackoverflow.com/questions/8803332/mongodb-database-encryption","q":"\n\n<p>I'm looking to design a webapp which stores private information securely using MongoDB. I would like to encrypt the entire database but it looks like it's not supported. What routes can I take to encrypt my database?</p>\n    ","a":"\n<p>As the commentator indicated, you can certainly encrypt the connection, which is a good start.</p>\n\n<p>MongoDB doesn't support complete database encryption at this time. Therefore, you'll have to encrypt the data selectively on the client side. Then you can store the encrypted data in a binary field within a BSON document.</p>\n\n<p>Note that the fields you want to query on should not be encrypted, especially if you need range queries.</p>\n\n<p>Can you speak more about your use case? </p>\n    "},{"t":"Anyone using HyperDex in production?","l":"http://stackoverflow.com/questions/10134753/anyone-using-hyperdex-in-production","q":"\n\n<p>I just noticed that the relatively new Open Source noSQL database \"<a href=\"http://hyperdex.org\">HyperDex</a>\" has no mentions in questions in S.O. yet - is someone using it? How does it compare with other noSQL engines?</p>\n    ","a":"\n<p>We're working with some folks at LinkedIn to use HyperDex to power some of their custom analytics applications.  We're also in discussion with a couple startups to build applications on top of HyperDex.</p>\n\n<p>Our mailing list is the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/hyperdex-discuss\">HyperDex discuss list</a> and has been relatively active as of late.  Many of these folks are using HyperDex and helping us to improve it.</p>\n\n<p>Finally, HyperDex holds its own against other popular NoSQL engines.  The <a href=\"http://hyperdex.org/performance/\">HyperDex performance benchmarks</a> show that HyperDex offers both higher throughput and lower latency than other popular systems.  The HyperDex tutorials show just how easy it is to deploy a cluster.  Start with the QuickStart and work your way up to deploying a truly fault tolerant cluster.</p>\n    "},{"t":"Anyone using NoSQL databases for medical record storage? [closed]","l":"http://stackoverflow.com/questions/2516752/anyone-using-nosql-databases-for-medical-record-storage","q":"\n\n<p>Electronic Medical records are composed of different types of data.  Visit information ( date/location/insurance info) seems to lend itself to a RDMS.  Other types of medical infomation, such as lab reports, x-rays, photos, and electronic signatures, are document based and would seem to be a good candidate for a 'document-oriented' database, such as MongoDB.  </p>\n\n<p>Traditionally, binary data would be stored as a BLOB in a RDBMS.  A hybrid approach using a traditional RDBMS along with a 'document-oriented' database would seem like good alternative to this.  Other alternative would be something like DB2 purexml.  </p>\n\n<p>The ultimate answer could be that 'it depends', but I really just wanted to get some general feedback/ideas on this. </p>\n\n<p>Is anyone using the NoSql approach for medical records?</p>\n\n<p>** Clarifying question ** \nTo clarify: is anyone using nosql databases such as: mongoDB, Cassandra, CouchDB for medical records, in a production environment? </p>\n    ","a":"\n<p>Perhaps the original NoSQL database was MUMPS, which dates from before Codd devised his rules (i.e. the 1960s).  As the name implies  (*M*assachusetts General Hospital *U*tility *M*ulti-*P*rogramming *S*ystem),  its original purpose was the storing of medical documents.  Apparently MUMPS is still in use in some healthcare systems and other environments.  <a href=\"http://en.wikipedia.org/wiki/MUMPS\" rel=\"nofollow\" title=\"Wikipedia article\">Find out more.</a></p>\n\n<p>But as for the more recent rash of NoSQL databases I would be suprised if there were any implementations - yet.  Most of these products are still extremely beta and - being largely open source - lacking in support.  Medical apps are inevitably going to be extremely conservative, because people could die if the IT system fouls up.   </p>\n    "},{"t":"Comparing MongoDB and RethinkDB Bulk Insert Performance","l":"http://stackoverflow.com/questions/15151554/comparing-mongodb-and-rethinkdb-bulk-insert-performance","q":"\n\n<p><em>This is my official first question here; I welcome any/all criticism of my post so that I can learn how to be a better SO citizen.</em></p>\n\n<p>I am vetting non-relational DBMS for storing potentially large email opt-out lists, leaning toward either MongoDB or RethinkDB, using their respective Python client libraries.  The pain point of my application is bulk insert performance, so I have set up two Python scripts to insert 20,000 records in batches of 5,000 into both a MongoDB and a RethinkDB collection.</p>\n\n<p>The MongoDB python script mongo_insert_test.py:</p>\n\n<pre><code>NUM_LINES = 20000\nBATCH_SIZE = 5000\n\ndef insert_records():\n    collection = mongo.recips\n    i = 0\n    batch_counter = 0\n    batch = []\n    while i &lt;= NUM_LINES:\n        i += 1\n        recip = {\n            'address': \"test%d@test%d.com\" % (i, i)\n        }\n        if batch_counter &lt;= BATCH_SIZE:\n            batch.append(recip)\n            batch_counter += 1\n        if (batch_counter == BATCH_SIZE) or i == NUM_LINES:\n            collection.insert(batch)\n            batch_counter = 0\n            batch = []\n\nif __name__ == '__main__':\n    insert_records()\n</code></pre>\n\n<p>The almost identical RethinkDB python script rethink_insert_test.py:</p>\n\n<pre><code>NUM_LINES = 20000\nBATCH_SIZE = 5000\n\ndef insert_records():\n    i = 0\n    batch_counter = 0\n    batch = []\n    while i &lt;= NUM_LINES:\n        i += 1\n        recip = {\n            'address': \"test%d@test%d.com\" % (i, i)\n        }\n        if batch_counter &lt;= BATCH_SIZE:\n            batch.append(recip)\n            batch_counter += 1\n        if (batch_counter == BATCH_SIZE) or i == NUM_LINES:\n            r.table('recip').insert(batch).run()\n            batch_counter = 0\n            batch = []\n\nif __name__ == '__main__':\n    insert_records()\n</code></pre>\n\n<p>In my dev environment, the MongoDB script inserts 20,000 records in under a second:</p>\n\n<pre><code>$ time python mongo_insert_test.py \nreal    0m0.618s\nuser    0m0.400s\nsys     0m0.032s\n</code></pre>\n\n<p>In the same environment, the RethinkDB script performs much slower, inserting 20,000 records in over 2 minutes:</p>\n\n<pre><code>$ time python rethink_insert_test.py\nreal    2m2.502s\nuser    0m3.000s\nsys     0m0.052s\n</code></pre>\n\n<p>Am I missing something huge here with regard to how these two DBMS work?  Why is RethinkDB performing so badly with this test?</p>\n\n<p>My dev machine had about 1.2GB available memory for these tests.</p>\n    ","a":"\n<p>RethinkDB currently implements batch inserts by doing a single insert at a time on the server. Since Rethink flushes every record to disk (because it's designed with safety first in mind), this has a really bad effect on workloads like this one.</p>\n\n<p>We're doing two things to address this:</p>\n\n<ol>\n<li>Bulk inserts will be implemented via a bulk insert algorithm on the server to avoid doing one insert at a time.</li>\n<li>We will give you the option to relax durability constraints to allow the cache memory to absorb high-throughput inserts if you'd like (in exchange for not syncing to disk as often).</li>\n</ol>\n\n<p>This will absolutely be fixed in 4-12 weeks (and if you need this ASAP, feel free to shoot me an email to slava@rethinkdb.com and I'll see if we can reprioritize).</p>\n\n<p>Here are the relevant github issues:</p>\n\n<p><a href=\"https://github.com/rethinkdb/rethinkdb/issues/207\">https://github.com/rethinkdb/rethinkdb/issues/207</a></p>\n\n<p><a href=\"https://github.com/rethinkdb/rethinkdb/issues/314\">https://github.com/rethinkdb/rethinkdb/issues/314</a></p>\n\n<p>Hope this helps. Please don't hesitate to ping us if you need help.</p>\n    "},{"t":"Scan HTable rows for specific column value using HBase shell","l":"http://stackoverflow.com/questions/11013197/scan-htable-rows-for-specific-column-value-using-hbase-shell","q":"\n\n<p>I want to scan rows in a HTable from <strong>hbase shell</strong> where a column family (i.e., Tweet) has a particular value (i.e., user_id).</p>\n\n<p>Now I want to find all rows where tweet:user_id has value <code>test1</code> as this column has value <code>'test1'</code></p>\n\n<pre><code>column=tweet:user_id, timestamp=1339581201187, value=test1\n</code></pre>\n\n<p>Though I can scan table for a particular using, </p>\n\n<pre><code>scan 'tweetsTable',{COLUMNS =&gt; 'tweet:user_id'}\n</code></pre>\n\n<p>but I did not find any way to scan a row for a value.</p>\n\n<p>Is it possible to do this via HBase Shell?</p>\n\n<p>I checked <a href=\"http://stackoverflow.com/questions/8189451/how-to-scan-for-a-particular-column-value-in-hbase\">this question</a> as well.</p>\n    ","a":"\n<p>It is possible without Hive:</p>\n\n<pre><code>scan 'filemetadata', { COLUMNS =&gt; 'colFam:colQualifier', LIMIT =&gt; 10, FILTER =&gt; \"ValueFilter( =, 'binaryprefix:someValue' )\" }\n</code></pre>\n    "},{"t":"What is the difference between Membase and Couchbase?","l":"http://stackoverflow.com/questions/6170909/what-is-the-difference-between-membase-and-couchbase","q":"\n\n<p>With the two merging under the same roof recently, it has become difficult to determine what the major differences between Membase and Couchbase. Why would one be used over the other?</p>\n    ","a":"\n<p>I want to elaborate on the answer given by James.</p>\n\n<p>At the moment Couchbase server is CouchDB with GeoCouch integration out of the box. What is great about CouchDB is that you have the ability to create structured documents and do map-reduce queries on those documents. </p>\n\n<p>Membase server is memcached with persistence and very simple cluster management interface. It's strengths are the ability to do very low latency queries as well as the ability to easily add and remove servers from a cluster. </p>\n\n<p>Late this summer however Membase and CouchDB will be merged together to form the next version of Couchbase. So what will the new version of Couchbase look like?</p>\n\n<p>Right now in Membase the persistence layer for memcached is implemented with SQLite. After the merger of these two products CouchDB will be the new persistence layer. This means that you will get the low latency requests and great cluster management that was provided by Membase and you will also get the great document oriented model that CouchDB is known for.</p>\n    "},{"t":"MongoDB - too much data for sort() with no index error","l":"http://stackoverflow.com/questions/4399068/mongodb-too-much-data-for-sort-with-no-index-error","q":"\n\n<p>I am using MongoDB 1.6.3, to store a big collection (300k+ records). I added a composite index.</p>\n\n<pre><code>db['collection_name'].getIndexes()\n[\n    {\n        \"name\" : \"_id_\",\n        \"ns\" : \"db_name.event_logs\",\n        \"key\" : {\n            \"_id\" : 1\n        }\n    },\n    {\n        \"key\" : {\n            \"updated_at.t\" : -1,\n            \"community_id\" : 1\n        },\n        \"ns\" : \"db_name.event_logs\",\n        \"background\" : true,\n        \"name\" : \"updated_at.t_-1_community_id_1\"\n    }\n]\n</code></pre>\n\n<p>However, when I try to run this code:</p>\n\n<pre><code>db['collection_name'].find({:community_id =&gt; 1}).sort(['updated_at.t', -1]).skip(@skip).limit(@limit)\n</code></pre>\n\n<p>i am getting:</p>\n\n<blockquote>\n  <p>Mongo::OperationFailure (too much data\n  for sort() with no index.  add an\n  index or specify a smaller limit)</p>\n</blockquote>\n\n<p>What am I doing wrong?</p>\n    ","a":"\n<p>Try adding <code>{community_id: 1, 'updated_at.t': -1}</code> index. It needs to search by <code>community_id</code> first and then sort.</p>\n    "},{"t":"Would MongoDB be a good idea for a social network site (developed in Ruby on Rails)?","l":"http://stackoverflow.com/questions/5061299/would-mongodb-be-a-good-idea-for-a-social-network-site-developed-in-ruby-on-rai","q":"\n\n<p>my project (in Ruby on Rails 3) is to develop a \"social network\" site with the following features:</p>\n\n<ul>\n<li>Users can be friends. It's mutual friendships; not asymetric like Twitter.</li>\n<li>Users can publish links, to share them. Friends of a user can see what this user has shared.</li>\n<li>Friends can comment on those shared links.</li>\n</ul>\n\n<p>So basically we have Users, Links, and Comments, and all that is connected. An interesting thing in social networks is that the User table has kind of a many-to-many relation with itself.</p>\n\n<p>I think I can handle that level of complexity with SQL and RoR.</p>\n\n<p>My question is: would it be a good idea to use MongoDB (or CouchDB) for such a site?</p>\n\n<p>To be honest, I think the answer is no. MongoDB doesn't seem to fit really well with many-to-many relationships. I can't think of a good MongoDB way to implement the friendship relationships. And I've read that Diaspora started with MongoDB but then switched back to classic SQL.</p>\n\n<p>But some articles on the web defend MongoDB for social networks, and above all I want to make a well-informed decision, and not miss a really cool aspect of MongoDB that would change my life.</p>\n\n<p>Also, I've heard about graph DB, which are probably great, but they really seem too young to me, and I don't know how they'd fit with RoR (and not mentioning heroku).</p>\n\n<p>So, am I missing something?</p>\n\n<p>Thanks,</p>\n\n<p>arthur</p>\n    ","a":"\n<p>I like MongoDB and use it a lot, but I am of the opinion that if you are dealing with relational data, you should use the right tool for it. We have relational databases for that. Mongo and Couch are document stores. </p>\n\n<p>Mongo has a serious disadvantage if you are going to be maintaining a lot of inter-document links.  Writes are only guaranteed to be atomic for one document. So you could have inconsistent updates for relations if you are not careful with your schema. </p>\n\n<p>The good thing about MongoDB is that it is very good at scaling.  You can shard and create replica sets.  Foursquare currently uses MongoDB and it has been working pretty well for them.  MongoDB also does map-reduce and has decent geospatial integration.   The team that develops MongoDB is excellent, and I live in NY where they are based and have met them.  You probably are not going to have scaling issues though I would think starting out.</p>\n\n<p>As far as Diaspora switching... I would not want to follow anything they are doing :)</p>\n\n<p>Your comment about graph dbs is interesting though.  I would probably not use a graph DB as my primary DB either, but when dealing with relationships, you can do amazing things with them. In fact usually the demo the guys from graph DB companies will give you is extracting relationship knowledge from a social network. However, there is nothing preventing you from playing with these in the future for network analysis.</p>\n\n<p>In conclusion, when you are starting out here, you are not running into the problems of massive scale yet, and are probably limited on time and money.  Keep in mind that even Facebook does not use just one technology, they have basically expanded to NoSQL for certain functionality (like Facebook messaging).  There is nothing stopping you in the future from using say Mongo and gridFS for handling image uploads or geo-location etc.  It is good to grow as your needs change. I think your gut feeling that you have an SQL app here is right, and the benefits gained with MongoDB would not be realized for a while.</p>\n    "},{"t":"Using a Relational Database for Schemaless Data - Best Practices","l":"http://stackoverflow.com/questions/4189709/using-a-relational-database-for-schemaless-data-best-practices","q":"\n\n<p>After reading a shocking article written by Bret Taylor (co-creator of FriendFeed; current CTO of Facebook),  <a href=\"http://bret.appspot.com/entry/how-friendfeed-uses-mysql\"><em>How FriendFeed uses MySQL to store schema-less data</em></a>, I began to wonder if there are best practices for using a RDBMS such as Oracle, MySQL, or PostgreSQL for storing and querying schemaless data?</p>\n\n<p>Few people like to admit they're using a relational database when NoSQL is the new hotness, which makes it difficult to find good articles on the topic. How do I implement a schemaless (or \"document-oriented\") database as a layer on top of a relational database?</p>\n    ","a":"\n<p>thats the classic article in this topic: <a href=\"http://yoshinorimatsunobu.blogspot.com/2010/10/using-mysql-as-nosql-story-for.html\">http://yoshinorimatsunobu.blogspot.com/2010/10/using-mysql-as-nosql-story-for.html</a> (Using MySQL as a NoSQL - A story for exceeding 750,000 qps on a commodity server)</p>\n    "},{"t":"looking for a NoSql DB to Embed into my Java Application","l":"http://stackoverflow.com/questions/6155601/looking-for-a-nosql-db-to-embed-into-my-java-application","q":"\n\n<p>looking for a NoSQL DB to <strong>Embed</strong> into my Java Application.</p>\n\n<p>so far I only found OrientDB as a <strong>Embeddable</strong> DB in Java Application.</p>\n\n<ul>\n<li>Is there any other options ?</li>\n</ul>\n\n<p>Thanks,\nMohammad</p>\n    ","a":"\n<p>Here are some alternatives:</p>\n\n<ul>\n<li><a href=\"http://jdbm.sourceforge.net/\">JDBM</a> An embeddable Java only key value store</li>\n<li><a href=\"http://www.oracle.com/technetwork/database/berkeleydb/overview/index.html\">Berkeley DB</a> Comes in different flavours: Native key value store library with Java bindings, pure Java key value store or native XML database with Java bindings.</li>\n<li><a href=\"http://fallabs.com/kyotocabinet/\">Kyoto cabinet</a> Native key value store with Java bindings</li>\n</ul>\n    "},{"t":"Clojure and NoSQL databases [closed]","l":"http://stackoverflow.com/questions/2720490/clojure-and-nosql-databases","q":"\n\n<p>I am currently trying to pick between different NoSQL databases for my project. The project is being written in clojure and javascript. I am currently looking at three candidates for storage. What are the relative strengths and weaknesses of MongoDB, FleetDB and CouchDB? Which one is better supported in Clojure? Which one is better supported under Linux? Did I miss a better product (has to be free and OSS)?</p>\n    ","a":"\n<p>We were using Clojure + MongoDB, and they worked together very well. Mostly because of JSON data model, provided by MongoDB, that could be easily transformed to/from Clojure internal structures.</p>\n    "},{"t":"Middleware for MongoDB or CouchDB with jQuery Ajax/JSON frontend","l":"http://stackoverflow.com/questions/1813612/middleware-for-mongodb-or-couchdb-with-jquery-ajax-json-frontend","q":"\n\n<p>I've been using the following web development stack for a few years:</p>\n\n<p>java/spring/hibernate/mysql/jetty/wicket/jquery</p>\n\n<p>For certain requirements, I'm considering switching to a NoSQL datastore with an AJAX frontend.  I would probably build the frontend with jQuery and communicate with the web application middleware using JSON.  I'm leaning toward MongoDB because of more dynamic query capabilities, but am still considering CouchDB.</p>\n\n<p>I'm not sure what to use in the middle.  Probably something RESTful?  My preference is to stick with Java (or maybe Scala or Groovy) since I'm using tools like Drools for rules and Shiro for security.  But then again, I want to pick something that is quick an easy to work with, so I'm open to other solutions.</p>\n\n<p>If you are building ajax/json/nosql solutions, I'd like to hear details about what tools you are using and any pros/cons you've found to using them.</p>\n    ","a":"\n<ol>\n<li><p>Pick whichever middleware you are most comfortable with. </p></li>\n<li><p><a href=\"http://github.com/couchapp/couchapp\" rel=\"nofollow\">CouchApp</a> is very experimental at the moment. The main issue is being able to add security to your app without having a standard HTTP pop-up box. This is obviously a big issue for standard web apps.</p></li>\n<li><p>Try and avoid parsing each DB request in the middleware and rebuilding the query for couchdb. You can make your middleware act like a proxy so most requests are forwarded on without modification. You can also add a security layer in the middlelayer on top of all requests that need authentication.</p></li>\n<li><p>Pick a middleware/framework with good URL routing capabilities. For example you could route all requests that go to mydomain.com/db/ to couchdb.</p></li>\n</ol>\n    "},{"t":".NET, JSON, Embedded, Free Commercial-Use data management solution? What to do? [closed]","l":"http://stackoverflow.com/questions/21592562/net-json-embedded-free-commercial-use-data-management-solution-what-to-do","q":"\n\n<p>I am trying to develop a data management solution for a commercial product that meets several criteria. The criteria and my reasoning are as follows:</p>\n\n<ol>\n<li>The solution should be in C# or support C#</li>\n<li>Manage data as JSON</li>\n<li>No external schema maintenance</li>\n<li>Be able to cache some or all data in memory and persist to disk</li>\n<li>Not require an additional installation</li>\n<li>If the solution involves third-party software, the license must support no-cost commercial use</li>\n</ol>\n\n<p><strong>Requirement #1</strong>: My application is written in C# and I would prefer that any solution does not involve integrating with applications, libraries, or code in another language.</p>\n\n<p><strong>Requirement #2</strong>: There are several JSON-based tools and libraries that I would like to utilize, so I need a solution where data is either in or easily converts to/from JSON. </p>\n\n<p><strong>Requirement #3</strong>: I want to avoid the schema maintenance that comes with using relational database solutions. I prefer to manage mismatched data-to-object mappings in code and have code update older data instead of managing schema updates separately.</p>\n\n<p><strong>Requirement #4</strong>: I require some or all data to be loaded into memory at all times, and all data to be persisted to disk. Whether data persists in memory or not should be optional per data type.</p>\n\n<p><strong>Requirement #5</strong>: When installing my product I don't want to have any secondary installations or have any external services running other than my application. A completely self-contained solution is best.</p>\n\n<p><strong>Requirement #6</strong>: The intended use is for a distributed commercial product. I would prefer to avoid any additional fees or licensing issues that come with many third-party solutions.</p>\n\n<p>To date I have tried several solutions. Originally I did not have as many constraints and went with a <a href=\"http://system.data.sqlite.org/index.html/doc/trunk/www/index.wiki\">SQLite.NET</a> and its use wasn't unpleasant, but the overhead from schema maintenance and data format was more than I would like. I investigated a lot of NoSQL solutions (such as <a href=\"http://ravendb.net/\">RavenDB</a>), other third-party solutions (Karvonite), and simple JSON file storage implementations, but I'm not satisfied with any of them.</p>\n\n<p>Is there a custom approach or solution that I am missing, that someone else has used successfully? I'm hoping that I am simply overlooking the option(s) that I am after, and that some NoSQL and .NET experts out there have enough experience in this area to point me in the right direction.</p>\n\n<p><strong>EDIT</strong>: In case any original commentators are confused, I updated the question and title to better adhere to SO's policies.</p>\n    ","a":"\n<p>Have you taken a look at the <a href=\"http://www.karvonite.com/\" rel=\"nofollow\">Karvonite Framework</a>? The Karvonite Framework provides a strongly-typed embedded database system that includes a portable library implementation for .NET / Windows Store / Silverlight / Windows Phone / Xbox development. I have only used this for small database implementations but so far it has met every one of my needs.</p>\n    "},{"t":"Riak performance - unexpected results","l":"http://stackoverflow.com/questions/6007833/riak-performance-unexpected-results","q":"\n\n<p>In the last days I played a bit with riak. The initial setup was easier then I thought. Now I have a 3 node cluster, all nodes running on the same vm for the sake of testing. </p>\n\n<p>I admit, the hardware settings of my virtual machine are very much downgraded (1 CPU, 512 MB RAM) but still I am a quite surprised by the slow performance of riak.</p>\n\n<p><strong>Map Reduce</strong></p>\n\n<p>Playing a bit with map reduce I had around 2000 objects in one bucket, each about 1k - 2k in size as json. I used this map function:</p>\n\n<pre><code>function(value, keyData, arg) {\n    var data = Riak.mapValuesJson(value)[0];\n\n    if (data.displayname.indexOf(\"max\") !== -1) return [data];\n    return [];\n}\n</code></pre>\n\n<p>And it took over 2 seconds just for performing the http request returning its result, not counting the time it took in my client code to deserialze the results from json. Removing 2 of 3 nodes seemed to slightly improve the performance to just below 2 seconds, but this still seems really slow to me.</p>\n\n<p>Is this to be expected? The objects were not that large in bytesize and 2000 objects in one bucket isnt that much, either. </p>\n\n<p><strong>Insert</strong></p>\n\n<p>Batch inserting of around 60.000 objects in the same size as above took rather long and actually didnt really work. </p>\n\n<p>My script which inserted the objects in riak died at around 40.000 or so and said it couldnt connect to the riak node anymore. In the riak logs I found an error message which indicated that the node ran out of memory and died. </p>\n\n<p><strong>Question</strong></p>\n\n<p>This is really my first shot at riak, so there is definately the chance that I screwed something up. </p>\n\n<ul>\n<li>Are there any settings I could tweak?</li>\n<li>Are the hardware settings too constrained?</li>\n<li>Maybe the PHP client library I used for interacting with riak is the limiting factor here?</li>\n<li>Running all nodes on the same physical machine is rather stupid, but if this is a problem - <em>how can i better test the performance of riak</em>?</li>\n<li>Is map reduce really that slow? I read about the performance hit that map reduce has on the riak mailing list, but if Map Reduce is slow, how are you supposed to perform \"queries\" for data needed nearly in realtime? I know that riak is not as fast as redis.</li>\n</ul>\n\n<p>It would really help me a lot if anyone with more experience in riak could help me out with some of these questions.</p>\n    ","a":"\n<p>I asked on the riak mailing list, you can read the answer by Sean Cribbs <a href=\"http://lists.basho.com/pipermail/riak-users_lists.basho.com/2011-May/004231.html\">here</a>. </p>\n    "},{"t":"Cassandra file structure - how are the files used?","l":"http://stackoverflow.com/questions/2359175/cassandra-file-structure-how-are-the-files-used","q":"\n\n<p>When experimenting with Cassandra I've observed that Cassandra writes to the following files:</p>\n\n<pre><code>/.../cassandra/commitlog/CommitLog-&lt;id&gt;.log\n/.../cassandra/data/Keyspace1/Standard1-1-Data.db\n/.../cassandra/data/Keyspace1/Standard1-1-Filter.db\n/.../cassandra/data/Keyspace1/Standard1-1-Index.db\n/.../cassandra/data/system/LocationInfo-1-Data.db\n/.../cassandra/data/system/LocationInfo-1-Filter.db\n/.../cassandra/data/system/LocationInfo-1-Index.db\n/.../cassandra/data/system/LocationInfo-2-Data.db\n/.../cassandra/data/system/LocationInfo-2-Filter.db\n/.../cassandra/data/system/LocationInfo-2-Index.db\n/.../cassandra/data/system/LocationInfo-3-Data.db\n/.../cassandra/data/system/LocationInfo-3-Filter.db\n/.../cassandra/data/system/LocationInfo-3-Index.db\n/.../cassandra/system.log\n</code></pre>\n\n<p>The general structure seems to be:</p>\n\n<pre><code>/.../cassandra/commitlog/CommitLog-ID.log\n/.../cassandra/data/KEYSPACE/COLUMN_FAMILY-N-Data.db\n/.../cassandra/data/KEYSPACE/COLUMN_FAMILY-N-Filter.db\n/.../cassandra/data/KEYSPACE/COLUMN_FAMILY-N-Index.db\n/.../cassandra/system.log\n</code></pre>\n\n<p>What is the Cassandra file structure? More specifically, how are the <code>data</code>, <code>commitlog</code> directories used, and what is the structure of the files in the <code>data</code> directory (<code>Data</code>/<code>Filter</code>/<code>Index</code>)?</p>\n    ","a":"\n<p>A write to a Cassandra node first hits the <strong>CommitLog</strong> (sequential). (Then Cassandra stores values to column-family specific, in-memory data structures called Memtables. The Memtables are flushed to disk whenever one of the configurable thresholds is exceeded. (1, datasize in memtable. 2, # of objects reach certain limit, 3, lifetime of a memtable expires.))</p>\n\n<p>The <strong>data</strong> folder contains a subfolder for each keyspace. Each subfolder contains three kind of files:</p>\n\n<ul>\n<li>Data files: An SSTable (nomenclature\nborrowed from Google) stands for\nSorted Strings Table and is a file of\nkey-value string pairs (sorted by\nkeys).</li>\n<li>Index file: (Key, offset) pairs (points into data file) </li>\n<li><a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">Bloom filter</a>: all keys in data file</li>\n</ul>\n    "},{"t":"Why are key value pair noSQL db's faster than traditional relational DBs","l":"http://stackoverflow.com/questions/2354254/why-are-key-value-pair-nosql-dbs-faster-than-traditional-relational-dbs","q":"\n\n<p>It has been recommended to me that I investigate Key/Value pair data systems to replace a relational database I have been using.</p>\n\n<p>What I am not quite understanding is how this improves efficiency of queries. From what I understand you are going to be throwing away a lot information that would help to make queries more efficient, by simply turning your structure database into one big long list of keys and values?</p>\n\n<p>Have I missed the point completely?</p>\n    ","a":"\n<p>The key advantage of a relational database is the ability to relate and index information. Most 'NoSQL' systems don't provide a relational algebra or a great query language.</p>\n\n<p>What you need to ask yourself is, does switching make sense for my intended use case?</p>\n\n<p>You have kind of missed the point. The point is, you sometimes don't have an index (in the way you do with a general relational DB anyways). Even when you do have an index, the ability to relate it together is difficult and what relational databases excel at. NoSQL solutions have a number of novel structure which make many usecases trivially easy, e.g. Redis is a data-structure oriented DB well-suited to rapidly building anything with queues or its pub-sub architecture. MongoDB is a freeform document database which stores documents as JSON (BSON) and excels at rapid development. BigTable solutions are a little less structured that that but expand the idea of a row to have families of columns â€” key value pairs contained in each row arranged efficiently on disk. You can build an inverted index on top of this with a technology like ElasticSearch.</p>\n\n<p>Not everything need the consistency guarantees or disk layout of a traditional RDBMS. Another major usecase of NoSQL is massive scale, many solutions (e.g. BigTable -- HBase/Cassandra) are designed to shard and scale horizontally easily (not so easy with SQL!). Cassandra in particular is designed for no SPOF. Further, column-oriented datastores are meant to optimize disk speeds via sequential reads (and reduce <a href=\"http://en.wikipedia.org/wiki/Write_amplification\" rel=\"nofollow\">write-amplification</a>). That being said, unless you really need it, a traditional SQL server is generally good enough.</p>\n\n<p>There's advantages and disadvantages. Personally, I use a mix of both. Use the right tool for the right job but that may end up being PostgreSQL or MySQL more often than not. </p>\n\n<p>You can liken a basic key-value system to making an SQL table with two columns, a unique key and a value. This is quite fast. You have no need to do any relations or correlations or collation of data. Just find the value and return it. This is an oversimplification, NoSQL databases do have a lot of interesting functionality and application beyond simple K,V stores.</p>\n\n<p>I don't know if your scientific data is well suited to most NoSQL implementations, that depends on the data. If you look at HBase or Cassandra, it may well suit a scientist's needs (with proper rowkey design -- timestamp must not be first, check out OpenTSDB). I know of many companies that store sensor readings in Cassandra by using a random-order partitioner and the UUID of the sensor to roll up readings into daily fat rows. Every day new databases are created around specific use cases, so that answer may change. For specific use cases, you can reap huge rewards for using specific datastores at the cost of flexibility and tooling.</p>\n    "},{"t":"Changing â€œschemaâ€ in RavenDB","l":"http://stackoverflow.com/questions/4776873/changing-schema-in-ravendb","q":"\n\n<p>Just for the interest of expanding my knowledge I have started looking at various NoSQL options. The first one I visited is RavenDB and it looks interesting. I am still trying to break my deep-seated relational thinking, along with the typical RDBMS maintenance routines.</p>\n\n<p>In my day-to-day dealing with Entity Framework we go through the routine of scripting DB changes, refreshing the EF mapping model, etc. How does it work in NoSQL stuff, especially RavenDB? Once an app has gone life how does one make changes to the various POCO objects, etc. and deploy it to production? What happens to data stored in the old POCO classes?</p>\n\n<p>I haven't delved deep or used Raven DB in anger yet. This may be obvious once I do but would love to know before hand so I don't code myself into a corner.</p>\n\n<p>Thanks,\nD.</p>\n    ","a":"\n<p>They stay as they are - properties not existing anymore will be ignored when loading (and lost on change), and missing properties will come back as null,</p>\n\n<p>Recommend you use set based operations to keep data in check with object model.</p>\n\n<p><em>Oh, look at me, I'm on a computer now!</em></p>\n\n<p>Right so basically, in moving to a document store you are right in recognising that you lose some functionality and gain some freedom in that in a database you have an up-front schema defined and trying to upload data that doesn't match that schema will result in an error.</p>\n\n<p>It is important to recognise however, that there is a difference between schema-less and structure-less, in that your documents all contain their own structure (key/value pairs denoting property name and property value).</p>\n\n<p>This makes it useful for the whole \"just getting on\" factor of writing some code and having your data persisted - but when being so easy to go around changing your code structure it can be harder to reconcile that with your already persisted data.</p>\n\n<p>A few strategies present themselves at this point:</p>\n\n<ul>\n<li>Make your structure immutable once you have persisted data, version your classes</li>\n<li>Allow modification of structure, but use set-based operations to update data to match new structure</li>\n<li>Allow modification of structure, and write code to deal with inconsistencies when loading data</li>\n</ul>\n\n<p>The third one is clearly a bad idea as it will lead to unmaintainable code, versioning your classes can work if you're just storing events or other such data but isn't really appropriate for most scenarios, so you're left with the middle option.</p>\n\n<p>I'd recommend doing just that, and following a few simple rules along the same lines as you'd follow when dealing with an up-front schema in a relational database.</p>\n\n<ul>\n<li>Use your VCS system to determine changes between deployed versions</li>\n<li>Write migration scripts that upgrade from one version to another</li>\n<li>Be careful of renames/removing properties - as loading a document and saving the document will result in lost data if those properties don't exist on the new document</li>\n</ul>\n\n<p>Etc.</p>\n\n<p>I hope this is more helpful :-)</p>\n    "},{"t":"Use Entity framework code first with nosql database","l":"http://stackoverflow.com/questions/12352757/use-entity-framework-code-first-with-nosql-database","q":"\n\n<p>Can I use Entity Framework Code First Approach with NoSql Database?\nAnd how NoSql can be advantage over SQL Database for a application with large data.</p>\n    ","a":"\n<p>I have been using BrightstarDb (<a href=\"http://www.brightstardb.com/\">http://www.brightstardb.com/</a>) for the past few months and it offers code-first model generation based on Entity Framework. </p>\n\n<p>BrightstarDb is a NoSql database that is also an RDF triplestore, so it's allows for more complicated relations between Entities than most traditional NoSql databases.</p>\n    "},{"t":"How to find a substring in a field in Mongodb","l":"http://stackoverflow.com/questions/10242501/how-to-find-a-substring-in-a-field-in-mongodb","q":"\n\n<p>How can I find all the objects in a database with where a field of a object contains a substring?</p>\n\n<p>If the field is A in an object of a collection with a string value:</p>\n\n<p>I want to find all the objects in the db \"database\" where A contains a substring say \"abc def\".</p>\n\n<p>I tried: </p>\n\n<pre><code>db.database.find({A: {$regex: '/^*(abc def)*$/''}})\n</code></pre>\n\n<p>but didn't work</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>A real string (in unicode):</p>\n\n<pre><code>Sujet  Commentaire sur  Star Wars  Episode III - La Revanche des Sith 1\n</code></pre>\n\n<p>Need to search for all entries with Star Wars</p>\n\n<pre><code>db.test.find({A: {$regex: '^*(star wars)*$''}}) not wokring\n</code></pre>\n    ","a":"\n<p>Instead of this:</p>\n\n<pre><code>db.database.find({A: {$regex: '/^*(abc def)*$/''}})\n</code></pre>\n\n<p>You should do this:</p>\n\n<pre><code>db.database.find({A: /abc def/i })\n</code></pre>\n\n<p>^* is not actually valid syntax as ^ and $ are anchors and not something that is repeatable. You probably meant ^.* here. But there is no need for ^.* as that simply means \"Everything up to the character following\" and (abc def)* means \"0 or more times \"abc def\", but it has to be at the end of the string, because of your $. The \"i\" at the end is to make it case insensitive.</p>\n    "},{"t":"Is it OK to query a MongoDB multiple times per request?","l":"http://stackoverflow.com/questions/5157265/is-it-ok-to-query-a-mongodb-multiple-times-per-request","q":"\n\n<p>Coming from an RDBMS background, I was always under the impression \"Try as hard as you can to use one query, assuming it's efficient,\" meaning that it's costly for every request you make to the database. When it comes to MongoDB, it seems like this might not be possible because you can't join tables.</p>\n\n<p>I understand that it's not supposed to be relational, but they're also pushing it for purposes like blogs, forums, and things I'd find an RDBMS easier to approach with.</p>\n\n<p>There are some hang ups I've had trying to understand the efficiency of MongoDB or NoSQL in general. If I wanted to get all \"posts\" related to certain users (as if they were grouped)... using MySQL I'd probably do some joins and get it with that.</p>\n\n<p>In MongoDB, assuming I need the collections separate, would it be efficient to use a large $in: ['user1', 'user2', 'user3', 'user4', ...] ? </p>\n\n<p>Does that method  get slow after a while? If I include 1000 users? \nAnd if I needed to get that list of posts related to users X,Y,Z, would it be efficient and/or fast using MongoDB to do:</p>\n\n<ul>\n<li>Get users array</li>\n<li>Get Posts IN users array</li>\n</ul>\n\n<p>2 queries for one request. Is that bad practice in NoSQL?</p>\n    ","a":"\n<p>To answer the Q about $in....</p>\n\n<p>I did some performance tests with the following scenario:</p>\n\n<p>~24 million docs in a collection<br>\nLookup 1 million of those documents based on a key (indexed)<br>\nUsing CSharp driver from .NET<br></p>\n\n<p><b>Results:</b><br>\nQuerying 1 at a time, single threaded : 109s<br>\nQuerying 1 at a time, multi threaded : 48s<br>\nQuerying 100K at a time using $in, single threaded=20s<br>\nQuerying 100K at a time using $in, multi threaded=9s<br></p>\n\n<p>So noticeably better performance using a large $in (restricted to max query size).</p>\n\n<p><b>Update:</b>\nFollowing on from comments below about how $in performs with different chunk sizes (queries multi-threaded):</p>\n\n<p>Querying 10 at a time (100000 batches) = 8.8s  <br>\nQuerying 100 at a time (10000 batches) = 4.32s<br>\nQuerying 1000 at a time (1000 batches) = 4.31s<br>\nQuerying 10000 at a time (100 batches) = 8.4s<br>\nQuerying 100000 at a time (10 batches) = 9s (per original results above)</p>\n\n<p>So there does look to be a sweet-spot for how many values to batch up in to an $in clause vs. the number of round trips</p>\n    "},{"t":"Best solution for finding 1 x 1 million set intersection? Redis, Mongo, other","l":"http://stackoverflow.com/questions/11095331/best-solution-for-finding-1-x-1-million-set-intersection-redis-mongo-other","q":"\n\n<p>Hi all and thanks in advance.\nI am new to the NoSQL game but my current place of employment has tasked me with set comparisons of some big data.</p>\n\n<p>Our system has customer tag set and targeted tag sets.\nA tag is an 8 digit number.<br>\nA customer tag set may have up to 300 tags but averages 100 tags<br>\nA targeted tag set may have up to 300 tags but averages 40 tags.  </p>\n\n<p>Pre calculating is not an option as we are shooting for a potential customer base of a billion users.</p>\n\n<p>(These tags are hierarchical so having one tag implies that you also have its parent and ancestor tags.  Put that info aside for the moment.)</p>\n\n<p>When a customer hits our site, we need to intersect their tag set against one million targeted tag sets as fast as possible. The customer set must contain all elements of the targeted set to match.   </p>\n\n<p>I have been exploring my options and the set intersection in Redis seems like it would be ideal.   However,  my trolling through the internet has not revealed how much ram would be required to hold one million tag sets.  I realize the intersection would be lightning fast, but is this a feasable solution with Redis.  </p>\n\n<p>I realize this is brute force and inefficient.  I also wanted to use this question as means to get suggestions for ways this type of problem has been handled in the past. As stated before, the tags are stored in a tree.  I have begun looking at Mongodb as a possible solution as well. </p>\n\n<p>Thanks again</p>\n    ","a":"\n<p>This is an interesting problem, and I think Redis can help here.</p>\n\n<p>Redis can store sets of integers using an optimized \"intset\" format. See <a href=\"http://redis.io/topics/memory-optimization\">http://redis.io/topics/memory-optimization</a> for more information.</p>\n\n<p>I believe the correct data structure here is a collection of targeted tag sets, plus a reverse index to map tags to their targeted tag sets.</p>\n\n<p>To store two targeted tag sets:</p>\n\n<pre><code> 0 -&gt; [ 1 2 3 4 5 6 7 8 ]\n 1 -&gt; [ 6 7 8 9 10 ]\n</code></pre>\n\n<p>I would use:</p>\n\n<pre><code> # Targeted tag sets\n sadd tgt:0 1 2 3 4 5 6 7 8\n sadd tgt:1 2 6 7 8 9 10\n # Reverse index\n sadd tag:0 0\n sadd tag:1 0\n sadd tag:2 0 1\n sadd tag:3 0\n sadd tag:4 0\n sadd tag:5 0\n sadd tag:6 0 1\n sadd tag:7 0 1\n sadd tag:8 0 1\n sadd tag:9 1\n sadd tag:10 1\n</code></pre>\n\n<p>This reverse index is quite easy to maintain when targeted tag sets are added/removed from the system.</p>\n\n<p>The global memory consumption depends on the number of tags which are common to multiple targeted tag sets. It is quite easy to store pseudo-data in Redis and simulate the memory consumption. I have done it using a <a href=\"https://gist.github.com/2956241\">simple node.js script</a>.</p>\n\n<p>For 1 million targeted tag sets (tags being 8 digits numbers, 40 tags per set), the memory consumption is close to <strong>4 GB</strong> when there are very few tags shared by the targeted tag sets (more than 32M entries in the reverse index), and about <strong>500 MB</strong> when the tags are shared a lot (only 100K entries in the reverse index).</p>\n\n<p>With this data structure, finding the targeted tag sets containing all the tags of a given customer is extremely efficient.</p>\n\n<pre><code>1- Get customer tag set (suppose it is 1 2 3 4)\n2- SINTER tag:1 tag:2 tag:3 tag:4\n   =&gt; result is a list of targeted tag sets having all the tags of the customer\n</code></pre>\n\n<p>The intersection operation is efficient because Redis is smart enough to order the sets per cardinality and starts with the set having the lowest cardinality.</p>\n\n<p>Now I understand you need to implement the converse operation (i.e. finding the targeted tag  sets having all their tags in the customer tag set). The reverse index can still help.</p>\n\n<p>Here in an example in ugly pseudo-code:</p>\n\n<pre><code>1- Get customer tag set (suppose it is 1 2 3 4)\n2- SUNIONSTORE tmp tag:1 tag:2 tag:3 tag:4\n   =&gt; result is a list of targeted tag sets having at least one tag in common with the customer\n3- For t in tmp (iterating on the selected targeted tag sets)\n      n = SCARD tgt:t (cardinality of the targeted tag sets)\n      intersect = SINTER customer tgt:t\n      if n == len(intersect), this targeted tag set matches\n</code></pre>\n\n<p>So you never have to test the customer tag set against 1M targeted tag sets. You can rely on the reverse index to restrict the scope of the search to an acceptable level.</p>\n    "},{"t":"Downsides of storing binary data in Riak?","l":"http://stackoverflow.com/questions/6102940/downsides-of-storing-binary-data-in-riak","q":"\n\n<p>What are the problems, if any, of storing binary data in Riak?</p>\n\n<p>Does it effect the maintainability and performance of the clustering?</p>\n\n<p>What would the performance differences be between using Riak for this rather than a distributed file system?</p>\n    ","a":"\n<p>Adding to @Oscar-Godson's excellent answer, you're likely to experience problems with values much larger than 50MBs. Bitcask is best suited for values that are up to a few KBs. If you're storing large values, you may want to consider alternative storage backends, such as <a href=\"http://wiki.basho.com/Setting-Up-Innostore.html\" rel=\"nofollow\">innostore</a>.</p>\n\n<p>I don't have experience with storing binary values, but we've a medium-sized cluster in production (5 nodes, on the order of 100M values, 10's of TBs) and we're seeing frequent errors related to inserting and retrieving values that are 100's of KBs in size. Performance in this case is inconsistent - some times it works, others it doesn't - so if you're going to test, test at scale.</p>\n\n<p>We're also seeing problems with large values when running map-reduce queries - they simply time out. However that may be less relevant to binary values... (as @Matt-Ranney mentioned).</p>\n\n<p>Also see @Stephen-C's answer <a href=\"http://stackoverflow.com/questions/6008576/bitcask-ok-for-simple-and-high-performant-file-store\">here</a></p>\n    "},{"t":"How does Meteor receive updates to the results of a MongoDB query?","l":"http://stackoverflow.com/questions/10103541/how-does-meteor-receive-updates-to-the-results-of-a-mongodb-query","q":"\n\n<p>I asked a question a few months ago, to which Meteor seems to have the answer.</p>\n\n<p><a href=\"http://stackoverflow.com/questions/5257177/which-if-any-of-the-nosql-databases-can-provide-stream-of-changes-to-a-query\">Which, if any, of the NoSQL databases can provide stream of *changes* to a query result set?</a></p>\n\n<p>How does Meteor receive updates to the results of a MongoDB query?</p>\n\n<p>Thanks,</p>\n\n<p>Chris.</p>\n    ","a":"\n<p>From the docs:</p>\n\n<blockquote>\n  <ul>\n  <li><p>On the server, a collection with that name is created on a backend Mongo server. When you call methods on that collection on the server,\n  they translate directly into normal Mongo operations.</p></li>\n  <li><p>On the client, a Minimongo instance is created. Minimongo is essentially an in-memory, non-persistent implementation of Mongo in\n  pure JavaScript. It serves as a local cache that stores just the\n  subset of the database that this client is working with. Queries on\n  the client (find) are served directly out of this cache, without\n  talking to the server.</p></li>\n  </ul>\n  \n  <p>When you write to the database on the client (insert, update, remove),\n  the command is executed immediately on the client, and,\n  simultaneously, it's shipped up to the server and executed there too.\n  The livedata package is responsible for this.</p>\n</blockquote>\n\n<p>That explains client to server</p>\n\n<p>Server to client from what I can gather is the livedata and mongo-livedata packages.</p>\n\n<p><a href=\"https://github.com/meteor/meteor/tree/master/packages/mongo-livedata\" rel=\"nofollow\">https://github.com/meteor/meteor/tree/master/packages/mongo-livedata</a></p>\n\n<p><a href=\"https://github.com/meteor/meteor/tree/master/packages/livedata\" rel=\"nofollow\">https://github.com/meteor/meteor/tree/master/packages/livedata</a></p>\n\n<p>Hope that helps.</p>\n    "},{"t":"HBase cassandra couchdb mongodb..any fundamental difference?","l":"http://stackoverflow.com/questions/3652310/hbase-cassandra-couchdb-mongodb-any-fundamental-difference","q":"\n\n<p>I just wanted to know if there is a fundamental difference between hbase, cassandra, couchdb and monogodb ? In other words, are they all competing in the exact same market and trying to solve the exact same problems. Or they fit best in different scenarios?</p>\n\n<p>All this comes to the question, what should I chose when. Matter of taste?</p>\n\n<p>Thanks, </p>\n\n<p>Federico </p>\n    ","a":"\n<p>Those are some long answers from <strong>@Bohzo</strong>. (but they are good links)</p>\n\n<p>The truth is, they're \"kind of\" competing. But they definitely have different strengths and weaknesses and they definitely don't all solve the same problems.</p>\n\n<p>For example Couch and Mongo both provide Map-Reduce engines as part of the main package. HBase is (basically) a layer over top of Hadoop, so you also get M-R via Hadoop. Cassandra is highly focused on being a Key-Value store and has plug-ins to \"layer\" Hadoop over top (so you can map-reduce).</p>\n\n<p>Some of the DBs provide MVCC (Multi-version concurrency control). Mongo does not.</p>\n\n<p>All of these DBs are intended to scale horizontally, but they do it in different ways. All of these DBs are also trying to provide flexibility in different ways. Flexible document sizes or REST APIs or high redundancy or ease of use, they're all making different trade-offs.</p>\n\n<p>So to your question: <em>In other words, are they all competing in the exact same market and trying to solve the exact same problems?</em></p>\n\n<ol>\n<li><strong>Yes</strong>: they're all trying to solve the issue of database-scalability and performance.</li>\n<li><strong>No</strong>: they're definitely making different sets of trade-offs.</li>\n</ol>\n\n<p>What should you start with?</p>\n\n<p>Man, that's a tough question. I work for a large company pushing tons of data and we've been through a few years. We tried Cassandra at one point a couple of years ago and it couldn't handle the load. We're using Hadoop everywhere, but it definitely has a steep learning curve and it hasn't worked out in some of our environments. More recently we've tried to do Cassandra + Hadoop, but it turned out to be a lot of configuration work.</p>\n\n<p>Personally, my department is moving several things to <strong>MongoDB</strong>. Our reasons for this are honestly just simplicity. </p>\n\n<p>Setting up Mongo on a linux box takes minutes and doesn't require root access or a change to the file system or anything fancy. There are no crazy config files or java recompiles required. So from that perspective, Mongo has been the easiest \"gateway drug\" for getting people on to KV/Document stores.</p>\n    "},{"t":"Cassandra - transaction support","l":"http://stackoverflow.com/questions/2976932/cassandra-transaction-support","q":"\n\n<p>I am going through apache cassandra and working on sample data insertion, retrieving etc.</p>\n\n<p>The documentation is very limited.</p>\n\n<p>I am interested in knowing</p>\n\n<ul>\n<li>can we completely replace relation db like mysql/ oracle with cassandra?</li>\n<li>does cassandra support rollback/ commit?</li>\n<li>does cassandra clients (thrift/ hector) support fetching associated object (objects where we save one super columns' key in another super column family)?</li>\n</ul>\n\n<p>This will help me a lot to proceed further.</p>\n\n<p>thank you in advance.</p>\n    ","a":"\n<p>Short answer: No.</p>\n\n<p>By design, Cassandra values availability and partition tolerance over consistency<a href=\"http://wiki.apache.org/cassandra/ArchitectureOverview\">1</a>. Basically, it's not possible to get acceptable latency while maintaining all three of qualities: one has to be sacrificed. This is called CAP theorem.</p>\n\n<p>The amount of consistency is configurable in Cassandra using consistency levels, but there doesn't exist any semantics for rollback. There's no guaranty that you'll be able to roll back your changes even if the first write succeeds.</p>\n\n<p>If you wan't to build application with transactions or locks on top of Cassandra, you probably want to look at Zookeeper, which can be used to provide distributed synchronization.</p>\n\n<p>You might've already guessed this, but Cassandra doesn't have foreign keys or anything like that. This has to be handled manually. I'm not that familiar with Hector, but a higher-level client could be able to do this semi-automatically.</p>\n\n<p>Whether or not you can use Cassandra to easily replace a RDBMS depends on your specific use case. In your use case (based on your questions), it might be hard to do so.</p>\n    "},{"t":"I cannot run Enmbedded RavenDB","l":"http://stackoverflow.com/questions/5480168/i-cannot-run-enmbedded-ravendb","q":"\n\n<p>I was able to successfully run a simple test for RavenDB based on the code found at: <a href=\"http://ravendb.net/tutorials/hello-world\" rel=\"nofollow\">http://ravendb.net/tutorials/hello-world</a></p>\n\n<p>Next I tried to run it in an Embedded Manner, but I keep on getting the following error:  </p>\n\n<pre><code>Message: Could not find transactional storage type: Raven.Storage.Esent.TransactionalStorage, Raven.Storage.Esent  \nStackTrace:    at Raven.Database.Config.InMemoryRavenConfiguration.CreateTransactionalStorage(Action notifyAboutWork) in c:\\Builds\\raven\\Raven.Database\\Config\\InMemoryRavenConfiguration.cs:line 272\n   at Raven.Database.DocumentDatabase..ctor(InMemoryRavenConfiguration configuration) in c:\\Builds\\raven\\Raven.Database\\DocumentDatabase.cs:line 109\n   at Raven.Client.Client.EmbeddableDocumentStore.InitializeInternal() in c:\\Builds\\raven\\Raven.Client.Embedded\\EmbeddableDocumentStore.cs:line 130\n   at Raven.Client.Document.DocumentStore.Initialize() in c:\\Builds\\raven\\Raven.Client.Lightweight\\Document\\DocumentStore.cs:line 388\n   at Tests.RavenEmbedded.RavenDB..ctor() in C:\\Users\\Pranav\\Documents\\Projects\\Repositories-Clone\\Common-clone\\Tests\\RavenDB.cs:line 114\n   at Tests.TestRavenDB.Basics() in C:\\Users\\Pranav\\Documents\\Projects\\Repositories-Clone\\Common-clone\\Tests\\RavenDB.cs:line 170 \n</code></pre>\n\n<hr>\n\n<p><strong>Setup:</strong> </p>\n\n<p>Target framework is <em>.NET Framework 4</em></p>\n\n<p>I added the following References to my project:</p>\n\n<ol>\n<li>\\RavenDB-Build-309\\EmbeddedClient\\Raven.Client.Embedded.dll</li>\n<li>\\RavenDB-Build-309\\Client\\Raven.Client.Lightweight.dll</li>\n<li>\\RavenDB-Build-309\\EmbeddedClient\\Raven.Storage.Esent.dll</li>\n<li>\\RavenDB-Build-309\\EmbeddedClient\\Raven.Storage.Managed.dll</li>\n</ol>\n\n<hr>\n\n<p><strong>The code is:</strong></p>\n\n<pre>namespace Tests.RavenEmbedded\n{\n    using Raven.Client.Client;\n    using Raven.Client.Document;\n    using Raven.Storage.Esent;\n    using Raven.Storage.Managed;\n    using Tests.RavenData;\n\n    class RavenDB\n    {\n        public RavenDB()\n        {\n            // EmbeddableDocumentStore store = new EmbeddableDocumentStore { DataDirectory = @\"C:\\Temp\\RavenData\" };\n            //Raven.Storage.Esent.TransactionalStorage\n            var store = new EmbeddableDocumentStore  { DataDirectory = @\"C:\\Temp\\RavenData\" };\n            store.Initialize();\n\n            #region Write Data\n            using (var session = store.OpenSession())\n            {\n                var product = new Product\n                {\n                    Cost = 3.99m,\n                    Name = \"Milk\",\n                };\n                session.Store(product);\n                session.SaveChanges();\n\n                session.Store(new Order\n                {\n                    Customer = \"customers/ayende\",\n                    OrderLines =\n                      {\n                          new OrderLine\n                          {\n                              ProductId = product.Id,\n                              Quantity = 3\n                          },\n                      }\n                });\n                session.SaveChanges();\n            }\n            #endregion\n\n            #region Read Data\n            using (var session = store.OpenSession())\n            {\n                var order = session.Load(\"orders/1\");\n                Debug.Print(\"Customer: {0}\", order.Customer);\n                foreach (var orderLine in order.OrderLines)\n                {\n                    Debug.Print(\"Product: {0} x {1}\", orderLine.ProductId, orderLine.Quantity);\n                }\n                session.SaveChanges();\n            }\n\n            #endregion\n\n        }\n    }\n}\n\nnamespace Tests\n{\n    public class TestRavenDB\n    {\n        public static void Basics()\n        {\n            try\n            {\n                //var db = new RavenClientServer.RavenDB();\n                var db = new RavenEmbedded.RavenDB();\n            }\n            catch (Exception ex)\n            {\n\n                Debug.Print(\"Message: {0} \",ex.Message);\n                Debug.Print(\"StackTrace: {0} \",ex.StackTrace);\n\n            }\n        }\n\n    }\n}\n</pre>\n\n<p>I have tried searching for this for a few days and tried a few different variations too.  I am not sure what's going on.</p>\n    ","a":"\n<p>Thanks to Ayende Rahien on groups.google.com/group/ravendb/topics.</p>\n\n<p>The solution was to add \"Raven.Storage.Esent\" reference to the main project.  It's an issue with Visual Studio and indirect references.</p>\n\n<p>Thanks @Derek for suggesting that I post there.</p>\n\n<p>-- Pranav</p>\n    "},{"t":"What is BSON and exactly how is it different from JSON?","l":"http://stackoverflow.com/questions/12438280/what-is-bson-and-exactly-how-is-it-different-from-json","q":"\n\n<p>I am just starting out with mongoDb and one of the things that I have noticed is that it uses bson to store data internally.However the documentation is not exactly clear on what BSON is and how it is used in mongoDb.Can someone explain it to me please?</p>\n    ","a":"\n<p><a href=\"http://bsonspec.org/\">BSON</a> is the binary encoding of JSON-like documents that MongoDB uses when storing documents in collections.  It adds support for data types like Date and binary that aren't supported in JSON.  In practice, you don't have to know much about BSON when working with MongoDB, you just need to use its data types when constructing documents.</p>\n    "},{"t":"Namespaces in Redis?","l":"http://stackoverflow.com/questions/8614858/namespaces-in-redis","q":"\n\n<p>Is it possible to create namespaces in Redis?</p>\n\n<p>From what I found, all the global commands (count, delete all) work on all the objects. Is there a way to create sub-spaces such that these commands will be limited in context?</p>\n\n<p>I don't want to set up different Redis servers for this purpose.</p>\n\n<p>I assume the answer is \"No\", and wonder why wasn't this implemented, as it seems to be a useful feature without too much overhead.</p>\n    ","a":"\n<p>A Redis server can handle multiple databases (namespaces) ... which are numbered.  I think it provides 32 of them by default; you can access them using the -n option to the redis-cli shell scripting command and by similar options to the connection arguments or using the \"select()\" method on its connection objects.  (In this case .select() is the method name for the Python Redis module ... I presume it's named similarly for other libraries and interfaces.</p>\n\n<p>There's an option to control how many separate databases (namespaces) you want in the configuration file for the Redis server daemon as well.  I don't know what the upper limit would be and there doesn't seem to be a way to dynamically change that (in other words it seems that you'd have to shutdown and re-start the server to add additional DBs).  Also there doesn't seem to be a away to associate these DB numbers with any sort of name nor to impose separate ACLS, nor even different passwords, to them.  Redis is, of course, schema-less as well.</p>\n    "},{"t":"Graph-structured databases and Php","l":"http://stackoverflow.com/questions/2843210/graph-structured-databases-and-php","q":"\n\n<p>I want to use a graph database using php. Can you point out some resources on where to get started? Is there any example code / tutorial out there? Or are there any other methods of storing data that relate to each other in totally random/abstract situations? </p>\n\n<p>-</p>\n\n<p>Very abstract example of the relations needed: John relates to Mary, both relate to School, John is Tall, Mary is Short, John has Blue Eyes, Mary has Green Eyes, query I want is which people are related to 'Short people that have Green Eyes and go to School' -&gt; answer John</p>\n\n<p>-</p>\n\n<p>Another example: </p>\n\n<pre><code>    TrackA -&gt; ArtistA\n           -&gt; ArtistB\n\n           -&gt; AlbumA -----&gt; [ label ]\n           -&gt; AlbumB -----&gt; [   A   ]\n\n           -&gt; TrackA:Remix\n           -&gt; Genre:House\n\n           -&gt; [ Album ] -----&gt; [ label ]\n   TrackB  -&gt; [   C   ]        [   B   ]\n</code></pre>\n\n<p>Example queries: </p>\n\n<p>Which Genre is TrackB closer to? answer: House - because it's related to Album C, which is related to TrackA and is related to Genre:House</p>\n\n<p>Get all Genre:House related albums of Label A : result: AlbumA, AlbumB - because they both have TrackA which is related to Genre:House</p>\n\n<p>-</p>\n\n<p>It is possible in MySQL but it would require a fixed set of attributes/columns for each item and a complex non-flexible query, instead I need every attribute to be an item by itself and instead of 'belonging' to something, to be 'related' to something.</p>\n    ","a":"\n<p>There's some work going on to make the <a href=\"http://neo4j.org/\">Neo4j</a> graph database available from PHP, see <a href=\"http://wiki.neo4j.org/content/PHP\">this wiki page</a> for more information! Regarding how to model your domain as a graph, the <a href=\"http://neo4j.org/community/list/\">user mailing list</a> tends to be pretty awesome.</p>\n\n<p>Update: there's now a short <a href=\"http://onewheeledbicycle.com/2010/06/01/getting-started-with-neo4j-rest-api-and-php/\">getting started blog post</a> for a PHP neo4j REST client.</p>\n    "},{"t":"Which CouchDB API to use for Rails?","l":"http://stackoverflow.com/questions/2443712/which-couchdb-api-to-use-for-rails","q":"\n\n<p>I am currently investigating possible applications of CouchDB on my current project (written in Rails) and would like to get some feedback from people who have actually used these APIs.  Which would you recommend and why?</p>\n\n<ul>\n<li>ActiveCouch</li>\n<li>CouchFoo</li>\n<li>CouchRest</li>\n<li>CouchRest-Rails</li>\n<li>CouchPotato</li>\n</ul>\n    ","a":"\n<p>The basic layer of CouchRest is probably the best to get started, CouchPotato is the most active for Rails integration, SimplyStored adds some nicities on top of CouchPotato</p>\n    "},{"t":"Best Model for Representing Many to Many relationships with attributes in MongoDB","l":"http://stackoverflow.com/questions/25344444/best-model-for-representing-many-to-many-relationships-with-attributes-in-mongod","q":"\n\n<p>What's the most 'mongo' way of representing many-to-many relationships that have attributes?</p>\n\n<p>So for example:</p>\n\n<h1>Intro</h1>\n\n<hr>\n\n<p>MYSQL tables</p>\n\n<p><code>people</code> =&gt; <code>firstName, lastName, ...</code></p>\n\n<p><code>Movies</code> =&gt; <code>name, length ..</code></p>\n\n<p><code>peopleMovies</code> =&gt; <code>movieId, personId, language, role</code></p>\n\n<h1>Solution 1</h1>\n\n<hr>\n\n<p>Embed people into movies...?</p>\n\n<p>In MongoDB I understand it's good to <code>denormalize and embed</code> but I don't want to <code>embed</code> people into movies, it just doesn't logically make any sense. Because people don't necessarily only have to belongs to movies.</p>\n\n<h1>Solution 2</h1>\n\n<hr>\n\n<p><code>People</code> and <code>Movies</code> will be two separate collections. \n<code>People</code> =&gt; embed <code>[{movieId: 12, personId: 1, language: \"English\", role: \"Main\"} ...]</code></p>\n\n<p><code>Movies</code> =&gt; embed <code>[{movieId: 12, personId: 1, language: \"English\", role: \"Main\"} ...]</code></p>\n\n<p>The issue with this solution is that when we want to update a person's <code>role</code> for a specific <code>movie</code> we'll need to run two update queries to ensure data is in sync in both collections.</p>\n\n<h1>Solution 3</h1>\n\n<hr>\n\n<p>We can also do something much more relational like and end up with three collections </p>\n\n<p><code>People</code> =&gt; <code>firstName, lastName, ...</code>\n<code>Movies</code> =&gt; <code>name, length ..</code>\n<code>Castings</code> =&gt; <code>movieId, personId, language, role</code></p>\n\n<p>The issue with this is that because of the lack of a join statement in MongoDB, it would take <code>3 queries</code> to go from people -&gt; movies and vice versa.</p>\n\n<p>Here is my question, what are some other ways to model something like this in <code>MongoDB</code> and in a more <code>NoSQL</code> way. In terms of the solutions provided, which one would be the best in terms of performance and convention in mongo.</p>\n    ","a":"\n<p>In many ways meteor's API encourages flat relational documents, however MongoDB is a non-relational data store. This conflict is, unfortunately, left as an exercise for the developer to solve.</p>\n\n<p>The notion of schema structure and joins is an enormous topic to cover within a single answer, so I will attempt to be as succinct as possible.</p>\n\n<h3>Reasons why you should choose a relational model</h3>\n\n<p>Assume you have comment and post data. Consider what would happen if you embedded comments within your posts.</p>\n\n<ul>\n<li><p>DDP operates on documents. All of the comments will be sent every time a new comment in the same post is added.</p></li>\n<li><p><code>allow</code> and <code>deny</code> rules operate on documents. It may be unreasonable to expect that the same rules apply simultaneously to both posts and comments.</p></li>\n<li><p>Publications tend to make more sense in terms of collections. In the above scenario, we could not easily publish a list of comments independent of their posts.</p></li>\n<li><p>Relational databases exist for good reasons. One of them is to avoid the multiple modification problem inherent in your second solution.</p></li>\n</ul>\n\n<h3>Reasons why you should choose an embedded model</h3>\n\n<ul>\n<li>Joins are not supported natively by MongoDB, and there isn't a core package to produce a reactive join.</li>\n</ul>\n\n<h3>Recommendations</h3>\n\n<p>Use your third solution. In my experience, the reasons for choosing a relational model far outweigh the restrictions imposed by the data store. Of course overcoming the lack of joins isn't easy, but the pain is likely to be isolated to only a handful of publish functions. Here are some resources I'd highly recommend:</p>\n\n<ul>\n<li><p><a href=\"https://www.eventedmind.com/feed/meteor-how-to-publish-a-many-to-many-relationship\">How to publish a many-to-many relationship</a> on EventedMind. Chris covers your exact use case in detail, however he manually does the reactive join with observe callbacks, which I don't recommend.</p></li>\n<li><p><a href=\"https://www.discovermeteor.com/blog/reactive-joins-in-meteor/\">Reactive joins in meteor</a> from the <a href=\"https://www.discovermeteor.com/encyclopedia\">Discover Meteor Encyclopedia</a>. This covers the basics of how and why one should do a reactive join.</p></li>\n<li><p>The denormalization chapter from <a href=\"https://www.discovermeteor.com/\">Discover Meteor</a>. This covers many of the points I made above and also talks about when and how to denormalize some of your data.</p></li>\n<li><p>You can use <a href=\"https://atmospherejs.com/tmeasday/publish-with-relations\">Publish with relations</a> to join your data. Alternative packages include: <a href=\"https://github.com/yeputons/meteor-smart-publish\">smart publish</a>, <a href=\"https://github.com/englue/meteor-publish-composite\">publish composite</a>, and <a href=\"https://github.com/copleykj/meteor-simple-publish\">simple publish</a>.</p></li>\n</ul>\n\n<p>If you need more information beyond this, please comment below and I will update my answer.</p>\n    "},{"t":"Need advice on MongoDB Schema for Chat App. Embedded vs Related Documents","l":"http://stackoverflow.com/questions/3652632/need-advice-on-mongodb-schema-for-chat-app-embedded-vs-related-documents","q":"\n\n<p>I'm starting a MongoDB project just for kicks and as a chance to learn MongoDB/NoSQL schemas.  It'll be a live chat app and the stack includes: Rails 3, Ruby 1.9.2, Devise, Mongoid/MongoDB, CarrierWave, Redis, JQuery.</p>\n\n<p>I'll be handling the live chat polling/message queueing separately.  Not sure how yet, either Node.js, APE or custom EventMachine app.  But in regards to Mongo, I'm thinking to use it for everything else in the app, specifically chat logs and historical transcripts.</p>\n\n<p>My question is how best to design the schema as all my previous experience has been with MySQL and relational DB schema's.  And as a sub-question, when is it best to us embedded documents vs related documents.</p>\n\n<p>The app will have:</p>\n\n<ul>\n<li>Multiple accounts which have multiple rooms</li>\n<li>Multiple rooms</li>\n<li>Multiple users per room</li>\n<li>List of rooms a user is allowed to be in</li>\n<li>Multiple user chats per room</li>\n<li>Searchable chat logs on a per room and per user basis</li>\n<li>Optional file attachment for a given chat</li>\n</ul>\n\n<p>Given Mongo (at least last time I checked) has a document limit of 4MB, I don't think having a collection for rooms and storing all room chats as embedded documents would work out so well.</p>\n\n<p>From what I've thought about so far, I'm thinking of doing something like:</p>\n\n<ul>\n<li>A collection for accounts</li>\n<li>A collection for rooms\n<ul>\n<li>Each room relates back to an account</li>\n<li>Related documents in chats collections for all chat messages in the room</li>\n<li>Embedded Document listing all users currently in the room</li>\n</ul></li>\n<li>A collection for users\n<ul>\n<li>Embedded Document listing all the rooms the user is currently in</li>\n<li>Embedded Document listing all the rooms the user is allowed to be in</li>\n</ul></li>\n<li>A collection for chats\n<ul>\n<li>Each chat relates back to a room in the rooms collection</li>\n<li>Each chat relates back to a user in the users collection</li>\n<li>Embedded document with info about optional uploaded file attachment.</li>\n</ul></li>\n</ul>\n\n<p>My main concern is how far do I go until this ends up looking like a relational schema and I defeat the purpose? There is definitely more relating than embedding going on.</p>\n\n<p>Another concern is that referencing related documents is much slower than accessing embedded documents I've heard. </p>\n\n<p>I want to make generic queries such as:</p>\n\n<ul>\n<li>Give me all rooms for an account</li>\n<li>Give me all chats in a room (or filtered via date range)</li>\n<li>Give me all chats from a specific user</li>\n<li>Give me all uploaded files in a given room or for a given org</li>\n<li>etc</li>\n</ul>\n\n<p>Any suggestions on how to structure the schema efficiently in a way that scales? Thanks everyone.</p>\n    ","a":"\n<p>I think you're pretty much on the right track. I'd use a <a href=\"http://www.mongodb.org/display/DOCS/Capped+Collections\" rel=\"nofollow\">capped collection</a> for chat lines, with each line containing the user ID, room ID, timestamp, and what was said. This data would expire once the capped collection's \"end\" is reached, so if you needed a historical log you'd want to copy data out of the capped collection into a \"log\" collection periodically, but capped collections are specifically designed for logging-style applications where you aren't going to be deleting documents, and insertion order matters. In the case of chat, it's a perfect match.</p>\n\n<p>The only other change I'd suggest would be to maintain uploads in a separate collection, as well.</p>\n    "},{"t":"Delete all nodes and relationships in neo4j 1.8","l":"http://stackoverflow.com/questions/14252591/delete-all-nodes-and-relationships-in-neo4j-1-8","q":"\n\n<p>I know this question is asked by many people already<br>\nfor my research, here's some questions asked before</p>\n\n<ol>\n<li><a href=\"http://stackoverflow.com/questions/12899538/how-to-delete-all-relationships-in-neo4j-graph\">How to delete all relationships in neo4j graph?</a></li>\n<li><a href=\"https://groups.google.com/forum/#!topic/neo4j/lgIaESPgUgE\">https://groups.google.com/forum/#!topic/neo4j/lgIaESPgUgE</a></li>\n</ol>\n\n<p>But after all, still can't solve our problems,<br>\nwe just want to delete \"ALL\" nodes and \"ALL\" relationships</p>\n\n<p><img src=\"http://i.stack.imgur.com/ZuKFY.png\" alt=\"enter image description here\"></p>\n\n<p>suppose delete \"ALL\" can see there are left <strong>0 nodes 0 properties and 0 relationships</strong></p>\n\n<p>This is the screenshot i took after executing the delete \"ALL\" suggested by forum </p>\n\n<p>My question still the same, how do delete all nodes and all relationships in neo4j</p>\n    ","a":"\n<p>The syntax for this recently changed. The ? is no longer used.</p>\n\n<pre><code>MATCH (n)\nOPTIONAL MATCH (n)-[r]-()\nDELETE n,r\n</code></pre>\n\n<p><a href=\"http://docs.neo4j.org/chunked/stable/query-delete.html#delete-delete-all-nodes-and-relationships\">Neo4j Docs</a></p>\n    "},{"t":"Has anyone used an object database with a large amount of data?","l":"http://stackoverflow.com/questions/2502742/has-anyone-used-an-object-database-with-a-large-amount-of-data","q":"\n\n<p>Object databases like MongoDB and db4o are getting lots of publicity lately.  Everyone that plays with them seems to love it.  I'm guessing that they are dealing with about 640K of data in their sample apps.</p>\n\n<p>Has anyone tried to use an object database with a large amount of data (say, 50GB or more)?  Are you able to still execute complex queries against it (like from a search screen)?  How does it compare to your usual relational database of choice?  </p>\n\n<p>I'm just curious.  I want to take the object database plunge, but I need to know if it'll work on something more than a sample app.</p>\n    ","a":"\n<p>Someone just went into production with a 12 terabytes of data in MongoDB.  The largest I knew of before that was 1 TB.  Lots of people are keeping really large amounts of data in Mongo.</p>\n\n<p>It's important to remember that Mongo works a lot like a relational database: you need the right indexes to get good performance.  You can use explain() on queries and contact <a href=\"http://groups.google.com/group/mongodb-user/\">the user list</a> for help with this.</p>\n    "},{"t":"MongoDB: match non-empty doc in array","l":"http://stackoverflow.com/questions/6607102/mongodb-match-non-empty-doc-in-array","q":"\n\n<p>I have a collection structured thusly:</p>\n\n<pre><code>{\n  _id: 1,\n  score: [\n    {\n      foo: 'a',\n      bar: 0,\n      user: {user1: 0, user2: 7}\n    }\n  ]\n}\n</code></pre>\n\n<p>I need to find all documents that have at least one 'score' (element in score array) that has a certain value of 'bar' and a non-empty 'user' sub-document.</p>\n\n<p>This is what I came up with (and it seemed like it should work):</p>\n\n<pre><code>db.col.find({score: {\"$elemMatch\": {bar:0, user: {\"$not\":{}} }}})\n</code></pre>\n\n<p>But, I get this error:</p>\n\n<pre><code>error: { \"$err\" : \"$not cannot be empty\", \"code\" : 13030 }\n</code></pre>\n\n<p>Any other way to do this?</p>\n    ","a":"\n<p>Figured it out: <code>{ 'score.user': { \"$gt\": {} } }</code> will match non-empty docs.</p>\n    "},{"t":"What is the recomended way to delete a large number of items from DynamoDB?","l":"http://stackoverflow.com/questions/9154264/what-is-the-recomended-way-to-delete-a-large-number-of-items-from-dynamodb","q":"\n\n<p>I'm writing a simple logging service in DynamoDB.</p>\n\n<p>I have a logs table that is keyed by a user_id hash and a timestamp (Unix epoch int) range.</p>\n\n<p>When a user of the service terminates their account, I need to delete all items in the table, regardless of the range value.</p>\n\n<p>What is the recommended way of doing this sort of operation (Keeping in mind there could be millions of items to delete)?</p>\n\n<p>My options, as far as I can see are:</p>\n\n<p>A: Perform a Scan operation, calling delete on each returned item, until no items are left</p>\n\n<p>B: Perform a BatchGet operation, again calling delete on each item until none are left</p>\n\n<p>Both of these look terrible to me as they will take a looooong time.</p>\n\n<p>What I ideally want to do is call LogTable.DeleteItem(user_id) - Without supplying the range, and have it delete everything for me.</p>\n\n<p>Any thoughts?</p>\n\n<p>Thanks</p>\n    ","a":"\n<blockquote>\n  <p>What I ideally want to do is call LogTable.DeleteItem(user_id) -\n  Without supplying the range, and have it delete everything for me.</p>\n</blockquote>\n\n<p>An understandable request indeed; I can imagine advanced operations like these might get added over time by the AWS team (they have a history of starting with a limited feature set first and evaluate extensions based on customer feedback), but here is what you should do to avoid the cost of a full scan at least:</p>\n\n<ol>\n<li><p>Use <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/API_Query.html\">Query</a> rather than <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/API_Scan.html\">Scan</a> to retrieve all items for <code>user_id</code> - this works regardless of the combined hash/range primary key in use, because <em>HashKeyValue</em> and <em>RangeKeyCondition</em> are separate parameters in this API and the former only targets the <em>Attribute value of the hash component of the composite primary key.</em>.\n</p><ul>\n<li>Please note that you''ll have to deal with the query API paging here as usual, see the <em>ExclusiveStartKey</em> parameter:<p></p>\n\n<blockquote>\n  <p>Primary key of the item from which to continue an earlier query. An\n  earlier query might provide this value as the LastEvaluatedKey if that\n  query operation was interrupted before completing the query; either\n  because of the result set size or the Limit parameter. The\n  LastEvaluatedKey can be passed back in a new query request to continue\n  the operation from that point.</p></blockquote></li>\n  </ul><p></p>\n</li>\n<li><p>Loop over all returned items and either facilitate <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/API_DeleteItem.html\">DeleteItem</a> as usual</p>\n\n<ul>\n<li><strong>Update</strong>: Most likely <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/API_BatchWriteItem.html\">BatchWriteItem</a> is more appropriate for a use case like this (see below for details).</li>\n</ul></li>\n</ol>\n\n<hr>\n\n<h2>Update</h2>\n\n<p>As highlighted by <a href=\"http://stackoverflow.com/questions/9154264/what-is-the-recomended-way-to-delete-a-large-number-of-items-from-dynamodb/9159431#comment18716993_9159431\">ivant</a>, the <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/API_BatchWriteItem.html\">BatchWriteItem</a> operation <em>enables you to put <strong>or delete</strong> several items across multiple tables in a single API call [emphasis mine]</em>:</p>\n\n<blockquote>\n  <p>To upload one item, you can use the PutItem API and to delete one\n  item, you can use the DeleteItem API. However, when you want to upload\n  or delete large amounts of data, such as uploading large amounts of\n  data from Amazon Elastic MapReduce (EMR) or migrate data from another\n  database in to Amazon DynamoDB, this API offers an efficient\n  alternative.</p>\n</blockquote>\n\n<p>Please note that this still has some relevant limitations, most notably:</p>\n\n<ul>\n<li><p><strong>Maximum operations in a single request</strong> â€” You can specify a total of up to 25 put or delete operations; however, the total request size cannot exceed 1 MB (the HTTP payload).</p></li>\n<li><p><strong>Not an atomic operation</strong> â€” Individual operations specified in a BatchWriteItem are atomic; however BatchWriteItem as a whole is a \"best-effort\" operation and not an atomic operation. That is, in a BatchWriteItem request, some operations might succeed and others might fail. [...]</p></li>\n</ul>\n\n<p>Nevertheless this obviously offers a potentially significant gain for use cases like the one at hand.</p>\n    "},{"t":"Riak on Windows","l":"http://stackoverflow.com/questions/1799958/riak-on-windows","q":"\n\n<p>I want to play with Riak <a href=\"http://riak.basho.com/\">http://riak.basho.com/</a> or a least get it running on a Windows system. I have downloaded the source code and compiled it but that's where I get stuck, how do I start it?</p>\n    ","a":"\n<p>It does run, altough I havent managed to run it as a service yet.</p>\n\n<p>Install CYGwin, install latest erlang, get source code, compile in cygwin</p>\n\n<p>then the fun part, adjust according to your paths and place into a batch</p>\n\n<p>c:\\riak\\rel\\riak\\erts-5.7.4\\bin\\erl -boot c:\\riak\\rel\\riak\\releases\\0.8\\riak -embedded -config c:\\riak\\rel\\riak\\etc\\app.config -args_file c:\\riak\\rel\\riak\\etc\\vm.args -- console</p>\n\n<p>Regards</p>\n    "},{"t":"Mongodb: sort documents by array objects","l":"http://stackoverflow.com/questions/5315658/mongodb-sort-documents-by-array-objects","q":"\n\n<p>I would like to return the documents in an order sorted by which holds the lowest <code>foo.bar</code> value (which are array objects). </p>\n\n<p>I can do <code>db.collection.find().sort({foo.0.bar: 1})</code>, but this only matches the first element in the array - and as you can see in the exampe below would sort item 1 first (foo.0.bar = 5), wheras I am looking to return item 2 first <code>(foo.2.bar = 4)</code> as it has the object with the lowest value.</p>\n\n<pre><code>{\n    \"name\": \"Item 1\",\n    \"foo\": [\n        {\n            \"bar\": 5\n        },\n        {\n            \"bar\": 6\n        },\n        {\n            \"bar\": 7\n        }\n    ]\n}\n{\n    \"name\": \"item 2\",\n    \"foo\": [\n        {\n            \"bar\": 6\n        },\n        {\n            \"bar\": 5\n        },\n        {\n            \"bar\": 4\n        }\n    ]\n}\n</code></pre>\n    ","a":"\n<p>It seems mongo <em>can</em> do this.</p>\n\n<p>For example, if I have the following documents:</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>{ a:{ b:[ {c:1}, {c:5 } ] } }\n{ a:{ b:[ {c:0}, {c:12} ] } }\n{ a:{ b:[ {c:4}, {c:3 } ] } }\n{ a:{ b:[ {c:1}, {c:9 } ] } }\n</code></pre>\n\n<p>And run the following:</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>db.collection.find({}).sort({ \"a.b.c\":1 });\n// produces:\n{ a:{ b:[ {c:0}, {c:12} ] } }\n{ a:{ b:[ {c:1}, {c:5 } ] } }\n{ a:{ b:[ {c:1}, {c:9 } ] } }\n{ a:{ b:[ {c:4}, {c:3 } ] } }\n\ndb.collection.find({}).sort({ \"a.b.c\":-1 });\n// produces:\n{ a:{ b:[ {c:0}, {c:12} ] } }\n{ a:{ b:[ {c:1}, {c:9 } ] } }\n{ a:{ b:[ {c:1}, {c:5 } ] } }\n{ a:{ b:[ {c:4}, {c:3 } ] } }\n</code></pre>\n\n<p>As you can see, the sort by <code>{\"a.b.c\":1}</code> takes the <strong>min</strong> of all values in the array and sorts on that, whereas the sort by <code>{\"a.b.c\":-1}</code> takes the <strong>max</strong> of all the values.</p>\n    "},{"t":"Getting values out of DynamoDB","l":"http://stackoverflow.com/questions/9166306/getting-values-out-of-dynamodb","q":"\n\n<p>I've just started looking into Amazon's DynamoDB.  Obviously the scalability appeals, but I'm trying to get my head out of SQL mode and into no-sql mode.  Can this be done (with all the scalability advantages of dynamodb):</p>\n\n<p>Have a load of entries (say 5 - 10 million) indexed by some number.  One of the fields in each entry will be a creation date.  Is there an effective way for dynamo db to give my web app all the entries created between two dates?  </p>\n\n<p>A more simple question - can dynamo db give me all entries in which a field matches a certain number.  That is, there'll be another field that is a number, for argument's sake lets say between 0 and 10.  Can I ask dynamodb to give me all the entries which have value e.g. 6?</p>\n\n<p>Do both of these queries need a scan of the entire dataset (which I assume is a problem given the dataset size?)</p>\n\n<p>many thanks</p>\n    ","a":"\n<blockquote>\n  <p>Is there an effective way for dynamo db to give my web app all the\n  entries created between two dates?</p>\n</blockquote>\n\n<p>Yup, please have a look at the of the <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/DataModel.html#DataModelPrimaryKey\">Primary Key</a> concept within <em>Amazon DynamoDB Data Model</em>, specifically the <em>Hash and Range Type Primary Key</em>:</p>\n\n<blockquote>\n  <p>In this case, the primary key is made of two attributes. The first\n  attributes is the hash attribute and the second one is the range\n  attribute. Amazon DynamoDB builds an unordered hash index on the hash\n  primary key attribute and a sorted range index on the range primary\n  key attribute. [...]</p>\n</blockquote>\n\n<p>The listed samples feature your use case exactly, namely the <em>Reply ( Id, ReplyDateTime, ... )</em> table facilitates a primary key of type <em>Hash and Range</em> with a hash attribute <em>Id</em> and a range attribute <em>ReplyDateTime</em>.</p>\n\n<p>You'll use this via the <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/API_Query.html\">Query</a> API, see <em>RangeKeyCondition</em> for details and <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/queryingdynamodb.html\">Querying Tables in Amazon DynamoDB</a> for respective examples.</p>\n\n<blockquote>\n  <p>can dynamo db give me all entries in which a field matches a certain\n  number. [...] Can I ask dynamodb to give\n  me all the entries which have value e.g. 6?</p>\n</blockquote>\n\n<p>This is possible as well, albeit by means of the <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/API_Scan.html\">Scan</a> API only (i.e. requires to read every item in the table indeed), see <em>ScanFilter</em> for details and <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/scandynamodb.html\">Scanning Tables in Amazon DynamoDB</a> for respective examples.</p>\n\n<blockquote>\n  <p>Do both of these queries need a scan of the entire dataset (which I\n  assume is a problem given the dataset size?)</p>\n</blockquote>\n\n<p>As mentioned the first approach works with a <em>Query</em> while the second requires a <em>Scan</em>, and <em>Generally, a query operation is more efficient than a scan operation</em> - this is a good advise to get started, though the details are more complex and depend on your use case, see section <em>Scan and Query Performance</em> within the <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/QueryAndScan.html\">Query and Scan in Amazon DynamoDB</a> overview:</p>\n\n<blockquote>\n  <p>For quicker response times, design your tables in a way that can use\n  the Query, Get, or BatchGetItem APIs, instead. Or, design your\n  application to use scan operations in a way that minimizes the impact\n  on your table's request rate. For more information, see <a href=\"http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/BestPractices.html\">Provisioned Throughput Guidelines in Amazon DynamoDB</a>.</p>\n</blockquote>\n\n<p>So, as usual when applying NoSQL solutions, you might need to adjust your architecture to accommodate these constraints.</p>\n    "},{"t":"Which of CouchDB or MongoDB suits my needs?","l":"http://stackoverflow.com/questions/3010794/which-of-couchdb-or-mongodb-suits-my-needs","q":"\n\n<p>Where I work, we use Ruby on Rails to create both backend and frontend applications. Usually, these applications interact with the same MySQL database. It works great for a majority of our data, but we have one situation which I would like to move to a NoSQL environment.</p>\n\n<p>We have clients, and our clients have what we call \"inventories\"--one or more of them. An inventory can have many thousands of items. This is currently done through two relational database tables, <code>inventories</code> and <code>inventory_items</code>.</p>\n\n<p>The problems start when two different inventories have different parameters:</p>\n\n<pre><code># Inventory item from inventory 1, televisions \n{\n  inventory_id: 1\n  sku: 12345\n  name: Samsung LCD 40 inches\n  model: 582903-4\n  brand: Samsung\n  screen_size: 40\n  type: LCD\n  price: 999.95\n}\n\n# Inventory item from inventory 2, accomodation\n{\n  inventory_id: 2\n  sku: 48cab23fa\n  name: New York Hilton\n  accomodation_type: hotel\n  star_rating: 5\n  price_per_night: 395\n}\n</code></pre>\n\n<p>Since we obviously can't use <code>brand</code> or <code>star_rating</code> as the column name in <code>inventory_items</code>, our solution so far has been to use generic column names such as <code>text_a</code>, <code>text_b</code>, <code>float_a</code>, <code>int_a</code>, etc, and introduce a third table, <code>inventory_schemas</code>. The tables now look like this:</p>\n\n<pre><code># Inventory schema for inventory 1, televisions \n{\n  inventory_id: 1\n  int_a: sku\n  text_a: name\n  text_b: model\n  text_c: brand\n  int_b: screen_size\n  text_d: type\n  float_a: price\n}\n\n# Inventory item from inventory 1, televisions \n{\n  inventory_id: 1\n  int_a: 12345\n  text_a: Samsung LCD 40 inches\n  text_b: 582903-4\n  text_c: Samsung\n  int_a: 40\n  text_d: LCD\n  float_a: 999.95\n}\n</code></pre>\n\n<p>This has worked well... up to a point. It's clunky, it's unintuitive and it lacks scalability. We have to devote resources to set up inventory schemas. Using separate tables is not an option.</p>\n\n<p>Enter NoSQL. With it, we could let each and every item have their own parameters and still store them together. From the research I've done, it certainly seems like a great alterative for this situation.</p>\n\n<p>Specifically, I've looked at CouchDB and MongoDB. Both look great. However, there are a few other bits and pieces we need to be able to do with our inventory:</p>\n\n<ul>\n<li>We need to be able to select items from only one (or several) inventories.</li>\n<li>We need to be able to filter items based on its parameters (eg. get all items from inventory 2 where type is 'hotel').</li>\n<li>We need to be able to group items based on parameters (eg. get the lowest price from items in inventory 1 where brand is 'Samsung').</li>\n<li>We need to (potentially) be able to retrieve thousands of items at a time.</li>\n<li>We need to be able to access the data from multiple applications; both backend (to process data) and frontend (to display data).</li>\n<li>Rapid bulk insertion is desired, though not required.</li>\n</ul>\n\n<p>Based on the structure, and the requirements, are either CouchDB or MongoDB suitable for us? If so, which one will be the best fit?</p>\n\n<p>Thanks for reading, and thanks in advance for answers.</p>\n\n<p>EDIT: One of the reasons I like CouchDB is that it would be possible for us in the frontend application to request data via JavaScript directly from the server after page load, and display the results without having to use any backend code whatsoever. This would lead to better page load and less server strain, as the fetching/processing of the data would be done client-side.</p>\n    ","a":"\n<p>I work on MongoDB, so you should take this with a grain of salt, but this looks like a great fit for Mongo.</p>\n\n<blockquote>\n  <ul>\n  <li>We need to be able to select items from only one (or several) inventories.</li>\n  </ul>\n</blockquote>\n\n<p>It's easy to ad hoc queries on any fields.</p>\n\n<blockquote>\n  <ul>\n  <li>We need to be able to filter items based on its parameters (eg. get all items from inventory 2 where type is 'hotel').</li>\n  </ul>\n</blockquote>\n\n<p>The query for this would be: <code>{\"inventory_id\" : 2, \"type\" : \"hotel\"}</code>.</p>\n\n<blockquote>\n  <ul>\n  <li>We need to be able to group items based on parameters (eg. get the lowest price from items in inventory 1 where brand is 'Samsung').</li>\n  </ul>\n</blockquote>\n\n<p>Again, super easy: <code>db.items.find({\"brand\" : \"Samsung\"}).sort({\"price\" : 1})</code></p>\n\n<blockquote>\n  <ul>\n  <li>We need to (potentially) be able to retrieve thousands of items at a time.</li>\n  </ul>\n</blockquote>\n\n<p>No problem.</p>\n\n<blockquote>\n  <ul>\n  <li>Rapid bulk insertion is desired, though not required.</li>\n  </ul>\n</blockquote>\n\n<p>MongoDB has much faster bulk inserts than CouchDB.</p>\n\n<p>Also, there's a REST interface for MongoDB: <a href=\"http://github.com/kchodorow/sleepy.mongoose\">http://github.com/kchodorow/sleepy.mongoose</a></p>\n\n<p>You might want to read <a href=\"http://chemeo.com/doc/technology\">http://chemeo.com/doc/technology</a>, who dealt with the arbitrary property search problem with MongoDB.</p>\n    "},{"t":"Django and NoSQL, any ready-to-use library?","l":"http://stackoverflow.com/questions/2293674/django-and-nosql-any-ready-to-use-library","q":"\n\n<p>So far Django has good integration with several RDBMS. NoSQL, schema-less and document-oriented DBMS are picking up. What's the status of integration those on-trend and fashionable DBMSes with Django? Are there any production-ready or at least ready-to-use libraries for Django?</p>\n\n<p>So far I have these at hand:</p>\n\n<ul>\n<li><a href=\"http://github.com/lethain/comfy-django-example\" rel=\"nofollow\">http://github.com/lethain/comfy-django-example</a></li>\n<li><a href=\"http://nosql.mypopescu.com/post/276069660/nosql-libraries#mongodb-python\" rel=\"nofollow\">http://nosql.mypopescu.com/post/276069660/nosql-libraries#mongodb-python</a></li>\n</ul>\n    ","a":"\n<p>Pre 1.0, django ORM underwent a major queryset re-factor. One of the reasons for this was \"This re-factor enables us to support non relational backends\".</p>\n\n<p>The official support I think is definitely on the cards; but I think there were more pressing matters for 1.1 and 1.2(now in beta).</p>\n\n<p>However, there are of course several independent efforts to use non relational databases with django, including, but not limited to the following:</p>\n\n<ul>\n<li><a href=\"http://www.allbuttonspressed.com/projects/django-nonrel\" rel=\"nofollow\">Django-nonrel</a> by Waldemar, who made django work on the appengine using the appengine patch.</li>\n<li>Using django with mongo db, by Kevin Fricovsky: <a href=\"http://bitbucket.org/gumptioncom/django-non-relational/\" rel=\"nofollow\">http://bitbucket.org/gumptioncom/django-non-relational/</a></li>\n<li>Using django with couch db, an old post, by Eric: <a href=\"http://www.eflorenzano.com/blog/post/using-couchdb-django/\" rel=\"nofollow\">http://www.eflorenzano.com/blog/post/using-couchdb-django/</a></li>\n</ul>\n    "},{"t":"Cassandra time series data","l":"http://stackoverflow.com/questions/2212279/cassandra-time-series-data","q":"\n\n<p>We are looking at using Cassandra to store a stream of information coming from various sources.</p>\n\n<p>One issue we are facing is the best way to query between two dates.</p>\n\n<p>For example we will need to retrieve an object between datetime dt1 and datetime dt2.</p>\n\n<p>We are currently considering the created unix timestamp as the key pointing to the actual object then using get_key_range to query to retrieve?</p>\n\n<p>Obviously this wouldn't work if two items have the same timestamp.</p>\n\n<p>Is this the best way to do datetime in noSQL stores in general? </p>\n    ","a":"\n<p>Cassandra rows can be very large, so consider modeling it as columns in a row rather than rows in a CF; then you can use the column slice operations, which are faster than row slices.  If there are no \"natural\" keys associated with this then you can use daily or hourly keys like \"2010/02/08 13:00\".</p>\n\n<p>Otherwise, yes, using range queries (get_key_range is deprecated in 0.5; use get_range_slice) is your best option.</p>\n    "},{"t":"Most Efficient One-To-Many Relationships in Google App Engine Datastore?","l":"http://stackoverflow.com/questions/11748695/most-efficient-one-to-many-relationships-in-google-app-engine-datastore","q":"\n\n<p>\nSorry if this question is too simple; I'm only entering 9th grade.</p>\n\n<p>I'm trying to learn about NoSQL database design. I want to design a Google Datastore model that minimizes the number of read/writes.</p>\n\n<p>Here is a toy example for a blog post and comments in a one-to-many relationship. Which is more efficient - storing all of the comments in a StructuredProperty or using a KeyProperty in the Comment model?</p>\n\n<p>Again, the objective is to minimize the number of read/writes to the datastore. You may make the following assumptions:</p>\n\n<ul>\n<li>Comments will not be retrieved independently of their respective blog post. (I suspect that this makes the StructuredProperty most preferable.)</li>\n<li>Comments will need to be sortable by date, rating, author, etc. (Subproperties in the datastore cannot be indexed, so perhaps this could affect performance?)</li>\n<li>Both blog posts and comments may be edited (or even deleted) after they are created.</li>\n</ul>\n\n<p>Using StructuredProperty:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from google.appengine.ext import ndb\n\nclass Comment(ndb.Model):\n    various properties...\n\nclass BlogPost(ndb.Model):\n    comments = ndb.StructuredProperty(Comment, repeated=True)\n    various other properties...\n</code></pre>\n\n<p>Using KeyProperty:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from google.appengine.ext import ndb\n\nclass BlogPost(ndb.Model):\n    various properties...\n\nclass Comment(ndb.Model):\n    blogPost = ndb.KeyProperty(kind=BlogPost)\n    various other properties...\n</code></pre>\n\n<p>Feel free to bring up any other considerations that relate to efficiently representing a one-to-many relationship with regards to minimizing the number of read/writes to the datastore.</p>\n\n<p>Thanks.</p>\n    ","a":"\n<p>I could be wrong, but from what I understand, a StructuredProperty is just a property within an entity, but with sub-properties.</p>\n\n<p>This means reading a BlogPost and all its comments would only cost one read.  So when you render your page, you only need one read op for your entire page.</p>\n\n<p>Writes would be cheaper each too.  You'll need one read op to get the BlogPost, and as long as you don't update any indexed properties, it'll just be one write op.</p>\n\n<p>You can handle the comment sorting on your own after you read the entity out of the datastore.</p>\n\n<p>You'll have to synchronize your comment updates/edits with transactions, to make sure one comment doesn't overwrite another, since they are both modifying the same entity.  You may run into unsolveable problems if everyone is commenting and editing the same blog post at the same time.</p>\n\n<p>In optimizing for cost though, you'll hit a wall with the maximum entity size of 1MB.  This will limit the number of comments you can store per blog post.</p>\n\n<p>Going with the KeyProperty would be quite a bit more expensive.</p>\n\n<p>You'll need one read to get the blog post, plus 1 query plus 1 small read op for each comment.</p>\n\n<p>Every comment is a new entity, so it'll be at least 4 write ops.  You may want to index for sort order, so that'll end up costing even more write ops.</p>\n\n<p>On the plus side, you'll have unlimited comments per blog post, you don't have to worry about synchronizing new comments.  You might need to worry about synchronization for editing comments, but if you limit the edit to the creator, that shouldn't really be a problem.  You don't have to do sorting yourself either.</p>\n\n<p>It's a cost vs features tradeoff.</p>\n    "},{"t":"Elasticsearch and NoSql database [duplicate]","l":"http://stackoverflow.com/questions/8026880/elasticsearch-and-nosql-database","q":"\n\n<div class=\"question-status question-originals-of-duplicate\">\n    <p>This question already has an answer here:</p>\n    <ul>\n        <li>\n            <a href=\"/questions/6636508/elasticsearch-as-a-database\" dir=\"ltr\">Elasticsearch as a database? [closed]</a>\n                <span class=\"question-originals-answer-count\">\n                    4 answers\n                </span>\n        </li>\n    </ul>\n</div>\n<p>What is the use to use both ElasticSearch and a separated Nosql database ?\nCan't Elasticsearch be used both as a database and for search indexing ?</p>\n    ","a":"\n<p>Yes, you can use ElasticSearch as a data source as well as an index.</p>\n\n<p>By default each document you send to the ElasticSearch system is index, and, the original document is stored as well. This means whenever you query ElasticSearch you can also retrieve the original JSON document that you indexed.</p>\n\n<p>If you have large documents and you want to be able to retrieve a smaller amount of data then when you can use the mapping API to set \"store\" to \"yes\" for specific fields, and then use the \"fields\" key to pull out specific fields you might want.</p>\n\n<p>In my system I have address autocompletion and I only fetch the address field of a property. Here is an example from my system:</p>\n\n<pre><code>_search?q=FullAddress:main&amp;fields:FullAddress\n</code></pre>\n\n<p>Then when a user selects the address I pull up the entire JSON document (along with others).</p>\n\n<p>Note:</p>\n\n<ol>\n<li>You cannot do updates like you can in SQL (update all items matching a query to increase an attribute, let's say)</li>\n<li>You can, however, add a new document and replace the existing one at the ID you want to update. Elastic search increments a _version property on each document which can be used by the developer to enforce optimistic concurrency, but it does not maintain a separate version history of each document. You can only retrieve the latest version of a document.</li>\n</ol>\n    "},{"t":"How can I calculate database design storage costs?","l":"http://stackoverflow.com/questions/9422025/how-can-i-calculate-database-design-storage-costs","q":"\n\n<p>I often have a couple different schema's in mind when starting project. After making rough guesses I realize that some are less optimized for growth or storage space than others. Obviously, the size of the column value is the main thing. But table metadata, indexes, and row headers all play a part as well. </p>\n\n<p>In addition, RDBMS use a completely different approach to data storage than object or key-value databases.</p>\n\n<p><strong>What are some good resources for trying to figure out the cost (or room needed) for database storage?</strong> </p>\n\n<p><strong>Note</strong>, my question has little to do with choosing the database, but rather knowing how to properly make use of each database's design for the <em>most efficiently</em>. Databases like PostgreSQL, MySQL, CouchDB, all have different target use cases and multiple ways to solve the same problem. So knowing the storage cost of each solution will help add to the choice of the best solution for the schema.</p>\n    ","a":"\n<blockquote>\n  <p>RDBMS use a completely different approach to data storage than object or key-value databases.</p>\n</blockquote>\n\n<p>The relational model assumes you don't know what data will be needed in the future, or how data will be accessed in the future. This has proven to be a pretty reliable assumption in my experience.</p>\n\n<p>That's one reason a SQL dbms will let you add indexes as they're needed, and let you drop indexes that have proven useless.  It will let you add constraints as they become known--constraints that sometimes require adding more tables--and drop constraints as the requirements change. It will let you add columns as you discover more things that would be good to know.  It will let you replace tables with views and replace views with tables.  Some dbms will let you create materialized views--their impact on query speed can be dramatic, and their impact on disk usage, devastating.</p>\n\n<p>Useful databases extend their reach. A SQL database, designed according to the relational model, makes it relatively easy to add features nobody dreamed of during the initial design, and <em>without crushing other parts of the system</em>. So they're called often called upon to do things their initial designers didn't imagine.</p>\n\n<p>All of these things</p>\n\n<ul>\n<li>adding and dropping indexes over time,</li>\n<li>adding and dropping constraints over time,</li>\n<li>adding and dropping columns over time,</li>\n<li>adding and dropping tables over time,</li>\n</ul>\n\n<p>make any estimate of disk usage look like a waste of time. Any one of them alone can drastically change the disk space required for a database.</p>\n\n<p>You can calculate the space required by a row and a page fairly accurately. (Try Google for \"YourDBMSname row layout\" and \"YourDBMSname page layout\".)  But when you try to multiply  by the number of rows required you have to estimate the number of rows. That puts you at the big end of what Steve McConnell calls \"the <a href=\"http://construx.com/Page.aspx?cid=1648\">cone of uncertainty</a>\".  </p>\n\n<p>If you haven't measured disk usage in multiple projects over time at your own company, estimating the impact of those bullet points above is just guessing.</p>\n\n<p>The last Fortune 100 company I worked for had an operational database that had been in production since the 1970s. Hundreds of applications, written in more than 25 programming languages over the course of 40 years hit that thing every day.  (I think it was built on IBM's IMS originally; today it runs on Oracle.)  </p>\n\n<p>Even just a few years ago, nobody there imagined that their database would be used to translate engineering drawings and bills of materials into Chinese, and also to produce the customs documents they'd need to get finished products out of China. Implementing those new features required storing additional data about every part and about every design document in their live inventory.  Early in that project, our estimates were pretty far off. That's the big end of the cone. (We estimated several things, but not disk usage. We were required to succeed, so whatever design I came up with, somebody would be required to supply the needed disk space.) But when we went live, we knew the exact value for every estimate, because we'd already done the work. (That's the narrow end of the cone.)</p>\n\n<p>So, how do you mitigate the risk of guesswork in a database design and deployment environment? Take a lesson from 1972.</p>\n\n<p><strong>Build a prototype, and measure it.</strong></p>\n\n<blockquote>\n  <p>Chemical engineers learned long ago that a process that works in the\n  laboratory cannot be implemented in a factory in only one step. An\n  intermediate step called the <em>pilot plant</em> is necessary to give\n  experience in scaling quantities up and in operating in nonprotective\n  environments. . . . </p>\n  \n  <p>. . . Project after project designs a set of algorithms and then plunges into construction of customer-deliverable software on a schedule that demands delivery of the first thing built. . . .</p>\n  \n  <p>The management question, therefore, is not <em>whether</em> to build a pilot system and throw it away. You <em>will</em> do that. The only question is whether to plan in advance to build a throwaway, or to promise to deliver the throwaway to customers.</p>\n</blockquote>\n\n<p>Fred Brooks, Jr., in <em>The Mythical Man-Month</em>, p 116. </p>\n    "},{"t":"Why exactly do we use NoSQL?","l":"http://stackoverflow.com/questions/3067866/why-exactly-do-we-use-nosql","q":"\n\n<p>Having understood some of the advantages that NoSQL offers (scalability, availability, etc.), I am still not clear why a website would want to use a non-relational database.\nCan I get some help on this, preferably with an example?</p>\n    ","a":"\n<p><strong>Better performance</strong></p>\n\n<p>NoSQL databases sometimes have better performance, although this depends on the situation and is disputed.</p>\n\n<p><strong>Adaptability</strong></p>\n\n<p>You can add and remove \"columns\" without downtime. In most SQL servers, this takes a long time and takes up a load of load.</p>\n\n<p><strong>Application design</strong></p>\n\n<p>It is desirable to separate the data storage from the logic. If you join and select things in SQL queries, you are mixing business logic with storage.</p>\n    "},{"t":"Best DataMining Database","l":"http://stackoverflow.com/questions/2577967/best-datamining-database","q":"\n\n<p>I am an occasional Python programer who only have worked so far with MYSQL or SQLITE databases. I am the computer person for everything in a small company and I have been started a new project where I think it is about time to try new databases. </p>\n\n<p>Sales department makes a CSV dump every week and I need to make a small scripting application that allow people form other departments mixing the information, mostly linking the records. I have all this solved, my problem is the speed, I am using just plain text files for all this and unsurprisingly it is very slow.</p>\n\n<p>I thought about using mysql, but then I need installing mysql in every desktop, sqlite is easier, but it is very slow. I do not need a full relational database, just some way of play with big amounts of data in a decent time.</p>\n\n<p>Update: I think I was not being very detailed about my database usage thus explaining my problem badly. I am working reading all the data ~900 Megas or more from a csv into a Python dictionary then working with it. My problem is storing and mostly reading the data quickly.</p>\n\n<p>Many thanks!</p>\n    ","a":"\n<h2>Quick Summary</h2>\n\n<ul>\n<li>You need enough memory(RAM) to solve your problem efficiently. I think you should upgrade memory?? When reading the excellent <a href=\"http://highscalability.com/\">High Scalability</a> Blog you will notice that for big sites to solve there problem efficiently they store the complete problem set in memory.</li>\n<li>You do need a central database solution. I don't think hand doing this with python dictionary's only will get the job done.</li>\n<li>How to solve \"your problem\" depends on your \"query's\". What I would try to do first is put your data in elastic-search(see below) and query the database(see how it performs). I think this is the easiest way to tackle your problem. But as you can read below there are a lot of ways to tackle your problem.</li>\n</ul>\n\n<h2>We know:</h2>\n\n<ul>\n<li>You used python as your program language.</li>\n<li>Your database is ~900MB (I think that's pretty large, but absolute manageable).</li>\n<li>You have loaded all the data in a python dictionary. Here I am assume the problem lays. Python tries to store the dictionary(also python dictionary's aren't the most memory friendly) in your memory, but you don't have enough memory(<strong>How much memory do you have????</strong>). When that happens you are going to have a lot of <a href=\"http://en.wikipedia.org/wiki/Virtual_memory\">Virtual Memory</a>. When you attempt to read the dictionary you are constantly swapping data from you disc into memory. This swapping causes \"<a href=\"http://en.wikipedia.org/wiki/Virtual_memory#Avoiding_thrashing\">Trashing</a>\". <strong>I am assuming that your computer does not have enough Ram. If true then I would first upgrade your memory with at least 2 Gigabytes extra RAM.</strong> When your problem set is able to fit in memory solving the problem is going to be a lot faster. I opened my computer architecture book where it(The memory hierarchy) says that main memory access time is about 40-80ns while disc memory access time is 5 ms. That is a BIG difference.</li>\n</ul>\n\n<h2>Missing information</h2>\n\n<ul>\n<li>Do you have a central server. You should use/have a server.</li>\n<li>What kind of architecture does your server have? Linux/Unix/Windows/Mac OSX? In my opinion your server should have linux/Unix/Mac OSX architecture.</li>\n<li>How much memory does your server have?</li>\n<li>Could you specify your data set(CSV) a little better.</li>\n<li>What kind of data mining are you doing? Do you need full-text-search capabilities? I am not assuming you are doing any complicated (SQL) query's. Performing that task with only python dictionary's will be a complicated problem. Could you formalize the query's that you would like to perform? For example:\n<ul>\n<li><code>\"get all users who work for departement x\"</code></li>\n<li><code>\"get all sales from user x\"</code></li>\n</ul></li>\n</ul>\n\n<h2>Database needed</h2>\n\n<blockquote>\n  <p>I am the computer person for\n  everything in a small company and I\n  have been started a new project where\n  I think it is about time to try new\n  databases.</p>\n</blockquote>\n\n<p>You are sure right that you need a database to solve your problem. Doing that yourself only using python dictionary's is difficult. Especially when your problem set can't fit in memory.</p>\n\n<h2>MySQL</h2>\n\n<blockquote>\n  <p>I thought about using mysql, but then\n  I need installing mysql in every\n  desktop, sqlite is easier, but it is\n  very slow. I do not need a full\n  relational database, just some way of\n  play with big amounts of data in a\n  decent time.</p>\n</blockquote>\n\n<p>A centralized(Client-server architecture) database is exactly what you need to solve your problem. Let all the users access the database from 1 PC which you manage. <strong>You can use MySQL to solve your problem</strong>.</p>\n\n<h2>Tokyo Tyrant</h2>\n\n<p>You could also use <a href=\"http://petewarden.typepad.com/searchbrowser/2009/03/tokyo-tyrant-tutorial.html\">Tokyo Tyrant</a> to store all your data. Tokyo Tyrant is pretty fast and it does not have to be stored in RAM. It handles getting data a more efficient(instead of using python dictionary's). However if your problem can completely fit in Memory I think you should have look at Redis(below).</p>\n\n<h2>Redis:</h2>\n\n<p>You could for example use <a href=\"http://code.google.com/p/redis/wiki/QuickStart\">Redis(quick start in 5 minutes)</a>(Redis is extremely fast) to store all sales in memory. Redis is extremely powerful and can do this kind of queries insanely fast. The only problem with Redis is that it has to fit completely in <a href=\"http://antirez.com/m/p.php?i=203\">RAM</a>, but I believe he is working on that(nightly build already supports it). Also like I already said previously solving your problem set completely from memory is how big sites solve there problem in a timely manner.</p>\n\n<h2>Document stores</h2>\n\n<p>This <a href=\"http://bcbio.wordpress.com/2009/05/10/evaluating-key-value-and-document-stores-for-short-read-data/\">article</a> tries to evaluate kv-stores with document stores like couchdb/riak/mongodb. These stores are better capable of searching(a little slower then KV stores), but aren't good at full-text-search.</p>\n\n<h2>Full-text-search</h2>\n\n<p>If you want to do full-text-search queries you could like at: </p>\n\n<ul>\n<li><a href=\"http://www.elasticsearch.com/videos/\">elasticsearch(videos)</a>: When I saw the video demonstration of elasticsearch it looked pretty cool. You could try put(post simple json) your data in elasticsearch and see how fast it is. I am following elastissearch on github and the author is commiting a lot of new code to it.</li>\n<li><a href=\"http://lucene.apache.org/solr/tutorial.html\">solr(tutorial)</a>: A lot of big companies are using solr(github, digg) to power there search. They got a big boost going from MySQL full-text search to solr.</li>\n</ul>\n    "},{"t":"MongoDB dot (.) in key name","l":"http://stackoverflow.com/questions/12397118/mongodb-dot-in-key-name","q":"\n\n<p>It seems mongo does not allow insertion of keys with a dot (.) or dollar sign ($) however when I imported a JSON file that contained a dot in it using the mongoimport tool it worked fine. The driver is complaining about trying to insert that element.</p>\n\n<p>This is what the document looks like in the database:</p>\n\n<pre><code>{\n    \"_id\": {\n        \"$oid\": \"...\"\n    },\n    \"make\": \"saab\",\n    \"models\": {\n        \"9.7x\": [\n            2007,\n            2008,\n            2009,\n            2010\n        ]\n    }\n}\n</code></pre>\n\n<p>Am I doing this all wrong and should not be using hash maps like that with external data (i.e. the models) or can I escape the dot somehow? Maybe I am thinking too much Javascript-like.</p>\n    ","a":"\n<p>MongoDB doesn't support <a href=\"http://docs.mongodb.org/manual/reference/limits/#Restrictions-on-Field-Names\">keys with a dot</a> in them so you're going to have to preprocess your JSON file to remove/replace them before importing it or you'll be setting yourself up for all sorts of problems.</p>\n\n<p>There isn't a standard workaround to this issue, the best approach is too dependent upon the specifics of the situation. But I'd avoid any key encoder/decoder approach if possible as you'll continue to pay the inconvenience of that in perpetuity, where a JSON restructure would presumably be a one-time cost.</p>\n    "},{"t":"MongoDb - Utilizing multi CPU server for a write heavy application","l":"http://stackoverflow.com/questions/4407336/mongodb-utilizing-multi-cpu-server-for-a-write-heavy-application","q":"\n\n<p>I am currently evaluating MongoDb for our write heavy application...</p>\n\n<p>Currently MongoDb uses single thread for write operation and also uses global lock whenever it is doing the write... Is it possible to exploit multiple CPU on a multi-CPU server to get better write performance? What are your workaround for global write lock?  </p>\n    ","a":"\n<p>So right now, the easy solution is to shard. </p>\n\n<p>Yes, normally sharding is done across servers. However, it is completely possible to shard on a single box. You simply fire up the shards on different ports and provide them with different folders. <a href=\"http://www.mongodb.org/display/DOCS/A+Sample+Configuration+Session\">Here's a sample configuration</a> of 2 shards on one box.</p>\n\n<p>The MongoDB team recognizes that this is kind of sub-par, and I know from talking to them that they're looking at better ways to do this.</p>\n\n<p>Obviously once you get multiple shards on one box and increase your write threads, you will have to be wary of disk IO. In my experience, I've been able to saturate disks with a single write thread. If your inserts/updates are relatively simple, you may find that extra write threads don't do anything. (Map-Reduces are the exception here, sharding definitely helps there)</p>\n    "},{"t":"RavenDB ASP.NET session provider?","l":"http://stackoverflow.com/questions/3415404/ravendb-asp-net-session-provider","q":"\n\n<p>A little background: I currently make use of <a href=\"http://memcachedproviders.codeplex.com/\">Memcached Providers</a> for managing session state in my ASP.NET application. It provides facilities for using SQL Server as a fallback storage mechanism (when sessions need to be purged from the memcached cache). I'd like to look at creating a provider for <a href=\"http://www.ravendb.net/\">RavenDB</a> as it would be much more performant for this sort of task.</p>\n\n<p>My question is, has anyone implemented such a thing? (or something similar?) - I'd hate to re-invent the wheel. Google does not yield any helpful results (other than <a href=\"http://groups.google.com/group/ravendb/browse_thread/thread/46eea78266d958ef?pli=1\">my question</a> about this in the RavenDB group itself), so I thought I'd take this question directly to the Stack Overflow community.</p>\n    ","a":"\n<p>I was also seeking a RavenDB session-state store, and my search also failed.</p>\n\n<p>So I created one:</p>\n\n<p><a href=\"https://github.com/mjrichardson/RavenDbSessionStateStoreProvider\">github.com/mjrichardson/RavenDbSessionStateStoreProvider</a> </p>\n\n<p>Also available via a <a href=\"http://nuget.org/List/Packages/Raven.AspNet.SessionState\">NuGet package</a>.</p>\n    "},{"t":"redis: Handling failover?","l":"http://stackoverflow.com/questions/3097324/redis-handling-failover","q":"\n\n<p>Redis really seems like a great product with the built in replication and the amazing speed. After testing it out, it feels definitely like the 2010 replacement of memcached.</p>\n\n<p>However, since when normally using memcached, a consistent hashing is being used to evenly spread out the data across the servers in a pool. If one of the servers in the pool goes down and stops being accessible, it is being handled transparently and only the keys that were lost will be recreated and evenly distributed across the remaining available servers in the pool.</p>\n\n<p>Redis has on the other hand also built-in sharding, but also another really interesting feature called automatic replication. Thanks to that, availability of the data could be greatly increased while utilizing slave servers to use in the event of the shit hitting the fan.</p>\n\n<p>However, I have yet not found any good solution to handle <strong>changing a redis server's status as a slave to become a new master automatically</strong> or by any other way automatically handling the failover with Redis. </p>\n\n<p>How could this be done? What would be an appropriate approach to this?</p>\n    ","a":"\n<blockquote>\n  <p>However, since when normally using memcached, a consistent hashing is being used to evenly spread out the data across the servers in a pool. If one of the servers in the pool goes down and stops being accessible, it is being handled transparently and only the keys that were lost will be recreated and evenly distributed across the remaining available servers in the pool.</p>\n</blockquote>\n\n<p>This is not what memcached does, the client library is doing all this magic ;)</p>\n\n<blockquote>\n  <p>However, I have yet not found any good solution to handle changing a redis server's status as a slave to become a new master automatically  or by any other way automatically handling the failover with Redis.</p>\n</blockquote>\n\n<p>Use the <a href=\"http://code.google.com/p/redis/wiki/SlaveofCommand\">SlaveofCommand</a> to change the characteristics. Automatic failover will need a bit more coding, connect to the server and if you loose the connection and can't establish it again for a time X then choose one slave to be master and change the slave-master-status of all other servers.</p>\n\n<p><strong>Update (01. Aug. 2012):</strong> There's now <a href=\"http://redis.io/topics/sentinel\">redis sentinel</a>, a monitoring and automatic failover solution for version 2.4.16 and higher.</p>\n    "},{"t":"Fast or Bulk Upsert in pymongo","l":"http://stackoverflow.com/questions/5292370/fast-or-bulk-upsert-in-pymongo","q":"\n\n<p>How can I do a bulk upsert in pymongo? I want to Update a bunch of entries and doing them one at a time is very slow.</p>\n\n<p>The answer to an almost identical question is here: <a href=\"http://stackoverflow.com/questions/4444023/bulk-update-upsert-in-mongodb\">Bulk update/upsert in MongoDB?</a></p>\n\n<p>The accepted answer doesn't actually answer the question. It simply gives a link to the mongo CLI for doing import/exports.</p>\n\n<p>I would also be open to someone explaining why doing a bulk upsert is no possible / no a best practice, but please explain what the preferred solution to this sort of problem is.</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p>MongoDB 2.6+ has support for bulk operations.  This includes bulk inserts, upserts, updates, etc.  The point of this is to reduce/eliminate delays from the round-trip latency of doing record-by-record operations ('document by document' to be correct).</p>\n\n<p>So, how does this work?  Example in Python, because that's what I'm working in.  </p>\n\n<pre><code>&gt;&gt;&gt; import pymongo\n&gt;&gt;&gt; pymongo.version\n'2.7rc0'\n</code></pre>\n\n<p>To use this feature, we create a 'bulk' object, add documents to it, then call execute on it and it will send all the updates at once.  Caveats:  The BSONsize of the collected operations (sum of the bsonsizes) cannot be over the document size limit of 16 MB.  Of course, the number of operations can thus vary significantly, Your Mileage May Vary.</p>\n\n<p>Example in Pymongo of Bulk upsert operation:</p>\n\n<pre><code>import pymongo\nconn = pymongo.MongoClient('myserver', 8839)\ndb = conn['mydbname']\ncoll = db.myCollection\nbulkop = coll.initialize_ordered_bulk_op()\nretval = bulkop.find({'field1':1}).upsert().update({'$push':{'vals':1})\nretval = bulkop.find({'field1':1}).upsert().update({'$push':{'vals':2})\nretval = bulkop.find({'field1':1}).upsert().update({'$push':{'vals':3})\nretval = bulkop.execute()\n</code></pre>\n\n<p>This is the essential method.  More info available at: </p>\n\n<p><a href=\"http://api.mongodb.org/python/2.7rc1/examples/bulk.html\">http://api.mongodb.org/python/2.7rc1/examples/bulk.html</a></p>\n    "},{"t":"NoSQL: What does it mean for MongoDB or BigTable to not always be â€œAvailableâ€","l":"http://stackoverflow.com/questions/7339374/nosql-what-does-it-mean-for-mongodb-or-bigtable-to-not-always-be-available","q":"\n\n<p>Reading Nathan Hurst's <a href=\"http://blog.nahurst.com/visual-guide-to-nosql-systems\" rel=\"nofollow\">Visual Guide to NoSQL Systems</a>, he includes the <code>CAP</code> triangle:</p>\n\n<ul>\n<li><code>C</code>onsistency</li>\n<li><code>A</code>vailibility</li>\n<li><code>P</code>artition Tolerance</li>\n</ul>\n\n<p><img src=\"http://i.stack.imgur.com/iMkdg.png\" alt=\"enter image description here\"></p>\n\n<p>With SQL Server being an <code>AC</code> system, and MongoDB being a <code>CP</code> system.</p>\n\n<p>These definitions from come a <a href=\"http://www.julianbrowne.com/article/viewer/brewers-cap-theorem\" rel=\"nofollow\">UC Berkley professor Eric Brewer, and his talk at PODC 2000</a> (Principles of Distributed Computing):</p>\n\n<blockquote>\n  <p><strong>Availability</strong></p>\n  \n  <p>Availability means just that - the service is available\n  (to operate fully or not as above). When you buy the book you want to\n  get a response, not some browser message about the web site being\n  uncommunicative. Gilbert &amp; Lynch in their proof of CAP Theorem make\n  the good point that availability most often deserts you when you need\n  it most - sites tend to go down at busy periods precisely because they\n  are busy. A service that's available but not being accessed is of no\n  benefit to anyone.</p>\n</blockquote>\n\n<p>What does it mean, in the context of MongoDB, or BigTable, for the system to not be \"available\"?</p>\n\n<p>Do you go to connect (e.g. over TCP/IP), and the server does not respond? Do you attempt execute a query, but the query never returns - or returns an error?</p>\n\n<p>What does it <em>mean</em> to not be available?</p>\n    ","a":"\n<p>Availability in this case means that <em>in the event of a network partition</em>, the server that a client connects to may not be able to guarantee the level of consistency that the client expects (or that the system is configured to provide).</p>\n\n<p>Assuming that you have 3 nodes, A, B, and C, in a hypothetical distributed system.  A, B, and C are each running in their own rack of servers, with 2 switches between them:</p>\n\n<pre><code>[Node A] &lt;- Switch #1 -&gt; [Node B] &lt;- Switch #2 -&gt; [ Node C ]\n</code></pre>\n\n<p>Now assume that said system is set up so that it is GUARANTEED that any write will go to at least 2 nodes before it is considered committed.  Now, lets assume that switch #2 gets unplugged, and some client is connected to node C:</p>\n\n<pre><code>[Node A] &lt;- Switch #1 -&gt; [Node B]                 [ Node C ] &lt;-- Some client\n</code></pre>\n\n<p>That client will not be able to issue Consistent writes, because the distributed system is currently in a partitioned state (namely, Node C cannot contact enough other nodes to guarantee the 2-node consistency required).</p>\n\n<p>I'd add to this that some NoSQL databases allow very dynamic selection of CAP attributes.  Cassandra, for instance, allows clients to specify the number of servers that a write must go to before it is committed on a per-write basis.  Writes going to a single server are \"AP\", writes going to a quorum (or all) servers are more \"CA\".</p>\n\n<p>EDIT - from the comments below:</p>\n\n<p>In MongoDB you can only have master/slave configuration within a replica set. What this means is that the choice of AP vs CP is made by the client at query time. The client can specify slaveOk, which will read from an arbitrarily selected slave (which may have stale data): mongodb.org/display/DOCS/â€¦. If the client is not OK with stale data, don't specify slaveOk and the query will go to the master. If the client cannot reach the master, then you'll get an error. I'm not sure exactly what that error will be.</p>\n    "},{"t":"How to Model Real-World Relationships in a Graph Database (like Neo4j)?","l":"http://stackoverflow.com/questions/7536142/how-to-model-real-world-relationships-in-a-graph-database-like-neo4j","q":"\n\n<p>I have a general question about modeling in a graph database that I just can't seem to wrap my head around.</p>\n\n<p>How do you model this type of relationship: \"Newton invented Calculus\"?</p>\n\n<p>In a <a href=\"http://docs.neo4j.org/chunked/snapshot/graphdb-neo4j-relationships.html\" rel=\"nofollow\">simple graph</a>, you could model it like this:</p>\n\n<pre><code>Newton (node) -&gt; invented (relationship) -&gt; Calculus (node)\n</code></pre>\n\n<p>...so you'd have a bunch of \"invented\" graph relationships as you added more people and inventions.</p>\n\n<p>The problem is, you start needing to add a bunch of properties to the relationship:</p>\n\n<ul>\n<li>invention_date</li>\n<li>influential_concepts</li>\n<li>influential_people</li>\n<li>books_inventor_wrote</li>\n</ul>\n\n<p>...and you'll want to start creating relationships between those properties and other nodes, such as:</p>\n\n<ul>\n<li>influential_people: relationship to person nodes</li>\n<li>books_inventor_wrote: relationship to book nodes</li>\n</ul>\n\n<p>So now it seems like the \"real-world relationships\" (\"invented\") should actually be a node in the graph, and the graph should look like this:</p>\n\n<pre><code>Newton (node) -&gt; (relationship) -&gt; Invention of Calculus (node) -&gt; (relationship) -&gt; Calculus (node)\n</code></pre>\n\n<p>And to complicate things more, other people are also participated in the invention of Calculus, so the graph now becomes something like:</p>\n\n<pre><code>Newton (node) -&gt; \n  (relationship) -&gt; \n    Newton's Calculus Invention (node) -&gt; \n      (relationship) -&gt; \n        Invention of Calculus (node) -&gt; \n          (relationship) -&gt; \n            Calculus (node)\nLeibniz (node) -&gt; \n  (relationship) -&gt; \n    Leibniz's Calculus Invention (node) -&gt; \n      (relationship) -&gt; \n        Invention of Calculus (node) -&gt; \n          (relationship) -&gt; \n            Calculus (node)\n</code></pre>\n\n<p>So I ask the question because it seems like <em>you don't want to set properties on the actual graph database \"relationship\" objects</em>, because you may want to at some point treat them as nodes in the graph.</p>\n\n<p>Is this correct?</p>\n\n<p>I have been studying the <a href=\"http://www.freebase.com/docs/mql/ch02.html\" rel=\"nofollow\">Freebase Metaweb Architecture</a>, and they seem to be treating everything as a node.  For example, Freebase has the idea of a <a href=\"http://wiki.freebase.com/wiki/CVT/Mediator_CVT_proposal\" rel=\"nofollow\">Mediator/CVT</a>, where you can create a \"Performance\" node that links an \"Actor\" node to a \"Film\" node, like here: <a href=\"http://www.freebase.com/edit/topic/en/the_last_samurai\" rel=\"nofollow\">http://www.freebase.com/edit/topic/en/the_last_samurai</a>.  Not quite sure if this is the same issue though.</p>\n\n<p>What are some guiding principles you use to figure out if the \"real-world relationship\" should actually be a graph node rather than a graph relationship?</p>\n\n<p>If there are any good books on this topic I would love to know.  Thanks!</p>\n    ","a":"\n<p>Some of these things, such as <code>invention_date</code>, can be stored as properties on the edges as in most graph databases edges can have properties in the same way that vertexes can have properties. For example you could do something like this (code follows <a href=\"https://github.com/tinkerpop/blueprints\">TinkerPop's Blueprints</a>):</p>\n\n<pre><code>Graph graph = new Neo4jGraph(\"/tmp/my_graph\");\nVertex newton = graph.addVertex(null);\nnewton.setProperty(\"given_name\", \"Isaac\");\nnewton.setProperty(\"surname\", \"Newton\");\nnewton.setProperty(\"birth_year\", 1643); // use Gregorian dates...\nnewton.setProperty(\"type\", \"PERSON\");\n\nVertex calculus = graph.addVertex(null);\ncalculus.setProperty(\"type\", \"KNOWLEDGE\");\n\nEdge newton_calculus = graph.addEdge(null, newton, calculus, \"DISCOVERED\");\nnewton_calculus.setProperty(\"year\", 1666);   \n</code></pre>\n\n<p>Now, lets expand it a little bit and add in Liebniz:</p>\n\n<pre><code>Vertex liebniz = graph.addVertex(null);\nliebniz.setProperty(\"given_name\", \"Gottfried\");\nliebniz.setProperty(\"surnam\", \"Liebniz\");\nliebniz.setProperty(\"birth_year\", \"1646\");\nliebniz.setProperty(\"type\", \"PERSON\");\n\nEdge liebniz_calculus = graph.addEdge(null, liebniz, calculus, \"DISCOVERED\");\nliebniz_calculus.setProperty(\"year\", 1674);\n</code></pre>\n\n<p>Adding in the books:</p>\n\n<pre><code>Vertex principia = graph.addVertex(null);\nprincipia.setProperty(\"title\", \"PhilosophiÃ¦ Naturalis Principia Mathematica\");\nprincipia.setProperty(\"year_first_published\", 1687);\nEdge newton_principia = graph.addEdge(null, newton, principia, \"AUTHOR\");\nEdge principia_calculus = graph.addEdge(null, principia, calculus, \"SUBJECT\");\n</code></pre>\n\n<p>To find out all of the books that Newton wrote on things he discovered we can construct a graph traversal. We start with Newton, follow the out links from him to things he discovered, then traverse links in reverse to get books on that subject and again go reverse on a link to get the author. If the author is Newton then go back to the book and return the result. This query is written in <a href=\"https://github.com/tinkerpop/gremlin\">Gremlin</a>, a Groovy based domain specific language for graph traversals:</p>\n\n<pre><code>newton.out(\"DISCOVERED\").in(\"SUBJECT\").as(\"book\").in(\"AUTHOR\").filter{it == newton}.back(\"book\").title.unique()\n</code></pre>\n\n<p>Thus, I hope I've shown a little how a clever traversal can be used to avoid issues with creating intermediate nodes to represent edges. In a small database it won't matter much, but in a large database you're going to suffer large performance hits doing that.</p>\n\n<p>Yes, it is sad that you can't associate edges with other edges in a graph, but that's a limitation of the data structures of these databases. Sometimes it makes sense to make everything a node, for example, in Mediator/CVT a performance has a bit more concreteness too it. Individuals may wish address only Tom Cruise's performance in \"The Last Samurai\" in a review. However, for most graph databases I've found that application of some graph traversals can get me what I want out of the database.</p>\n    "},{"t":"How do you query DynamoDB?","l":"http://stackoverflow.com/questions/9131191/how-do-you-query-dynamodb","q":"\n\n<p>I'm looking at Amazon's DynamoDB as it looks like it takes away all of the hassle of maintaining and scaling your database server. I'm currently using MySQL, and maintaining and scaling the database is a complete headache.</p>\n\n<p>I've gone through the documentation and I'm having a hard time trying to wrap my head around how you would structure your data so it could be easily retrieved.</p>\n\n<p>I'm totally new to NoSQL and non-relational databases, so any help is really appreciated (and needed).</p>\n\n<p>From the Dynamo documentation it sounds like you can only query a table on the primary hash key, and the primary range key with a limited number of comparison operators. </p>\n\n<p>Or you can run a full table scan and apply a filter to it. The catch is that it will only scan 1Mb at a time, so you'd likely have to repeat your scan to find X number of results.</p>\n\n<p>I realize these limitations allow them to provide predictable performance, but it seems like it makes it really difficult to get your data out. And performing full table scans <em>seems</em> like it would be really inefficient, and would only become less efficient over time as your table grows.</p>\n\n<p>For Instance, say I have a Flickr clone. My Images table might look something like:</p>\n\n<ul>\n<li>Image ID (Number, Primary Hash Key)</li>\n<li>Data Added (Number, Primary Range Key)</li>\n<li>User ID (String)</li>\n<li>Tags (String Set)</li>\n<li>etc</li>\n</ul>\n\n<p>So using query I would be able to list all images from the last 7 days and limit it to X number of results pretty easily.</p>\n\n<p>But if I wanted to list all images from a particular user I would need to do a full table scan and filter by username. Same would go for tags.</p>\n\n<p>And because you can only scan 1Mb at a time you may need to do multiple scans to find X number of images. I also don't see a way to easily stop at X number of images. If you're trying to grab 30 images, your first scan might find 5, and your second may find 40. </p>\n\n<p>Do I have this right? Is it basically a trade-off? You get really fast predictable database performance that is virtually maintenance free. But the trade-off is that you need to build way more logic to deal with the results?</p>\n\n<p>Or am I totally off base here? I'm totally new to all of this, so please correct me if I'm wrong. I'm here to learn.</p>\n    ","a":"\n<p>Yes, you are correct about the trade-off between performance and query flexibility. </p>\n\n<p>But there are a few tricks to reduce the pain - secondary indexes/denormalising probably being the most important. </p>\n\n<p>You would have another table keyed on user ID, listing all their images, for example. When you add an image, you update this table as well as adding a row to the table keyed on image ID.</p>\n\n<p>You have to decide what queries you need, then design the data model around them.</p>\n    "},{"t":"Is there any nosql flat file database just as sqlite? [closed]","l":"http://stackoverflow.com/questions/4245438/is-there-any-nosql-flat-file-database-just-as-sqlite","q":"\n\n<p>Short Question:\nIs there any nosql flat-file database available as sqlite?</p>\n\n<p>Explanation:\nFlat file database can be opened in different processes to read, and keep one process to write. I think its perfect for read cache if there's no strict consistent needed. Say 1-2 secs write to the file or even memory block and the readers get updated data after that.</p>\n\n<p>So I almost choose to use sqlite, as my python server read cache. But there's still one problem. I don't like to rewrite sqls again in another place and construct another copy of my data tables in sqlite just as the same as I did in PostgreSql which used as back-end database.</p>\n\n<p>so is there any other choice?thanks!</p>\n    ","a":"\n<p>Maybe <code>shelve</code>? It's basically a key-value store where you can store python objects. <a href=\"http://docs.python.org/library/shelve.html\">http://docs.python.org/library/shelve.html</a></p>\n\n<p>Or maybe you could just use the filesystem?</p>\n    "},{"t":"Cassandra Vs Amazon SimpleDB","l":"http://stackoverflow.com/questions/1839218/cassandra-vs-amazon-simpledb","q":"\n\n<p>I'm working on an application where data size and SQL queries are going to be heavy. I am thinking between Cassandra or Amazon SimpleDB. Can you please suggest which is more suitable in this kind of scenario?</p>\n\n<p>Cassandra data indexing seems better than Amazon simpleDB, but the queries have fewer options compared to Amazon SimpleDB. Seems Amazon SimpleDB has heavy I/O rates.</p>\n\n<p>Few of the complex use cases are user activities with different filters that user can put to narrow down to some interesting activities.</p>\n\n<p>If you think there is anyother cleaner and better solution apart from these two, please suggest.</p>\n    ","a":"\n<p>SimpleDB can only scale by sharding, has 10 GB data size limit per table, and query performance is parallel to record count (eg: poor if you have 1 million records). And google's datastore is slower than simpledb. Cassandra is much more scalable, high traffic sites began to use it, there is nothing better for free if you need high write rates with massive data. <a href=\"http://n2.nabble.com/Cassandra-users-survey-td4040068.html\">cassandra survey</a></p>\n\n<p>If your read/write ratio is something like %90 for read and %10 for write, then terracotta  or infinispan with postgres is a better fit. There some free clustering options for postgresql but none of them matured (mostly prototypes).</p>\n\n<p>Another option is sharding. Hiberntae and NHibernate has sharding support. You can use them with postgres or mysql but you loose joins.</p>\n\n<p>Regards</p>\n    "},{"t":"Does it make sense to use the repository pattern with a document database?","l":"http://stackoverflow.com/questions/7497959/does-it-make-sense-to-use-the-repository-pattern-with-a-document-database","q":"\n\n<p>I'm currently experimenting with <strong>MongoDB</strong>.  I'm moving from a <strong>NHibernate/SQL</strong> mindset, and so initially I implemented a <strong>repository pattern</strong> for data access.</p>\n\n<p>This was all looking fine until I started using nested documents.  Now it's starting to seem like there's a bit of a mismatch.  However, I'm comfortable with repositories, and like the abstraction, separation of concerns, and testability they provide.</p>\n\n<p><strong>Are people successfully using the repository pattern with document databases?  If not, what data access methodology to you use? What about abstraction/SoC?</strong> </p>\n    ","a":"\n<p>It's an interesting question.  In my usage of MongoDB, I chose to not have a repository.  This was primary because the document database was used as a read store (therefore simplifying the data that was stored there).</p>\n\n<p>I guess you have to strip the consideration back to what a repository is and what advantages you get from the extra layer of abstraction.  A good design will have the least number of layers <em>possible</em>.  </p>\n\n<p>A repository therefore gives you some persistence ignorance and the ability to use unit of work over a common context of data.  It also can increase your ability to test queries against the data in isolation (because these are usually abstracted as queryables or specifications).</p>\n\n<p>Also, some document databases already provide a repository pattern (RavenDB etc), so there is no need to have yet another layer.</p>\n\n<p>So, it seems to me that using a repository is not so much about whether your data is stored as a relational table or a document, but more about what you gain from the abstraction.</p>\n    "},{"t":"Mongo Schema-less Collections & C#","l":"http://stackoverflow.com/questions/7279633/mongo-schema-less-collections-c-sharp","q":"\n\n<p>I'm exploring Mongo as an alternative to relational databases but I'm running into a problem with the concept of schemaless collections. </p>\n\n<p>In theory it sounds great, but as soon as you tie a model to a collection, the model becomes your defacto schema. You can no longer just add or remove fields from your model and expect it to continue to work. I see the same problems here managing changes as you have with a relational database in that you need some sort of script to migrate from one version of the database schema to the other. </p>\n\n<p>Am I approaching this from the wrong angle? What approaches do members here take to ensure that their collection items stay in sync with their domain model when making updates to their domain model?</p>\n\n<p>Edit: It's worth noting that these problems obviously exist in relational databases as well, but I'm asking specifically for strategies in mitigating the problem using schemaless databases and more specifically Mongo. Thanks!</p>\n    ","a":"\n<p>Schema migration with MongoDB is actually a lot less painful than with, say, SQL server.</p>\n\n<p>Adding a new field is easy, old records will come in with it set to null or you can use attributes to control the default value <code>[BsonDefaultValue(\"abc\", SerializeDefaultValue = false)]</code></p>\n\n<p>The <code>[BsonIgnoreIfNull]</code> attribute is also handy for omitting objects that are null from the document when it is serialized.</p>\n\n<p>Removing a field is fairly easy too, you can use <code>[BSonExtraElements]</code> (see <a href=\"http://www.mongodb.org/display/DOCS/CSharp+Driver+Serialization+Tutorial#CSharpDriverSerializationTutorial-Supportingextraelements\">docs</a>) to collect them up and preserve them or you can use <code>[BsonIgnoreExtraElements]</code> to simply throw them away.</p>\n\n<p>With these in place there really is no need to go convert every record to the new schema, you can do it lazily as needed when records are updated, or slowly in the background.</p>\n\n<hr>\n\n<p>PS, since you are also interested in using dynamic with Mongo, here's an <a href=\"http://blog.abodit.com/2011/05/class-free-persistence-multiple-inheritance-in-c-sharp-mongodb/\">experiment</a> I tried along those lines.  And here's an updated post with a complete <a href=\"http://blog.abodit.com/2011/09/dynamic-persistence-with-mongodb-look-no-classes-polymorphism-in-c/\">serializer and deserializer for dynamic objects</a>.</p>\n    "},{"t":"Alternatives to traditional relational databases for activity streams","l":"http://stackoverflow.com/questions/1342741/alternatives-to-traditional-relational-databases-for-activity-streams","q":"\n\n<p>I'm wondering if some other non-relational database would be a good fit for activity streams - sort of like what you see on Facebook, Flickr (<a href=\"http://www.flickr.com/activity\" rel=\"nofollow\">http://www.flickr.com/activity</a>), etc. Right now, I'm using MySQL but it's pretty taxing (I have tens of millions of activity records) and since they are basically read-only once written and always viewed chronologically, I was thinking that an alternative DB might work well.</p>\n\n<p>The activities are things like:</p>\n\n<ul>\n<li>6 PM: John favorited Bacon</li>\n<li>5:30 PM: Jane commented on Snow Crash</li>\n<li>5:15 PM: Jane added a photo of Bacon to her album</li>\n</ul>\n\n<p>The catch is that unlike Twitter and some other systems, I can't just simply append activities to lists for each user who is interested in the activity - if I could it looks like <a href=\"http://code.google.com/p/redis/wiki/TwitterAlikeExample\" rel=\"nofollow\">Redis would be a good fit</a> (with its list operations). </p>\n\n<p>I need to be able to do the following:</p>\n\n<ul>\n<li>Pull activities for a set <strong>or subset</strong> of people who you are following (\"John\" and \"Jane\"), in reverse date order</li>\n<li>Pull activities for a thing (like \"Bacon\") in reverse date order</li>\n<li>Filter by activity type (\"favorite\", \"comment\") </li>\n<li>Store at least 30 million activities</li>\n<li>Ideally, if you added or removed a person who you are following, your activity stream would reflect the change.</li>\n</ul>\n\n<p>I have been doing this with MySQL. My \"activities\" table is as compact as I could make it, the keys are as small as possible, and the it is indexed appropriately. It works, but it just feels like the wrong tool for this job.</p>\n\n<p><strong>Is anybody doing anything like this outside of a traditional RDBMS?</strong></p>\n\n<p><em>Update November 2009</em>: It's too early to answer my own question, but my current solution is to stick with MySQL but augment with Redis for fast access to the fresh activity stream data. More information in my answer here: <a href=\"http://stackoverflow.com/questions/1443960/how-to-implement-the-activity-stream-in-a-social-network/1766371#1766371\"></a><a href=\"http://stackoverflow.com/questions/1443960/how-to-implement-the-activity-stream-in-a-social-network\">How to implement the activity stream in a social network</a>...</p>\n\n<p><strong>Update August 2014</strong>: Years later, I'm still using MySQL as the system of record and using Redis for very fast access to the most recent activities for each user. Dealing with schema changes on a massive MySQL table has become a non-issue thanks to <a href=\"http://pt-online-schema-change.html\" rel=\"nofollow\">pt-online-schema-change</a></p>\n    ","a":"\n<p>I'd really, really, suggest stay with MySQL (or a RDBMS) until you fully understand the situation.</p>\n\n<p>I have no idea how much performance or much data you plan on using, but 30M rows is not very many. </p>\n\n<p>If you need to optimise certain range scans, you can do this with (for example) InnoDB by choosing a (implicitly clustered) primary key judiciously, and/or denormalising where necessary.</p>\n\n<p>But like most things, make it work first, then fix performance problems you detect in your performance test lab on production-grade hardware.</p>\n\n<p></p><hr><p></p>\n\n<p>EDIT:Some other points:</p>\n\n<ul>\n<li>key/value database such as Cassandra, Voldermort etc, do not generally support secondary indexes</li>\n<li>Therefore, you cannot do a CREATE INDEX</li>\n<li>Most of them also don't do range scans (even on the main index) because they're using hashing to implement partitioning (which they mostly do).</li>\n<li>Therefore they also don't do range expiry (DELETE FROM tbl WHERE ts &lt; NOW() - INTERVAL 30 DAYS)</li>\n<li>Your application must do ALL of this itself or manage without it; secondary indexes are really the killer</li>\n<li>ALTER TABLE ... ADD INDEX takes quite a long time in e.g. MySQL with a large table, but at least you don't have to write much code to do it. In a \"nosql\" database, it will also take a long time BUT also you have to write heaps and heaps of code to maintain the new secondary index, expire it correctly, AND modify your queries to use it.</li>\n</ul>\n\n<p>In short... you can't use a key/value database as a shortcut to avoid ALTER TABLE.</p>\n    "},{"t":"What is Hash and Range Primary Key?","l":"http://stackoverflow.com/questions/27329461/what-is-hash-and-range-primary-key","q":"\n\n<p>I am not able to understand what is Range primary key here - </p>\n\n<p><a href=\"http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithTables.html#WorkingWithTables.primary.key\">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithTables.html#WorkingWithTables.primary.key</a></p>\n\n<p>and how does it work?</p>\n\n<p>what is they mean by - \"unordered hash index on the hash attribute and a sorted range index on the range attribute\"?</p>\n    ","a":"\n<p>\"<em>Hash and Range Primary Key</em>\" means that a single row in DynamoDB has a unique primary key made up of both the <strong>hash</strong> and the <strong>range</strong> key. For example with a hash key of <em>X</em> and range key of <em>Y</em>, your primary key is effectively <em>XY</em>. You can also have multiple range keys for the same hash key but the combination must be unique, like <em>XZ</em> and <em>XA</em>.  Let's use their examples for each type of table:</p>\n\n<blockquote>\n  <p>Hash Primary Key â€“ The primary key is made of one attribute, a hash\n  attribute. For example, a ProductCatalog table can have ProductID as\n  its primary key. DynamoDB builds an unordered hash index on this\n  primary key attribute.</p>\n</blockquote>\n\n<p>This means that every row is keyed off of this value. <strong>Every row in DynamoDB will have a required, unique value for this attribute</strong>. Unordered hash index means what is says - the data is not ordered and you are not given any guarantees into how the data is stored. <strong>You won't be able to make queries on an unordered index</strong> such as <em>Get me all rows that have a ProductID greater than X</em>. You write and fetch items based on the hash key. For example, <em>Get me the row from that table that has ProductID X</em>. You are making a query against an unordered index so your gets against it are basically key-value lookups, are very fast, and use very little throughput.</p>\n\n<hr>\n\n<blockquote>\n  <p>Hash and Range Primary Key â€“ The primary key is made of two\n  attributes. The first attribute is the hash attribute and the second\n  attribute is the range attribute. For example, the forum Thread table\n  can have ForumName and Subject as its primary key, where ForumName is\n  the hash attribute and Subject is the range attribute. DynamoDB builds\n  an unordered hash index on the hash attribute and a sorted range index\n  on the range attribute.</p>\n</blockquote>\n\n<p>This means that every row's primary key is the <strong>combination of the hash and range key</strong>. You can make direct gets on single rows if you have both the hash and range key, or you can make a query against the <strong>sorted range index</strong>. For example, get <em>Get me all rows from the table with Hash key X that have range keys greater than Y</em>, or other queries to that affect. They have better performance and less capacity usage compared to Scans and Queries against fields that are not indexed. From <a href=\"http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScan.html\">their documentation</a>:</p>\n\n<blockquote>\n  <p>Query results are always sorted by the range key. If the data type of\n  the range key is Number, the results are returned in numeric order;\n  otherwise, the results are returned in order of ASCII character code\n  values. By default, the sort order is ascending. To reverse the order,\n  set the ScanIndexForward parameter to false</p>\n</blockquote>\n\n<p>I probably missed some things as I typed this out and I only scratched the surface. There are <em>a lot</em> more <a href=\"http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html\">aspects to take into consideration when working with DynamoDB tables</a> (throughput, consistency, capacity, other indices, key distribution, etc.). You should take a look at the <a href=\"http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SampleTablesAndData.html\">sample tables and data</a> page for examples.</p>\n    "},{"t":"Why NoSQL is better at â€œscaling outâ€ than RDBMS?","l":"http://stackoverflow.com/questions/8729779/why-nosql-is-better-at-scaling-out-than-rdbms","q":"\n\n<p>I have read the following text in a <a href=\"http://tekedia.com/12083/nosql-database-advantages-and-disadvantages/\">technical blog</a> discussing the advantages and disadvantages of NoSQL</p>\n\n<p><strong>\"</strong>\nFor years, in order to improve performance on database servers, database administrators have had to buy bigger servers as the database load increases (scaling up) instead of distributing the database across multiple â€œhostsâ€ as the load increases (scaling out). RDBMS do not typically scale out easily, but the newer NoSQL databases are actually designed to expand easily to take advantage of new nodes and are usually designed with low-cost commodity hardware in mind. <strong>\"</strong></p>\n\n<p>I became confused about the scalability of RDBMS and NoSQL.</p>\n\n<p>My confusion are:</p>\n\n<ol>\n<li>Why RDBMS are less able to scale out? And the reason of buying bigger servers instead of buying more cheap ones.</li>\n<li>Why NoSQL is more able to scale out?</li>\n</ol>\n    ","a":"\n<p>RDBMS have ACID ( <a href=\"http://en.wikipedia.org/wiki/ACID\">http://en.wikipedia.org/wiki/ACID</a> ) and supports transactions. Scaling \"out\" with RDBMS is harder to implement due to these concepts.</p>\n\n<p>NoSQL solutions usually offer record-level atomicity, but cannot guarantee a series of operations will succeed (transaction).</p>\n\n<p>It comes down to: to keep data integrity and support transactions, a multi-server RDBMS would need to have a fast backend communication channel to synchronize all possible transactions and writes, while preventing/handling deadlock.</p>\n\n<p>This is why you usually only see 1 master (writer) and multiple slaves (readers).</p>\n    "},{"t":"Lucene as data store","l":"http://stackoverflow.com/questions/3803045/lucene-as-data-store","q":"\n\n<p>Is it possible to use <a href=\"http://lucene.apache.org/java/docs/index.html\">Lucene</a> as full fledged data store (like other(mongo,couch) nosql variants). </p>\n\n<p>I know there are some limitations like newly updated documents by one indexer will not be shown in other indexer. So we need to restart the indexer to get the updates.</p>\n\n<p>But i stumble upon <a href=\"http://lucene.apache.org/solr/\">solr</a> lately, it seems these problems are avoided by some kind of snapshot replication.</p>\n\n<p>So i thought i could use lucene as a data store since this also uses same kind of documents(JSON based) used by mongo and couch internally to manage documents, and its proven indexing algorithm fetches the records super fast. </p>\n\n<p>But i am curious has anybody tried that before..? if not what are reasons not choosing this approach.</p>\n\n\n    ","a":"\n<p>There is also the problem of durability. While a Lucene index should not get corrupted ever, I've seen it happen. And the approach Lucene takes to repairing a broken index is \"throw it away and rebuild from the original data\". Which makes perfect sense for an indexing tool. But it does require you to have the data stored somewhere else.</p>\n    "},{"t":"SQL vs NoSQL: what about other issues than ACID and scalibility?","l":"http://stackoverflow.com/questions/23308576/sql-vs-nosql-what-about-other-issues-than-acid-and-scalibility","q":"\n\n<p>I have read quite a few articles lately which describe SQL and NoSQL from both sides of the divide, such as <a href=\"http://use-the-index-luke.com/blog/2013-04/whats-left-of-nosql\">http://use-the-index-luke.com/blog/2013-04/whats-left-of-nosql</a>. These articles very often touch the subjects of things like ACID and scalability. However, a number of issues I usually have with SQL seem to be rarely mentioned in these articles and I was wondering why, and whether that had to do with me not entirely understanding SQL. If anyone can enlighten me, at least partly on one or more of the following items, I would highly appreciate it.</p>\n\n<p>My issues with SQL:</p>\n\n<ol>\n<li>SQL is inherently insecure: SQL is a language which was made for insertion and doesn't have any methods to prevent insertion of code instead of data. The only way to prevent insertion is to completely isolate SQL from the application using it. Why hasn't this been solved yet in SQL itself?</li>\n<li>SQL seems to have been made for the smallest storage size possible for the data contained in it. While that still makes a lot of sense for huge amounts of data, it doesn't really for smaller databases anymore, or does it?</li>\n<li>SQL forces everything to fit a two dimensional relational model, with specific relation tables to do the other dimensions. To me this poses as two problems:\n<ul>\n<li>the consistency of data is completely relying on relation tables</li>\n<li>the data is very difficult for humans to make sense of in cases of failures</li>\n</ul></li>\n<li>SQL doesn't maintain a history as it does destructive updates by default: there are of course all kinds of ways to create a history, but that requires custom written stuff with extra tables and the use of time stamps, or writing a new record for every change, leading to exponentially growing table sizes. </li>\n<li>SQL seems to prefer data loss to loss of consistency: if an error occurs or loss of consistency, the only way of restoring the situation to a consistent state is to use a backup, which means that the latest changes will be destroyed. This is partly because of the lack of history (see 4), but also, because of the lack of human readability, there is no real way to have a human try to correct errors.</li>\n<li>Especially in a web-environment, the use of SQL usually means to have models created often more than once. In a normal (simple) PHP web application twice: once in PHP, once in SQL. In a full stack web application three times: once in the client app, once in the middleware, and once in the SQL database (if no ORM is used). Because of the different programming languages, and the type differences between them, it means that there is a lot of possible conflicts between those models. I am aware that ORMs like ActiveRecord and Django solve at least part of these problems, but the amount of extra work you still need to do because an SQL table contains a VARCHAR(25) and none of the languages used (JavaScript, Ruby, PHP, Perl, Python etc) know that kind of a construct, is huge.</li>\n<li>Data structure changes seem to be regarded as a consistency issue: if something changes in the data structure, table changes are applied to every existing record in that table, even if a record didn't have that field originally and whether or not it makes sense for that record to have that field. A set of those changes lead to automated migrations, which add another layer of possible problems, especially with regards to consistency.</li>\n<li>Mixing up of storage logic and application logic: SQL seems to be eager to gobble up parts of the application logic as stored procedures (CouchDB also does this through views). \nWhile I do understand that for some types of operations you need server side and very strictly controlled procedures, I don't understand why they are stored in the database and as such part of the storage engine, instead of being a part of the application (middleware).</li>\n</ol>\n\n<p>I am aware of (but not very familiar with) things like PostgreSQL Hstore, but I don't see entirely how that solves the things mentioned above.\nThanks for any insights!</p>\n    ","a":"\n<ol>\n<li><p>Is SQL inherently insecure?</p>\n\n<p>I think you are referring to <a href=\"http://en.wikipedia.org/wiki/SQL_injection\">SQL Injections</a> which is one of the most dangerous security vulnerabilities at all.</p>\n\n<p>However, SQL injection is primarily a problem of education because most textbook and courses don't explain <a href=\"http://use-the-index-luke.com/sql/where-clause/bind-parameters\">bind parameters</a> at all. Writing literal values into the SQL statement itself is handy for ad-hoc queries when humans use the database directly, but is just the plain wrong way in programs. Programs should always use bind parameters (very few exceptions for performance) effectively protecting the program 100% against SQL injection. The problem is that SQL textbooks don't say that.</p>\n\n<p>Even otherwise, SQL has sound security systems that allow you to limit access to tables, views, and sometimes even selected rows based on some rules (\"row level security\").</p></li>\n<li><p>\"smallest storage size possible\"</p>\n\n<p>Sorry, I don't get that question.</p></li>\n<li><p>About normalization.</p>\n\n<p>You are right. Normalization solves a few problems (de-duplication and preventing unintentional inconsistencies) but opens some others. Namely:</p>\n\n<ul>\n<li>How to easily access data from many tables. </li>\n<li>How to maintain consistency across many tables.</li>\n<li>How to cope with intentional \"inconsistencies\" aka. histories (master data changes)</li>\n</ul>\n\n<p>In principle, SQL should provide tools to compensate for these challenges raised by normalization.</p>\n\n<p>Accessing data from many tables should be done using joins and similar operations of SQL. SQL does more than storing and retrieving data in a 1:1 fashion, it provides tools (joins, subqueries, set operations,...) to transform the normalized data into the form that is most suitable for a specific task. This is done intentionally at runtime because the tasks don't need to be known beforehand. The nature of the data, on the other hand, is considered to be static so that storing it an a normalized fashion is valid. This is a very important key concept of the relational model and SQL: The <em>nature</em> of the data doesn't change so that's the way it should be persistent. How you use that data varies widely and does often change over time â€” hence this has to be done dynamic. This is of course a very regular task so that it makes sense to have a solid tool to make it easy. We call this tool SQL ;) The <a href=\"http://en.wikipedia.org/wiki/Dont_repeat_yourself\">DRY</a> rule can be accomplished by using views or <a href=\"http://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression\">CTEs</a> but both may hurt performance because the <em>implementations</em> ain't well optimized for that (something that I openly criticize!).</p>\n\n<p>Keeping data consistent across many tables is mostly done with the help of constraints.</p>\n\n<p>Handling intended \"inconsistencies\" (histories) was finally covered by <a href=\"http://en.wikipedia.org/wiki/SQL:2011\">SQL:2011</a>: This will allow \"AS OF\" queries and provide also tools to maintain temporal consistencies (e.g. validity of one row may not overlap with the validity of another row). Arguably, it is pretty poor that it took 40 years or so to come up with a solution for that. And I don't even know when this will be commonly available!</p>\n\n<p>I think that part is pretty much true for every system: \"the data is very difficult for humans to make sense of <em>in cases of failures</em>\" (my emphasis). However, I think you might mean that it is hard to investigate issues because the required data might be spread across several tables. SQL's answer for that is: VIEWs which are basically just stored queries. However, depending on the database band VIEWs might introduce performance issues. That is, however, a limitation of some database bands, not a limitation of SQL or the relational model.</p></li>\n<li><p>Keeping History</p>\n\n<p>I've already mentioned that above (SQL:2011).</p>\n\n<p>The following is also true for every system that want's to keep history: \"leading to exponentially growing table sizes.\" Although I'd say its \"ever growing\" not \"exponentially\".</p>\n\n<p>The tools to cope with it are triggers or the ORMs. If you want to be sure nobody does a \"destructive update\" you can just revoke UPDATE rights on that table (and also DELETE to be on the save side).</p></li>\n<li><p>\"prefer data loss to loss of consistency: \"</p>\n\n<p>I find that an interesting point of view. However, the SQL answer to this is that you try very hard to not get wrong data into the system in the first place. Mostly by using a proper schema, constraints + ACID. In that way your statement is somehow right: instead of accepting inconsistent data is <em>rejected</em> (which is something different than lost!). So, you must handle the error at the time somebody is entering the rejected data as opposed to some later time when you try to resolve inconsistencies because you accepted the bad data in the first place. So yes, that's the philosophy for the relational model and SQL!</p>\n\n<p>The lack of human readability is obviously depended on your background. However, the correct-ability using SQL is pretty nice, I'd say. Here I'd also like to quote from the original IBM paper about <a href=\"http://www.almaden.ibm.com/cs/people/chamberlin/sequel-1974.pdf\">SEQUEL</a> (at that time it was the real name of it):</p>\n\n<blockquote>\n  <p>SEQUEL is intended as a data base sublanguage for both the professional programmer and the more infrequent data base user.</p>\n</blockquote>\n\n<p>In my observation, it is absolutely true: I've recently had an assignment to teach SQL to support staff so that they can investigate cases directly in the database. They were no programmers, yet understood SQL quite quickly. And I think here kicks your \"human\" argument in: What they had problems with is navigating a real world relational model consisting of several hundred tables. But that issue was quickly solved by asking development to provide views for some common tasks that involve more than a few tables. Joining those view then was no problem anymore.</p>\n\n<p>For relational thinking you need a different mind set very much you need a different mid set to functional programming. That's not good or badâ€”yet it might be uncommon for you. Once you use it regularly you'll get used to it.</p></li>\n<li><p>Object/Relational Impedance Mismatch</p>\n\n<p>I think that topic doesn't need any long discussion: yes it exists, yes there are tools to cope with it in some way or another. I've made my point about excessive use in my article.</p></li>\n<li><p>Data structure changes</p>\n\n<p>I think that is mostly due to a poor understanding of the relational model in the first place. Compare above: \"the nature of the data\"</p>\n\n<p>It's also a pretty well discussed argument: schema vs. \"schema less\". Choose your flavor. \"Schema less\" quite often just means \"doesn't provide schema management tools\" nevertheless you have to cope with the fact that we sometimes want to add more properties to an existing entity. RDBMSs provide tools for that: new columns can be nullable or have default values. More drastic changes such as moving one attribute to an extra table (e.g. 1:n) can be done with CREATE AS SELECT. You might even provide a compatibility view that still delivers the data as it ought to be (as though the moved attribute would still be stored in the table). Once you changed your schema, your application can rely on it's constraints (such as the existence of columns or the validity of constraints). That's quite a lot of stuff the database can do for you in an extremely reliable manner. Stuff you don't need to care about in your app anymore.</p>\n\n<p>An argument you didn't mention is that those schema changes often involve downtimes. That's definitively true for the past, and to some extent also today. E.g. MySQL introduce online ALTER TABLE just in 5.6 recently. However, that's quite often an implementation limitation, not a problem inherently tied to the relational model or SQL. Even some more complex changes (like moving an attribute to another table) can be done online when done right and planned carefully (I've done that with one of the expensive database that provide all the tools you need for it). Generally it is about keeping the migration code out of your application and coping in the database with it. After the migration you should neither have migration artifacts in the DB nor in the application code. Of course, there are cases where a downtime is inevitable (I think ;).</p></li>\n<li><p>\"Mixing up of storage logic and application logic\"</p>\n\n<p>SQL actually does the exact opposite: SQL abstracts the storage layer completely away.</p>\n\n<p>Nobody forces you to use stored procedures. I personally also think that stored procedures are overused, mostly because stored procedures are stored in the database and can thus be changed (optimized) by database administrators that might not have access to other source code. In other words: I think it is often done out of desperation.</p>\n\n<p>The second argument is, of course, the excessive use of ORMs again and policies that disallow the use of real SQL in the application.</p></li>\n</ol>\n    "},{"t":"Mongoose: populate() / DBref or data duplication?","l":"http://stackoverflow.com/questions/7970082/mongoose-populate-dbref-or-data-duplication","q":"\n\n<p><strong>I have two collections:</strong></p>\n\n<ol>\n<li>Users</li>\n<li>Uploads</li>\n</ol>\n\n<p><br>\nEach upload has a <code>User</code> associated with it and I need to know their details when an <code>Upload</code> is viewed. Is it best practice to duplicate this data inside the the Uploads record, or use <a href=\"http://mongoosejs.com/docs/populate.html\">populate()</a> to pull in these details from the Users collection referenced by <code>_id</code>?</p>\n\n<p><br>\n<strong>OPTION 1</strong></p>\n\n<pre><code>var UploadSchema = new Schema({\n    _id: { type: Schema.ObjectId },\n    _user: { type: Schema.ObjectId, ref: 'users'},\n    title: { type: String },\n});\n</code></pre>\n\n<p><br>\n<strong>OPTION 2</strong></p>\n\n<pre><code>var UploadSchema = new Schema({\n    _id: { type: Schema.ObjectId },\n    user: { \n           name: { type: String },\n           email: { type: String },\n           avatar: { type: String },\n           //...etc\n          },\n    title: { type: String },\n});\n</code></pre>\n\n<p><br>\nWith 'Option 2' if any of the data in the <code>Users</code> collection changes I will have to update this across all associated <code>Upload</code> records. With 'Option 1' on the other hand I can just chill out and let <code>populate()</code> ensure the latest User data is always shown.</p>\n\n<p><strong>Is the overhead of using <code>populate()</code> significant? What is the best practice in this common scenario?</strong></p>\n    ","a":"\n<p>If You need to query on your Users, keep users lonely. If You need to query on your uploads, keep uploads lonely.</p>\n\n<p>Another question you should make yourself is: each time i need this data i need the embedded objects (and viceversa)? How many time this data will be updated? How many times this data will be read?</p>\n\n<p><strong>Think about a friendship request:</strong>\nEach time you need the request you need the user which made the request, then embed the request inside the user document.</p>\n\n<p>You will be able to create an index on the embedded object too, and your search will be mono query / fast / consistent.</p>\n\n<hr>\n\n<p><em>Just a link to my previous reply on a similar question:</em>\n<a href=\"http://stackoverflow.com/questions/3128200/mongo-db-relations-between-objects/7287321#7287321\">Mongo DB relations between objects</a></p>\n\n<p>I think this post will be right for you <a href=\"http://www.mongodb.org/display/DOCS/Schema+Design\" rel=\"nofollow\">http://www.mongodb.org/display/DOCS/Schema+Design</a></p>\n\n<p><strong>Use Cases</strong></p>\n\n<p><em>Customer / Order / Order Line-Item</em></p>\n\n<blockquote>\n  <p>Orders should be a collection. customers a collection. line-items should be an array of line-items embedded in the order object.</p>\n</blockquote>\n\n<p><em>Blogging system.</em></p>\n\n<blockquote>\n  <p>Posts should be a collection. post author might be a separate collection, or simply a field within posts if only an email address. comments should be embedded objects within a post for performance.</p>\n</blockquote>\n\n<p><strong>Schema Design Basics</strong></p>\n\n<p><em>Kyle Banker, 10gen</em></p>\n\n<blockquote>\n  <p><a href=\"http://www.10gen.com/presentation/mongosf2011/schemabasics\" rel=\"nofollow\">http://www.10gen.com/presentation/mongosf2011/schemabasics</a></p>\n</blockquote>\n\n<p><em>Indexing &amp; Query Optimization\nAlvin Richards, Senior Director of Enterprise Engineering</em></p>\n\n<blockquote>\n  <p><a href=\"http://www.10gen.com/presentation/mongosf-2011/mongodb-indexing-query-optimization\" rel=\"nofollow\">http://www.10gen.com/presentation/mongosf-2011/mongodb-indexing-query-optimization</a></p>\n</blockquote>\n\n<p>*<em>These 2 videos are the bests on mongoddb ever seen imho</em></p>\n    "},{"t":"In Cassandra terminology, what is TimeUUID?","l":"http://stackoverflow.com/questions/2614195/in-cassandra-terminology-what-is-timeuuid","q":"\n\n<p>In Cassandra terminology, what is TimeUUID and when is it used?</p>\n    ","a":"\n<p><a href=\"http://svn.apache.org/repos/asf/cassandra/trunk/src/java/org/apache/cassandra/db/marshal/TimeUUIDType.java\">TimeUUID</a> is one of six concrete implementations of the abstract class <a href=\"http://svn.apache.org/repos/asf/cassandra/trunk/src/java/org/apache/cassandra/db/marshal/AbstractType.java\">AbstractType</a>.</p>\n\n<p>For ColumnFamilies you have the possiblity to specify an attribute called CompareWith. (SuperColumns have a similar CompareSubcolumnsWith attribute).  </p>\n\n<p>Valid values for this attribute are classes that implements the abstract class AbstractType (eg. TimeUUID). The CompareWith attribute tells Cassandra how to sort the columns for slicing operations.</p>\n\n<p>If you are using Java and using cassandra with TimeUUID I would recommend to read <a href=\"http://wiki.apache.org/cassandra/FAQ#working_with_timeuuid_in_java\">this section of the cassandra FAQ</a>.  </p>\n    "},{"t":"Is it possible to construct complex queries against noSQL DB","l":"http://stackoverflow.com/questions/2351040/is-it-possible-to-construct-complex-queries-against-nosql-db","q":"\n\n<p>I have been researching noSQL DB and have not been able to satisfactorily answer this for myself:  Is it possible to construct complex queries against noSQL DB?</p>\n\n<p>The type of query I'm wondering about would be something like this:</p>\n\n<pre><code>select * from DB where\nvara &gt; x AND\nvarb = 2 AND\nvarc &lt; x AND\nvard in (x,y,z) AND\nvare like '%texthere%' AND\nvarf = 2 AND\nvarg = 3 and\netc...\n</code></pre>\n\n<p>NOTE:  I'm aware that I can't use SQL, as above, what I'm asking is how would I query using the psuedo logic above, in other words a whole bunch of different conditions.  So far the best answer I have found is that you have an RDBMS to query and then grab data based on key from cloud.  That doesn't necessarily seem more efficient to me.</p>\n\n<p>So as a follow up.  If just trying to solve a 'search 4 million rows' problem as opposed to a 'we have billions of rows of data' problem, should I even bother looking at a noSQL DB?</p>\n    ","a":"\n<p>In <a href=\"http://www.mongodb.org/\">mongodb</a>, you would just do something like <code>db.mytbl.find({\"vara\": { $gt: 10}, \"varb\":  2, \"varc\": {$lt: 100 }})</code></p>\n\n<p>See <a href=\"http://www.mongodb.org/display/DOCS/Advanced+Queries\">here</a>, and <a href=\"http://www.mongodb.org/display/DOCS/Queries+and+Cursors\">here</a> for examples</p>\n    "},{"t":"Recommend a good db4o viewer [closed]","l":"http://stackoverflow.com/questions/2411424/recommend-a-good-db4o-viewer","q":"\n\n<p>I'm playing around with db4o, and I have the Object Manager viewer thingy for Visual Studio.  It seems okay, but not exactly on par with tools like HeidiSQL/SQL Studio/etc., not to mention that it locks the db4o file--I can't use my db4o app and Object Manager at the same time.</p>\n\n<p>Maybe I'm using it wrong, but regardless, I'd like to know what else is out there.  What tools would you recommend for looking at and manipulating db4o files?</p>\n\n<p>UPDATE: I've been using LINQPad.  There is some yak-shaving involved, but it's working pretty well.  It still leaves a lot to be desired in terms of the functionality available in applications like HeidiSQL/SQL Studio/etc., but it's a start.</p>\n    ","a":"\n<p>I suggest you use <a href=\"http://www.linqpad.net/\" rel=\"nofollow\">LINQPad</a> as a first start.</p>\n\n<p>Gamlor has <a href=\"http://www.gamlor.info/wordpress/?p=949\" rel=\"nofollow\">a great tutorial on how to use LINQPad with db4o</a>. You'll need to modify that a bit so you use client-server access (thus not locking your db).</p>\n\n<p>I don't want to re-post his code here, because I think it's pointless and he's also a user here, don't wanna win his laurels. </p>\n\n<p>For me, LINQPad seems to be the best approach, also because I can use it on my server, where I'd be having trouble with OME for obvious reasons.</p>\n    "},{"t":"What's the difference been NoSQL and a Column-Oriented database?","l":"http://stackoverflow.com/questions/2798251/whats-the-difference-been-nosql-and-a-column-oriented-database","q":"\n\n<p>The more I read about NoSQL, the more it begins to sound like a column oriented database to me.</p>\n\n<p>What's the difference between NoSQL (e.g. CouchDB, Cassandra, MongoDB) and a column oriented database (e.g. Vertica, MonetDB)?</p>\n    ","a":"\n<p>Some NoSQL databases are column-oriented databases, and some SQL databases are column-oriented as well.  Whether the database is column or row-oriented is a physical storage implementation detail of the database and can be true of both relational and non-relational (NoSQL) databases.</p>\n\n<p>Vertica, for example, is a column-oriented relational database so it wouldn't actually qualify as a NoSQL datastore.</p>\n\n<p>A \"NoSQL movement\" datastore is better defined as being non-relational, shared-nothing, horizontally scalable database without (necessarily) ACID guarantees. Some column-oriented databases can be characterized this way. Besides column stores, NoSQL implementations also include document stores, object stores, tuple stores, and graph stores.</p>\n    "},{"t":"Using a Filesystem (Not a Database!) for Schemaless Data - Best Practices","l":"http://stackoverflow.com/questions/4189898/using-a-filesystem-not-a-database-for-schemaless-data-best-practices","q":"\n\n<p>After reading over my other question, <a href=\"http://stackoverflow.com/questions/4189709/using-a-relational-database-for-schema-less-data\"><em>Using a Relational Database for Schema-Less Data</em></a>, I began to wonder if a filesystem is more appropriate than a relational database for storing and querying schemaless data. </p>\n\n<p>Rather than just building a file system on top of MySQL, why not just save the data directly to the filesystem? Indexing needs to be figured out, but modern filesystems are very stable, have great features like replication, snapshot and backup facilities, and are flexible at storing schema-less data.</p>\n\n<p>However, I can't find <em>any</em> examples of someone using a filesystem instead of a database. </p>\n\n<p>Where can I find more resources on how to implement a schemaless (or \"document-oriented\") database as a layer on top of a filesystem? Is anyone using a modern filesystem as a schemaless database?</p>\n    ","a":"\n<p>Yes a filesystem could be taken as a special case of a NOSQL-like database system. It may have some limitations that should be considered during any design decisions:</p>\n\n<p>pros:\n -\n -  simple, intuitive.</p>\n\n<ul>\n<li>takes advantage of years of tuning and caching algorithms</li>\n<li>easy backup, potentially easy clustering</li>\n</ul>\n\n<p>things to think about:</p>\n\n<ul>\n<li><p>richness of metadata - what types of\ndata does it store, how does it let\nyou query them, can you have\nhierarchal or multivalued attributes</p></li>\n<li><p>speed of querying metadata - not all\nfs's are particularly well optimized\nwith anything other than size, dates.</p></li>\n<li><p>inability to join queries (though\nthat's pretty much common to NoSQL)</p></li>\n<li><p>inefficient storage usage (unless the file\nsystem performs block suballocation,\nyou'll typically blow 4-16K per item\nstored regardless of size)</p></li>\n<li>May not have the kind of caching algorithm\nyou want for it's directory structure</li>\n<li>tends to be less tunable, etc. </li>\n<li>backup solutions may have trouble\ndepending on how you store things -\ntoo deep, too many items per node,\netc - which might obviate an obvious\nadvantage of such a structure.\nlocking for a LOCAL filesystem works\npretty well of course if you call the\nright routines, but not necessarily\nfor a network base fileesytem (those\nproblems have been solved in various\nways, but it's certainly a design\nissue)</li>\n</ul>\n    "},{"t":"Any Open Source software using Orient DB database? Have you any experiences with that database? [closed]","l":"http://stackoverflow.com/questions/3028156/any-open-source-software-using-orient-db-database-have-you-any-experiences-with","q":"\n\n<p>Do you know any open source software that uses <a href=\"http://www.orientechnologies.com/\">Orient DB</a>?  Or have you used that product yourself? Any experiences to share?</p>\n\n<p>I have recently looked into Orient DB, and it has nice and interesting feature set (fast, embeddable in Java, simple API) but it seems that it is not widely used. Is it just because the Orient DB is a new player on the field?</p>\n    ","a":"\n<p>After the total failure of ODBMS (at least from an adoption point of view), it seems obvious to me that the NoSQL movement is perceived by (ex) ODBMS players (like Versant, db4o, Orient) as an opportunity for a resurrection.</p>\n\n<p>This IMHO exactly the case of OrientDB which is the result of the rewrite of the Orient ODBMS engine as a document oriented database (in other words, re-branded to fit in the NoSQL niche market).</p>\n\n<p>But while OrientDB benefits from the experience acquired in the ODBMS field (the author has more 10+ years of experience in this field and is member of the JDO expert group, how surprising), I'm not aware of any projects/customers using it (and I believe they would publish some testimonials if they had many of them). Some possible reaons:</p>\n\n<ol>\n<li>The product is new.</li>\n<li>Only a <strong>very few people</strong> might need a NoSQL solution.</li>\n</ol>\n\n<p>The conjunction of both points means you won't see \"mass adoption\". At least, this is my opinion.</p>\n\n<p>That being said, I agree that OrientDB looks interesting. </p>\n    "},{"t":"'schema' design for a social network","l":"http://stackoverflow.com/questions/2839725/schema-design-for-a-social-network","q":"\n\n<p>I'm working on a proof of concept app for a twitter style social network with about 500k users.  I'm unsure of how best to design the 'schema'</p>\n\n<p>should I embed a user's subscriptions or have a separate 'subscriptions' collection and use db references?  If I embed, I still have to perform a query to get all of a user's followers. e.g.</p>\n\n<p>Given the following user:</p>\n\n<pre><code>{\n \"username\" : \"alan\",\n \"photo\": \"123.jpg\",\n \"subscriptions\" : [\n    {\"username\" : \"john\", \"status\" : \"accepted\"},\n    {\"username\" : \"paul\", \"status\" : \"pending\"}\n  ]\n}\n</code></pre>\n\n<p>to find all of alan's subscribers, I'd have to run something like this:</p>\n\n<pre><code>db.users.find({'subscriptions.username' : 'alan'});\n</code></pre>\n\n<p>from a performance point of view, is that any worse or better than having a separate subscriptions collection?</p>\n\n<p>also, when displaying a list of subscriptions/subscribers, I am currently having problems with n+1 because the subscription document tells me the username of the target user but not other attributes I may need such as the profile photo.  Are there any recommended practices for such situations?</p>\n\n<p>thanks\nAlan</p>\n    ","a":"\n<p>First off, you should know the tradeoffs you are going to get with MongoDB and any other NoSQL database (but realize that I am a fan of it). If you are trying to normalize your data completely, you are making a big mistake. Even in relational databases, the larger your app gets, the more your data gets denormalized (see <a href=\"http://blog.hotpotato.com/post/574255351/hot-potato-infrastructure-mongodb\">this post</a> by Hot Potato). I've seen this time and time again. You should not go nuts and make a huge mess, but don't worry about repeating information in two places. One of the major points (in my opinion) of NoSQL is that your schema moves into your code and not solely into the database.</p>\n\n<p>Now, to answer your question, I think your initial strategy is what I would do. MongoDB can place indexes on elements which are arrays, so that will make things a lot faster if you are looking for how many friendships a user has. But in reality, the only way to really be sure is to run some sort of test program that generates a database full of names and relationships. </p>\n\n<p>You can script up some input in Python or Perl or whatever you like, and use a file of names to generate some relationships. Check out the <a href=\"http://www.census.gov/genealogy/names/names_files.html\">Census website</a>, which has a list of last names. Download the file <code>dist.all.last</code> and write some program like:</p>\n\n<pre><code>#! /usr/bin/env python\nimport random as rand\n\nf = open('dist.all.last')\nnames = []\nfor line in f:\n  names.append(line.split()[0])\n\nrels = {}\nfor name in names:\n  numOfFriends = rand.randint(0, 1000)\n  rels[name] = []\n  for i in range(numOfFriends):\n    newFriend = rand.choice(names)\n    if newFriend != name: #cannot be friends with yourself\n      rels[name].append(newFriend)\n\n# take relationships (i.e. rels) and write them to MongoDB\n</code></pre>\n\n<p>Also, as a general note, your fieldnames seem kind of long. Remember that the fieldnames are repeated with <em>every document</em> in that collection because you cannot rely on one field being in any other document. To save space, a general strategy is to use shorter fieldnames like \"unam\" instead of \"username\", but that's a small thing. See the great advice in <a href=\"http://blog.boxedice.com/2009/07/25/choosing-a-non-relational-database-why-we-migrated-from-mysql-to-mongodb/\">these</a> <a href=\"http://blog.boxedice.com/2010/02/28/notes-from-a-production-mongodb-deployment/\">two</a> posts.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Actually, in pondering your problem a little more, I would make one more suggestion: break up the subscription types into different fields to make the indexes more efficient. For example, instead of:</p>\n\n<pre><code>{\n \"username\" : \"alan\",\n \"photo\": \"123.jpg\",\n \"subscriptions\" : [\n    {\"username\" : \"john\", \"status\" : \"accepted\"},\n    {\"username\" : \"paul\", \"status\" : \"pending\"}\n  ]\n}\n</code></pre>\n\n<p>As you said above, I would do this:</p>\n\n<pre><code>{\n \"username\" : \"alan\",\n \"photo\": \"123.jpg\",\n \"acc_subs\" : [ \"john\" ],\n \"pnd_subs\" : [ \"paul\" ]\n}\n</code></pre>\n\n<p>So that you could have an index for each type of subscription, thus making queries like \"Hoy many people have Paul as pending?\" and \"How many people subscribe to Paul?\" super fast either way. Mongo's indexing over array'd values is truly an epic win.</p>\n    "},{"t":"What are good NoSQL and non-relational database solutions for audit/logging database","l":"http://stackoverflow.com/questions/2774341/what-are-good-nosql-and-non-relational-database-solutions-for-audit-logging-data","q":"\n\n<p>What would be suitable database for following? I am especially interested about your experiences with non-relational <a href=\"http://en.wikipedia.org/wiki/NoSQL\" rel=\"nofollow\">NoSQL</a> systems. \nAre they any good for this kind of usage, which system you have used and would recommend, or should I go with normal relational database (DB2)?</p>\n\n<p>I need to gather audit trail/logging type information from bunch of sources to a\ncentralized server where I could generate reports efficiently and examine what is happening in the system.</p>\n\n<p>Typically a audit/logging event would consist always of some mandatory fields, for example</p>\n\n<ul>\n<li>globally unique id (some how generated by program that generated this event)</li>\n<li>timestamp</li>\n<li>event type (i.e. user logged in,  error happened etc)</li>\n<li>some information about source (server1, server2)</li>\n</ul>\n\n<p>Additionally the event could contain 0-N key-value pairs, where value might be up to few kilobytes of text.</p>\n\n<ul>\n<li>It must run on Linux server</li>\n<li>It should work with high amount of data (100GB for example)</li>\n<li>it should support some kind of efficient full text search</li>\n<li>It should allow concurrent reading and writing</li>\n<li>It should be flexible to add new event types and add/remove key-value pairs to  new events. Flexible=no changes should be required to database schema, application generating the events can just add new event types/new fields as needed.</li>\n<li>it should be efficient to make queries against database. For reporting and exploring what happened.  For example:\n<ul>\n<li>How many events with type=X occurred in some time period. </li>\n<li>Get all events where field A has value Y. </li>\n<li>Get all events with type X and field A has value 1 and field B is not 2 and event occurred in last 24h</li>\n</ul></li>\n</ul>\n    ","a":"\n<p>The two I've seen used successfully are <a href=\"http://www.mongodb.org/\" rel=\"nofollow\">MongoDB</a> and <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Cassandra</a>.</p>\n    "},{"t":"In MongoDB, strategy for maximizing performance of writes to daily log documents","l":"http://stackoverflow.com/questions/8010643/in-mongodb-strategy-for-maximizing-performance-of-writes-to-daily-log-documents","q":"\n\n<p>We have a collection of log data, where each document in the collection is identified by a MAC address and a calendar day. Basically:</p>\n\n<pre><code>{\n  _id: &lt;generated&gt;,\n  mac: &lt;string&gt;,\n  day: &lt;date&gt;,\n  data: [ \"value1\", \"value2\" ]\n}\n</code></pre>\n\n<p>Every five minutes, we append a new log entry to the data array within the current day's document. The document rolls over at midnight UTC when we create a new document for each MAC.</p>\n\n<p>We've noticed that IO, as measured by bytes written, increases all day long, and then drops back down at midnight UTC. This shouldn't happen because the rate of log messages is constant. We believe that the unexpected behavior is due to Mongo moving documents, as opposed to updating their log arrays in place. For what it's worth, <code>stats()</code> shows that the paddingFactor is 1.0299999997858227.</p>\n\n<p>Several questions:</p>\n\n<ol>\n<li>Is there a way to confirm whether Mongo is updating in place or moving? We see some moves in the slow query log, but this seems like anecdotal evidence. I know I can <code>db.setProfilingLevel(2)</code>, then <code>db.system.profile.find()</code>, and finally look for <code>\"moved:true\"</code>, but I'm not sure whether it's ok to do this on a busy production system.</li>\n<li>The size of each document is very predictable and regular. Assuming that mongo is doing a lot of moves, what's the best way to figure out why isn't Mongo able to presize more accurately? Or to make Mongo presize more accurately? Assuming that the above description of the problem is right, tweaking the padding factor does not seem like it would do the trick.</li>\n<li>It should be easy enough for me to presize the document and remove any guesswork from Mongo. (I know the <a href=\"http://www.mongodb.org/display/DOCS/Padding+Factor\">padding factor</a> docs say that I shouldn't have to do this, but I just need to put this issue behind me.) What's the best way to presize a document? It seems simple to write a document with a garbage byte array field, and then immediately remove that field from the document, but are there any gotchas that I should be aware of? For example, I can imagine having to wait on the server for the write operation (i.e. do a safe write) before removing the garbage field.</li>\n<li>I was concerned about preallocating all of a day's documents at around the same time because it seems like this would saturate the disk at that time. Is this a valid concern? Should I try to spread out the preallocation costs over the previous day?</li>\n</ol>\n    ","a":"\n<p>The following combination seems to cause write performance to fall off a cliff:</p>\n\n<ol>\n<li>Journaling is on.</li>\n<li>Writes append entries to an array that makes up the bulk of a larger document</li>\n</ol>\n\n<p>Presumably I/O becomes saturated. Changing either of these factors seems to prevent this from happening:</p>\n\n<ol>\n<li>Turn journaling off. Use more replicas instead.</li>\n<li>Use smaller documents. Note that document size here is measured in bytes, not in the length of any arrays in the documents.</li>\n<li>Journal on a separate filesystem.</li>\n</ol>\n\n<p>In addition, here are some other tricks that improve write throughput. With the exception of sharding, we found the improvements to be incremental, whereas we were trying to solve a \"this doesn't work at all\" kind of problem, but I'm including them here in case you're looking for incremental improvements. The 10Gen folks <a href=\"http://groups.google.com/group/mongodb-user/browse_thread/thread/8a24e7f3faf95f71/7b26f9b8cd057e69#7b26f9b8cd057e69\" rel=\"nofollow\">did some testing and got similar results</a>:</p>\n\n<ol>\n<li>Shard.</li>\n<li>Break up long arrays into several arrays, so that your overall structure looks more like a nested tree. If you use hour of the day as the key, then the daily log document becomes:<br> <code>{\"0\":[...], \"1\":[...],...,\"23\":[...]}</code>.</li>\n<li>Try manual preallocation. (This didn't help us. Mongo's padding seems to work as advertised. My original question was misguided.)</li>\n<li>Try different --syncdelay values. (This didn't help us.)</li>\n<li>Try without safe writes. (We were already doing this for the log data, and it's not possible in many situations. Also, this seems like a bit of a cheat.)</li>\n</ol>\n\n<p>You'll notice that I've copied some of the suggestions from 10Gen here, just for completeness. Hopefully I did so accurately! If they publish a cookbook example, then I'll post a link here.</p>\n    "},{"t":"Database for super-fast querying","l":"http://stackoverflow.com/questions/2229420/database-for-super-fast-querying","q":"\n\n<p>We have a 300 Gb+ data array we'd like to query as fast as possible. Traditional SQL databases (specifically, SQL Server) cannot handle this volume as effectively as we need (like, perform a <code>select</code> with 10-20 conditions in <code>where</code> clause in less than 10 sec), so I'm investigating other solutions for this problem.</p>\n\n<p>I've been reading about <a href=\"http://en.wikipedia.org/wiki/NoSQL\">NoSQL</a> and this whole thing looks promising, but I'd prefer to hear from those who have used it in real life.</p>\n\n<p>What can you suggest here?</p>\n\n<p><strong>EDIT</strong> to clarify what we're after.</p>\n\n<p>We're a company developing an app whereby users can search for tours and perform bookings of said tours, paying for them with their plastic cards. This whole thing can surely be Russia-specific, so bear with me.</p>\n\n<p>When a user logs on to the site, she is presented with a form similar to this:</p>\n\n<p><img src=\"http://queenbee.alponline.ru/searchform.png\" alt=\"alt text\"></p>\n\n<p>Here, user selects where she leaves from and where she goes to, dates, duration and all that.</p>\n\n<p>After hitting \"Search\" a request goes to our DB server, which, with cannot handle such load: queries include various kinds of parameters. Sharding doesn't work well either.</p>\n\n<p>So what I'm after is a some kind of a pseudo-database, which can do lightning fast queries.</p>\n    ","a":"\n<p>If you want to do ad-hoc queries for reporting or analysis you're probably better off using something that will play nicely with off-the-shelf reporting tools.  Otherwise you are likely to find yourself getting dragged off all the time to write little report programs to query the data.  This is a strike against NoSQL type databases, but it may or may not be an issue depending on your circumstances.</p>\n\n<p>300GB should not be beyond the capabilities of modern RDBMS platforms, even MS SQL Server.  Some other options for large database queries of this type are:</p>\n\n<ul>\n<li><p>See if you can use a SSAS cube and aggregations to mitigate your query performance issues.  Usage-based optimiisation might get you adequate performance without having to get another database system.  SSAS can also be used in shared-nothing configurations, allowing you to stripe your queries across a cluster of relatively cheap servers with direct-attach disks.  Look at ProClarity for a front-end if you do go this way.</p></li>\n<li><p>Sybase IQ is a RDBMS platform that uses an underlying data structure optimised for reporting queries.  It has the advantage that it plays nicely with a reasonable variety of conventional reporting tools.  Several other systems of this type exist, such as Red Brick, Teradata or Greenplum (which uses a modified version of PostgreSQL).  The principal strike against these systems is that they are not exactly mass market items and can be quite expensive.</p></li>\n<li><p>Microsoft has a shared-nothing version of SQL Server in the pipeline, which you might be able to use.  However they've tied it to third party hardware manufacturers so you can only get it with dedicated (and therefore expensive) hardware.</p></li>\n<li><p>Look for opportunities to build data marts with aggregated data to reduce the volumes for some of the queries.</p></li>\n<li><p>Look at tuning your hardware.  Direct attach SAS arrays and RAID controllers can put through streaming I/O of the sort used in table scans pretty quickly.  If you partition your tables over a large number of mirrored pairs you can get very fast streaming performance - easily capable of saturating the SAS channels.<br><br>Practically, you're looking at getting 10-20GB/sec from your I/O subsystem if you want the performance targets you describe, and it is certianly possible to do this without resorting to really exotic hardware.</p></li>\n</ul>\n    "},{"t":"How to get Schema of mongoose database which defined in another model","l":"http://stackoverflow.com/questions/8730255/how-to-get-schema-of-mongoose-database-which-defined-in-another-model","q":"\n\n<p>This is my folder structure:</p>\n\n<pre><code>+-- express_example\n|---- app.js\n|---- models\n|-------- songs.js\n|-------- albums.js\n|---- and another files of expressjs\n</code></pre>\n\n<p>My code in file songs.js </p>\n\n<pre><code>var mongoose = require('mongoose')\n, Schema = mongoose.Schema\n, ObjectId = Schema.ObjectId;\n\nvar SongSchema = new Schema({\nname: {type: String, default: 'songname'}\n, link: {type: String, default: './data/train.mp3'}\n, date: {type: Date, default: Date.now()}\n, position: {type: Number, default: 0}\n, weekOnChart: {type: Number, default: 0}\n, listend: {type: Number, default: 0}\n});\n\nmodule.exports = mongoose.model('Song', SongSchema);\n</code></pre>\n\n<p>And here is my code in file albums.js</p>\n\n<pre><code>var mongoose = require('mongoose')\n, Schema = mongoose.Schema\n, ObjectId = Schema.ObjectId;\n\nvar AlbumSchema = new Schema({\nname: {type: String, default: 'songname'}\n, thumbnail: {type:String, default: './images/U1.jpg'}\n, date: {type: Date, default: Date.now()}\n, songs: [SongSchema]\n});\n\nmodule.exports = mongoose.model('Album', AlbumSchema);\n</code></pre>\n\n<p><br> How can I make albums.js know <strong>SongSchema</strong> to be defined AlbumSchema</p>\n    ","a":"\n<p>You can get models defined elsewhere directly with Mongoose:</p>\n\n<pre><code>require('mongoose').model(name_of_model)\n</code></pre>\n\n<p>To get the schema in your example in albums.js you can do this:</p>\n\n<pre><code>var SongSchema = require('mongoose').model('Song').schema\n</code></pre>\n    "},{"t":"How to search nested objects with elastic-search","l":"http://stackoverflow.com/questions/8140651/how-to-search-nested-objects-with-elastic-search","q":"\n\n<p>OK, I've not been able to figure this out thus far. Hoping someone can offer some insight.</p>\n\n<p>Given the documents below, how would I search for all documents with a video that has \"test\" in the video title? I'm using the HTTP API.  <strong>(Basically, how do you search nested objects with elastic search? I know there has to be docs out there, but I haven't really been able to find any.)</strong></p>\n\n<pre><code>[{\n    id:4635,\n    description:\"This is a test description\",\n    author:\"John\",\n    author_id:51421,\n    video: {\n        title:\"This is a test title for a video\",\n        description:\"This is my video description\",\n        url:\"/url_of_video\"\n    }\n},\n{\n    id:4636,\n    description:\"This is a test description 2\",\n    author:\"John\",\n    author_id:51421,\n    video: {\n        title:\"This is an example title for a video\",\n        description:\"This is my video description2\",\n        url:\"/url_of_video2\"\n    }\n},\n{\n    id:4637,\n    description:\"This is a test description3\",\n    author:\"John\",\n    author_id:51421,\n    video: {\n        title:\"This is a test title for a video3\",\n        description:\"This is my video description3\",\n        url:\"/url_of_video3\"\n    }\n}]\n</code></pre>\n    ","a":"\n<p>OK, RTFM! I finally found these pages (should have taken more time with the docs beforehand) and it seems we set the property that holds the video to type:nested, then use nested queries. </p>\n\n<p><a href=\"http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html\" rel=\"nofollow\">http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html</a></p>\n\n<p><a href=\"http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-filter.html\" rel=\"nofollow\">http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-filter.html</a></p>\n\n<p>Hope this helps someone down the road.</p>\n    "},{"t":"Basic NoSQL document design question","l":"http://stackoverflow.com/questions/4798525/basic-nosql-document-design-question","q":"\n\n<p>I am trying to figure out how to best implement this for my system...and get my head out of the RDBMS space for now...</p>\n\n<p>A part of my current DB has three tables: Show, ShowEntry, and Entry. Basically ShowEntry is a many-to-many joining table between Show and Entry. In my RDBMS thinking it's quite logical since any changes to Show details can be done in one place, and the same with Entry.</p>\n\n<p>What's the best way to reflect this in a document-based storage? I'm sure there is no one way of doing this but I can't help but think if document-based storage is appropriate for this case at all.</p>\n\n<p>FYI, I am currently considering implementing RavenDB. While discussions on general NoSQL design will be good a more RavenDB focused one will be fantastic!</p>\n\n<p>Thanks,\nD.</p>\n    ","a":"\n<p>When modelling a many-to-many relationship in a document database, you usually store a collection of foreign keys in just one of the documents. The document you choose largely depends on the direction you intend to traverse the relationship. Traversing it one way is trivial, traversing it the other way requires an index.</p>\n\n<p>Take the shopping basket example. It's more important to know exactly which items are in a particular basket than which baskets contain a particular item. Since we're usually following the relationship in the basket-to-item direction, it makes more sense to store item IDs in a basket than it does to store basket IDs in an item.</p>\n\n<p>You can still traverse the relationship in the opposite direction (e.g. find baskets containing a particular item) by using an index, but the index will be updated in the background so it won't always be 100% accurate. (You can wait for the index to become accurate with <code>WaitForNonStaleResults</code>, but that delay will show in your UI.)</p>\n\n<p>If you require immediate 100% accuracy in both directions, you can store foreign keys in both documents, but your application will have to update two documents whenever a relationship is created or destroyed.</p>\n    "},{"t":"Move MongoDB data from Staging server to Production","l":"http://stackoverflow.com/questions/4679835/move-mongodb-data-from-staging-server-to-production","q":"\n\n<p>I have 500,000 documents inside a collection on a staging server, I need to move these documents to the production server.</p>\n\n<p>What is the best way to move this data, can I let mongodb replicate it from staging to production, do I move the data files or do I do an export and re-import?</p>\n\n<p>Thanks</p>\n    ","a":"\n<p>Take a look at the mongodump and mongorestore tools. If you only want some of the documents in the collection you can use the --query parameter.</p>\n    "},{"t":"NoSQL or Ehcache caching?","l":"http://stackoverflow.com/questions/2862370/nosql-or-ehcache-caching","q":"\n\n<p>I'm building a Route Planner Webapp using Spring/Hibernate/Tomcat and a mysql database, \nI have a database containing read only data, such as Bus Stop Coordinates, Bus times which is never updated. I'm trying to make the app run faster, each time the application is run it will preform approx 1000 reads to the database to calculate a route. </p>\n\n<p>I have setup a Ehcache which greatly improves the read from database times.\nI'm now setting terracotta + Ehcache distributed caching to share the cache with multiple Tomcat JVMs. This seems a bit complicated. I've tried memcached but it was not performing as fast as ehcache.</p>\n\n<p>I'm wondering if a MongoDb or Redis would be better suited. I have no experience with nosql but I would appreciate if anyone has any ideas. What i need is quick access to the read only database.</p>\n    ","a":"\n<p>if you want to try routing, you even might look at Neo4j, see <a href=\"http://blogs.neotechnology.com/peter/2010/04/cool-spatial-algos-with-neo4j-part1-routing-with-a.html\">the blog on using an A* algo for </a></p>\n    "},{"t":"Does MongoDB support floating point types?","l":"http://stackoverflow.com/questions/7682714/does-mongodb-support-floating-point-types","q":"\n\n<p>im migrating a mysql database to mongodb. But i have read in MongoDb data types and then there is no reference to floating point types like, float, double, decimal.\nAnd how i have some fields with decimal types in my sql schema , how can i do or what can i do?\nAny sugestions?</p>\n\n<p>Thanks</p>\n    ","a":"\n<p>MongoDB stores data in a binary format called <a href=\"http://bsonspec.org/#/specification\">BSON</a> which supports these numeric data types:</p>\n\n<ul>\n<li><code>int32</code> - 4 bytes (32-bit signed integer)</li>\n<li><code>int64</code> - 8 bytes (64-bit signed integer)</li>\n<li><code>double</code> - 8 bytes (64-bit IEEE 754 floating point)</li>\n</ul>\n\n<p>There's no exact value fixed-point equivalent to mySQL's <code>decimal</code> type in MongoDB, but you can store 64-bit floating point numbers in Mongo as a <code>double</code>.</p>\n\n<p>It's worth mentioning that the MongoDB shell - being a JavaScript shell - doesn't recognise the distinction between integer and floating-point values, it treats all numbers the same because JavaScript represents all numbers as 64-bit floating point, regardless of their underlying BSON type.</p>\n\n<p>Most <a href=\"http://www.mongodb.org/display/DOCS/Drivers\">MongoDB language drivers</a>, however, make the distinction between integer and floating point types.</p>\n    "},{"t":"RavenDB: Id Generation For Sub-Documents","l":"http://stackoverflow.com/questions/3167002/ravendb-id-generation-for-sub-documents","q":"\n\n<p>I'm trying migrating an existing web application to use RavenDB.</p>\n\n<p>I currently have pages in my web application which allow you to view Categories, SubCategories and Resources based on an id in the querystring.</p>\n\n<p>However I notice that RavenDB generates ids for aggregate roots, but not for child entities.  </p>\n\n<p>I don't think subcategory is an aggregate root (a Category has SubCategories), so am making it a sub-document of my Category document.</p>\n\n<p>Am I wrong to make it a sub-document as I'm accessing it directly by its id passed in on the querystring?  But if not, how should I access individual SubCategories as RavenDB does not seem to generate ids for entities that are not aggregate roots?</p>\n    ","a":"\n<p>There's a <a href=\"http://groups.google.com/group/ravendb/browse_thread/thread/1bc4089443ff35bb\">long but interesting discussion</a> over on the Raven mailing list about this exact situation.</p>\n\n<p>The short answer is that Raven isn't designed to do this, only root entities get an id, everything else is treated as a value type. But you can implement it yourself, see the code sample at the end of the thread for info.</p>\n    "},{"t":"Disadvantages of CouchDB","l":"http://stackoverflow.com/questions/7858699/disadvantages-of-couchdb","q":"\n\n<p>I've very recently fallen in love with <em>CouchDB</em>. I'm pretty excited by its enormous benefits and by its beauty. Now I want to make sure that I haven't missed any show-stopping disadvantages.</p>\n\n<p>What comes to your mind? Attached is a list of points that I have collected. Is there anything to add?</p>\n\n<ul>\n<li>Blog posts from as late as 2010 claim \"not mature enough\" (whatever that's worth).</li>\n<li>Slower than in-memory DBMS.</li>\n<li>In-place updates require server-side logic <a href=\"https://wiki.apache.org/couchdb/Document_Update_Handlers\">(update handlers)</a>.</li>\n<li>Trades disk vs. speed: Databases can become huge compared to other DBMS (compaction functionality exists, though).</li>\n<li>\"Only\" <em>eventual</em> consistency.</li>\n<li>Temporary views on large datasets are <em>very</em> slow.</li>\n<li>Replication of large databases <a href=\"https://issues.apache.org/jira/browse/COUCHDB-690\">may fail</a>.</li>\n<li>Map/reduce paradigm requires rethinking (only for completeness).</li>\n</ul>\n\n<p>The only point that worries <em>me</em> is #3 (in-place updates), because it's quite inconvenient.</p>\n    ","a":"\n<ul>\n<li><strong>The data is in JSON</strong></li>\n</ul>\n\n<p>Which means that documents are quite large (BigData, network bandwidth, speed), and having descriptive key names actually hurts, since they add up to the document size.</p>\n\n<ul>\n<li><p><strong>No built in full text search</strong></p>\n\n<p>Although there are ways: <a href=\"https://github.com/rnewson/couchdb-lucene\">couchdb-lucene</a>, <a href=\"http://www.elasticsearch.org/\">elasticsearch</a></p></li>\n</ul>\n\n<p><em>plus <a href=\"http://wiki.fluidproject.org/display/fluid/Schema-less+Databases\">some more</a>:</em></p>\n\n<ul>\n<li><strong>It doesn't support transactions</strong></li>\n</ul>\n\n<p>It means that enforcing uniqueness of one field across all documents is not safe, for example, enforcing that a username is unique. Another consequence of CouchDB's inability to support the typical notion of a transaction is that things like inc/decrementing a value and saving it back are also dangerous. There aren't many instances that we would want to simply inc/decrement some value where we couldn't just store the individual documents separately and aggregate them with a view.</p>\n\n<ul>\n<li><strong>Relational data</strong> </li>\n</ul>\n\n<p>If the data makes a lot of sense to be in 3rd normal form, and we try to follow that form in CouchDB,  we are going to run into a lot of trouble. A possible way to solve this problem is with view collations, but we might constantly going to be fighting with the system. If  the data can be reformatted to be much more denormalized, then CouchDB will work fine.</p>\n\n<ul>\n<li><strong>Data warehouse</strong></li>\n</ul>\n\n<p>The problem with this is that temporary views in CouchDB on large datasets are really slow. Using CouchDB and permanent views could work quite well. However, in most of cases, a Column-Oriented Database of some sort is a much better tool for the data warehousing job.</p>\n\n<p><strong>But CouchDB Rocks!</strong></p>\n\n<p>But don't let it discorage you: NoSQL DBs that are written in Erlang (CouchDB, Riak) are the best, since Erlang is meant for distributed systems. Have fun with Couch!</p>\n    "},{"t":"NoSQL Injection? (PHP->phpcassa->Cassandra)","l":"http://stackoverflow.com/questions/5998838/nosql-injection-php-phpcassa-cassandra","q":"\n\n<p>Anyone familiar enough with the Cassandra engine (via PHP using phpcassa lib) to know offhand whether there's a corollary to the sql-injection attack vector?  If so, has anyone taken a stab at establishing best practices to thwart them?  If not, would anyone like to ; )</p>\n    ","a":"\n<p>No. The Thrift layer used by phpcassa is an rpc framework, not based on string parsing.</p>\n    "},{"t":"Embeddable document store database [closed]","l":"http://stackoverflow.com/questions/6080343/embeddable-document-store-database","q":"\n\n<p>Is there something akin to SQLite but a document-oriented database as opposed to an RDBMS?</p>\n\n<p><a href=\"http://code.google.com/p/orient/\">Orient</a> seemed to be something similar to what I was looking for, but it is written in Java, and I'm looking for something I can use from C++.</p>\n\n<p>Ideally this would be a serverless system, like SQLite.</p>\n    ","a":"\n<p><a href=\"http://unqlite.org/\">UnQLite</a> </p>\n\n<blockquote>\n  <p>UnQLite is a in-process software library which implements a\n  self-contained, serverless, zero-configuration, transactional NoSQL\n  database engine. UnQLite is a document store database similar to\n  MongoDB, Redis, CouchDB etc. as well a standard Key/Value store\n  similar to BerkeleyDB, LevelDB, etc.</p>\n  \n  <p>UnQLite is 100% hand-coded, written in ANSI C, Thread-safe, Full\n  reentrant, compiles unmodified and should run in most platforms\n  including restricted embedded devices with a C compiler. UnQLite is\n  extensively tested on Windows and UNIX systems especially Linux,\n  FreeBSD, Oracle Solaris and Mac OS X.</p>\n</blockquote>\n    "},{"t":"What is the killer reason for using Mongoose ORM?","l":"http://stackoverflow.com/questions/5747806/what-is-the-killer-reason-for-using-mongoose-orm","q":"\n\n<p>I've been using it with a new project, but it is also my first time using MongoDB. Defining a schema seems unnecessary because I thought the upside of mongo was that it didn't need defined schemes. Can't I just save objects on the fly no matter the schema? Then why would I want to? Also the documentation is lacking, making some things I can easily do in the mongo shell harder then they should be. </p>\n    ","a":"\n<p>The best thing about Mongoose for MongoDB is the fact that you can have built-in automatic validation of the data which you are inserting/updating.  Mongoose also gives you the ability to pre-define events to happen, say, before a document gets saved.  This is very powerful because it consolidates the code you would have to write, and it places that code where it should be next to the document logic and not in the application logic.</p>\n\n<p>Check out <a href=\"http://mongoosejs.com/docs/middleware.html\">middleware</a> and validation for some examples.\nalexyoung/Nodepad on Github has some good examples in the <a href=\"https://github.com/alexyoung/nodepad/blob/master/models.js\">models.js file</a>.</p>\n    "},{"t":"NoSQL database for storing big files? [closed]","l":"http://stackoverflow.com/questions/4262254/nosql-database-for-storing-big-files","q":"\n\n<p>I need to store very large (more than 512Mb) binary files into a NoSQL database. What particular NoSQL database implementation allows that?</p>\n    ","a":"\n<p>No experience, but Mongos GridFS is the only thing I have heard of that is specifically for storing files</p>\n    "},{"t":"Tree structures in a nosql database","l":"http://stackoverflow.com/questions/3281685/tree-structures-in-a-nosql-database","q":"\n\n<p>I'm developing an application for Google App Engine which uses BigTable for its datastore.</p>\n\n<p>It's an application about writing a story collaboratively. It's a very simple hobby project that I'm working on just for fun. It's open source and you can see it here: <a href=\"http://story.multifarce.com/\">http://story.multifarce.com/</a></p>\n\n<p>The idea is that anyone can write a paragraph, which then needs to be validated by two other people. A story can also be branched at any paragraph, so that another version of the story can continue in another direction.</p>\n\n<p>Imagine the following tree structure:</p>\n\n<p><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Binary_tree.svg/220px-Binary_tree.svg.png\" alt=\"\"></p>\n\n<p>Every number would be a paragraph. I want to be able to select all the paragraphs in every unique story line. Basically, those unique story lines are (2, 7, 2); (2, 7, 6, 5); (2, 7, 6, 11) and (2, 5, 9, 4). Ignore that the node \"2\" appears twice, I just took a tree structure diagram from Wikipedia.</p>\n\n<p>I also made a diagram of a proposed solution: <a href=\"https://docs.google.com/drawings/edit?id=1fdUISIjGVBvIKMSCjtE4xFNZxiE08AoqvJSLQbxN6pc&amp;hl=en\">https://docs.google.com/drawings/edit?id=1fdUISIjGVBvIKMSCjtE4xFNZxiE08AoqvJSLQbxN6pc&amp;hl=en</a></p>\n\n<p>How can I set up a structure is performance efficient both for writing, but most importantly for reading?</p>\n    ","a":"\n<p>There are a number of well known ways to represent trees in databases; each of them have their pros and cons. Here are the most common:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Adjacency_list\">Adjacency list</a>, where each node stores the ID of its parent.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Materialized_path\">Materialized path</a>, which is the strategy Keyur describes. This is also the approach used by entity groups (eg, parent entities) in App Engine. It's also more or less what you're describing in your update.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Nested_set_model\">Nested sets</a>, where each node has 'left' and 'right' IDs, such that all child nodes are contained in that range.</li>\n<li>Adjacency lists agumented with a root ID.</li>\n</ul>\n\n<p>Each of these has its own advantages and disadvantages. Adjacency lists are simple, and cheap to update, but require multiple queries to retrieve a subtree (one for each parent node). Augmented adjacency lists make it possible to retrieve an entire tree by storing the ID of the root node in every record.</p>\n\n<p>Materialized paths are easy to implement and cheap to update, and permit querying arbitrary subtrees, but impose increasing overhead for deep trees.</p>\n\n<p>Nested sets are tougher to implement, and require updating, on average, half the nodes each time you make an insertion. They allow you to query arbitrary subtrees, without the increasing key length issue materialized path has.</p>\n\n<p>In your specific case, though, it seems like you don't actually need a tree structure at all: each story, branched off an original though it may be, stands alone. What I would suggest is having a 'Story' model, which contains a list of keys of its paragraphs (Eg, in Python a db.ListProperty(db.Key)). To render a story, you fetch the Story, then do a batch fetch for all the Paragraphs. To branch a story, simply duplicate the story entry - leaving the references to Paragraphs unchanged.</p>\n    "},{"t":"Should I use redis to store large number of binary files in redis?","l":"http://stackoverflow.com/questions/8786395/should-i-use-redis-to-store-large-number-of-binary-files-in-redis","q":"\n\n<p>I need to store huge amount of binary files (10 - 20 TB, each file ranging from 512 kb to 100 MB).</p>\n\n<p>I need to know if redis will be efficient for my system. \nI need following properties in my system:</p>\n\n<ul>\n<li>High Availability </li>\n<li>Failover</li>\n<li>Sharding</li>\n</ul>\n\n<p>I intend to use a cluster of commodity hardware to reduce costing as much as possible. Please suggest pros and cons of building such system using redis. I am also concerned about high ram requirements of Redis.</p>\n\n<p>Also if you require additional information please let me know.</p>\n    ","a":"\n<p>I would not use Redis for such a task. Other products will be a better fit IMO.</p>\n\n<p>Redis is an in-memory data store. If you want to store 10-20 TB of data, you will need 10-20 TB of RAM, which is expensive. Furthermore, the memory allocator is optimized for small objects, not big ones. You would probably have to cut your files in various small pieces,  it would not be really convenient.</p>\n\n<p>Redis does not provide an ad-hoc solution for HA and failover. A master/slave replication is provided (and works quite well), but with no support for the automation of this failover. Clients have to be smart enough to switch to the correct server. Something on server-side (but this is unspecified) has to switch the roles between master and slaves nodes in a reliable way. In other words, Redis only provides a do-it-yourself HA/failover solution.</p>\n\n<p>Sharding has to be implemented on client-side (like with memcached). Some clients have support for it, but not all of them. The fastest client (hiredis) does not. Anyway, things like rebalancing has to be implemented on top of Redis. Redis Cluster which is supposed to support such sharding capabilities is not ready yet.</p>\n\n<p>I would suggest to use some other solutions. MongoDB with <a href=\"http://www.mongodb.org/display/DOCS/GridFS\" rel=\"nofollow\">GridFS</a> can be a possibility. Hadoop with <a href=\"http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\" rel=\"nofollow\">HDFS</a> is another one. If you like cutting edge projects, you may want to give the <a href=\"http://www.ioremap.net/projects/elliptics\" rel=\"nofollow\">Elliptics Network</a> a try.</p>\n    "},{"t":"MongoDB: How to represent a schema diagram in a thesis?","l":"http://stackoverflow.com/questions/11323841/mongodb-how-to-represent-a-schema-diagram-in-a-thesis","q":"\n\n<p>I am currently writing a thesis and need to display the schema of my MongoDB in a diagram. I have found no resources about diagrams for document-based databases. </p>\n\n<p>There are Entity Relationship Diagrams (ERD) for relational databases. What options do I have for MongoDB? I've noticed that a lot of blogs just display the raw JSON as their \"diagram\" but this isn't feasible in my thesis. </p>\n\n<p>Here is a sample of one of my JSON structures: </p>\n\n<pre><code>//MultiChoiceQuestion\n{\n    \"title\": \"How are you?\",\n    \"valid_answers\" : [\n        {\n            \"_id\" : ObjectID(xxxx),\n            \"title\": \"Great\",\n            \"isCorrect\": true,\n        },\n        {\n            \"_id\" : ObjectID(yyyy),\n            \"title\": \"OK\",\n            \"isCorrect\": false,\n        },\n        {\n            \"_id\" : ObjectID(zzzz),\n            \"title\": \"Bad\",\n            \"isCorrect\": false,\n        }\n    ],\n    \"user_responses\" : [\n        {\n            \"user\": ObjectID(aaaa),\n            \"answer\": ObjectID(xxxx)\n        },\n        {\n            \"user\": ObjectID(bbbb),\n            \"answer\": ObjectID(xxxx)\n        },\n        {\n            \"user\": ObjectID(cccc),\n            \"answer\": ObjectID(yyyy)\n        }\n    ]\n}\n\n//User\n{\n    \"_id\": ObjectID(aaaa),\n    \"name\": \"Person A\"\n}\n//User\n{\n    \"_id\": ObjectID(bbbb),\n    \"name\": \"Person B\"\n}\n//User\n{\n    \"_id\": ObjectID(cccc),\n    \"name\": \"Person C\"\n}\n</code></pre>\n\n<p>Could this be a possible diagram: \n<img src=\"http://i.stack.imgur.com/2NC81.png\" alt=\"Possible Diagram?\"></p>\n    ","a":"\n<p>We found class diagrams to actually be one of the best ways to represent a mongo schema design. </p>\n\n<p>It can capture most of the items that a document will have such as arrays, embedded objects and even references. </p>\n\n<p>General guidelines we use to relate onto concepts to uml</p>\n\n<p><strong>Embed</strong> = Composition aggregation</p>\n\n<p><strong>Reference</strong> = Association class</p>\n\n<p>If you're unfamiliar with the uml terminology then this is a decent intro.</p>\n\n<p><a href=\"http://www.ibm.com/developerworks/rational/library/content/RationalEdge/sep04/bell/\">UML intro from IBM site</a></p>\n    "},{"t":"Can redis fully replace mysql?","l":"http://stackoverflow.com/questions/3683436/can-redis-fully-replace-mysql","q":"\n\n<p>Simple question, could I conceivably use redis instead of mysql for all sorts of web applications: social networks, geo-location services etc?</p>\n    ","a":"\n<p>Nothing is impossible in IT. But some things might get extremely complicated.</p>\n\n<p>Using key-value storage for things like full-text search might be extremely painfull.</p>\n\n<p>Also, as far as I see, it lack support for large, clustered databases: so on MySQL you have no problems if you grow over 100s of Gb in Database, and on Redis... Well, it will require more effort :-)</p>\n\n<p>So use it for what it was developed for, storing simple things which just need to be retreived by id.</p>\n    "},{"t":"What to use for session management?","l":"http://stackoverflow.com/questions/8570659/what-to-use-for-session-management","q":"\n\n<p>I'm trying to do some research to find the best option for sessions management in a multi-server environment and was wondering what people have found successful and why.  Pros and cons.</p>\n\n<p>RDBMS - Slower. Better used for other data.</p>\n\n<p>Memcached - You can't take down a memcached server without losing sessions</p>\n\n<p>Redis - Fixes the problem of memcached, but what about ease of scalability? Fault tolerance? </p>\n\n<p>Cassandra - Has good fault tolerance.  Pros and cons?</p>\n\n<p>MongoDB, Others?</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p>Personally, I use Cassandra to persist php session data.  It stores it in a single column on a single row with session_id:{session_data_as_json} and I set the TTL on the column so that it does garbage cleanup automatically.  Works a treat.  </p>\n\n<p>I went with cassandra as it has all other user data already ... For caching, I enabled APC on all front end webservers and haven't had any issues ... </p>\n\n<p>Is this the best approach?  Not sure.  it was fit for purpose for the environment, technologies and business rules I needed to fulfill. ... </p>\n\n<p>Side note, I did start working on a native php -&gt; cassandra session handler:  <a href=\"https://github.com/sdolgy/php-cassandra-sessions\">https://github.com/sdolgy/php-cassandra-sessions</a> -- this shows how the TTL's are set with PHPCassa and Cassandra</p>\n    "},{"t":"CouchDB â€œJoinâ€ two documents","l":"http://stackoverflow.com/questions/6380045/couchdb-join-two-documents","q":"\n\n<p>I have two documents that looks a bit like so:</p>\n\n<pre><code>Doc\n{\n  _id: AAA,\n  creator_id: ...,\n  data: ...\n}\n\nDataKey\n{\n  _id: ...,\n  credits_left: 500,\n  times_used: 0,\n  data_id: AAA\n}\n</code></pre>\n\n<p>What I want to do is create a view which would allow me to pass the DataKey id (key=DataKey _id) and get both the information of the DataKey and the Doc.</p>\n\n<p><strong>My attempt:</strong></p>\n\n<p>I first tried embedding the DataKey inside the Doc and used a map function like so:</p>\n\n<pre><code>function (doc)\n{\n  if (doc.type == \"Doc\")\n  {\n    var ids = [];\n    for (var i in doc.keys)\n      ids.push(doc.keys[i]._id);\n\n    emit(ids, doc);\n  }\n}\n</code></pre>\n\n<p>But i ran into two problems:</p>\n\n<ol>\n<li>There can be multiple DataKey's per\nDoc so using startkey=[idhere...]\nand endkey=[idhere..., {}] didn't\nwork (only worked if the key happend\nto be the first one in the array).</li>\n<li>All the data keys need to be unique, and I would prefer not making a seperate document like {_id = datakey} to reserve the key.</li>\n</ol>\n\n<p>Does anyone have ideas how I can accomplish this? Let me know if anything is unclear.</p>\n\n<p><strong>-----EDIT-----</strong></p>\n\n<p>I forgot to mention that in my application I do not know what the Doc ID is, so I need to be able to search on the DataKey's ID.</p>\n    ","a":"\n<p>I think what you want is</p>\n\n<pre><code>function (doc)\n{\n  if (doc.type == \"Doc\")\n  {\n    emit([doc._id, 0], doc);\n  }\n\n  if(doc.type == \"DataKey\")\n  {\n    emit([doc.data_id, 1], doc);\n  }\n}\n</code></pre>\n\n<p>Now, query the view with <code>key=[\"AAA\"]</code> and you will see a list of all docs. The first one will be the real \"Doc\" document. All the rest will be \"DataKey\" documents which reference the first doc.</p>\n\n<p>This is a common technique, called <a href=\"http://wiki.apache.org/couchdb/View_collation\">CouchDB view collation</a>.</p>\n    "},{"t":"Help me understand mnesia (NoSQL) modeling","l":"http://stackoverflow.com/questions/4113315/help-me-understand-mnesia-nosql-modeling","q":"\n\n<p>In my Quest to understanding Mnesia, I still struggle with thinking in relational terms. So I will put my struggles up here and ask for the best way to solve them. </p>\n\n<p><strong>one-to-many-relations</strong>\nSay I have a bunch of people,</p>\n\n<pre><code>-record(contact, {name, phone}). \n</code></pre>\n\n<p>Now, I know that I can define phone to always be saved as a list, so people can have multiple phone numbers, and I suppose that's the way to do it <strong>(is it? How would I then look this up the other way around, say, finding a name to a number?)</strong>. </p>\n\n<p><strong>many-to-many-relations</strong>\nnow let's suppose I have multiple groups I can put people in. <em>The group names don't have any significance, they are just names; the concept is \"unix system groups\" or \"labels\"</em>.  Naively, I would model this membership as a proplist, like </p>\n\n<pre><code>{groups [{friends, bool()}, {family, bool()}, {work, bool()}]} %% and so on...\n</code></pre>\n\n<p>as a field within the \"contact\" record from above, for example. What is the best way to model this within mnesia if I want to be able to lookup all members of a group based on group name quickly, and also want to be able to lookup all group an individual is registered in? I also could just model this as a list containing just the group identifiers, of course. For use with mnesia, <strong>what is the best way to model this?</strong></p>\n\n<p>I apologize if this question is dumb. There's plenty of documentation on mnesia, but it's lacking (IMO) some good examples for the overall use. </p>\n    ","a":"\n<p>For the first example, consider this record:</p>\n\n<pre><code>-record(contact, {name, [phonenumber, phonenumber, ...]}).\n</code></pre>\n\n<p><code>contact</code> is a record with two fields, <code>name</code> and <code>phone</code> where phone is a list of phone numbers. As user425720 said it could make sense to store these as something else than strings, if you have extreme requirements for small storage footprint, for example.</p>\n\n<p>Now here comes the part that is hard to \"get\" with key-value stores: you need to also store the inverse relationship. In other words, you need something similar to the following:</p>\n\n<pre><code>-record(phone, {phonenumber, contactname}).\n</code></pre>\n\n<p>If you have a layer in your application to abstract away database handling, you could make it always add/change the phone records when adding/changing a contact.</p>\n\n<p>--</p>\n\n<p>For the second example, consider these two records:</p>\n\n<pre><code>-record(contact, {uuid, name, [group_id, group_id]}).\n-record(group, {uuid, name, [contact_id, contact_id]}).\n</code></pre>\n\n<p>The easiest way is to just store ids pointing to the related records. As Mnesia has no concept of referential integrity, this can become out of sync if you for example delete a group without removing that group from all users.</p>\n\n<p>If you need to store the type of group on the contact record, you could use the following:</p>\n\n<pre><code>-record(contact, {name, [{family, [group_id, group_id]}, {work, [..]}]}).\n</code></pre>\n\n<p>--</p>\n\n<p>Your second problem could also be solved by using a intermediate record, which you can think of as \"membership\".</p>\n\n<pre><code>-record(contact, {uuid, name, ...}).\n-record(group, {uuid, name, ...}).\n-record(membership, {contact_uuid, group_uuid}). # must use 'bag' table type\n</code></pre>\n\n<p>There can be any number of \"membership\" records. There will be one record for every users group.</p>\n    "},{"t":"How to implement Object Databases in Asp.net MVC","l":"http://stackoverflow.com/questions/2452169/how-to-implement-object-databases-in-asp-net-mvc","q":"\n\n<p>I started my project in Asp.net MVC(c#) &amp; SQL Server 2005.I want to implement Object Databases in my project.\nWhile searched in google i found \"<a href=\"http://www.mongodb.org/display/DOCS/Home\" rel=\"nofollow\">MongoDb</a>\" &amp; <a href=\"http://www.db4o.com/about/productinformation/\" rel=\"nofollow\">db4o</a></p>\n\n<p>I didn't have enough knowledge in Object Databases &amp; which one best suited for SQL Server 2005.</p>\n\n<p>Please suggest a good example/reference regarding Object Databases implementation in Asp.net MVC application</p>\n    ","a":"\n<p>For a good introduction to MongoDB with C#, you might look at this series:</p>\n\n<ul>\n<li><a href=\"http://mookid.dk/oncode/archives/1057\" rel=\"nofollow\">http://mookid.dk/oncode/archives/1057</a></li>\n<li><a href=\"http://mookid.dk/oncode/archives/1107\" rel=\"nofollow\">http://mookid.dk/oncode/archives/1107</a></li>\n<li><a href=\"http://mookid.dk/oncode/archives/1145\" rel=\"nofollow\">http://mookid.dk/oncode/archives/1145</a></li>\n<li><a href=\"http://mookid.dk/oncode/archives/1165\" rel=\"nofollow\">http://mookid.dk/oncode/archives/1165</a></li>\n</ul>\n\n<p>As for using it from ASP.net MVC, I don't know of any reference-implementation yet.</p>\n    "},{"t":"How would a â€œNOSQLâ€ database be designed for consumer apps (e.g. social bookmarking) [closed]","l":"http://stackoverflow.com/questions/1905688/how-would-a-nosql-database-be-designed-for-consumer-apps-e-g-social-bookmark","q":"\n\n<p>I've been reading up on a lot of posts about non-relational databases, the whole NOSQL movement, and there's a lot of fresh new activity around it. It seems like a very interesting approach to building highly scalable web applications but unfortunately (but also a good thing at this nascent stage) there isn't quite a clear leader/standard at the moment.</p>\n\n<p>My background is in the LAMP stack (with MySQL as the dB) and I wanted to understand what differences and limitations there would be - but with using the example of real life web applications out there. There's a ton of good articles about the theory of pros/cons of RDBMS vs non-RDBMS, but I haven't found anything that walks through an example of how an existing web app (for example social bookmarking) would be built different to take advantage of the new dB structure and what features one would have to leave out if any - where the NOSQL dB could be key-value or document-centric or graphs.</p>\n\n<p><strong>Would anyone be willing to take a stab at a high-level comparison of a NOSQL vs RDBMS model/architecture of a real-world web app (such as social bookmarking or any other example that might explain the concepts well)?</strong></p>\n\n<p><em>For reference to others, here are a few of the articles I have come across:</em></p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/1189911/non-relational-database-design\">http://stackoverflow.com/questions/1189911/non-relational-database-design</a></li>\n<li><a href=\"http://nosql-databases.org/\">http://nosql-databases.org/</a></li>\n<li><a href=\"http://linux-mag.com/cache/7579/1.html\">http://linux-mag.com/cache/7579/1.html</a></li>\n<li><a href=\"http://blog.boxedice.com/2009/07/25/choosing-a-non-relational-database-why-we-migrated-from-mysql-to-mongodb/\">http://blog.boxedice.com/2009/07/25/choosing-a-non-relational-database-why-we-migrated-from-mysql-to-mongodb/</a></li>\n<li><a href=\"http://bret.appspot.com/entry/how-friendfeed-uses-mysql\">http://bret.appspot.com/entry/how-friendfeed-uses-mysql</a></li>\n<li><a href=\"http://metabrew.com/article/anti-rdbms-a-list-of-distributed-key-value-stores/\">http://metabrew.com/article/anti-rdbms-a-list-of-distributed-key-value-stores/</a></li>\n<li><a href=\"http://rackspacecloud.com/blog/2009/11/09/nosql-ecosystem/\">http://rackspacecloud.com/blog/2009/11/09/nosql-ecosystem/</a></li>\n<li><a href=\"http://horicky.blogspot.com/2009/11/nosql-patterns.html\">http://horicky.blogspot.com/2009/11/nosql-patterns.html</a></li>\n</ul>\n    ","a":"\n<p>The main reason is scale (Facebook, MySpace)</p>\n\n<p>Check out these articles:</p>\n\n<p><a href=\"http://highscalability.com/product-facebooks-cassandra-massive-distributed-store\" rel=\"nofollow\">Facebook's Cassandra - A Massive Distributed Store</a> </p>\n\n<p><a href=\"http://qizmt.myspace.com/\" rel=\"nofollow\">MySpace Qizmt - MySpace's Mapreduce Framework</a></p>\n\n<p>As you are already using Lucene, this may be of interest:</p>\n\n<p><a href=\"http://katta.sourceforge.net/\" rel=\"nofollow\">Katta - Lucene in the cloud</a></p>\n    "},{"t":"Geo program design suggestions using redis","l":"http://stackoverflow.com/questions/29979085/geo-program-design-suggestions-using-redis","q":"\n\n<p>I am in the process of learning redis and am building a geo program for learning purposes. I would like to only use redis to store the data and am trying to avoid any relational databases. My question is how to best design the database for the program. This is what the how the program goes:</p>\n\n<p>1) I will create millions of random robots around the world which wander so they can have different geo coordinates (some robots can be in the exact same space).</p>\n\n<p>2) Each robot will randomly send a post to the server (every few hours possibly on average) which will contain:\n    a) the location of where the robot sent this data from (in either coordinates or geohash depending on the best implementation idea)\n    b) some small text</p>\n\n<p>3) I will have a map with all the robots and would like to be able to click on a robot and get this information:\n    a) all the posts which were posted nearby the robot I just clicked</p>\n\n<p>4) Due to the fact I will be hosting this on AWS I will need to delete the posts every couple of hours to keep the memory usage low so some type of expiration is mandatory. </p>\n\n<p>My main concern is performance and I am interested in how to design the redis database.</p>\n\n<p>In a single day (I will work out the math for random posts to do this) about ~500,000,000 posts will be generated.</p>\n\n<p>My Incomplete ideas so far:</p>\n\n<p><strong>Idea 1</strong></p>\n\n<p>1) a post would be stored as such:</p>\n\n<pre><code>`HSET [Geohash of location] [timestamp] [small text] (&lt;-- the value will be used in a later feature to increment the number of manual modification I make to a post).\n</code></pre>\n\n<p>2) I then would be able to get all the posts near a robot by sending the geohash location he is in. The downfall here is I would also need to include his 8 geohash neighbors which would require 8 more queries. Which is why I am also looking into concept of spatial proximity for this feature. </p>\n\n<pre><code>HGETALL [GeoHash Location of robot] \n</code></pre>\n\n<p>This would then return the field ([timestamp]) and value (\"0\");</p>\n\n<p>3) Expiration of old posts. Since I can't use the EXPIRE command to delete fields from a hashset, I would then need to scan through all the hashset fields periodicly and find old timestamps and remove them. Since Redis only allows pattern searching this could will be difficult when all the timestamps are different.</p>\n\n<p><strong>Idea 2:</strong></p>\n\n<p>Use Redis-geo (<a href=\"https://matt.sh/redis-geo\">https://matt.sh/redis-geo</a>).</p>\n\n<p>1) To store the posts I would run:</p>\n\n<pre><code>geoadd globalSet [posts_long] [posts_lat] \"small text\";\n</code></pre>\n\n<p>2) To get all the post information for a robot nearby:</p>\n\n<pre><code>georadius globalSet [robots_long] [robots_lat] [X] km\n</code></pre>\n\n<p>This would return all posts near the robot within X kms.</p>\n\n<p>3) Then I am now stuck how to remove old posts</p>\n    ","a":"\n<p>Let me give you an idea base on how i understood your problem:</p>\n\n<p>Instead of storing values in hash, simply store everything in redis.\nConstruct the key as GeoLocation:[Geohash location of robot]:1[indicating the number of post, this will keep on incrementing whenever a new request comes]:timestamp and value will be the timestamp.\nSimilarly for small text  GeoLocation:[Geohash location of robot]:1[indicating the number of post]:smallText. \nUse set expire to set the values and set the expire time as your wish.</p>\n\n<p>Ex: setex GeoLocation:12.31939:1:timestamp 1432423232 (timestamp) 14400 (4 hrs)\n    setex GeoLocation:12.31939:1:smalltext ronaldo 14400</p>\n\n<p>Thus you will get any number of posts from all robots with a distinct key to access and setting expire has also become easy. </p>\n\n<p>Now to get all the info posted by a particular robot, use keys GeoLocation:(location of particular robot):* and get values of each.</p>\n\n<p>In this way you don't need to scan through all the keys in redis. You will get the info relatively quicker and keys are expired by it's own.</p>\n    "},{"t":"HBase Error - assignment of -ROOT- failure","l":"http://stackoverflow.com/questions/6007725/hbase-error-assignment-of-root-failure","q":"\n\n<p>I've just installed hadoop and hbase from cloudera (3) but when I try to go to <a href=\"http://localhost:60010\">http://localhost:60010</a> it just sits there continually loading.</p>\n\n<p>I can get to the regionserver fine - <a href=\"http://localhost:60030\">http://localhost:60030</a>... Looking at the master hbase server logs I can see the following. </p>\n\n<p>Looks like a problem with the root region. </p>\n\n<p>All of this is installed on a ext4 1TB partition running Ubuntu (Natty) 11. No cluster/other boxes).</p>\n\n<p>Any help would be great!</p>\n\n<p>Cheers!</p>\n\n<p>11/05/15 19:58:27 WARN master.AssignmentManager: Failed assignment of -ROOT-,,0.70236052 to serverName=localhost,60020,1305452402149, load=(requests=0, regions=0, usedHeap=24, maxHeap=995), trying to assign elsewhere instead; retry=0\norg.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /127.0.0.1:60020 after attempts=1\n    at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:355)\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:957)\n    at org.apache.hadoop.hbase.master.ServerManager.getServerConnection(ServerManager.java:606)\n    at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:541)\n    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:901)\n    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:730)\n    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:710)\n    at org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor.chore(AssignmentManager.java:1605)\n    at org.apache.hadoop.hbase.Chore.run(Chore.java:66)\nCaused by: java.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)\n    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)\n    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:328)\n    at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:883)\n    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)\n    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)\n    at $Proxy6.getProtocolVersion(Unknown Source)\n    at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)\n    at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)\n    at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)\n    at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)\n    ... 8 more\n11/05/15 19:58:27 WARN master.AssignmentManager: Unable to find a viable location to assign region -ROOT-,,0.70236052</p>\n    ","a":"\n<p>Fixed this issue for anyone else who finds this. Was a problem with the host file (/etc/hosts). Need to remove entries relating to 127.0.1.1 COMPNAME - just put a hash (#) in front of this line and then restart all hadoop and hbase services. </p>\n\n<p>More on the solution here: <a href=\"http://blog.nemccarthy.me/?p=110\">http://blog.nemccarthy.me/?p=110</a></p>\n    "},{"t":"Querying with Redis?","l":"http://stackoverflow.com/questions/5386607/querying-with-redis","q":"\n\n<p>I've been learning Node.js so I decided to make a simple ad network, but I can't seem to decide on a database to use. I've been messing around with Redis but I can't seem to find a way to query the database by specific criteria, instead I can only get the value of a key or a list or set inside a key.</p>\n\n<p>Am I missing something, or should I be using a more robust database like MongoDB?</p>\n    ","a":"\n<p>I would recommend to read <a href=\"http://redis.io/topics/data-types-intro\"><strong>this</strong></a> tutorial about Redis in order to understand its concepts and data types. I also had problems to understand why there is no querying support similar to other (no) SQL databases until I read few articles and try to test and compare Redis with other solutions. Maybe it isn't the right database for your use case, although it is very fast and supports advanced data structures, but lacks querying which is crucial for you. If you are looking for a database which allows you to query your data then you should try <a href=\"http://www.mongodb.org/\">mongodb</a> or maybe <a href=\"http://wiki.basho.com/\">riak</a>.</p>\n    "},{"t":"Any Validity to the NoSQL movement?","l":"http://stackoverflow.com/questions/4069892/any-validity-to-the-nosql-movement","q":"\n\n<p>First of all im relatively new to the Database world, Im graduating with my B.S. in Comp Science this semester and Database Technologies have really caught my eye so ive been studying alot of T-SQL because I want to in the end get a SQL Development job (MS SQL server seemed like the best choice right now because it's on the rise)</p>\n\n<p>ANYWAYS, i've heard alot of hoopla about this NOsql movement of Non-relational database management systems. Trying to keep this question and non-subjective as possible i mainly want to know the advantages/disadvantages of NRDBMS's (like Nosql) and if there is really a future in them. Perhaps as a side question, is it a bad time to be studying SQL in general (specifically the normal RDBMS's we are so used to). I forsee people sticking with this for a long time, but then again.....I dont know. I'd hate to see my interest suddenly be taking a dive in the market.</p>\n    ","a":"\n<p>There is definitely validity to the NoSQL movement, but I wouldn't worry about your SQL skills going to waste.  NoSQL storage architectures were born out of the need for highly available and scalable data stores that went beyond what a typical relational database could provide.  This comes at a cost though, and typically that cost is guaranteed consistency.  This isn't always a large concern.  In the case of something like Facebook doesn't have complete consistency for a period of time for things like your pictures, status updates, etc.  As long as they get consistent at some point, it's okay. On the other end, take your bank account.  That type of data store needs to provide the strong ACID characteristics that a relational database provides.</p>\n\n<p>NoSQL isn't something that I see taking over the world, it's an alternative to the common approach of RDBMS's and as with everything else it has it's strengths and weaknesses.    </p>\n\n<p>Here is an excellent <a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxwcmFjdGljYWxjbG91ZGNvbXB1dGluZ3xneDo2NDc2ODVjY2ExY2Y1Zjcz&amp;pli=1\">article</a> on the subject written about NetFlix.</p>\n    "},{"t":"Finding all records containing a given subfield in mongodb","l":"http://stackoverflow.com/questions/6767949/finding-all-records-containing-a-given-subfield-in-mongodb","q":"\n\n<p>In mongodb, i can find all those records in a collection in database db that contain a particular field using the following query </p>\n\n<pre><code>var doc = db.collection_name.find({field_name:{$exists:true}})\n</code></pre>\n\n<p>Now consider the following document:</p>\n\n<pre><code>{\n  \"somefield\":\"someval\",\n  \"metadata\": {\"id\":\"someval\",\n               \"client_url\":\"http://www.something.com\"\n\n              }\n}\n</code></pre>\n\n<p>What would be the query for getting all records having the id field in metadata ?</p>\n\n<p>Please Help.\nThank You</p>\n    ","a":"\n<p>You can use dot notation to reference sub-document fields</p>\n\n<pre><code>var doc = db.collection_name.find({\"metadata.id\":{$exists:true}})\n</code></pre>\n    "},{"t":"Mongo db select where in array of _id?","l":"http://stackoverflow.com/questions/7713363/mongo-db-select-where-in-array-of-id","q":"\n\n<p>is possible in mongo db to select collection's documents like in SQL :</p>\n\n<pre><code>SELECT * FROM collection WHERE _id IN (1,2,3,4);\n</code></pre>\n\n<p>or if i have a <code>_id array</code> i must select one by one and then recompose the <code>array/object</code> of results?</p>\n    ","a":"\n<p>Easy :)</p>\n\n<pre><code>db.collection.find( { _id : { $in : [1,2,3,4] } } );\n</code></pre>\n\n<p>taken from: <a href=\"http://www.mongodb.org/display/DOCS/Advanced+Queries#AdvancedQueries-%24in\">http://www.mongodb.org/display/DOCS/Advanced+Queries#AdvancedQueries-%24in</a></p>\n    "},{"t":"Store enum as integer in RavenDB","l":"http://stackoverflow.com/questions/6776615/store-enum-as-integer-in-ravendb","q":"\n\n<p>I would like to store Enums as integer-values inside a RavenDB-document instead of there full-name. Doing so, I would like to ensure, that changing the name of an enum-value, does not break persistence.</p>\n\n<p>With FluentNHibernate, I can create a custom convention, but I didn't find anything matching with RavenDB.</p>\n    ","a":"\n<p>You can now just do:</p>\n\n<pre><code>store.Conventions.SaveEnumsAsIntegers = true;\n</code></pre>\n    "},{"t":"Why use SQL and NoSQL data in a single web application [closed]","l":"http://stackoverflow.com/questions/9411190/why-use-sql-and-nosql-data-in-a-single-web-application","q":"\n\n<blockquote>\n  <p>To all downvoters: This question has been heavily edited so it became much more specific.</p>\n</blockquote>\n\n<ol>\n<li><p>What are the <strong>pros</strong> and <strong>cons</strong> of doing this?</p></li>\n<li><p>How does one decide what goes where?</p></li>\n<li><p>What about joining data between the two? If I had Users in SQL and related data in NoSQL document database...</p></li>\n<li><p>How about file storage?</p>\n\n<p>If you'd have a web application where users can upload files (of whatever type, but mostly between few kB to few MB in size) what strategy would you take in this regard? <strong>SQL Server Filestream</strong> seems a fine solution. What about NoSQL alternative? Is it even worth the hassle to store files in NoSQL or should they be rather saved as written?</p></li>\n<li><p>What if I used an ACID document database like RavenDB?</p></li>\n</ol>\n    ","a":"\n<p>I guess it is undisputed, that RDBMS and NoSQL datastores have different strengths and weaknesses. </p>\n\n<p>Starting from this, the main reason (and by definition the biggest <strong>pro</strong>) to combine both in a single app is, that it needs both set of strengths. </p>\n\n<p>Let's beef this up with a (non fictious) example: Say you want to create an app for an ad and affiliate marketing network, that tracks billions of clicks (else you don't get payed), but you need calculation-intensive single-page reports for your customers.</p>\n\n<ul>\n<li>Tracking the <strong>clicks</strong> in a conventional RDBMS will quickly drain your budget, as RDBMS do not scale well by cost. So going NoSQL seems a good idea here - if you lose a click or two because <code>BASE &lt; ACID</code>, no harm done (And the E in BASE might mean, you get that click next month)</li>\n<li>Creating the reports is the textbook example of what a NoSQL store is good at.</li>\n<li>Tracking your <strong>customers</strong> in a NoSQL datastore seems quite ... intensive, so using a RDBMS here seems quite natural. </li>\n<li>For tracking your <strong>payment data</strong> you want some ACID guarantees, again you chose the RDBMS</li>\n</ul>\n\n<p>So, after all, you need both (or a really, really big RDBMS). Needing it is the biggest possible  <strong>pro</strong>.</p>\n\n<p>The <strong>cons</strong> concentrate on implementation:</p>\n\n<ul>\n<li>You need both skillsets, both sets of libraries, everything twice</li>\n<li>Joining RDBMS data with NoSQL data is only possible via IDs in a very limited way</li>\n<li>You will waste storage - this is unavoidable, as you will need to store a lot of cross-infrastructure keys</li>\n</ul>\n\n<p><strong>Edit</strong></p>\n\n<p>After you edited your question, here are my thoughts:</p>\n\n<ul>\n<li>What are the pros and cons of doing this: See above</li>\n<li>How does one decide what goes where: I can just repeat the strengths and weaknesses argument: Data, that is tabular by nature, doesn't need ACID, etc. goes to NoSQL, the opposite goes to the RDBMS. There is a large grey zone, for which there is no universal answer: These cases have to be decided after looking into what the app and its data really do and need.</li>\n<li>What about joining data between the two? If I had Users in SQL and\nrelated data in NoSQL document database: See above - this might be the biggest <strong>con</strong>, but in reallity it turns out to be quite manageable. In any bigger RDBMS app you will have tables that are never joined, so they might just as well be in different datastores. Be prepared to run one RDBMS query and one NoSQL query in many (most) places, where a single <code>JOIN</code> would suffice in the classic model.</li>\n<li>How about file storage. That's an easy one - If the DB can't understand the file's <strong>content</strong>, it is useless to store the file in the DB. Use a file system to store your files. If scaling becomes a problem (many files), scale the same way you scale the NoSQL store: Use sharding to distribute them over servers, best let a cluster file system do this for you.</li>\n</ul>\n    "},{"t":"NoSql/Raven DB implementation best practices","l":"http://stackoverflow.com/questions/4219525/nosql-raven-db-implementation-best-practices","q":"\n\n<p>I'm investigating a new project which will be a social networking style site. I'm reading up on RavenDb and I like the look of a lot of its features. I've not read up on nosql all that much but I'm wondering if there's a niche it fits best with and old school sql is still the best choice for other stuff.</p>\n\n<p>I'm thinking that the permissions plug in would be ideal for a social net style site - but will it really perform in an environment where the database will be getting hammered - or is it optimised for a more reporting style system where it's possible to keep throwing new data structures at the database  and report on those structures. </p>\n\n<p>I'm eager to use the right tool for the job - I'll be using MVC3, Windsor + either Nhibernate+Sql server or RavenDb.</p>\n\n<p>Should I stick with the old school sql or go with the new kid on the block: ravendb?</p>\n    ","a":"\n<p>This question can get very close to being subjective (even though it's really not), you're talking about NoSQL as if it is just one thing, and that is not the case.</p>\n\n<p>You have </p>\n\n<ul>\n<li>graph databases (Neo4j etc), </li>\n<li>map/reduce style document databases (Couch,Raven), </li>\n<li>document databases which attempt to feel like ordinary databases (Mongo), </li>\n<li>Key/value stores (Cassandra etc)</li>\n<li>moar goes here.</li>\n</ul>\n\n<p>Each of them attempts to solve a different problem via different means, and whether you'd use one of them over a traditional relational store is</p>\n\n<ul>\n<li>A matter of suitability</li>\n<li>A matter of personal preference</li>\n</ul>\n\n<p>At the end of the day, for the <strong>primary</strong> data-storage for a single system, a document database or relational store is probably what you want, although for different parts of your system you may well end up utilising a graph database (For calculating neighbours etc), or a key/value store (like Facebook does/did for inbox messages).</p>\n\n<p>The main benefit of choosing a document store as your primary store over that of a relational one, is that you haven't got to worry about trying to map your objects into a collection of tables, and there is less configuration overhead involved in doing so.</p>\n\n<p>The other downside/upside would be that you have to learn something new and make mistakes along the way.</p>\n\n<p>So my answer if I am going to be direct?</p>\n\n<ul>\n<li>RavenDB would be suitable</li>\n<li>SQL would be suitable</li>\n</ul>\n\n<p>Which do you prefer to use? These days I'd probably just go for Raven, knowing I can dump data into a relational store for reporting purposes and probably do likewise for other parts of my system, and getting free-text search and fastish-writes/fast-reads without going through the effort of defining separate read/write stores is an overall win.</p>\n\n<p>But that's me, and I am biased.</p>\n    "},{"t":"Neo4j and big log files","l":"http://stackoverflow.com/questions/14696819/neo4j-and-big-log-files","q":"\n\n<p>I try to use n4j in my app, but I have problem with big log files. Are they necessary or is there some way to reduce the number and size of them?</p>\n\n<p>At the moment I see files like:</p>\n\n<blockquote>\n  <p>nioneo_logical.log.v0</p>\n  \n  <p>nioneo_logical.log.v1</p>\n  \n  <p>nioneo_logical.log.v2</p>\n</blockquote>\n\n<p>etc\nand they are ~26MB each (over 50% of neo4j folder).</p>\n    ","a":"\n<p>These files are created whenever the logical logs are rotated.</p>\n\n<p>You can configure rules for them in the server properties file.</p>\n\n<p>See details here: <a href=\"http://docs.neo4j.org/chunked/stable/configuration-logical-logs.html\">http://docs.neo4j.org/chunked/stable/configuration-logical-logs.html</a></p>\n\n<p>You can safely remove them (but only the *.v*) if your database is shutdown and in a clean state. Don't remove them while the db is running because they could be needed in case of recovery on a crash.</p>\n    "},{"t":"mongoDB vs mySQL â€” why one is better than another in some aspects [closed]","l":"http://stackoverflow.com/questions/17017010/mongodb-vs-mysql-why-one-is-better-than-another-in-some-aspects","q":"\n\n<p>I am really new to database and am interested in some high level basic knowledge. I have read <a href=\"http://stackoverflow.com/questions/1476295/when-to-use-mongodb-or-other-document-oriented-database-systems\">this wonderful SO post</a>. I under one is better than another in some cases, but not sure why. </p>\n\n<ol>\n<li><p>Why is MySQL faster than MongoDB at join operations? </p></li>\n<li><p>Why does MongoDB scale better in distributed system? </p></li>\n<li><p>Why is MongoDB faster if I am \"just selecting a bunch of tables and putting all the objects together, AKA what most people do in a web app\" ? </p></li>\n</ol>\n\n<p>Thanks a lot!</p>\n    ","a":"\n<p>This question lacks any real research, I mean you say you read that question but either that question has some real problems with the source of its information or...well; anyway:</p>\n\n<blockquote>\n  <p>Why is MySQL faster than MongoDB at join operations?</p>\n</blockquote>\n\n<p>Because it doesn't have any? MongoDB HAS NO SERVER SIDE JOINS. I am sorry to put that in capitals but I say it soooooo often, I just feel like placing it as the defacto answer for most questions.</p>\n\n<p>Any joins you do are client side. This means they will actually be slower than MySQL, or other SQL techs. The important idea behind doing joins client side is that doing them server-side becomes very hard to scale in huge distributed environments, if not impossible. That is why many big SQL users actually attempt to prevent huge joins and are effectively trying to do in SQL what MongoDB does.</p>\n\n<p>The case for this is scenario dependant of course.</p>\n\n<blockquote>\n  <p>Why does MongoDB scale better in distributed system? </p>\n</blockquote>\n\n<p><a href=\"http://docs.mongodb.org/manual/replication/\">http://docs.mongodb.org/manual/replication/</a> is very important here and so too is <a href=\"http://docs.mongodb.org/manual/core/sharded-clusters/\">http://docs.mongodb.org/manual/core/sharded-clusters/</a> and I would recommend reading both carefully and how they scale to data partitions and what not.</p>\n\n<blockquote>\n  <p>Why is MongoDB faster if I am \"just selecting a bunch of tables and putting all the objects together, AKA what most people do in a web app\" ? </p>\n</blockquote>\n\n<p>Dunno what you mean by that.</p>\n\n<p>I realise this isn't much of an answer but your question is one of those defacto questions and so I answered with a defacto answer.</p>\n\n<p>Since you are new to databases in general I would personally recommend you go use one...</p>\n    "},{"t":"Modeling friend of friend relationships in MongoDB","l":"http://stackoverflow.com/questions/7395304/modeling-friend-of-friend-relationships-in-mongodb","q":"\n\n<p>We need to be able to quickly perform queries across the set of a user's friends and friends of friends. This would be relatively straightforward in a relational database, but I'm somewhat stuck on the best way to accomplish it in MongoDB. We store the user IDs of a user's friends in an array in the user document, so the obvious solution is to do this:</p>\n\n<ul>\n<li>Pull all friend user IDs from user doc</li>\n<li>Pull all friend arrays from user docs of those friends (using an $in query across all friend IDs), combine application-side into one set, then combine that with first-level friend user IDs to get set of all friends and friends of friends</li>\n<li>Use that set to perform the final query (using $in) across all friends and friends of friends</li>\n</ul>\n\n<p>While straightforward, this seems like a huge amount of back and forth, as compared to what we could do with a join in a relational database. Is there a more efficient way to do this in MongoDB, or is this a problem best suited for a RDBMS?</p>\n    ","a":"\n<p>I asked Eliot Horowitz this very same question recently at MongoDB SV conference. He said the way he would structure it is to store each users friends as embedded documents within each user. For example, the structure might look like this:</p>\n\n<pre><code>{\n  _id : ObjectId(\"4e77bb3b8a3e000000004f7a\"),\n  username : \"alex\",\n  friends : [\"283956723823626626aa\", \"226567377578888888as\", \"8738783888aas88a8a88\" ]\n}\n</code></pre>\n\n<p>then you can have an index on user.friends</p>\n\n<p><a href=\"http://www.mongodb.org/display/DOCS/Indexes#Indexes-IndexingArrayElements\">http://www.mongodb.org/display/DOCS/Indexes#Indexes-IndexingArrayElements</a></p>\n\n<p>\"When a document's stored value for a index key field is an array, MongoDB indexes each element of the array. See the Multikeys page for more information.\"</p>\n\n<p>so to find all of \"alex\"'s friends I can just do:</p>\n\n<p>db.user.find( { 'friends' : '4e77bb3b8a3e000000004f7a'});</p>\n    "},{"t":"Is using a load balancer with ElasticSearch unnecessary?","l":"http://stackoverflow.com/questions/24751025/is-using-a-load-balancer-with-elasticsearch-unnecessary","q":"\n\n<p>I have a cluster of 3 ElasticSearch nodes running on AWS EC2.  These nodes are setup using OpsWorks/Chef.  My intent is to design this cluster to be very resilient and elastic (nodes can come in and out when needed).</p>\n\n<p>From everything I've read about ElasticSearch, it seems like no one recommends putting a load balancer in front of the cluster; instead, it seems like the recommendation is to do one of two things:</p>\n\n<ol>\n<li><p>Point your client at the URL/IP of one node, let ES do the load balancing for you and hope that node never goes down.</p></li>\n<li><p>Hard-code the URLs/IPs of ALL your nodes into your client app and have the app handle the failover logic.</p></li>\n</ol>\n\n<p>My background is mostly in web farms where it's just common sense to create a huge pool of autonomous web servers, throw an ELB in front of them and let the load balancer decide what nodes are alive or dead.  Why does ES not seem to support this same architecture?</p>\n    ","a":"\n<p>You don't need a load balancer â€” ES is already providing that functionality. You'd just another component, which could misbehave and which would add an unnecessary network hop.</p>\n\n<p>ES will shard your data (by default into 5 shards), which it will try to evenly distribute among your instances. In your case 2 instances should have 2 shards and 1 just one, but you might want to change the shards to 6 for an equal distribution.</p>\n\n<p>By default replication is set to <code>\"number_of_replicas\":1</code>, so one replica of each shard. Assuming you are using 6 shards, it could look something like this (R is a replicated shard):</p>\n\n<ul>\n<li>node0: 1, 4, R3, R6</li>\n<li>node1: 2, 6, R1, R5</li>\n<li>node2: 3, 5, R2, R4</li>\n</ul>\n\n<p>Assuming node1 dies, the cluster would change to the following setup:</p>\n\n<ul>\n<li>node0: 1, 4, 6, R3 + new replicas R5, R2</li>\n<li>node2: 3, 5, 2, R4 + new replicas R1, R6</li>\n</ul>\n\n<p>Depending on your connection setting, you can either connect to one instance (transport client) or you could join the cluster (node client). With the node client you'll avoid double hops, since you'll always connect to the correct shard / index. With the transport client, your requests will be routed to the correct instance.</p>\n\n<p>So there's nothing to load balance for yourself, you'd just add overhead. The auto-clustering is probably ES's greatest strength.</p>\n    "},{"t":"Why many refer to Cassandra as a Column oriented database?","l":"http://stackoverflow.com/questions/13010225/why-many-refer-to-cassandra-as-a-column-oriented-database","q":"\n\n<p>Reading several papers and documents on internet, I found many contradictory informations about the Cassandra data model. There are many which identify it as a column oriented database, other as a row-oriented and then who define it as a hybrid way of both.</p>\n\n<p>According to what I know about how Cassandra stores file, it uses the *-Index.db  file to access at the right position of the *-Data.db file where it is stored the bloom filter, column index and then the columns of the required row.</p>\n\n<p>In my opinion, this is strictly row-oriented. Is there something I'm missing?</p>\n    ","a":"\n<p>Yes, the \"column-oriented\" terminology is a bit confusing.</p>\n\n<p>The model in Cassandra is that rows contain columns. To access the smallest unit of data (a column) you have to specify first the row name (key), then the column name.</p>\n\n<p>So in a columnfamily called <code>Fruit</code> you could have a structure like the following example (with 2 rows), where the fruit types are the row keys, and the columns each have a name and value.</p>\n\n<pre><code>apple -&gt; colour  weight  price variety\n         \"red\"   100     40    \"Cox\"\n\norange -&gt; colour    weight  price  origin\n          \"orange\"  120     50     \"Spain\"\n</code></pre>\n\n<p>One difference from a table-based relational database is that one can omit columns (orange has no variety), or add arbitrary columns (orange has origin) at any time.  You can still imagine the data above as a table, albeit a sparse one where many values might be empty.</p>\n\n<p>However, a \"column-oriented\" model can also be used for lists and time series, where every column name is unique (and here we have just one row, but we could have thousands or millions of columns):</p>\n\n<pre><code>temperature -&gt;  2012-09-01  2012-09-02  2012-09-03 ...\n                40          41          39         ...\n</code></pre>\n\n<p>which is quite different from a relational model, where one would have to model the entries of a time series as <code>rows</code> not <code>columns</code>.</p>\n    "},{"t":"Looking for a lightweight java-compatible in-memory key-value store [closed]","l":"http://stackoverflow.com/questions/2574689/looking-for-a-lightweight-java-compatible-in-memory-key-value-store","q":"\n\n<p>Berkeley DB would be the best choice probably but I can't use it due to licensing issues.</p>\n\n<p>Are there any alternatives? </p>\n    ","a":"\n<p>You can try <a href=\"http://www.hazelcast.com\" rel=\"nofollow\">Hazelcast</a>. Just add hazelcast.jar to your classpath. And start coding</p>\n\n<pre><code>java.util.Map map = Hazelcast.getMap(\"myMap\");\n</code></pre>\n\n<p>You'll get an in-memory, distributed, dynamically scalable data grid which performs super fast.</p>\n    "},{"t":"Storing images in NoSQL stores","l":"http://stackoverflow.com/questions/2278186/storing-images-in-nosql-stores","q":"\n\n<p>Our application will be serving a large number of small, thumbnail-size images (about 6-12KB in size) through HTTP. I've been asked to investigate whether using a NoSQL data store is a viable solution for data storage. Ideally, we would like our data store to be fault-toerant and distributed. Anyone in the StackOverflow community have any experiences they would like to share regarding storing blobs in NoSQL stores, and which one they used? Also, is NoSQL a good solution for our problem, or would we be better served storing the images in the file system and serving them directly from the web server (as an aside, CDN is currently not an option for us)? Thanks.</p>\n    ","a":"\n<p><a href=\"http://www.mongodb.org/display/DOCS/Home\" rel=\"nofollow\">Mongo DB</a> should work well for you.  I haven't used it for blobs yet, but here is a nice FLOSS Weekly <a href=\"http://twit.tv/floss105\" rel=\"nofollow\">podcast interview with Michael Dirolf</a> from the Mongo DB team where he addresses this use case.</p>\n    "},{"t":"SQL vs NOSQL: Which to use for this schema?","l":"http://stackoverflow.com/questions/7705478/sql-vs-nosql-which-to-use-for-this-schema","q":"\n\n<p>I've got an upcoming project and I can't decide whether to stick with SQL or switch over to NoSQL. It's basically a reporting system with the main interface being reporting on the data entered in by users.</p>\n\n<p>Here's the schema I've got mapped out:</p>\n\n<p><img src=\"http://i.stack.imgur.com/CIuMW.png\" alt=\"enter image description here\"></p>\n\n<p>Because this schema is so nested, I started thinking about NoSQL.  With SQL, I'm afraid I'm going to have a crap-ton of joins to get to the bottom of the tree (the Record model).</p>\n\n<p><strong>My concerns, though, are two-fold:</strong></p>\n\n<ol>\n<li>I'm only just starting to get into NoSQL and I'm worried my\nknowledge may limit me because of the tight timeframe.</li>\n<li>Although creating data at the bottom of the tree will probably be relatively simple, I'm worried that it may be hard to report on without getting into some heavy map/reduce stuff (that I have zero experience with)</li>\n</ol>\n\n<p><strong>My question:</strong>\nGiven my concerns, do you think this schema -- because of how deeply nested it is -- lends itself more to NoSQL? If so, do you think the reporting on the \"records\" will be difficult?  </p>\n\n<p>I realize that it may be difficult to answer these questions without more info, so please let me know what other info may be helpful in coming up with an answer.</p>\n\n<p>Thanks in advance for your help!</p>\n    ","a":"\n<p>Just my opinion:</p>\n\n<p>I Stared at diagram for approx 3 sec, this is clearly relational.  Benefits of an RDBMS heavily outweigh a NoSQL solution here.  <em>Why</em> would you want to use NoSQL?  Are there 100,000+ records (may a million plus)?  You need microsecond/millisecond performance?</p>\n\n<p>NoSQL, as I understand, is not because you don't like lots of joins.  It's because big systems for hierarchical data don't suit every situation.  This suit this perfectly, however.</p>\n    "},{"t":"Are RDBMS that bad as described in Hadoop: The definitive guide?","l":"http://stackoverflow.com/questions/4289079/are-rdbms-that-bad-as-described-in-hadoop-the-definitive-guide","q":"\n\n<p>I'm reading Hadoop: The definitive guide by Tom White. In chapter 13.6 \"HBase vs RDMS\" he said that if you have a lot of data, even simple queries like getting 10 recent items are extreamly expensive and they had to rewrite them using python and PL/SQL.</p>\n\n<p>He gives the following query as an example:</p>\n\n<pre><code>SELECT id, stamp, type FROM streams \nWHERE type IN ('type1','type2','type3','type4',...,'typeN')\nORDER BY stamp DESC LIMIT 10 OFFSET 0;\n</code></pre>\n\n<p>And says: \"an RDBMS query planner treats this query as follows:</p>\n\n<pre><code>MERGE (\n  SELECT id, stamp, type FROM streams\n    WHERE type = 'type1' ORDER BY stamp DESC,\n  ...,\n  SELECT id, stamp, type FROM streams\n    WHERE type = 'typeK' ORDER BY stamp DESC\n) ORDER BY stamp DESC LIMIT 10 OFFSET 0;\n</code></pre>\n\n<blockquote>\n  <p>The problem here is that we are after\n  only the top 10 IDs, but the query\n  planner actually materializes an\n  entire merge and then limits at the\n  end. .... We actually went so far as\n  to write a custom PL/Python script\n  that performed a heapsort. ... In\n  nearly all cases, this outperformed\n  the native SQL implementation and the\n  query plannerâ€™s strategy...</p>\n</blockquote>\n\n<p><strong>Expected perforamnce and expermiental results</strong></p>\n\n<p>I couldn't imagine the data set that will cause such problems that you have to write pl/python to do such simple query right. So I've played for a while about this problem and came up with following observations:</p>\n\n<p>The performance of such query is be bounded by O(KlogN). Because it can be translated to so something as follows:</p>\n\n<pre><code>SELECT * FROM (\n  SELECT id, stamp, type FROM streams\n    WHERE type = 'type1' ORDER BY stamp DESC LIMIT 10,\n  UNION\n  ...,\n  SELECT id, stamp, type FROM streams\n    WHERE type = 'typeK' ORDER BY stamp DESC LIMIT 10\n) t ORDER BY stamp DESC LIMIT 10;\n</code></pre>\n\n<p><em>(note the 'LIMIT 10' at each query. BTW I know that I can't limit and order unions but i've stripped out wrapping selects for sake of readability)</em></p>\n\n<p>Each subquery should run as fast as finding the right postion in an index O(logN) and returning 10 items. If we repeat that K times we get O(KlogN).</p>\n\n<p>And even if query planner is so bad that it can not optimize the first query we can always translate it to the query with unions and get the desired performance without writing anything in pl/python.</p>\n\n<p>To double check my calculations I've run the queries above one postgresql filled with  9,000,000 of test records. The results confirmed my expectations both queries were quite fast 100ms for the first query and 300ms for second (the one with unions). </p>\n\n<p>So if the query runs in 100ms for 9,000,000 (logn=23) of records then for 9,000,000,000 (logn=33) of records it should run in 140ms.</p>\n\n<p><strong>Questions</strong> </p>\n\n<ul>\n<li>Do you see any flaws in above reasoning? </li>\n<li>Can you imagine a data set where you would need to rewrite such query as above in pl/python?</li>\n<li>Do you see any situation in which such query wouldn't work in O(K log n)?</li>\n</ul>\n    ","a":"\n<p>Their assertion that an RDMBS query planner takes that solution to the query is incorrect, at least for Postgresql 9.0, and I should imagine for other platforms too. I did a quick test with a similar query:</p>\n\n<pre><code>explain select * from client_attribute where client_attribute_type_code in ('UAG', 'RFR', 'IPA', 'FVD') order by client_attribute_id desc limit 10;\n\n                                                      QUERY PLAN\n-----------------------------------------------------------------------------------------------------------------------\n Limit  (cost=0.00..0.93 rows=10 width=85)\n   -&gt;  Index Scan Backward using client_attribute_pkey on client_attribute  (cost=0.00..15516.47 rows=167234 width=85)\n         Filter: (client_attribute_type_code = ANY ('{UAG,RFR,IPA,FVD}'::bpchar[]))\n(3 rows)\n</code></pre>\n\n<p>Here client_attribute_id is indexed, so it does exactly as desired- walks back through the index, applies the filter and stops when the output hits the limit.</p>\n\n<p>If the ordering column is not indexed, a table scan and sort is requierd, but only one table scan:</p>\n\n<pre><code>explain analyze select * from client_attribute where client_attribute_type_code in ('UAG', 'RFR', 'IPA', 'FVD') order by updated desc limit 10;\n\n                                                              QUERY PLAN\n---------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=13647.00..13647.03 rows=10 width=85) (actual time=180.961..180.964 rows=10 loops=1)\n   -&gt;  Sort  (cost=13647.00..14065.09 rows=167234 width=85) (actual time=180.960..180.961 rows=10 loops=1)\n         Sort Key: updated\n         Sort Method:  top-N heapsort  Memory: 26kB\n         -&gt;  Seq Scan on client_attribute  (cost=0.00..10033.14 rows=167234 width=85) (actual time=0.010..106.791 rows=208325 loops=1)\n               Filter: (client_attribute_type_code = ANY ('{UAG,RFR,IPA,FVD}'::bpchar[]))\n</code></pre>\n\n<p>This uses a heapsort to maintain the top 10 results through the course of the sequential scan, which sounds exactly like the solution they wrote themselves.</p>\n    "},{"t":"I need an advice about NoSQL/MongoDb and data/models structure","l":"http://stackoverflow.com/questions/1815731/i-need-an-advice-about-nosql-mongodb-and-data-models-structure","q":"\n\n<p>Recently I'm exploring NoSQL Databases. I need an advice about how to store data in the most optimal and efficient way for a given problem. I'm targeting MongoDB, now. However it should be the same with CouchDB.</p>\n\n<p>Let's say we have these 3 Models:</p>\n\n<pre><code>Story:\n id\n title\n\nUser:\n id\n name\n\nVote:\n  id\n  story_id\n  user_id\n</code></pre>\n\n<p>I want to be able to ask the database these questions:</p>\n\n<ul>\n<li>Who has voted for this Story?</li>\n<li>What this User has Voted for?</li>\n</ul>\n\n<p>I'm doing simple joins while working with a relational DB. The question is, how should I store the data for those objects in order to be most efficient.</p>\n\n<p>For example, if I store the Vote objects as a subcollection of Stories it wont be easy to get the info - \"What a user has voted for\".</p>\n    ","a":"\n<p>I would suggest storing votes as a list of story <code>_id</code>s in each user. That way you can find out what stories a user has voted for just by looking at the list. To get the users who have voted for a story you can do something like:</p>\n\n<p><code>db.users.find({stories: story_id})</code></p>\n\n<p>where <code>story_id</code> is the <code>_id</code> of the story in question. If you create an index on the <code>stories</code> field both of those queries will be fast.</p>\n    "},{"t":"HBase standalone failed to connect (fail to create table)","l":"http://stackoverflow.com/questions/15752608/hbase-standalone-failed-to-connect-fail-to-create-table","q":"\n\n<p>I am trying to deploy Hbase in standalone mode following this article: <a href=\"http://hbase.apache.org/book.html#quickstart\">http://hbase.apache.org/book.html#quickstart</a>. The version is 0.92.1-cdh4.1.2</p>\n\n<p>But I am getting these errors when try to create a table:</p>\n\n<p>Error message:</p>\n\n<pre><code>    13/04/01 14:07:10 ERROR zookeeper.RecoverableZooKeeper: ZooKeeper exists failed after 3 retries\n13/04/01 14:07:10 WARN zookeeper.ZKUtil: hconnection Unable to set watcher on znode /hbase/master\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n    at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1021)\n    at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:154)\n    at org.apache.hadoop.hbase.zookeeper.ZKUtil.watchAndCheckExists(ZKUtil.java:226)\n    at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:82)\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:580)\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.&lt;init&gt;(HConnectionManager.java:569)\n</code></pre>\n\n<p>Output log:</p>\n\n<pre><code>13/04/01 14:06:39 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=180000 watcher=hconnection\n13/04/01 14:06:39 INFO zookeeper.RecoverableZooKeeper: The identifier of this process is 10231@localhost\n13/04/01 14:06:39 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)\n13/04/01 14:06:39 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)\n13/04/01 14:06:39 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid\n</code></pre>\n\n<p>My configurations:</p>\n\n<ol>\n<li>Added JAVA_HOME in hbase-env.sh</li>\n<li><p>hbase_site.xml</p>\n\n<p></p>\n\n<p></p>\n\n<pre><code>&lt;name&gt;hbase.rootdir&lt;/name&gt;\n\n&lt;value&gt;file:///home/hadoop/data&lt;/value&gt;\n</code></pre>\n\n<p></p>\n\n<p></p>\n\n<pre><code>&lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n\n&lt;value&gt;file:///home/hadoop/zookeeper&lt;/value&gt;\n</code></pre>\n\n<p></p>\n\n<p></p></li>\n</ol>\n\n<p>I have tried to modify /etc/hosts, which looks likes this (oracle is the hostname):</p>\n\n<pre><code>127.0.0.1   localhost oracle\n</code></pre>\n\n<p>or </p>\n\n<pre><code>[server ip]   localhost oracle\n</code></pre>\n\n<p>But these do not work for me.</p>\n\n<p>My exact codes are:</p>\n\n<pre><code>[usr@oracle bin]$ ./start-hbase.sh \nstarting master, logging to /home/hadoop/hbase-0.94.6/bin/../logs/hbase-usr-master-oracle.out\n[usr@oracle bin]$ hbase shell\n13/04/01 14:57:55 WARN conf.Configuration: hadoop.native.lib is deprecated. Instead, use io.native.lib.available\nHBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.\nType \"exit&lt;RETURN&gt;\" to leave the HBase Shell\nVersion 0.92.1-cdh4.1.2, rUnknown, Thu Nov  1 18:02:07 PDT 2012\n\nhbase(main):001:0&gt; create 'test','cf'\n</code></pre>\n\n<p>Thanks in advance!</p>\n    ","a":"\n<p>Looks like you are missing <code>hbase.zookeeper.quorum</code> in your configuration.\nPlease check this HBase guide chapter, it may help: <a href=\"http://hbase.apache.org/book/zookeeper.html\">http://hbase.apache.org/book/zookeeper.html</a></p>\n\n<p>Also please check zookeeper is started on right port and (what happens more often) correct IP interfaces.</p>\n    "},{"t":"Are there any REAL advantages to NoSQL over RDBMS for structured data on one machine?","l":"http://stackoverflow.com/questions/24921175/are-there-any-real-advantages-to-nosql-over-rdbms-for-structured-data-on-one-mac","q":"\n\n<p>So I've been trying hard to figure out if NoSQL is really bringing that much value outside of auto-sharding and handling UNSTRUCTURED data.</p>\n\n<p>Assuming I can fit my STRUCTURED data on a single machine OR have an effective 'auto-sharding' feature for SQL, what advantages do any NoSQL options offer? I've determined the following:</p>\n\n<ol>\n<li><p><strong>Document-based (MongoDB, Couchbase, etc)</strong> - Outside of it's 'auto-sharding' capabilities, I'm having a hard time understanding where the benefit is. Linked objects are quite similar to SQL joins, while Embedded objects significantly bloat doc size and causes a challenge regarding to replication (a comment could belong to both a post AND a user, and therefore the data would be redundant). Also, loss of ACID and transactions are a big disadvantage.</p></li>\n<li><p><strong>Key-value based (Redis, Memcached, etc)</strong> - Serves a different use case, ideal for caching but not complex queries</p></li>\n<li><p><strong>Columnar (Cassandra, HBase, etc )</strong> - Seems that the big advantage here is more how the data is stored on disk, and mostly useful for aggregations rather than general use</p></li>\n<li><p><strong>Graph (Neo4j, OrientDB, etc)</strong> - The most intriguing, the use of both edges and nodes makes for an interesting value-proposition, but mostly useful for highly complex relational data rather than general use.</p></li>\n</ol>\n\n<p>I can see the advantages of Key-value, Columnar and Graph DBs for specific use cases (Caching, social network relationship mapping, aggregations), but can't see any reason to use something like MongoDB for STRUCTURED data outside of it's 'auto-sharding' capabilities. </p>\n\n<p>If SQL has a similar 'auto-sharding' ability, would SQL be a no-brainer for structured data? Seems to me it would be, but I would like the communities opinion...</p>\n\n<p>NOTE: This is in regards to a typical CRUD application like a Social Network, E-Commerce site, CMS etc.</p>\n    ","a":"\n<p>If you're starting off on a single server, then many advantages of NoSQL go out the window.  The biggest advantages to the most popular NoSQL are high availability with less down time.  Eventual consistency requirements can lead to performance improvements as well.  It really depends on your needs.</p>\n\n<ol>\n<li><p><strong>Document-based</strong> - If your data fits well into a handful of small buckets of data, then a document oriented database.  For example, on a classifieds site we have Users, Accounts and Listings as the core data.  The bulk of search and display operations are against the Listings alone.  With the legacy database we have to do nearly 40 join operations to get the data for a single listing.  With NoSQL it's a single query.  With NoSQL we can also create indexes against nested data, again with results queried without Joins.  In this case, we're actually mirroring data from SQL to MongoDB for purposes of search and display (there are other reasons), with a longer-term migration strategy being worked on now.  ElasticSearch, RethinkDB and others are great databases as well.  RethinkDB actually takes a very conservative approach to the data, and ElasticSearch's out of the box indexing is second to none.</p></li>\n<li><p><strong>Key-value store</strong> - Caching is an excellent use case here, when you are running a medium to high volume website where data is mostly read, a good caching strategy alone can get you 4-5 times the users handled by a single server.</p></li>\n<li><p><strong>Columnar</strong> - Cassandra in particular can be used to distribute significant amounts of load for even single-value lookups.  Cassandra's scaling is very linear to the number of servers in use.  Great for heavy read and write scenarios.  I find this less valuable for live searches, but very good when you have a <em>VERY</em> high load and need to distribute.  It takes a lot more planning, and may well not fit your needs.  You can tweak settings to suite your CAP needs, and even handle distribution to multiple data centers in the box.  NOTE: Most applications do emphatically <em>NOT</em> need this level of use.  ElasticSearch may be a better fit in most scenarios you would consider HBase/Hadoop or Cassandra for.</p></li>\n<li><p><strong>Graph</strong> - I'm not as familiar with graph databases, so can't comment here.</p></li>\n</ol>\n\n<p>Given that you then comment on MongoDB specifically vs SQL ... even if both auto-shard.  PostgreSQL in particular has made a lot of strides in terms of getting unstrictured data usable (JSON/JSONB types) not to mention the power you can get from something like PLV8, it's probably the most suited to handling the types of loads you might throw at a document store with the advantages of NoSQL.  Where it happens to fall down is that replication, sharding and failover are bolted on solutions not really in the box.</p>\n\n<p>For small to medium loads sharding really isn't the best approach.  Most scenarios are mostly read so having a replica-set where you have additional read nodes is usually better when you have 3-5 servers.  MongoDB is great in this scenario, the master node is automagically elected, and failover is pretty fast.  The only weirdness I've seen is when Azure went down in late 2014, and only one of the servers came up first, the other two were almost 40 minutes later.  With replication any given read request can be handled in whole by a single server.  Your data structures become simpler, and your chances of data loss are reduced.</p>\n\n<p>Again in my own example above, for a mediums sized classifieds site, the vast majority of data belongs to a single collection... it is searched against, and displayed from that collection.  With this use case a document store works much better than structured/normalized data.  The way the objects are stored are much closer to their representation in the application.  There's less of a cognitive disconnect and it simply works.</p>\n\n<p>The fact is that SQL JOIN operations kill performance, especially when aggregating data across those joins.  For a single query for a single user it's fine, even with a dozen of them.  When you get to dozens of joins with thousands of simultaneous users, it starts to fall apart.  At this point you have several choices...</p>\n\n<ul>\n<li><p><strong>Caching</strong> - caching is always a great approach, and the less often your data changes, the better the approach.  This can be anything from a set of memcache/redis instances to using something like MongoDB, RethinkDB or ElasticSearch to hold composite records.  The challenge here comes down to updating or invalidating your cached data.</p></li>\n<li><p><strong>Migrating</strong> - migrating your data to a data store that better represents your needs can be a good idea as well.  If you need to handle massive writes, or very massive read scenarios no SQL database can keep up.  You could <em>NEVER</em> handle the likes of Facebook or Twitter on SQL.</p></li>\n<li><p><strong>Something in between</strong> - As you need to scale it depends on what you are doing and where your pain points are as to what will be the best solution for a given situation.  Many developers and administrators fear having data broken up into multiple places, but this is often the best answer.  Does your analytical data really need to be in the same place as your core operational data?  For that matter do your logins need to be tightly coupled?  Are you doing a lot of correlated queries?  It really depends.</p></li>\n</ul>\n\n<hr>\n\n<p><em>Personal Opinions Ahead</em></p>\n\n<p>For me, I like the safety net that SQL provides.  Having it as the central store for core data it's my first choice.  I tend to treat RDBMS's as dumb storage, I don't like being tied to a given platform.  I feel that many people try to over-normalize their data.  Often I will add an XML or JSON field to a table so additional pieces of data can be stored without bloating the scheme, specifically if it's unlikely to ever be queried... I'll then have properties in my objects in the application code that store in those fields.  A good example may be a payment... if you are currently using one system, or multiple systems (one for CC along with Paypal, Google, Amazon etc) then the details of the transaction really don't affect your records, why create 5+ tables to store this detailed data.</p>\n\n<p>When data is a natural fit for a document store, I say go for it... if the vast majority of your queries are for something that fits better to a single record or collection, denormalize away.  Having this as a mirror to your primary data is great.</p>\n\n<p>For write-heavy data you want multiple systems in play... It depends heavily on your needs here...  Do you need fast hot-query performance? Go with ElasticSearch.  Do you need absolute massive horizontal scale, HBase or Cassandra.</p>\n\n<p>The key take away here is not to be afraid to mix it up... there really isn't a one size fits all.  As an aside, I feel that if PostgreSQL comes up with a good in the box (for the open-source version) solution for even just replication and automated fail-over they're in a much better position than most at that point.</p>\n\n<p>I didn't really get into, but feel I should mention that there are a number of SaaS solutions and other providers that offer hybrid SQL systems.  You can develop against MySQL/MariaDB locally and deploy to a system with SQL on top of a distributed storage cluster.  I still feel that HBase or ElasticSearch are better for logging and analitical data, but the SQL on top solutions are also compelling.</p>\n\n<p>More: <a href=\"http://www.mongodb.com/nosql-explained\">http://www.mongodb.com/nosql-explained</a></p>\n    "},{"t":"Is it too soon to use Couchbase Mobile?","l":"http://stackoverflow.com/questions/7063402/is-it-too-soon-to-use-couchbase-mobile","q":"\n\n<p>For an iPhone App I decided to give a try to a NoSQL DB, because the nature of the data I need to store locally. The most sophisticated solution I found is <a href=\"http://www.couchbase.com/products-and-services/mobile-couchbase\">Couchbase Mobile</a>. But it seems, that <a href=\"https://github.com/couchbaselabs/iOS-Couchbase\">the project has only beta status</a>. Is it too soon to use it?</p>\n    ","a":"\n<p>Couchbase Mobile is currently beta, with plans for a GA/1.0 at the end of September (2011). </p>\n\n<p>By the next developer preview release at the end of August the iOS version should be entirely ready for you to start development with. The Android version is lagging a little in terms of documentation, but should also be ready for active development at the end of August.</p>\n\n<p>If you are hard core, you can start today, but if you wait a few weeks we'll be ready for even the non-hardcore.</p>\n    "},{"t":"Understanding MongoDB (and NoSQL in general) and how to make the best use of it","l":"http://stackoverflow.com/questions/2573657/understanding-mongodb-and-nosql-in-general-and-how-to-make-the-best-use-of-it","q":"\n\n<p>I am beginning to think that my next project I am wanting to do would work better with a NoSQL solution. The project would either involve a ton of 2-column tables or a ton of dynamic queries with dynamically generated columns in a traditional SQL database. So I feel a NoSQL database would be much cleaner. </p>\n\n<p>I'm looking at MongoDB and it looks pretty promising. Anyway, I'm attempting to make sense of it all. Also, I will be using MongoMapper in Ruby. </p>\n\n<p>Anyway though, I'm confused as to how to layout things in such a freeform database. I've read <a href=\"http://stackoverflow.com/questions/2170152/nosql-best-practices\">http://stackoverflow.com/questions/2170152/nosql-best-practices</a> and the answer there says that normalization is usually bad in a NoSQL DB. So how would be the best way of laying out say a simple blog with users, posts, and comments? </p>\n\n<p>My natural thought was to have three collections for each and then link them by a unique ID. But this apparently is wrong? So, what are some of the ways to lay out such a thing? My concern with the answer given in the other question is, what if the author's name changed? You'd have to go through updating a ton of posts and comments. But is this an okay thing to do with NoSQL? </p>\n    ","a":"\n<p>Ok, I've found two pages that are helpful <a href=\"http://www.mongodb.org/display/DOCS/Schema+Design\">Schema Design</a> and <a href=\"http://www.mongodb.org/display/DOCS/MongoDB+Data+Modeling+and+Rails\">Data Modeling (a full application in RoR)</a> </p>\n\n<p>Also, the #mongodb channel on IRC is extremely helpful. The user <code>dacort</code> there helped me to find those very useful pages. </p>\n    "},{"t":"How to store 800 billion GPS markers in database [closed]","l":"http://stackoverflow.com/questions/12825810/how-to-store-800-billion-gps-markers-in-database","q":"\n\n<p>I need to store GPS tracks that users record into a database. The tracks will consist of a marker every 5 meter of movement for the purpose of drawing a line on a map. I am estimating 200 km tracks which means 40,000 lnlt markers. I estimate 50,000 users minimum and 20 pieces of 200 km tracks for each. That means at least 40 billion lnlt markers. </p>\n\n<p>This needs to scale too, so for 1 million users I need capacity for 800 billion GPS markers.</p>\n\n<p>Since each set of 40,000 markers belong to a single track, we are talking 1 - 20 million records/sets of GPS tracks.</p>\n\n<p>Requirements:\nUsers will request to view these tracks on top of a Google map in a mobile application.</p>\n\n<p>Relations:\nI currently have 2 tables. Table one has:[trackid], [userid], [comment], [distance], [time], [top speed]. </p>\n\n<p>Table 2 has [trackid] [longitude] [latitude] and this is where all GPS markers are stored. What is an efficient way of storing this volume of GPS data while maintaining read performance?</p>\n\n<p>New information:</p>\n\n<p>Storing the GPS data in a KML file for the purpose of displaying them as a track on top of a Google map is a good solution that saves database space. Compressing the KML into a KMZ (basically a zipped KML wit a KMZ extension) greatly reduces file size further. KMZ loads much quicker than GPX and can be integrated with the Google Maps API as a KML layer. <a href=\"https://developers.google.com/maps/documentation/javascript/layers#KMLLayers\">See this information from Google</a> for further assistance. This seems to be the best solution so far for the intended requirement.</p>\n    ","a":"\n<p>The choice of a particular database, as always, is tied to how you want to store the information and how you want to use it. As such, without knowing the exact requirements of your project, as well as the relationships of the data, the best thing to do would be to <strong>do some reading on the topic to determine what particular product or storage model is best suited to you.</strong></p>\n\n<p>A good place to start is reading blogs that compare the performance and uses of the databases (see attached):</p>\n\n<p><a href=\"http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis\" rel=\"nofollow\">http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis</a></p>\n    "},{"t":"Does NoSQL databases use or need indexes?","l":"http://stackoverflow.com/questions/10085022/does-nosql-databases-use-or-need-indexes","q":"\n\n<p>I am a newbie in NoSQL databases and this may sound a bit stupid but I was wondering if  NoSQL databases use or need indexes?\nIf yes, how to make or manage them? any links?\nThanks</p>\n    ","a":"\n<p>CouchDB and MongoDB definitely yes. I mentioned that in my book:</p>\n\n<ul>\n<li><a href=\"http://use-the-index-luke.com/sql/testing-scalability/response-time-throughput-scaling-horizontal\" rel=\"nofollow\">http://use-the-index-luke.com/sql/testing-scalability/response-time-throughput-scaling-horizontal</a></li>\n</ul>\n\n<p>Here are the respective docs:</p>\n\n<ul>\n<li><a href=\"http://guide.couchdb.org/draft/btree.html\" rel=\"nofollow\">http://guide.couchdb.org/draft/btree.html</a></li>\n<li><a href=\"http://www.mongodb.org/display/DOCS/Indexes\" rel=\"nofollow\">http://www.mongodb.org/display/DOCS/Indexes</a></li>\n</ul>\n\n<p>NoSQL is, however, too fragmented to give a definite \"yes, all NoSQL systems need indexes\", I believe. Most systems require and provide indexes but not at level most SQL databases do. Recently, the Cassandra people were proudly introducing secondary indexes, i.e., more than a single clustered index.</p>\n\n<ul>\n<li><a href=\"http://www.datastax.com/dev/blog/whats-new-cassandra-07-secondary-indexes\" rel=\"nofollow\">http://www.datastax.com/dev/blog/whats-new-cassandra-07-secondary-indexes</a> (well, not so recently as I remember)</li>\n</ul>\n    "},{"t":"I need an introduction to MongoDB/NoSQL Database [closed]","l":"http://stackoverflow.com/questions/8676895/i-need-an-introduction-to-mongodb-nosql-database","q":"\n\n<p>I use PHP/mySQL/CodeIgniter pretty heavily, writing sql statements to handle/manipulate data. I feel doing all that is primitive, and I've heard good things about MongoDB, schema-less database.</p>\n\n<p>In MySQL, schemas helps me figure out the structure of the model. Usually, I draw out a class diagram with basic things like: id, title, description, date</p>\n\n<p>What blows my mind is, MongoDB seems insanely simple, it's hard to grasp where to begin. From what I hear/read, it doesn't have a schema. How do I know what type will it return?</p>\n\n<p>How do I build my models, how do I add relations between different \"tables\"?</p>\n\n<p>What is the standard way to add relations and map out data? I've tried playing with it, but wasn't sure what I was doing was the correct way.</p>\n\n<p>I've tried reading manuals and such, but couldn't find a good article helping me transition from mySQL to MongoDB.</p>\n\n<p>Is there anyway I could see comparisons of Models with <em>mySQL</em> and <em>MongoDB</em>? Simple things like CRUD.</p>\n\n<p>How do I start, where do I begin?</p>\n    ","a":"\n<p>You could start <a href=\"http://www.mongodb.org/display/DOCS/SQL+to+Mongo+Mapping+Chart\">here</a>.</p>\n\n<blockquote>\n  <p>How do I build my models, how do I add relations between different \"tables\"?</p>\n</blockquote>\n\n<p><a href=\"http://www.mongodb.org/display/DOCS/Philosophy\">Answer:</a></p>\n\n<blockquote>\n  <p>A non-relational approach is the best path to database solutions which scale horizontally to &gt; many machines.</p>\n</blockquote>\n\n<p><a href=\"http://How%20do%20I%20know%20what%20type%20will%20it%20return?\">Answer:</a></p>\n\n<blockquote>\n  <p>MongoDB stores data in JSON documents (which we serialize to BSON). JSON provides us a rich data model that seamlessly maps to native programming language types, and since its schema-less, makes it much easier to evolve your data model than with a system with enforced schemas such as a RDBMS.</p>\n</blockquote>\n\n<p>Check also <a href=\"http://stackoverflow.com/questions/1145726/what-is-nosql-how-does-it-work-and-what-benefits-does-it-provide\">What is NoSQL, how does it work, and what benefits does it provide?</a>, <a href=\"http://stackoverflow.com/questions/1815731/i-need-an-advice-about-nosql-mongodb-and-data-models-structure\">I need an advice about NoSQL/MongoDb and data/models structure</a> and <a href=\"http://stackoverflow.com/questions/6201433/converting-simple-mysql-database-to-a-nosql-solution\">Converting simple MySQL database to a NoSQL solution</a></p>\n    "},{"t":"Are there any â€œgotchasâ€ in deploying a Cassandra cluster to a set of Linode VPS instances?","l":"http://stackoverflow.com/questions/2291442/are-there-any-gotchas-in-deploying-a-cassandra-cluster-to-a-set-of-linode-vps","q":"\n\n<p>I am learning about the Apache Cassandra database [sic].</p>\n\n<p>Does anyone have any good/bad experiences with deploying Cassandra to less than dedicated hardware like the offerings of Linode or Slicehost?  </p>\n\n<p>I think Cassandra would be a great way to scale a web service easily to meet read/write/request load...  just add another Linode running a Cassandra node to the existing cluster.  Yes, this implies running the public web service and a Cassandra node on the same VPS (which many can take exception with).</p>\n\n<p>Pros of Linode-like deployment for Cassandra:</p>\n\n<ul>\n<li>Private VLAN; the Cassandra nodes could communicate privately</li>\n<li>An API to provision a new Linode (and perhaps configure it with a \"StackScript\" that installs Cassandra and its dependencies, etc.)</li>\n<li>The price is right</li>\n</ul>\n\n<p>Cons:</p>\n\n<ul>\n<li>Each host is a VPS and is not dedicated of course</li>\n<li>The RAM/cost ratio is not that great once you decide you want 4GB RAM (cf. dedicated at say SoftLayer)</li>\n<li>Only 1 disk where one would prefer 2 disks I suppose (1 for the commit log and another disk for the data files themselves).  Probably moot since this is shared hardware anyway.</li>\n</ul>\n\n<p>EDIT: found this which helps a bit: <a href=\"http://wiki.apache.org/cassandra/CassandraHardware\">http://wiki.apache.org/cassandra/CassandraHardware</a> </p>\n\n<p>I see that 1GB is the minimum but is this a recommendation?  Could I deploy with a Linode 720 for instance (say 500 MB usable to Cassandra)?  See <a href=\"http://www.linode.com/\">http://www.linode.com/</a></p>\n    ","a":"\n<p>How much ram you needs really depends on your workload: if you are write-mostly you can get away with less, otherwise you will want ram for the read cache.</p>\n\n<p>You do get more ram for you money at my employer, rackspace cloud:  <a href=\"http://www.rackspacecloud.com/cloud_hosting_products/servers/pricing\">http://www.rackspacecloud.com/cloud_hosting_products/servers/pricing</a>.  (our machines also have raided disks so people typically see better i/o performance vs EC2.  Dunno about linode.)</p>\n\n<p>Since with most VPSes you pay roughly 2x for the next-size instance, i.e., about the same as adding a second small instance, I would recommend going with fewer, larger instances than more, smaller ones, since in small numbers network overhead is not negligible.</p>\n\n<p>I do know someone using Cassandra on 256MB VMs but you're definitely in the minority if you go that small.</p>\n    "},{"t":"What nosql means? can someone explain it to me in simple words?","l":"http://stackoverflow.com/questions/1245338/what-nosql-means-can-someone-explain-it-to-me-in-simple-words","q":"\n\n<p>in this post <a href=\"http://highscalability.com/stack-overflow-architecture\">Stack Overflow Architecture</a> i read about something called nosql, i didn't understand what it means, and i tried to search on google but seams that i can't get exactly whats it.</p>\n\n<p>Can anyone explain what nosql means in simple words?</p>\n    ","a":"\n<p>From the <a href=\"http://www.strozzi.it/cgi-bin/CSA/tw7/I/en%5FUS/nosql/Home%20Page\" rel=\"nofollow\">NoSQL Homepage</a></p>\n\n<blockquote>\n  <p>NoSQL is a fast, portable, relational database management system without arbitrary limits, (other than memory and processor speed) that runs under, and interacts with, the UNIX  1 Operating System. It uses the \"Operator-Stream Paradigm\" described in \"Unix Review\", March, 1991, page 24, entitled \"A 4GL Language\". There are a number of \"operators\" that each perform a unique function on the data. The \"stream\" is supplied by the UNIX Input/Output redirection mechanism. Therefore each operator processes some data and then passes it along to the next operator via the UNIX pipe function. This is very efficient as UNIX pipes are implemented in memory. NoSQL is compliant with the \"Relational Model\".</p>\n</blockquote>\n\n<p>I would also see this answer on <a href=\"http://stackoverflow.com/questions/1145726/what-is-nosql-how-does-it-work-and-what-benefits-does-it-profide/1145751#1145751\">Stackoverflow</a>.</p>\n    "},{"t":"Redis-cli - Selecting which instance?","l":"http://stackoverflow.com/questions/6206971/redis-cli-selecting-which-instance","q":"\n\n<p>I'm currently running multiple redis instances on one box. Each have their own config, init.d, and listen on different ports. My application(s) have no problem connecting via the redis clients, but I'd like to be able to connect to each one using redis-cli. I couldn't find any information on $:redis-cli [options] in either the redis-doc or on redis.io. Any ideas?</p>\n    ","a":"\n<p>You can specify the server host and port using -h and -p parameters. E.g.:</p>\n\n<pre><code>redis-cli -h 127.0.0.1 -p 6379\n</code></pre>\n    "},{"t":"Mongoose populate embedded","l":"http://stackoverflow.com/questions/13077609/mongoose-populate-embedded","q":"\n\n<p>I use Mongoose.js and cannot solve problem with 3 level hierarchy document.</p>\n\n<p>There 2 ways to do it.</p>\n\n<p><strong>First</strong> - without refs.</p>\n\n<pre><code>C = new Schema({\n    'title': String,\n});\n\nB = new Schema({\n    'title': String,\n    'c': [C]\n});\n\nA = new Schema({\n    'title': String,\n    'b': [B]\n});\n</code></pre>\n\n<p>I need to show C record. How can i populate / find it, knowing only _id of C?</p>\n\n<p>I was try use:</p>\n\n<pre><code>A.findOne({'b.c._id': req.params.c_id}, function(err, a){\n    console.log(a);\n});\n</code></pre>\n\n<p>But i dont know how to get from returnet a object only c object that i need.</p>\n\n<p><strong>Second</strong> if working with refs:</p>\n\n<pre><code>C = new Schema({\n    'title': String,\n});\n\nB = new Schema({\n    'title': String,\n    'c': [{ type: Schema.Types.ObjectId, ref: 'C' }]\n});\n\nA = new Schema({\n    'title': String,\n    'b': [{ type: Schema.Types.ObjectId, ref: 'B' }]\n});\n</code></pre>\n\n<p>How to populate all B, C records to get hierarchy?</p>\n\n<p>I was try to use something like this:</p>\n\n<pre><code>A\n.find({})\n.populate('b')\n.populate('b.c')\n.exec(function(err, a){\n    a.forEach(function(single_a){\n        console.log('- ' + single_a.title);\n        single_a.b.forEach(function(single_b){\n            console.log('-- ' + single_b.title);\n            single_b.c.forEach(function(single_c){\n                console.log('--- ' + single_c.title);\n            });\n        });\n    });\n});\n</code></pre>\n\n<p>But it will return undefined for single_c.title. I there way to populate it?</p>\n\n<p>Thanks.</p>\n    ","a":"\n<p>Nested sub document population is not supported. This link to the open github issue.</p>\n\n<p><a href=\"https://github.com/LearnBoost/mongoose/issues/601\" rel=\"nofollow\">https://github.com/LearnBoost/mongoose/issues/601</a></p>\n    "},{"t":"Migrate mongodb database from localhost to remote servers","l":"http://stackoverflow.com/questions/21303456/migrate-mongodb-database-from-localhost-to-remote-servers","q":"\n\n<p>I created a db on my local ubuntu machine.</p>\n\n<p>How can I transafer it to my remote server(ec2 ubuntu)</p>\n    ","a":"\n<h3>TL;DR</h3>\n\n<p>Use <a href=\"http://docs.mongodb.org/manual/tutorial/backup-databases-with-binary-database-dumps/\"><code>mongodump</code> and <code>mongorestore</code></a> to take (and restore) a full binary backup of your MongoDB database. Compress the backup <code>dump</code> directory to make it faster to copy to your Amazon instance (BSON tends to compress very well).</p>\n\n<h3>Best practices</h3>\n\n<p>Rather than following adhoc instructions, I would strongly recommend reading the standard <a href=\"http://docs.mongodb.org/manual/tutorial/backup-databases-with-binary-database-dumps/\">Backup and Restore with MongoDB Tools</a> tutorial in the MongoDB manual.</p>\n\n<p>You can also use a <a href=\"http://docs.mongodb.org/manual/tutorial/back-up-databases-with-filesystem-snapshots/\">Filesystem snapshot</a>, but <code>mongodump</code> and <code>mongorestore</code> only export the data so your backup will be smaller (i.e. your remote server will not inherit any <a href=\"http://docs.mongodb.org/manual/faq/storage/#why-are-the-files-in-my-data-directory-larger-than-the-data-in-my-database\">excessive storage allocation</a> due to preallocation).</p>\n    "},{"t":"MongoDB: How to get distinct list of sub-document field values?","l":"http://stackoverflow.com/questions/16155266/mongodb-how-to-get-distinct-list-of-sub-document-field-values","q":"\n\n<p>Let's say I have the following documents in collection:</p>\n\n<pre><code>{\n   \"family\": \"Smith\",\n   \"children\": [\n        {\n            \"child_name\": \"John\"\n        },\n        {\n            \"child_name\": \"Anna\"\n        },\n    ]\n}\n\n{\n   \"family\": \"Williams\",\n   \"children\": [\n        {\n            \"child_name\": \"Anna\"\n        },\n        {\n            \"child_name\": \"Kevin\"\n        },\n    ]\n}\n</code></pre>\n\n<p>Now I want to get somehow the following list of unique child names cross all families:</p>\n\n<pre><code>[ \"John\", \"Anna\", \"Kevin\" ]\n</code></pre>\n\n<p>Structure of result might be different. How to achieve that in MongoDB? Should be something simple but I can't figure out. I tried aggregate() function on collection but then I don't know how to apply distinct() function.</p>\n    ","a":"\n<p>You can just do:</p>\n\n<pre><code>db.collection.distinct(\"children.child_name\");\n</code></pre>\n\n<p>In your case it returns:</p>\n\n<pre><code>[ \"John\", \"Anna\", \"Kevin\" ]\n</code></pre>\n    "},{"t":"What is the major difference between Redis and Membase?","l":"http://stackoverflow.com/questions/5071588/what-is-the-major-difference-between-redis-and-membase","q":"\n\n<p>What are the major differences between Redis and Membase? </p>\n    ","a":"\n<p><strong>Scalability:</strong>\nMembase offers a distributed key/value store (just like Memcache), so writes and reads will always be performed in predictably constant time regardless of how large your data set is.  Redis on the other hand offers just master-slave replication, which speeds up read but does not speed up writes.</p>\n\n<p><strong>Data Redundancy</strong>\nIt's simple to setup a cluster with a set amount of replicated copy for each key-value pair, allow for servers to failover a inoperative node in a cluster without losing data.  Redis' master-slave replication doesn't offer this same type of data redundancy, however.</p>\n\n<p><strong>Data Type:</strong>\nRedis offers ability to handle lists in an atomic fashion out of the box, but one can implement similar functionality in the application logic layer with Membase.</p>\n\n<p><strong>Adoption:</strong>\nCurrently Redis is more widely adopted and a bit more mature than Membase.  Membase does have a few high profile use case, such as Zynga and their slew of social games.</p>\n\n<p>Membase has recently merged with Couchbase and they will have a version of Membase that will offer CouchDB's Map/Reduce and query/index ability in the next major release (scheduled around early 2011).</p>\n    "},{"t":"Neo4j and ORM (Hibernate)","l":"http://stackoverflow.com/questions/9196276/neo4j-and-orm-hibernate","q":"\n\n<p>I've been using RDBMSes since college and am really <em>struggling</em> with the underlying concepts of NoSQL databases...but I think their concept is really cool.</p>\n\n<p>I <em>believe</em> I understand the following (please correct me if I'm wrong, because these play into my question here!):</p>\n\n<ul>\n<li>NoSQL is not some formal specification; it is a concept underlying a new \"breed\" of databases that are not relational and do not use SQL</li>\n<li>As such, each NoSQL system is different (for instance, MongoDB is JSON-centric)</li>\n</ul>\n\n<p>If these are true, then let us redirect our attention to Neo4j, a \"graph-based\" database.</p>\n\n<p>After perusing the site and the PDF, it seems like Neo4j is not only a database, but it also provides a Java API that essentially replaces the need for traditional ORM tools like Hibernate.</p>\n\n<p>So, my final question is actually a request for clarification/confirmation of that last assertion, specifically:</p>\n\n<ul>\n<li>Is it true that if my backend is entirely Neo4j-based, that I would have no need for Hibernate (which is my usual ORM)? Are these two APIs mutually-exclusive, or is there some way to benefit between using both of them?</li>\n</ul>\n\n<p>Thanks in advance!</p>\n    ","a":"\n<p>AFAIK, Hibernate is an object/relational mapping framework that only supports SQL-like databases. So you won't need / be able to use it if you use Neo4j, you would use Neo4j's API instead.</p>\n\n<p>But nothing prevents you from using both Neo4j and an SQL database therefore mixing Hibernate and the neo4j API (most likely to store/query different objects within your project).</p>\n\n<p>Have you checked the basic examples given on Neo4j website, such as <a href=\"http://docs.neo4j.org/chunked/snapshot/tutorials-java-embedded-hello-world.html\" rel=\"nofollow\">http://docs.neo4j.org/chunked/snapshot/tutorials-java-embedded-hello-world.html</a> ?</p>\n\n<p>EDIT:</p>\n\n<p>You are right, NoSql does not define a specific standard. You might want to have a look at this (short) introduction: <a href=\"http://martinfowler.com/articles/nosql-intro.pdf\" rel=\"nofollow\">http://martinfowler.com/articles/nosql-intro.pdf</a></p>\n    "},{"t":"Using MongoDB vs MySQL with lots of JSON fields?","l":"http://stackoverflow.com/questions/12934385/using-mongodb-vs-mysql-with-lots-of-json-fields","q":"\n\n<p>There is a microblogging type of application. Two main basic database stores zeroed upon are:\nMySQL or MongoDB.</p>\n\n<p>I am planning to denormalize lot of data I.e. A vote done on a post is stored in a voting table, also a count is incremented in the main posts table. There are other actions involved with the post too (e.g. Like, vote down). </p>\n\n<p>If I use MySQL, some of the data better suits as JSON than fixed schema, for faster lookups.</p>\n\n<p>E.g.</p>\n\n<pre><code>POST_ID   |  activity_data\n\n213423424 | { 'likes': {'count':213,'recent_likers' :\n             ['john','jack',..fixed list of recent N users]} , 'smiles' : \n             {'count':345,'recent_smilers' :\n             ['mary','jack',..fixed list of recent N users]}  }\n</code></pre>\n\n<p>There are other components of the application as well, where usage of JSON is being proposed.\nSo, to update a JSON field, the sequence is:</p>\n\n<ol>\n<li><p>Read the JSON in python script.</p></li>\n<li><p>Update the JSON</p></li>\n<li><p>Store the JSON back into MySQL.</p></li>\n</ol>\n\n<p>It would have been single operation in MongoDB with atomic operations like <code>$push</code>,<code>$inc</code>,<code>$pull</code> etc. Also\ndocument structure of MongoDB suits my data well.</p>\n\n<p>My considerations while choosing the data store.</p>\n\n<p>Regarding MySQL:</p>\n\n<ol>\n<li>Stable and familiar.</li>\n<li>Backup and restore is easy.</li>\n<li>Some future schema changes can be avoided using some fields as schemaless JSON.</li>\n<li>May have to use layer of memcached early.</li>\n<li>JSON blobs will be static in some tables like main Posts, however will be updated alot in some other tables like Post votes and likes.</li>\n</ol>\n\n<p>Regarding MongoDB:</p>\n\n<ol>\n<li>Better suited to store schema less data as documents.</li>\n<li>Caching might be avoided till a later stage.</li>\n<li>Sometimes the app may become write intensive, MongoDB can perform better at those points where unsafe writes are not an issue.</li>\n<li>Not sure about stability and reliability.</li>\n<li>Not sure about how easy is it to backup and restore.</li>\n</ol>\n\n<p>Questions:</p>\n\n<ol>\n<li>Shall we chose MongoDB if half of data is schemaless, and is being stored as JSON if using MySQL?</li>\n<li><p>Some of the data like main posts is critical, so it will be saved using safe writes, the counters etc\nwill be saved using unsafe writes. Is this policy based on importance of data, and write intensiveness correct? </p></li>\n<li><p>How easy is it to monitor, backup and restore MongoDB as compared to MySQL? We need to plan periodic backups ( say daily ), and restore them with ease in case of disaster. What are the best options I have with MongoDB to make it a safe bet for the application.</p></li>\n</ol>\n\n<p>Stability, backup, snapshots, restoring, wider adoption I.e.database durability are the reasons pointing me\nto use MySQL as RDBMS+NoSql even though a NoSQL document storage could serve my purpose better.</p>\n\n<p>Please focus your views on the choice between MySQL and MongoDB considering the database design I have in mind. I know there could be better ways to plan database design with either RDBMS or MongoDB documents. But that is not the current focus of my question.</p>\n    ","a":"\n<p>So, to directly answer the questions...</p>\n\n<blockquote>\n  <p>Shall we chose mongodb if half of data is schemaless, and is being stored as JSON if using MySQL?</p>\n</blockquote>\n\n<p>Schemaless storage is certainly a compelling reason to go with MongoDB, but as you've pointed out, it's fairly easy to store JSON in a RDBMS as well. The power behind MongoDB is in the rich queries against schemaless storage. </p>\n\n<p>If I might point out a small flaw in the illustration about updating a JSON field, it's not simply a matter of getting the current value, updating the document and then pushing it back to the database. The process must all be wrapped in a transaction. Transactions tend to be fairly straightforward, until you start denormalizing your database. Then something as simple as recording an upvote can lock tables all over your schema. </p>\n\n<p>With MongoDB, there are no transactions. But operations can almost always be structured in a way that allow for atomic updates. This usually involves some dramatic shifts from the SQL paradigms, but in my opinion they're fairly obvious once you stop trying to force objects into tables. At the very least, lots of other folks have run into the same problems you'll be facing, and the Mongo community tends to be fairly open and vocal about the challenges they've overcome.</p>\n\n<blockquote>\n  <p>Some of the data like main posts is critical , so it will be saved using safe writes , the counters etc will be saved using unsafe writes. Is this policy based on importance of data, and write intensiveness correct? </p>\n</blockquote>\n\n<p>By \"safe writes\" I assume you mean the option to turn on an automatic \"getLastError()\" after every write. We have a very thin wrapper over a DBCollection that allows us fine grained control over when getLastError() is called. However, our policy is not based on how \"important\" data is, but rather whether the code following the query is expecting any modifications to be immediately visible in the following reads.</p>\n\n<p>Generally speaking, this is still a poor indicator, and we have instead migrated to findAndModify() for the same behavior. On the occasion where we still explicitly call getLastError() it is when the database is likely to reject a write, such as when we insert() with an _id that may be a duplicate.</p>\n\n<blockquote>\n  <p>How easy is it to monitor,backup and restore Mongodb as compared to mysql? We need to plan periodic backups (say daily), and restore them with ease in case of disaster. What are the best options I have with mongoDb to make it a safe bet for the application?</p>\n</blockquote>\n\n<p>I'm afraid I can't speak to whether our backup/restore policy is effective as we have not had to restore yet. We're following the MongoDB recommendations for backing up; @mark-hillick has done a great job of summarizing those. We're using replica sets, and we have migrated MongoDB versions as well as introduced new replica members. So far we've had no downtime, so I'm not sure I can speak well to this point.</p>\n\n<blockquote>\n  <p>Stability,backup,snapshots,restoring,wider adoption i.e.database durability are the reasons pointing me to use MySQL as RDBMS+NoSql even though a NoSQL document storage could serve my purpose better.</p>\n</blockquote>\n\n<p>So, in my experience, MongoDB offers storage of schemaless data with a set of query primitives rich enough that transactions can often be replaced by atomic operations. It's been tough to unlearn 10+ years worth of SQL experience, but every problem I've encountered has been addressed by the community or 10gen directly. We have not lost data or had any downtime that I can recall. </p>\n\n<p>To put it simply, MongoDB is hands down the best data storage ecosystem I have ever used in terms of querying, maintenance, scalability, and reliability. Unless I had an application that was so clearly relational that I could not in good conscience use anything other than SQL, I would make every effort to use MongoDB.</p>\n\n<p>I don't work for 10gen, but I'm very grateful for the folks who do.</p>\n    "},{"t":"Which NoSQL DB is best fitted for OLTP financial systems?","l":"http://stackoverflow.com/questions/6209834/which-nosql-db-is-best-fitted-for-oltp-financial-systems","q":"\n\n<p>We're designing an OLTP financial system. it should be able to support 10.000 transactions per second and have reporting features.</p>\n\n<p>So we have come to the idea of using:</p>\n\n<ul>\n<li>a NoSQL DB as our main storage</li>\n<li>a MySQL DB (Percona server actually) making some ETLs from the NoSQL DB for store reporting data</li>\n</ul>\n\n<p>We're considering MongoDB and Riak for the NoSQL job. we have read that Riak scales more smoothly than MongoDB. And we would like to listen your opinion.</p>\n\n<ul>\n<li><strong>Which NoSQL DB would you use for a\nOLTP financial system?</strong></li>\n<li><strong>How has been\nyour experience scaling MongoDB/Riak?</strong></li>\n</ul>\n    ","a":"\n<p>There is no conceivable circumstance where I would use a NOSQl database for anything to do with finance. You simply don't have the data integrity needed or the internal controls.  Dow Jones uses SQL Server to do its transactions and if they can properly design a high performance, high transaction Relational datbase so can you. You will have to invest in some people who know what they are doing though.</p>\n    "},{"t":"MongoDB nested array query","l":"http://stackoverflow.com/questions/17303833/mongodb-nested-array-query","q":"\n\n<p>I've asked this as a comment on <a href=\"http://stackoverflow.com/questions/5250652/query-a-nested-array-in-mongodb\" title=\"another\">another</a> question, and also posted a <a href=\"https://groups.google.com/forum/#!searchin/mongodb-user/advertised/mongodb-user/SetYxmdvjTE/NiKGhu6cCKsJ\">question</a> on mongodb-user. No responses so far, so I'm resorting to asking a separate question. </p>\n\n<p>The <a href=\"http://docs.mongodb.org/manual/reference/operator/in/#op._S_in\">documentation</a> states:</p>\n\n<blockquote>\n  <p>If the field holds an array, then the $in operator selects the\n  documents whose field holds an array that contains at least one\n  element that matches a value in the specified array (e.g. ,\n  , etc.)</p>\n</blockquote>\n\n<p>I'm using:</p>\n\n<pre><code>mongod --version:\ndb version v2.2.2, pdfile version 4.5\nThu May 30 12:19:12 git version: d1b43b61a5308c4ad0679d34b262c5af9d664267\n\nmongo --version:\nMongoDB shell version: 2.0.4\n</code></pre>\n\n<p>In MongoDB shell:</p>\n\n<pre><code>db.nested.insert({'level1': {'level2': [['item00', 'item01'], ['item10', 'item11']]}})\n</code></pre>\n\n<p>Here's a list of queries that should work according to the documentation, and the results they produce:</p>\n\n<p>Why doesn't this work?</p>\n\n<pre><code>&gt; db.nested.findOne({'level1.level2.0': 'item00'})\nnull\n</code></pre>\n\n<p>Why do I need the $all?</p>\n\n<pre><code>&gt; db.nested.findOne({'level1.level2.0': {'$all': ['item00']}})\n{\n    \"_id\" : ObjectId(\"51a7a4c0909dfd8872f52ed7\"),\n    \"level1\" : {\n        \"level2\" : [\n            [\n                \"item00\",\n                \"item01\"\n            ],\n            [\n                \"item10\",\n                \"item11\"\n            ]\n        ]\n    }\n}\n</code></pre>\n\n<p>At least one of the following should work, right?</p>\n\n<pre><code>&gt; db.nested.findOne({'level1.level2.0': {'$in': ['item00']}})\nnull\n\n&gt; db.nested.findOne({'level1.level2': {'$in': ['item00']}})\nnull\n</code></pre>\n\n<p>Any ideas? We're considering abandoning MongoDB if the query syntax doesn't work as advertised.</p>\n\n<p>Thanks!</p>\n    ","a":"\n<p>Use nested <code>elemMatch</code> to search nested levels within arrays.</p>\n\n<p>Details <a href=\"http://stackoverflow.com/questions/12629692/querying-an-array-of-arrays-in-mongodb\">here</a>.</p>\n    "},{"t":"Is there a reliable (single server) MongoDB alternative?","l":"http://stackoverflow.com/questions/14962084/is-there-a-reliable-single-server-mongodb-alternative","q":"\n\n<p>I like the idea of document databases, especially MongoDB. It allows for faster development as we don't have to adjust database schema's. However MongoDB doesn't support multi-document transactions and doesn't guarantee that modifications get written to disk immediately like normal databases (I know that you can make the time between flushes quite small, but it's still no guarantee). </p>\n\n<p>Most of our projects are not that big that they need things like multi-server environments. So keeping that in mind. Are there any single server MongoDB-like document databases that support multi-document transactions and reliable flushing to disk? </p>\n    ","a":"\n<p>A very short answer to your specific (but brief) requirements:</p>\n\n<blockquote>\n  <p>Are there any single server MongoDB-like document databases that support multi-document transactions and reliable flushing to disk?</p>\n</blockquote>\n\n<ol>\n<li><p>RavenDB [<a href=\"http://ravendb.net\" rel=\"nofollow\">1</a>] provides support for multi-doc transactions [<a href=\"http://ravendb.net/docs/client-api/advanced/transaction-support\" rel=\"nofollow\">2</a>]. Unfortunately I don't know it handles durability.</p></li>\n<li><p>CouchDB [<a href=\"http://couchdb.apache.org\" rel=\"nofollow\">3</a>] provides durable writes, but no multi-doc transactions</p></li>\n<li><p>RethinkDB [<a href=\"http://www.rethinkdb.com\" rel=\"nofollow\">4</a>] provides durable writes, but no multi-doc transactions.</p></li>\n</ol>\n\n<p>So you might wonder what's different about these 3 solutions? Most of the time is their querying support (I'd say RethinkDB has the most advanced one covering pretty much all types of queries: sub-queries, JOINs, aggregations, etc.), their history (read: production readiness -- here I'd probably say CouchDB is in the lead), their distribution model (you mentioned that's not interesting for you), their licensing (RavenDB: commercial, CouchDB: Apache License, Rethinkdb: AGPL). </p>\n\n<p>The next step would be for you to briefly look over their feature set and figure out which one comes close to your needs and give it a try.</p>\n    "},{"t":"Which database to choose (Cassandra, MongoDB, ?) for storing and querying event / log / metrics data?","l":"http://stackoverflow.com/questions/5594458/which-database-to-choose-cassandra-mongodb-for-storing-and-querying-event","q":"\n\n<p>In sql terms we're storing data like this:</p>\n\n<pre><code>table events (\n  id\n  timestamp\n  dimension1\n  dimension2\n  dimension3\n  etc.\n)\n</code></pre>\n\n<p>All dimension values are integers. This table is becoming very large.</p>\n\n<p>We want stupidly fast reads for queries like this:</p>\n\n<pre><code>SELECT dimension1, dimension2, COUNT(*) \nFROM   events\nWHERE  dimension8 = 'foo'\nAND    dimension9 = 'bar'\nGROUP BY 1, 2\n</code></pre>\n\n<p>We want fast writes, and don't care about transactions and consistency. We care about eventual availability and partition tolerance.</p>\n\n<p>I was looking at \"NoSQL\" alternatives. Can Casandra do the kind of queries I'm looking for?? This isn't immediately obvious from reading their docs... if it can do that, what is it's performance for those types of queries?</p>\n\n<p>Was also looking at MongoDB, but their \"group()\" function has severe limitations as far as I could read (max of 10,000 rows).</p>\n\n<p>Do you have experience with any of these databases, and would you recommend it as a solution to the problem described above? </p>\n\n<p>Are there any other databases I should consider that can do these kind of queries fast?</p>\n\n<p>Cheers,\njimmy</p>\n    ","a":"\n<p>\"Group by\" and \"stupidly fast\" do not go together.  That's just the nature of that beast... Hence the limitations on Mongo's group operation; Cassandra doesn't even support it natively (although it does for Hive or Pig queries via Hadoop... but those are not intended to be stupidly fast).</p>\n\n<p>Systems like Twitter's Rainbird (which uses Cassandra) doing realtime analytics do it by denormalizing/pre-computing the counts: <a href=\"http://www.slideshare.net/kevinweil/rainbird-realtime-analytics-at-twitter-strata-2011\">http://www.slideshare.net/kevinweil/rainbird-realtime-analytics-at-twitter-strata-2011</a></p>\n    "},{"t":"Saving numpy array in mongodb","l":"http://stackoverflow.com/questions/6367589/saving-numpy-array-in-mongodb","q":"\n\n<p>I have a couple of MongoDB documents wherein one my the fields is best represented as a matrix (numpy array). I would like to save this document to MongoDB, how do I do this?</p>\n\n<pre><code>{\n'name' : 'subject1',\n'image_name' : 'blah/foo.png',\n'feature1' : np.array(...)\n}\n</code></pre>\n    ","a":"\n<p>For a 1D numpy array, you can use lists:</p>\n\n<pre><code># serialize 1D array x\nrecord['feature1'] = x.tolist()\n\n# deserialize 1D array x\nx = np.fromiter( record['feature1'] )\n</code></pre>\n\n<p>For multidimensional data, I believe you'll need to use pickle and pymongo.binary.Binary:</p>\n\n<pre><code># serialize 2D array y\nrecord['feature2'] = pymongo.binary.Binary( pickle.dumps( y, protocol=2) ) )\n\n# deserialize 2D array y\ny = pickle.loads( record['feature2'] )\n</code></pre>\n    "},{"t":"Most efficient way to save way points and do comparisons?","l":"http://stackoverflow.com/questions/12568726/most-efficient-way-to-save-way-points-and-do-comparisons","q":"\n\n<p>I would like to know your opinion. I created an application, where users create routes and we track this route and save all the way points in the database. Then, the application does comparisons of users way points.</p>\n\n<p>Currently, I use a <code>MSSQL</code> Server, using two tables, one for Routes and the other for storing way points (with spatial data type). The comparisons are made in a stored procedure using SQL Server geographic functions such as st_distance...</p>\n\n<p>I have investigated other options. One that I implemented is with Oracle 11g using objects. I store all data in only one Object Table, and the way points are stored in a Varray of a type with Latitude and Longitude attributes. This way is very efficient saving and retrieving data, but gets some complicated when comparing.</p>\n\n<p>I'm looking for a <code>NoSQL</code> solution, some algorithm or method to do this efficiently. What do you think?</p>\n    ","a":"\n<p>Using database functions like <a href=\"http://msdn.microsoft.com/en-us/library/bb933808.aspx\" rel=\"nofollow\">STDistance</a> for all n records is suboptimal. Your CPU overhead will increase exponentially.</p>\n\n<p>What you should do is check for the amount of points within a rectangle around the current epicenter you are searching. Here's an example (in MySQL):</p>\n\n<pre><code>SELECT * FROM `points`\n    WHERE `latitude` &gt;= X1 AND `latitude` &lt;= X2\n    AND `longitude` &gt;= Y1 AND `longitude` &lt;= Y2\n</code></pre>\n\n<p>This provides a reduced <code>superset</code> of points that should then be further reduced by calculating the orthodromic distance (with respect to the curvature of the Earth) using the <a href=\"http://en.wikipedia.org/wiki/Haversine_formula\" rel=\"nofollow\">Haversine formula</a>.</p>\n\n<p><strong>Don't forget</strong> to set up a <a href=\"http://dev.mysql.com/doc/refman/5.0/en/multiple-column-indexes.html\" rel=\"nofollow\">composite index</a> on <code>latitude</code> and <code>longitude</code>.</p>\n\n<p><img src=\"http://i.stack.imgur.com/d78lX.jpg\" alt=\"Orthodromic distance\"></p>\n\n<p>Here it is in PHP:</p>\n\n<pre><code>&lt;?php\nfunction haversine($latitude1, $longitude1,\n                   $latitude2, $longitude2, $unit = 'Mi') {\n    $theta = $longitude1 - $longitude2;\n    $distance = (sin(deg2rad($latitude1)) * sin(deg2rad($latitude2))) +\n    (cos(deg2rad($latitude1)) * cos(deg2rad($latitude2)) * cos(deg2rad($theta)));\n    $distance = acos($distance);\n    $distance = rad2deg($distance);\n    $distance = $distance * 60 * 1.1515;\n    switch ($unit) {\n    case 'Mi':\n        break;\n    case 'Km':\n        $distance = $distance * 1.609344;\n    }\n    return (round($distance, 2));\n}\n?&gt;\n</code></pre>\n\n<p><strong>To recap</strong>:</p>\n\n<p>Here's an example image illustrating what to do:</p>\n\n<p><img src=\"http://i.stack.imgur.com/gg7Zp.png\" alt=\"Example with CN Tower\"></p>\n\n<p>The first search would involve a bounding box collision search (MySQL example) to determine the <code>superset</code>, excluding the red points. The second verification process would involve calculating if the points are within an appropriate orthodromic distance with the Haversine formula (PHP example) and taking a <code>subset</code> (composed of the black points).</p>\n    "},{"t":"Would relational databases scale as well (or better) than their NoSQL counterparts if we drop the relationships?","l":"http://stackoverflow.com/questions/8695008/would-relational-databases-scale-as-well-or-better-than-their-nosql-counterpar","q":"\n\n<p>Disclaimer: This is a broad question, so it could be moved to a different source (if the admins find it appropriate). </p>\n\n<p>All the cool kids seem to be dropping relational databases in favor of their NoSQL counterparts. Everyone will have their reasons, from scaling issues to simply being on the bleeding edge of tech. And, I am not here to question their motives.</p>\n\n<p>However, what I am interested in is whether any NoSQL transitions ever validated the performance (maintenance) gains over a traditional RDBMS when relationships were dropped. Why would we want to use a RDBMS when the core reason it exists is dropped? A few reasons come to mind</p>\n\n<ol>\n<li>30+ years of academic and work research in developing these systems</li>\n<li>A well-known language in Structured Query Language (SQL).</li>\n<li>Stable and mature ORM support across technologies (Hibernate, ActiveRecord)</li>\n</ol>\n\n<p>Clearly, in the modern world where horizontal scaling is important, there is a need to make sure that shards are fault tolerant, updated within the time intervals required by the app, etc. However, those needs shouldn't necessarily be the responsibility of a system that stores data (case in point: ZooKeeper).</p>\n\n<p>Also, I acknowledge that research should be dedicated to NoSQL and that time spent in this arena will clearly lead to better more internet worthy technologies. However, a comparison of sorts between NoSQL and traditional RDBMS offerings (minus relationships) would be useful in making business decisions.</p>\n\n<p><em><strong>UPDATE 1</strong></em>: When I refer to NoSQL databases, I am talking about data stores that may not require fixed table schemas and usually avoid join operations. Hence, the emphasis in the question on dropping the relationships in a traditional SQL RDBMS</p>\n    ","a":"\n<p>I don't find that inter-table relationships are the main limiter for scalability.  I use queries with joins regularly and get good scalability if indexes are defined well.</p>\n\n<p>The greater limiter for scalability is the cost of synchronous I/O.  The requirements of consistency and durability -- that the DBMS <em>actually</em> and reliably saves data when it tells you it saved data -- is expensive.</p>\n\n<p>Several NoSQL products that are currently in vogue achieve great performance by weakening their consistency and durability guarantees in their default configuration.  There are many reports of CouchDB or MongoDB losing data.</p>\n\n<p>There are ways you can configure those NoSQL products to be more strict about durability, but then you sacrifice their impressive performance numbers.</p>\n\n<p>Likewise, you can make an SQL database achieve high performance like the NoSQL products, by disabling the default features that ensure data safety.  See <a href=\"http://it.toolbox.com/blogs/database-soup/runningwithscissorsdb-39879\">RunningWithScissorsDB</a>.</p>\n\n<p>PS: If you think document-oriented databases are \"cutting edge\", I invite you to read about <a href=\"http://en.wikipedia.org/wiki/MUMPS\">MUMPS</a>.  Everything old is new again.  :-)</p>\n    "},{"t":"How does Amazon.com function with a key-value datastore?","l":"http://stackoverflow.com/questions/1997069/how-does-amazon-com-function-with-a-key-value-datastore","q":"\n\n<p>I have heard that Amazon uses a key-value data store - that it does not use a traditional relational normalized db.  Speaking as someone who only has used the traditional approach, how does this work?  Don't you need to do the following?</p>\n\n<p><code>select * from book where book_id = n</code> </p>\n\n<p>Or a: </p>\n\n<p><code>select * from book where author_id = y</code></p>\n\n<p>How can you build a site/app with so much data and so many relationships without a normalized db?</p>\n    ","a":"\n<p>The Amazon.com architecture is very interesting. They moved to a service oriented architecture, if you look at all the different content areas on their site, each one is served by a different service. So there is a 'wish list' service and a 'Related to Items You've Viewed' service, and Bestsellers service, Shopping cart service, etc. </p>\n\n<p>Each of the services has its own set of requirements and features. The requirements include things like response time and availability. Internally each service is implemented using whatever database best suits the needs. The key value store is good for a shopping cart, because you never need to do:</p>\n\n<pre><code>select * from book where book_id = n\n</code></pre>\n\n<p>on a shopping cart.</p>\n\n<p>One of the important things to realize is the enormous role that availability plays at Amazon scale. Consider that Amazon 2008 revenue was $19.166 billion. The total retail revenue from from the Amazon.com site may be more than $1000 per second during the day (it may be double that, for all I know, during peak hours. It could be 5 times that during peak holiday shopping). Think of the cost if the shopping cart service goes down for 3 minutes during peak usage. It is clear that the loss would be a large dollar value in abandon carts.</p>\n\n<p>Using a key-value store doesn't mean embracing rampant data duplication, it means redesigning applications so the necessary data doesn't need sit all in one monolithic database.</p>\n\n<p>Amazon is really more of a platform for applications than anything else. Here is a <a href=\"http://www.infoq.com/presentations/vogels-amazon-platform\" rel=\"nofollow\">video of Amazon's CTO</a> talking about just that.</p>\n    "},{"t":"Why don't you start off with a â€œsingle & smallâ€ Cassandra server as you usually do it with MySQL?","l":"http://stackoverflow.com/questions/18462530/why-dont-you-start-off-with-a-single-small-cassandra-server-as-you-usually","q":"\n\n<p>For any website just starting out, the load initially is minimal &amp; grows with a  slow pace initially. People usually start with their MySQL based sites with a single server(***that too a VPS not a dedicated server) running as both app server as well as DB server &amp; usually get too far with this setup &amp; only as they feel the need they separate the DB from the app server giving it a separate VPS server. This is what a start up expects the things to be while planning about resources procurement.</p>\n\n<p>But so far what I have seen, it's something very different with Cassandra. People usually recommend starting out with atleast a 3 node cluster, (on dedicated servers) with lots &amp; lots of RAM. 4GB or 8GB RAM is what they suggest to start with. So is it that Cassandra requires more hardware resources in comparison to MySQL,  for a website to deliver similar performance, serve similar load/ traffic &amp; same amount of data. I understand about higher storage requirements of Cassandra due to replication but what about other hardware resources ? </p>\n\n<p>Can't we start off with Cassandra based apps just like MySQL. Starting with 1 or 2 VPS &amp; adding more whenever there's a need ?</p>\n\n<h1>Edit:</h1>\n\n<p>I don't want to compare apples with oranges. I just want to know how much more dangerous situation I may be in when I start out with a single node VPS based cassandra installation Vs a single node VPS based MySQL installation. Difference between these two situations. Are cassandra servers more prone to be unavailable than MySQL servers ? What is bad if I put tomcat too along with Cassandra as people use LAMP stack on single server.</p>\n    ","a":"\n<p><strong>TL;DR;</strong><br>\nYou can even start with a single node, but you loose the <em>highly available</em> factor of c*.</p>\n\n<p>Cassandra is built for systems that handle huge volumes of data, terabytes and <a href=\"http://www.datastax.com/wp-content/uploads/2012/08/C2012-BuyItNow-JayPatel.pdf\">in some cases petabyte</a>s. Many users typically switch from MySQL (and lots of other RDBMS) to Cassandra once they find that their current DB system can't handle the data load efficiently (querying gets slow, managing storage becomes challenging etc.) </p>\n\n<p><br></p>\n\n<h3>Why 4-8GB gb of ram?</h3>\n\n<p>The 4-8 GB of ram is to do with the JVM and the size of ram on efficient garbage collection. The advice is stating not that you should start on 8 GB, but hat <a href=\"http://www.datastax.com/docs/1.1/operations/tuning#heap-sizing\">you shouldn't have more than 8GB</a> </p>\n\n<p>This doesn't mean to say that you cant use Cassandra to start up a single node on a very basic machine (some people actually have cassandra running on a <a href=\"http://www.slideshare.net/acobley/data-stax-cassandrasummit2013cassandraraspberrypirc1\">raspberry pi</a>).</p>\n\n<p><br></p>\n\n<h3>Why do people recommend 3 nodes?</h3>\n\n<p><strong>Availability</strong> is one of cassandra's main selling points. If you have 2 nodes with <code>RF</code>=2 then you cant perform writes if a single node goes down. If you have 3 nodes you can still perform both reads and writes.</p>\n    "},{"t":"Quick Sending of 4[GB] To Be Processed From 100 Machines?","l":"http://stackoverflow.com/questions/6062396/quick-sending-of-4gb-to-be-processed-from-100-machines","q":"\n\n<p>I have <code>100 servers</code> in my cluster.</p>\n\n<p>At time <code>17:35:00</code>, all <code>100 servers</code> are provided with data (of size <code>1[MB]</code>). Each server processes the data, and produces an output of about <code>40[MB]</code>. The processing time for each server is <code>5[sec]</code>.</p>\n\n<p>At time <code>17:35:05</code> (<code>5[sec] later</code>), there's a need for a <em>central machine</em> to read all the output from all <code>100 servers</code> (remember, the total size of data is: 100 [machines] x 40 [MB] ~ 4[GB]), aggregate it, and produce an output. </p>\n\n<p>It is of <strong>high importance</strong> that the entire process of <code>gathering the 4[GB] data</code> from all <code>100 servers</code> takes <em>as little time as possible</em>. How do I go about solving this problem? </p>\n\n<p>Are there any existing tools (ideally, in <code>python</code>, but would consider other solutions)  that can help?</p>\n    ","a":"\n<p>Look at the flow of data in your application, and then look at the data rates that your (I assume shared) disk system provides and the rate your GigE interconnect provides, and the topology of your cluster. Which of these is a bottleneck?</p>\n\n<p>GigE provides theoretical maximum 125 MB/s transmission rate between nodes - thus 4GB will take ~30s to move 100 40MB chunks of data into your central node from the 100 processing nodes over GigE.</p>\n\n<p>A file system shared between all your nodes provides an alternative to over-Ethernet RAM to RAM data transfer. </p>\n\n<p>If your shared file system is fast at the disk read/write level (say: a bunch of many-disk RAID 0 or RAID 10 arrays aggregated into a Lustre F/S or some such) and it uses 20Gb/s or 40 Gb/s interconnect btwn block storage and nodes, then 100 nodes each writing a 40MB file to disk and the central node reading those 100 files may be faster than transferring the 100 40 MB chunks over the GigE node to node interconnect.</p>\n\n<p>But if your shared file system is a RAID 5 or 6 array exported to the nodes via NFS over GigE Ethernet, that will be slower than RAM to RAM transfer via GigE using RPC or MPI because you have to write and read the disks over GigE anyway.</p>\n\n<p>So, there have been some good answers and discussion or your question. But we do (did) not know your node interconnect speed, and we do not know how your disk is set up (shared disk, or one disk per node), or whether shared disk has it's own interconnect and what speed that is.</p>\n\n<p>Node interconnect speed is now known. It is no longer a free variable.</p>\n\n<p>Disk set up (shared/not-shared) is unknown, thus a free variable.</p>\n\n<p>Disk interconnect (assuming shared disk) is unknown, thus another free variable.</p>\n\n<p>How much RAM does your central node have is unknown (can it hold 4GB data in RAM?) thus is a free variable.</p>\n\n<p>If everything including shared disk uses the same GigE interconnect then it is safe to say that 100 nodes each writing a 40MB file to disk and then the central node reading 100 40MB files from disk is the slowest way to go. Unless your central node cannot allocate 4GB RAM without swapping, in which case things probably get complicated.</p>\n\n<p>If your shared disk is high performance it may be the case that it is faster for 100 nodes to each write a 40MB file, and for the central node to read 100 40MB files. </p>\n    "},{"t":"Is Cassandra suitable to use as a primary data store?","l":"http://stackoverflow.com/questions/1849204/is-cassandra-suitable-to-use-as-a-primary-data-store","q":"\n\n<p>I'm evaluating a storage platform for an upcoming project and keep coming back to Cassandra. For this project loosing <em>any</em> amount of data is unacceptable. So far we've used a relational database (Microsoft SQL Server), but the data is so varied and large that it has become an issue to store and query.</p>\n\n<p>Is Cassandra robust enough to use as a primary data store? Or should it only be used to mirror existing data to speed up access?</p>\n    ","a":"\n<p>Anecdotally: yes, Twitter, Digg, Ooyala, SimpleGeo, Mahalo, and others are using or moving to Cassandra for a primary data store (<a href=\"http://n2.nabble.com/Cassandra-users-survey-td4040068.html\">http://n2.nabble.com/Cassandra-users-survey-td4040068.html</a>).</p>\n\n<p>Technically: yes; besides supporting replication (including to multiple datacenters), each Cassandra node has an fsync'd commit log to make sure writes are durable; from there writes are turned into SSTables which are immutable until compaction (which combines multiple SSTables to GC old versions).  Snapshotting is supported at any time, including automatic snapshot-before-compaction.</p>\n    "},{"t":"Delphi and NoSQL","l":"http://stackoverflow.com/questions/4828846/delphi-and-nosql","q":"\n\n<p>Have anyone ever used Delphi with NoSQL databases like Mongo, CouchDB or others? Which one would you recommend?</p>\n    ","a":"\n<p>For Mongo, theres <a href=\"https://github.com/stijnsanders/TMongoWire\">TMongoWire</a> and <a href=\"http://code.google.com/p/pebongo/\">pebongo</a> (early stages). For couchDB, I believe one would interact mainly through HTTP/JSON\nFor Cassandra, I believe the best bet would be to somehow incorporate a supported language inside your Delphi app and use that to interact with Cassandra, or else implement a web service in a supported language and make it accessible to your Delphi application.</p>\n\n<p>Hope it helps.</p>\n    "},{"t":"What is the best api/library for Java to use Cassandra? [closed]","l":"http://stackoverflow.com/questions/3232842/what-is-the-best-api-library-for-java-to-use-cassandra","q":"\n\n<p>I'm looking for an API with the following requirements</p>\n\n<ol>\n<li>It's simple to use and is concise. It is not bloated.</li>\n<li>Works with Spring way of doing things, or is at least easy to make it work with Spring</li>\n<li>Has a Maven repository, preferably it's already in the main repositories</li>\n<li>Is production-tested, meaning a fair number of people are using it in production applications.</li>\n</ol>\n\n<p>Help? Thanks!</p>\n    ","a":"\n<p><a href=\"http://github.com/rantav/hector\" rel=\"nofollow\">Hector</a> and <a href=\"http://code.google.com/p/pelops/\" rel=\"nofollow\">Pelops</a> are, as far as I know, the two that are most widely used (4). I dont think any of the two are mavenized (3).  Both should work in a Spring framework environment(2). Your first criteria might be a little bit subjective. I dont find any of these two bloated. You might do, if you do, please tell me.</p>\n    "},{"t":"MongoDB C# Driver Unable to Find by Object ID?","l":"http://stackoverflow.com/questions/2453513/mongodb-c-sharp-driver-unable-to-find-by-object-id","q":"\n\n<p>Using MongoDB C# driver (http://github.com/samus/mongodb-csharp), seems that I'm unable to get the data by ObjectId. Below the command that I'm using: </p>\n\n<pre><code>var spec = new Document { { \"_id\", id } };\nvar doc = mc.FindOne(spec);\n</code></pre>\n\n<p>I also tried this:</p>\n\n<pre><code>var spec = new Document { { \"_id\", \"ObjectId(\\\"\" + id + \"\\\")\" } };\nvar doc = mc.FindOne(spec);\n</code></pre>\n\n<p>Both return nothing. Meanwhile, if I query it from the mongo console, it returns the expected result.</p>\n\n<p>My question is, does that driver actually support the lookup by ObjectId?</p>\n\n<p>Thanks..</p>\n    ","a":"\n<p>It does support fetching by object ID.  Your id variable should be an Oid.  Is it the correct type?</p>\n\n<p>Here is a complete program that will</p>\n\n<ul>\n<li>Connect to Mongo</li>\n<li>Insert a document</li>\n<li>Fetch the document back using its ID</li>\n<li>Print the document's details.</li>\n</ul>\n\n<pre><code>\n// Connect to Mongo\nMongo db = new Mongo();\ndb.Connect();\n\n// Insert a test document\nvar insertDoc = new Document { { \"name\", \"my document\" } };\ndb[\"database\"][\"collection\"].Insert(insertDoc);\n\n// Extract the ID from the inserted document, stripping the enclosing quotes\nstring idString = insertDoc[\"_id\"].ToString().Replace(\"\\\"\", \"\");\n\n// Get an Oid from the ID string\nOid id = new Oid(idString);\n\n// Create a document with the ID we want to find\nvar queryDoc = new Document { { \"_id\", id } };\n\n// Query the db for a document with the required ID \nvar resultDoc = db[\"database\"][\"collection\"].FindOne(queryDoc);\ndb.Disconnect();\n\n// Print the name of the document to prove it worked\nConsole.WriteLine(resultDoc[\"name\"].ToString());\n</code></pre>\n    "},{"t":"How to model Student/Classes with DynamoDB (NoSQL)","l":"http://stackoverflow.com/questions/9179605/how-to-model-student-classes-with-dynamodb-nosql","q":"\n\n<p>I'm trying to get my way with DynamoDB and NoSQL.</p>\n\n<p>What is the best (right?) approach for modeling a student table and class tables with respect to the fact that I need to have a student-is-in-class relationship.\nI'm taking into account that there is no second-index available in DynamoDB.</p>\n\n<p>The model needs to answer the following questions:</p>\n\n<p>Which students are in a specific class?</p>\n\n<p>Which classes a student take?</p>\n\n<p>Thanks</p>\n    ","a":"\n<p>A very simple suggestion (without range keys) would be to have two tables: One per query type. This is not unusual in NoSQL databases.</p>\n\n<p>In your case we'd have:</p>\n\n<ul>\n<li>A table <code>Student</code> with attribute <code>StudentId</code> as (hash type) primary key. Each item might then have an attribute named <code>Attends</code>, the value of which was a list of Ids on classes.</li>\n<li>A table <code>Class</code> with attribute <code>ClassId</code> as (hash type) primary key. Each item might then have an attribute named <code>AttendedBy</code>, the value of which was a list of Ids on students.</li>\n</ul>\n\n<p>Performing your queries would be simple. Updating the database with one \"attends\"-relationship between a student and a class requires two separate writes, one to each table.</p>\n\n<p>Another design would have one table <code>Attends</code> with a hash and range primary key. Each record would represent the attendance of one student to one class. The hash attribute could be the Id of the class and the range key could be the Id of the student. Supplementary data on the class and the student would reside in other tables, then.</p>\n    "},{"t":"Geo spatial index in mongodb with node.js","l":"http://stackoverflow.com/questions/7347686/geo-spatial-index-in-mongodb-with-node-js","q":"\n\n<p>I am finding problem in defining the geo spatial index '2d' as shown\nbelow. Any clue as to what is going wrong ?</p>\n\n<pre><code>var Address = new Schema({\n      loc           : {lat: Number,  lng: Number },\n      Address       : String,\n      create_date       : {type: Date, default: Date.now}\n});\nAddress.index ({\n       loc : \"2d\"\n});\n</code></pre>\n\n<p>It throws error like,</p>\n\n<blockquote>\n  <p>events.js:45\n         throw arguments[1]; // Unhandled 'error' event\n                        ^ Error: <strong>point not in range</strong>    at [object\n  Object]. (/cygdrive/c/Personal/software/ nodejs/NODE/no\n  de_modules/mongoose/node_modules/mongodb/lib/mongodb/db.js:503:20)</p>\n</blockquote>\n\n<p><strong>EDIT: added the code</strong></p>\n\n<pre><code>var Address = new Schema({\n      type              : {type: String, enum: ['Apartment', 'House', 'Serviced Apartment'], default: 'Apartment'}, \n      loc               : {lat: Number,  lng: Number },\n      Address           : String,\n      create_date       : {type: Date, default: Date.now}\n});\n\n/*\nAddress.index ({\n    loc : \"2d\"\n});\n*/\n\nmongoose.connect('mongodb://127.0.0.1:27017/test123', function(err) {\n    if (err) {\n        console.log(\"error in mongo connection\");\n        throw err;\n    }\n    console.log(\"connected to mongo\");\n});\n\nvar RentModel = mongoose.model('Rent', Address);\n\n\n\nsocket = io.listen(app);\n\nsocket.sockets.on('connection', function(client){ \n\n        console.log('inside on connection');\n\n        client.on('register', function(msg){ \n                console.log(\"msg.geometry.type\", msg.geometry.type);\n\n                var rent = new RentModel();\n                rent.type = 'Apartment';\n                rent.loc.lat = 23;\n                rent.loc.lng = 56;\n                rent.Address = \"LLLLLLLLIIIIIIIOOOOOOONNNNNNNN\"\n\n                console.log(\"before save\");\n                rent.save(function(err){\n                    console.log(\"rent.save start\");\n                    if(err) { \n                        throw err; \n                        console.log(\"error in save\");\n                    }\n                    console.log(\"saved\");\n\n                });\n\n            }); \n\n\n            RentModel.find({loc : { $near : [20, 50], $maxDistance: 30 }} , function(err, docs){\n                if (err) {\n                    console.log(\"error in finding near\", err);\n                    throw err;\n                }\n                console.log('docs.length : ' , docs.length);\n                console.log('docs : ',docs)\n            })\n</code></pre>\n    ","a":"\n<p>It's also worth noting that you will want longitude to come before latitude in your array. This will not affect you when you are using 2D, but it will when you are using 3D. Mathematically this makes sense as longitude is the X coordinate and latitude is the Y coordinate (x,y), but most of us are familiar with lat coming before long (and one of the best Mongo books out there has an example with lat before long, but it does not cover 2D).</p>\n\n<p>Ultimately you're likely going to want to use 3D as 2D calculations are not accurate as you move away from the equator.</p>\n    "},{"t":"NoSql Referential Data","l":"http://stackoverflow.com/questions/7591943/nosql-referential-data","q":"\n\n<p><strong>Disclaimer: by referential Data, i do not mean referential integrity</strong></p>\n\n<p>I am learning nosql and would like to understand how data should by modeled.  In a typical relational database for an CMS application, for example, you may have two table: article and author, where article have an reference to the author.  </p>\n\n<p>In nosql system, you may create an article document this way since they are just <a href=\"http://www.jroller.com/dmdevito/entry/thinking_about_nosql_databases_classification\">disguised object graph</a></p>\n\n<pre><code>{\ntitle: \"Learn nosql in 5 minutes\",\nslug: \"nosql_is_easy\", \nauthor: {firstName: \"Smarty\"\n          lastName: \"Pants\"\n}\n\n{\ntitle: \"Death to RDBMS\",\nslug: \"rdbms_sucks\", \nauthor: {firstName: \"Smarty\"\n          lastName: \"Pants\"\n}\n</code></pre>\n\n<p>and so on... </p>\n\n<p>Say one day Mr. Smarty Pants decided to change his name to Regular Joe because nosql has become ubiquitous.  In such uses case, every article would need to be scanned and the author's name updated.</p>\n\n<p>So my questions is, how should the data be modeled in nosql to fit the basic uses cases for an CMS so that <strong>the performance is on par or faster than RDBMS</strong>? <a href=\"http://www.mongodb.org/display/DOCS/Use+Cases\">mongodb</a>, for example, claims CMS as an use-case ...</p>\n\n<p><strong>Edit</strong>:</p>\n\n<p>Few people have already suggesting normalizing the data like:</p>\n\n<pre><code>article \n{\ntitle: \"Death to RDBMS\",\nslug: \"rdbms_sucks\", \nauthor: {id: \"10000001\"}\n}\n\nauthor\n{\nname: \"Big Brother\",\nid: \"10000001\"\n}\n</code></pre>\n\n<p>However, since nosql, by design, lack joins, you would have to use mapreduce-like functions to bring the data together.  If this is your suggestion, please comment on the performance of such operation.</p>\n\n<p><strong>Edit 2:</strong></p>\n\n<p>If you think nosql is not suitable solution for any kind of data that requires referential data, please also explain why.  This would seem to make the use case for nosql rather limited since any reasonable application would contain relational data.</p>\n\n<p><strong>Edit 3:</strong></p>\n\n<p><a href=\"http://www.xaprb.com/blog/2010/03/08/nosql-doesnt-mean-non-relational/\">Nosql doesn't mean non-relational</a></p>\n    ","a":"\n<p>I suppose CouchDB is a NoSQL database, if you say so.</p>\n\n<p>But really, we have <em>general-purpose</em> programming languages, and <em>domain-specific</em> languages. Similarly, CouchDB is a <em>domain-specific database</em>.</p>\n\n<p>I use CouchDB a lot but I really don't care whether it uses SQL or NoSQL. CouchDB is valuable (to me) because the API is 100% HTTP, JSON, and Javascript. You can build web applications with the browser fetching HTML from CouchDB and then querying for data over AJAX. To say this is \"not SQL\" is an understatement!</p>\n\n<p>Anyway, back to Smarty Pants and Regular Joe. Maybe he has 100,000 documents. What if we just updated them all, the hard way? Well, that is a pretty small amount of Javascript.</p>\n\n<pre><code>$.getJSON('/db/_design/cms/_view/by_user?key=Smarty+Pants', {\n  success: function(result) {\n    // Change the name right here, in the result objects.\n    var docs = result.rows.map(function(row) {\n      row.value.firstName = \"Regular\";\n      row.value.lastName = \"Joe\";\n      return row.value;\n    })\n\n    // Store it!\n    $.post('/db/_bulk_docs', {\"docs\":docs}, function() {\n      console.log(\"Done! Renamed Smarty Pants in \" + docs.length + \" documents!\");\n    })\n  }\n})\n</code></pre>\n\n<p>Yes, this technique would get you an F in computer science class. However, I like it. I would write this code <em>in Firebug. In my browser</em>. The rename is not atomic and it has no referential integrity. On the other hand, it would probably complete in a couple of seconds and nobody would care.</p>\n\n<p>You might say CouchDB fails at the buzzwords and benchmarks but aces the school of hard knocks.</p>\n\n<p>P.S. The <code>by_user</code> view is built from map-reduce. In CouchDB, map-reduce is <em>incremental</em> which means it performs like most SQL indexes. All queries finish in a short, predictable (logarithmic) time.</p>\n    "},{"t":"When I remove rows in Cassandra I delete only columns not row keys","l":"http://stackoverflow.com/questions/2981483/when-i-remove-rows-in-cassandra-i-delete-only-columns-not-row-keys","q":"\n\n<p>If I delete every keys in a ColumnFamily in a Cassandra db using <code>remove(key)</code>, then if I use <code>get_range_slices</code>, rows are still there but without columns. How could I remove entire rows?</p>\n    ","a":"\n<p>Just been having the same issue and I found that:</p>\n\n<p>This has been fixed in 0.7\n(https://issues.apache.org/jira/browse/CASSANDRA-1027).\nAnd backported to 0.6.3</p>\n\n<p>This is also relevant:\n<a href=\"https://issues.apache.org/jira/browse/CASSANDRA-494\" rel=\"nofollow\">https://issues.apache.org/jira/browse/CASSANDRA-494</a></p>\n    "},{"t":"MongoDB Schema Design - Real-time Chat","l":"http://stackoverflow.com/questions/2936598/mongodb-schema-design-real-time-chat","q":"\n\n<p>I'm starting a project which I think will be particularly suited to MongoDB due to the speed and scalability it affords.</p>\n\n<p>The module I'm currently interested in is to do with real-time chat.  If I was to do this in a traditional RDBMS I'd split it out into:</p>\n\n<ul>\n<li>Channel (A channel has many users)</li>\n<li>User (A user has one channel but many messages)</li>\n<li>Message (A message has a user)</li>\n</ul>\n\n<p>The the purpose of this use case, I'd like to assume that there will be typically 5 channels active at one time, each handling at most 5 messages per second.</p>\n\n<p>Specific queries that need to be fast:</p>\n\n<ul>\n<li>Fetch new messages (based on an bookmark, time stamp maybe, or an incrementing counter?)</li>\n<li>Post a message to a channel</li>\n<li>Verify that a user can post in a channel</li>\n</ul>\n\n<p>Bearing in mind that the document limit with MongoDB is 4mb, how would you go about designing the schema?  What would yours look like?  Are there any gotchas I should watch out for?</p>\n    ","a":"\n<p>I used <a href=\"http://code.google.com/p/redis/\" rel=\"nofollow\">Redis</a>, NGINX &amp; PHP-FPM for my chat project. Not super elegant, but it does the trick. There are a few pieces to the puzzle.</p>\n\n<ol>\n<li><p>There is a very simple PHP script that receives client commands and puts them in one massive LIST. It also checks all room LISTs and the users private LIST to see if there are messages it must deliver. This is polled by a client written in jQuery &amp; it's done every few seconds.</p></li>\n<li><p>There is a command line PHP script that operates server side in an infinite loop, 20 times per second, which checks this list and then processes these commands. The script handles who is in what room and permissions in the scripts memory, this info is not stored in Redis. </p></li>\n<li><p>Redis has a LIST for each room &amp; a LIST for each user which operates as a private queue. It also has multiple counters for each room the user is in. If the users counter is less than the total messages in the room, then it gets the difference and sends it to the user.</p></li>\n</ol>\n\n<p>I haven't been able to stress test this solution, but at least from my basic benchmarking it could probably handle many thousands of messages per second. There is also the opportunity to port this over to something like Node.js to increase performance. Redis is also maturing and has some interesting features like Pub/Subscribe commands, which might be of interest, that would possibly remove the polling on the server side possibly.</p>\n\n<p>I looked into Comet based solutions, but many of them were complicated, poorly documented or would require me learning an entirely new language(e.g. Jetty-&gt;Java, APE-&gt;C),etc... Also delivery and going through proxies can sometimes be an issue with Comet. So that is why I've stuck with polling.</p>\n\n<p>I imagine you could do something similar with MongoDB. A collection per room, a collection per user &amp; then a collection which maintains counters. You'll still need to write a back-end daemon or script to handle manging where these messages go. You could also use MongoDB's \"limited collections\", which keeps the documents sorted &amp; also automatically clears old messages out, but that could be complicated in maintaining proper counters.</p>\n    "},{"t":"Android + NoSQL","l":"http://stackoverflow.com/questions/6941543/android-nosql","q":"\n\n<p>I am developing an app which is supposed to send data to a MySQL DB in a remote server so as to be later displayed in a webpage that grabs the data from that server, and I was wondering if it's possible to use instead MySQL some NoSQL solution?</p>\n\n<p>I have been reading about CouchDB and MongoDB but I still don't have very clear if I could use them for my purposes, as for example with MongoDB, I have to install the app on the Android phone and I still have no clue if I can install it in a remote server.</p>\n\n<p>Thanks a lot in advance!</p>\n    ","a":"\n<p><a href=\"http://www.couchbase.com/products-and-services/couchbase-mobile\" rel=\"nofollow\">CouchBase Mobile</a> is probably what you are looking for. I don't think there is an equivalent solution for MongoDB yet, and it's not really what it is designed for anyway.</p>\n\n<p>EDIT: But what is wrong with the MySQL option?</p>\n    "},{"t":"Need Advice: Is this a good use case for a 'NoSQL' Database? If so, which one?","l":"http://stackoverflow.com/questions/3789458/need-advice-is-this-a-good-use-case-for-a-nosql-database-if-so-which-one","q":"\n\n<p>I have recently been researching NoSql options. My scenario is as follows:</p>\n\n<p>We collect and store data from custom hardware at remote locations around the world. We record data from every site every 15 minutes. We would eventually like to move to every 1 minute. Each record has between 20 and 200 measurements. Once set up the hardware records and reports the same measurements every time.</p>\n\n<p>The biggest issue we are facing is that we get a different set of measurements from every project. We measure about 50-100 different measurement types, however any project can have any number of each type of measurement. There is no preset set of columns that can accommodate the data. Because of this we create and build each projects data table with the exact columns it needs as we set up and configure the project on the system.</p>\n\n<p>We provide tools to help analyze the data. This typically includes more calculations and data aggregation, some of which we also store.</p>\n\n<p>We are currently using a mysql database with a table for each client. There are no relations between tables.</p>\n\n<p>NoSql seems promising because we could store a project_id, timestamp then the rest would not be preset. This means one table, more relationships in the data, yet still handling the variety of measurements.</p>\n\n<p>Is a 'NoSql' solution right for this job? If so which ones?</p>\n\n<p>I have been investigation MongoDB and it seems promising...</p>\n\n<p>Example for Clarification:</p>\n\n<p>Project 1 has 5 data points recorded, the mysql table columns look like:\ntimestamp, temp, wind speed, precipitation, irradiance, wind direction</p>\n\n<p>Project 2 has 3 data points recorded mysql table columns:\ntimestamp, temp, irradiance, temp2</p>\n    ","a":"\n<p>The simple answer is that there is no simple answer to these sort of problems, the only way to find out what works for your scenario is to invest R&amp;D time into it. </p>\n\n<p>The question is hard to answer because the performance requirements aren't spelled out by the OP. It appears to be  75M/year records over a number of customers with a write rate of num_customers*1minute (which is low), but I don't have figures for the required read / query performance. </p>\n\n<p>Effectively you have already a <a href=\"http://en.wikipedia.org/wiki/Shard_%28database_architecture%29\" rel=\"nofollow\">sharded</a> database using <a href=\"http://en.wikipedia.org/wiki/Partition_%28database%29\" rel=\"nofollow\">horizontal partitioning</a> because you're storing each customer in a seperate table. This is good and will increase performance. However you haven't yet established that you have a performance problem, so this needs to be measured and the problem size assessed before you can fix it.</p>\n\n<p>A NoSQL database is indeed a good way of fixing performance problems with traditional RDBMS, but it will not provide automatic scalabity and is not a general solution. You need to find your performance problem fix and then design the (nosqL) data model to provide the solution.</p>\n\n<p>Depending on what you're trying to achieve I'd look at <a href=\"http://www.mongodb.org/\" rel=\"nofollow\">MongoDB</a>, <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra</a>, <a href=\"http://hbase.apache.org/\" rel=\"nofollow\">Apache HBase</a> or <a href=\"http://hibari.sourceforge.net/\" rel=\"nofollow\">Hibari</a>.</p>\n\n<p>Remember that NoSQL is a vague term typically encompassing</p>\n\n<ul>\n<li>Applications that are either performance intensive in read or write. Often sacrificing read or write performance at the expense of the other.</li>\n<li>Distribution and scalability</li>\n<li>Different methods of persistency (RAM/Disk)</li>\n<li>A more structured/defined access pattern making ad-hoc queries harder.</li>\n</ul>\n\n<p>So, in the first instance I'd see if a traditional RDBMS can achieve the required performance, using all available techniques, get a copy of <a href=\"http://rads.stackoverflow.com/amzn/click/0596101716\" rel=\"nofollow\">High Performance MySQL</a> and read <a href=\"http://www.mysqlperformanceblog.com/\" rel=\"nofollow\">MySQL Performance Blog</a>.</p>\n\n<h2>Rev1:</h2>\n\n<p>In light of your comments I think it is fair to say that you could achieve what you want with one of the above NOSQL engines. </p>\n\n<p>My primary recommendation would be to get your data model designed and implemented, what you're using at the moment isn't really right.</p>\n\n<p>So look at <a href=\"http://en.wikipedia.org/wiki/Entity-attribute-value_model\" rel=\"nofollow\">Entity-attribute-value model</a> as I think it is exactly right for what you need. </p>\n\n<p>You need to get your data model right before you can consider which technology to use, being honest modifying schemas dynamically isn't a datamodel.</p>\n\n<p>I'd use a traditional SQL database to validate and test the new datamodel as the management tools are better and it's generally easier to work with the schemas as you refine the datamodel.</p>\n    "},{"t":"Fastest way to update (populate) 1,000,000 records into a database using .NET","l":"http://stackoverflow.com/questions/17281412/fastest-way-to-update-populate-1-000-000-records-into-a-database-using-net","q":"\n\n<p>I am using this code to insert 1 million records into an empty table in the database.  Ok so without much code I will start from the point I have already interacted with data, and read the schema into a <code>DataTable</code>:</p>\n\n<p>So: </p>\n\n<pre><code>DataTable returnedDtViaLocalDbV11 = DtSqlLocalDb.GetDtViaConName(strConnName, queryStr, strReturnedDtName);\n</code></pre>\n\n<p>And now that we have <code>returnedDtViaLocalDbV11</code> lets create a new <code>DataTable</code> to be a clone of the source database table:</p>\n\n<pre><code>DataTable NewDtForBlkInsert = returnedDtViaLocalDbV11.Clone();\n\nStopwatch SwSqlMdfLocalDb11 = Stopwatch.StartNew();\nNewDtForBlkInsert.BeginLoadData();\n\nfor (int i = 0; i &lt; 1000000; i++)\n{\n   NewDtForBlkInsert.LoadDataRow(new object[] { null, \"NewShipperCompanyName\"+i.ToString(), \"NewShipperPhone\" }, false);\n}\nNewDtForBlkInsert.EndLoadData();\n\nDBRCL_SET.UpdateDBWithNewDtUsingSQLBulkCopy(NewDtForBlkInsert, tblClients._TblName, strConnName);\n\nSwSqlMdfLocalDb11.Stop();\n\nvar ResSqlMdfLocalDbv11_0 = SwSqlMdfLocalDb11.ElapsedMilliseconds;\n</code></pre>\n\n<p>This code is populating 1 million records to an embedded SQL database (localDb) in <strong>5200ms</strong>. The rest of the code is just implementing the bulkCopy but I will post it anyway.</p>\n\n<pre><code> public string UpdateDBWithNewDtUsingSQLBulkCopy(DataTable TheLocalDtToPush, string TheOnlineSQLTableName, string WebConfigConName)\n {\n    //Open a connection to the database. \n    using (SqlConnection connection = new SqlConnection(ConfigurationManager.ConnectionStrings[WebConfigConName].ConnectionString))\n    {\n       connection.Open();\n\n       // Perform an initial count on the destination table.\n       SqlCommand commandRowCount = new SqlCommand(\"SELECT COUNT(*) FROM \"+TheOnlineSQLTableName +\";\", connection);\n       long countStart = System.Convert.ToInt32(commandRowCount.ExecuteScalar());\n\n       var nl = \"\\r\\n\";\n       string retStrReport = \"\";\n       retStrReport = string.Concat(string.Format(\"Starting row count = {0}\", countStart), nl);\n       retStrReport += string.Concat(\"==================================================\", nl);\n       // Create a table with some rows. \n       //DataTable newCustomers = TheLocalDtToPush;\n\n       // Create the SqlBulkCopy object.  \n       // Note that the column positions in the source DataTable  \n       // match the column positions in the destination table so  \n       // there is no need to map columns.  \n       using (SqlBulkCopy bulkCopy = new SqlBulkCopy(connection))\n       {\n          bulkCopy.DestinationTableName = TheOnlineSQLTableName;\n\n          try\n          {\n             // Write from the source to the destination.\n             for (int colIndex = 0; colIndex &lt; TheLocalDtToPush.Columns.Count; colIndex++)\n             {\n                bulkCopy.ColumnMappings.Add(colIndex, colIndex);\n             }\n             bulkCopy.WriteToServer(TheLocalDtToPush);\n          }\n\n          catch (Exception ex)\n          {\n             Console.WriteLine(ex.Message);\n          }\n       }\n\n       // Perform a final count on the destination  \n       // table to see how many rows were added. \n       long countEnd = System.Convert.ToInt32(\n       commandRowCount.ExecuteScalar());\n\n       retStrReport += string.Concat(\"Ending row count = \", countEnd, nl);\n       retStrReport += string.Concat(\"==================================================\", nl);\n       retStrReport += string.Concat((countEnd - countStart),\" rows were added.\", nl);\n       retStrReport += string.Concat(\"New Customers Was updated successfully\", nl, \"END OF PROCESS !\");\n       //Console.ReadLine();\n       return retStrReport;\n   }\n}\n</code></pre>\n\n<p>Trying it via a connection to SQL server was around 7000ms(at best) &amp; ~7700ms average. Also via a random kv nosql database took around 40 sec (really I did not even keep records of it as it passed over the x2 of sql variants). So... is there a faster way than what I was testing in my code? </p>\n\n<p><strong>Edit</strong> </p>\n\n<p>i am using win7 x64 8gb ram and most important i should think (as i5 3ghz) is not so great by now\nthe x3 500Gb Wd on Raid-0 does the job even better \nbut i am just saying if you will check on your pc \nthough just compare it to any other method in your configuration </p>\n    ","a":"\n<p>Have you tried SSIS? I have never written an SSIS package with a loacldb connection, but this is the sort of activity SSIS should be well suited. </p>\n\n<p>If your data source is a SQL Server, another idea would be setting up a linked server. Not sure if this would work with localdb. If you can set up a linked server, you could bypass the C# all together and load your data with an INSERT .. SELECT ... FROM ... SQL statement.</p>\n    "},{"t":"Suggest Cassandra data model for an existing schema","l":"http://stackoverflow.com/questions/2479589/suggest-cassandra-data-model-for-an-existing-schema","q":"\n\n<p>I hope there's someone who can help me suggest a suitable data model to be implemented using nosql database Apache Cassandra. More of than I need it to work under high loads and large amounts of data.</p>\n\n<p>Simplified I have 3 types of objects:</p>\n\n<ul>\n<li>Product</li>\n<li>Tag</li>\n<li>ProductTag</li>\n</ul>\n\n<p>Product:</p>\n\n<pre><code>key - string key\nname - string\n.... - some other fields\n</code></pre>\n\n<p>Tag:</p>\n\n<pre><code>key - string key\nname - unique tag words\n</code></pre>\n\n<p>ProductTag:</p>\n\n<pre><code>product_key - foreign key referring to product\ntag_key  - foreign key referring to tag\nrating - this is rating of tag for this product\n</code></pre>\n\n<p>Each product may have 0 or many tags. Tag may be assigned to 1 or many products. Means relation between products and tags is many-to-many in terms of relational databases.</p>\n\n<p>Value of \"rating\" is updated \"very\" often. </p>\n\n<p>I need to be run the following queries</p>\n\n<ul>\n<li>Select objects by keys</li>\n<li>Select tags for product ordered by rating</li>\n<li>Select products by tag order by rating</li>\n<li>Update rating by product_key and tag_key</li>\n</ul>\n\n<p>The most important is to make these queries really fast on large amounts of data, considering that rating is constantly updated.</p>\n    ","a":"\n<p>Something like this:  </p>\n\n<pre><code>Products : { // Column Family  \n    productA : { //Row key  \n        name: 'The name of the product' // column\n        price: 33.55 // column\n        tags : 'fun, toy' // column\n    }  \n}\n\nProductTag : { // Column Family\n    fun : { //Row key\n        timeuuid_1 : productA // column\n        timeuuid_2 : productB // column\n    },\n    toy : { //Row key\n        timeuuid_3 : productA // column\n    }\n}\n</code></pre>\n\n<p><strong>UPDATE</strong><br>\nCheck this <a href=\"http://www.mail-archive.com/user@cassandra.apache.org/msg00108.html\" rel=\"nofollow\">Model to store biggest score</a></p>\n    "},{"t":"How do you handle relations in a NoSQL database?","l":"http://stackoverflow.com/questions/4995265/how-do-you-handle-relations-in-a-nosql-database","q":"\n\n<p>With a standard RDMS I can find relations by using primary keys and foreign keys. If I want recent comments then I just order by a datetime. If I want all the comments by a user, then I fetch where a comment belongs to that user.</p>\n\n<p>In other words, I can use indexes to filter results. <em>Not just the primary key.</em></p>\n\n<p>However, with the document and key-value NoSQL I can't figure out how I could use them for much more than a text dump. The only thing you can do is fetch the value by an ID.</p>\n\n<p><strong>I need some examples of how you model data in NoSQL when you can no longer use indexes or filters. How do you sort and search data?</strong></p>\n    ","a":"\n<p>If you need secondary indexes like you're describing, then you can't just use any non-relational database. BigTable databases like Cassandra (and probably others) allow for secondary indexes.</p>\n\n<p>If you need to search for things within a Key-Value store based on the <em>values</em> then you'll need to get creative. You could:<br>\n1) Create your own keys that point at the original keys and then maintain those pairs on new inserts, updates, and deletes of the original pairs.<br>\n2) Just look at every value, brute force, off-line, once a day and save the answer somewhere. Clearly this won't work if you need the new data right away.</p>\n\n<p>Sorting the data will probably need to be done on the application layer or with custom sorted sets if you use technique (1) and Redis.</p>\n    "},{"t":"Firebase data structure and url","l":"http://stackoverflow.com/questions/16638660/firebase-data-structure-and-url","q":"\n\n<p>I'm new in Firebase and nosql so bear with me to use reference to sql.\nSo my question is how to structure the data in firebase?</p>\n\n<p>In firebase, is that mean every \"new firebase\" = \"new Database\" or \"table\" in mysql?</p>\n\n<p>If in my real time web app, I have users and comments.\nIn mysql, i will create a users and a comments table then link them together.</p>\n\n<p>How do I structure this in firebase?</p>\n\n<p>Thank in adv</p>\n    ","a":"\n<p>If you have users and comments, you could easily model it like this</p>\n\n<pre><code>ROOT\n |\n +-- vzhen\n |     |\n |     +-- Vzhen's comment 1\n |     |\n |     +-- Vzhen's comment 2\n |\n +-- Frank van Puffelen\n       |\n       +-- Frank's comment 1\n       |\n       +-- Frank's comment 2\n</code></pre>\n\n<p>However it is more likely that there is a third entity, like an article, and that users are commenting on (each other's) articles.</p>\n\n<p>Firebase doesn't have the concept of a foreign key, but it's easy to mimic it. If you do that, you can model the user/article/comment structure like this:</p>\n\n<pre><code>ROOT\n |\n +-- ARTICLES\n |     |\n |     +-- Text of article 1 (AID=1)\n |     |\n |     +-- Text of article 2 (AID=2)\n |\n +-- USERS\n |     |\n |     +-- vzhen (UID=1056201)\n |     |\n |     +-- Frank van Puffelen (UID=209103)\n |\n +-- COMMENTS\n |     |\n |     +-- Vzhen's comment on Article 1 (CID=1)\n |     |\n |     +-- Frank's response (CID=2)\n |     |\n |     +-- Frank's comment on article 2 (AID=2,UID=209103)\n |\n +-- ARTICLE_USER_COMMENT\n       |\n       +-- (AID=1,UID=1056201,CID=1)\n       |\n       +-- (AID=1,UID=209103,CID=2)\n       |\n       +-- (AID=2,UID=209103,CID=3)\n</code></pre>\n\n<p>This is a quite direct mapping of the way you'd model this in a relational database. The main problem with this model is the number of lookups you'll need to do to get the information you need for a single screen.</p>\n\n<ol>\n<li>Read the article itself (from the ARTICLES node)</li>\n<li>Read the information about the comments (from the ARTICLE_USER_COMMENT node)</li>\n<li>Read the content of the comments (from the COMMENTS node)</li>\n</ol>\n\n<p>Depending on your needs, you might even need to also read the USERS node.</p>\n\n<p>And keep in mind that Firebase does not have the concept of a WHERE clause that allows you to select just the elements from ARTICLE_USER_COMMENT that match a specific article, or a specific user.</p>\n\n<p>In practice this way of mapping the structure is not usable. Firebase is a hierarchical data structure, so we should use the unique abilities that gives us over the more traditional relational model. For example: we don't need a ARTICLE_USER_COMMENT node, we can just keep this information directly under each article, user and comment itself.</p>\n\n<p>A small snippet of this:</p>\n\n<pre><code>ROOT\n |\n +-- ARTICLES\n |     |\n |     +-- Text of article 1 (AID=1)\n |     .    |\n |     .    +-- (CID=1,UID=1056201)\n |     .    |\n |          +-- (CID=2,UID=209103)\n |\n +-- USERS\n |     |\n |     +-- vzhen (UID=1056201)\n |     .    |\n |     .    +-- (AID=1,CID=1)\n |     .    \n |\n +-- COMMENTS\n       |\n       +-- Vzhen's comment on Article 1 (CID=1)\n       |\n       +-- Frank's response (CID=2)\n       |\n       +-- Frank's comment on article 2 (CID=3)\n</code></pre>\n\n<p>You can see here, that we're spreading the information from ARTICLE_USER_COMMENT over the article and user nodes. This is denormalizing the data a bit. The result is that we'll need to update multiple nodes when a user adds a comment to an article. In the example above we'd have to add the comment itself and then the nodes to the relevant user node and article node. The advantage is that we have fewer nodes to read when we need to display the data.</p>\n\n<p>If you take this denormalization to its most extreme, you end up with a data structure like this:</p>\n\n<pre><code>ROOT\n |\n +-- ARTICLES\n |     |\n |     +-- Text of article 1 (AID=1)\n |     |    |\n |     |    +-- Vzhen's comment on Article 1 (UID=1056201)\n |     |    |\n |     |    +-- Frank's response (UID=209103)\n |     |\n |     +-- Text of article 2 (AID=2)\n |          |\n |          +-- Frank's comment on Article 2 (UID=209103)\n |\n +-- USERS\n       |\n       +-- vzhen (UID=1056201)\n       |    |\n       |    +-- Vzhen's comment on Article 1 (AID=1)\n       |\n       +-- Frank van Puffelen (UID=209103)\n            |\n            +-- Frank's response (AID=1)\n            |\n            +-- Frank's comment on Article 2 (AID=2)\n</code></pre>\n\n<p>You can see that we got rid of the COMMENTS and ARTICLE_USER_COMMENT nodes in this last example. All the information about an article is now stored directly under the article node itself, including the comments on that article (with a \"link\" to the user who made the comment). And all the information about a user is now stored under that user's node, including the comments that user made (with a \"link\" to the article that the comment is about).</p>\n\n<p>The only thing that is still tricky about this model is the fact that Firebase doesn't have an API to traverse such \"links\", so you will have to look up the user/article up yourself. This becomes a lot easier if you use the UID/AID (in this example) as the name of the node that identifies the user/article.</p>\n\n<p>So that leads to our final model:</p>\n\n<pre><code>ROOT\n |\n +-- ARTICLES\n |     |\n |     +-- AID_1\n |     |    |\n |     |    +-- Text of article 1\n |     |    |\n |     |    +-- COMMENTS\n |     |         |\n |     |         +-- Vzhen's comment on Article 1 (UID=1056201)\n |     |         |\n |     |         +-- Frank's response (UID=209103)\n |     |\n |     +-- AID_2\n |          |\n |          +-- Text of article 2\n |          |\n |          +-- COMMENTS\n |               |\n |               +-- Frank's comment on Article 2 (UID=209103)\n |\n +-- USERS\n       |\n       +-- UID_1056201\n       |    |\n       |    +-- vzhen\n       |    |\n       |    +-- COMMENTS\n       |         |\n       |         +-- Vzhen's comment on Article 1 (AID=1)\n       |\n       +-- UID_209103\n            |\n            +-- Frank van Puffelen\n            |\n            +-- COMMENTS\n                 |\n                 +-- Frank's response (AID=1)\n                 |\n                 +-- Frank's comment on Article 2 (AID=2)\n</code></pre>\n\n<p>I hope this helps in understanding hierarchical data-modelling and the trade-offs involved.</p>\n    "},{"t":"Using a NoSQL database over MySQL [closed]","l":"http://stackoverflow.com/questions/4832911/using-a-nosql-database-over-mysql","q":"\n\n<p>I have a web application running on Java stack (Struts 2 + Spring + Hibernate) and persisted in MySQL. I looked at NoSQL databases and they are certainly easy to reason about and work with than a RDBMS. It's a music streaming app which stores artist information and allows users to save playlists. </p>\n\n<p>I am wondering whether there are any advantages (performance?, hardware cost?, simplified code?, scalability?) of switching to a NoSQL DB (CouchDB?, MongoDB?, Cassandra?). What would I lose/gain by switching to a NoSQL database?</p>\n\n<p>Please advise.</p>\n    ","a":"\n<p>The polite interpretation of \"NoSQL\" has become <code>Not Only SQL</code>.  If you have data that is indeed truly relational, or if your functionality depends on things like joins and ACIDity, then you should store that data in a relational way.  In this post, I'll explain how I use MySQL alongside <em>two</em> NoSQL data stores.  Modern, web-scale data storage is all about understanding how to pick the best tool(s) for the job(s).</p>\n\n<p>That said, NoSQL is really a reaction to the fact that the relational method and way of thinking has been applied to problems where it's not actually a very good fit (typically huge tables with tens of millions of rows or more).  Once tables get that large, the typical SQL \"best practice\" has been to manually <em>shard</em> the data -- that is, putting records 1 through 10,000,000 in table A, 10,000,001 through 20,000,001 in table B, and so on.  Then, typically in the application model layer, the lookups are performed according to this scheme.  This is what's called <code>application-aware</code> scaling.  It's time-intensive and error prone, but to scale something up while maintaining MySQL for the long table store, it's become a more or less standard MO.  NoSQL represents, to me, the <code>application-unaware</code> alternative.</p>\n\n<hr>\n\n<p><strong>Key-Value</strong></p>\n\n<p>When I had a MySQL prototype start getting too big for its own good, I personally moved as much data as possible to the lightning-fast <a href=\"http://membase.org\">Membase</a>, which outperforms Memcached and adds persistence.  Membase is a distributed key-value store that scales more or less linearly (Zynga uses it to handle a half-million ops per second, for instance) by adding more commodity servers into a cluster -- it's therefore a <em>great</em> fit for the cloud age of <a href=\"http://aws.amazon.com/ec2/\">Amazon EC2</a>, <a href=\"https://www.joyent.com/\">Joyent</a>, etc.</p>\n\n<p>It's well known that distributed key-value stores are the best way to get enormous, linear scale.  The weakness of key-value is queryability and indexing.  But even in the relational world, the best practice for scalability is to offload as much effort onto the application servers as possible, doing joins in memory on commodity app servers instead of asking the central RDB cluster to handle all of that logic.  Since <code>simple select</code> plus <code>application logic</code> are really the best way to achieve massive scale <em>even on</em> MySQL, the transition to something like Membase (or its competitors like <a href=\"https://www.basho.com/Riak.html\">Riak</a>) isn't really too bad.</p>\n\n<hr>\n\n<p><strong>Document Stores</strong></p>\n\n<p>Sometimes -- though I would argue less often than many think -- an application's design inherently requires secondary indices, range queryability, etc.  The NoSQL approach to this is through a <code>document store</code> like <a href=\"http://mongodb.org\">MongoDB</a>.  Like Membase, Mongo is very good in some areas where relational databases are particularly weak, like <code>application-unaware</code> scaling, <code>auto-sharding</code>, and <code>maintaining flat response times even as dataset size balloons</code>.  It's significantly slower than Membase and a bit trickier to do pure horizontal scale, but the benefit is that it's highly queryable.  You can query on parameters and ranges in real time, or you can use Map/Reduce to perform complex batch operations on truly enormous data sets.</p>\n\n<p>On the same project I mentioned above, which uses Membase to serve tons of live player data, we use MongoDB to store analytics/metrics data, which is really where MongoDB shines.</p>\n\n<hr>\n\n<p><strong>Why to keep things in SQL</strong></p>\n\n<p>I touched briefly on the fact that 'truly relational' information should stay in relational databases. As commenter Dan K. points out, I missed the part where I discuss the disadvantages of leaving RDBMS, or at least of leaving it entirely.</p>\n\n<p><strong>First, there's SQL itself.</strong>  SQL is well-known and has been an industry standard for a long time.  Some \"NoSQL\" databases like Google's <a href=\"https://code.google.com/appengine/\">App Engine</a> Datastore (built on Big Table) implement their own SQL-like language (Google's is called, cutely, GQL for <code>Google Query Language</code>).  MongoDB takes a fresh approach to the querying problem with its delightful <a href=\"http://www.mongodb.org/display/DOCS/Querying\">JSON query objects</a>.  Still, SQL itself is a powerful tool for getting information out of data, which is often the whole point of databases to begin with.</p>\n\n<p>The most important reason to stay with RDBMS is <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a>,  or <code>Atomicity, Consistency, Isolation, Durability</code>.  I won't re-hash the state of Acid-NoSQL, as it's well-addressed in <a href=\"http://stackoverflow.com/questions/2608103/is-there-any-nosql-that-is-acid-compliant\">this post</a> on SO.  Suffice it to say, there's a rational reason <a href=\"http://www.oracle.com/us/products/database/index.html\">Oracle's RDBMS</a> has such a huge market that isn't going anywhere:  <em>some data needs pure ACID compliance</em>.  If your data does (and if it does, you're probably well aware of that fact), then so does your database.  Keep that <a href=\"http://en.wikipedia.org/wiki/PH\">pH</a> low!</p>\n\n<p><strong>Edit:</strong>  Check out Aaronaught's post <a href=\"http://stackoverflow.com/questions/2571098/moving-to-nosql/2571516#2571516\">here.</a>  He represents the business-to-business perspective far better than I could, in part because I've spent my entire career in the consumer space.</p>\n    "},{"t":"Command like SQL LIMIT in HBase","l":"http://stackoverflow.com/questions/14002948/command-like-sql-limit-in-hbase","q":"\n\n<p>Does HBase have any command that works like <code>SQL LIMIT</code> query?</p>\n\n<p>I can do it by <code>setStart</code> and <code>setEnd</code>, but I do not want to iterate all rows.</p>\n    ","a":"\n<p>From the shell you can use LIMIT</p>\n\n<p>scan 'test-table', {'LIMIT' =&gt; 5}</p>\n\n<p>from the java API you can use Scan.setMaxResultSize(N) or scan.setMaxResultsPerColumnFamily(N)</p>\n\n<p><a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setMaxResultSize%28long%29\">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setMaxResultSize%28long%29</a></p>\n\n<p><a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setMaxResultsPerColumnFamily%28int%29\">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setMaxResultsPerColumnFamily%28int%29</a></p>\n    "},{"t":"C# and NoSql databases [closed]","l":"http://stackoverflow.com/questions/5056895/c-sharp-and-nosql-databases","q":"\n\n<p>Is there any particular NoSQL database suitable for C#?</p>\n\n<p>Thank you!</p>\n    ","a":"\n<p>I agree with Marc's comment above, but I guess what you're looking for is probably something like <a href=\"http://ravendb.net/\">RavenDB</a>.  It's developed specifically with .NET in mind, so would probably seem more \"C# friendly\" than others such as CouchDB or MongoDB, etc.</p>\n\n<p>Keep in mind, however, that the different NoSQL implementations have different strengths and weaknesses beyond simply what language/platform is more natively considered during their development.  It may very well be true that RavenDB would seem more natural to a .NET developer, but a different database may be better suited to the job at hand depending on the data persistence needs.</p>\n\n<p>Here's <a href=\"http://nosql.mypopescu.com/post/978742866/document-databases-compared-couchdb-mongodb-ravendb\">a quick breakdown</a> of some other feature comparisons that I just found.  As you can see there's more to each tool than its .NET driver.  A little Googling of NoSQL database comparisons will yield more information.  If you have more specific data persistence needs or can elaborate more on the project in question we may be able to provide more advice.</p>\n\n<p><strong>Edit:</strong> (In response to your comment above)  To perhaps help you narrow down your choices, here's my experience so far:</p>\n\n<p>Of the three that I've mentioned, the only one I've actually used in .NET is MongoDB.  It didn't \"feel\" as native for .NET purposes, but it wasn't difficult or unwieldy in any way.  It was easy enough to use and performed its intended task very well.</p>\n\n<p>I've used CouchDB from JavaScript code, as opposed to from .NET code.  It's considered to be a very JavaScript friendly database and I've been toying with the idea of connecting to it directly from client-side AJAX calls.  But it should be just as easy from within .NET.  That's the beauty of a RESTful API, really.  Anything should be able to interact with it as easily as interacting with any service.  From within .NET code, something like RestSharp may make using CouchDB very easy and feel more .NET-native.</p>\n    "}]